{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle\n",
    "from helpers import TrainingAttnScoresLog\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '/home/wdwatson2/projects/CAT-Transformer/model')\n",
    "from testingModel import CATTransformer, MyFTTransformer, Combined_Dataset, train, test, EarlyStopping\n",
    "\n",
    "device_in_use = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device_in_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2]\n"
     ]
    }
   ],
   "source": [
    "#Lets do this with income since there are not a ton of features, and the target is binary\n",
    "\n",
    "df_train = pd.read_csv('/home/wdwatson2/projects/CAT-Transformer/datasets/income/train.csv')\n",
    "df_test = pd.read_csv('/home/wdwatson2/projects/CAT-Transformer/datasets/income/test.csv')\n",
    "df_val = pd.read_csv('/home/wdwatson2/projects/CAT-Transformer/datasets/income/validation.csv') \n",
    "\n",
    "cont_columns = ['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss',\n",
    "       'hours-per-week']\n",
    "cat_columns = ['workclass', 'education', 'marital-status', 'occupation',\n",
    "       'relationship', 'race', 'sex', 'native-country']\n",
    "target = ['income']\n",
    "\n",
    "#CHECKING TO MAKE SURE YOUR LIST IS CORRECT (NO NEED TO TOUCH)\n",
    "yourlist = cont_columns + cat_columns+target\n",
    "yourlist.sort()\n",
    "oglist = list(df_train.columns)\n",
    "oglist.sort()\n",
    "\n",
    "assert(yourlist == oglist), \"You may of spelled feature name wrong or you forgot to put on of them in the list\"\n",
    "\n",
    "cat_features = (10,16,7,16,6,5,2,43)\n",
    "\n",
    "target_classes = [max(len(df_train[target].value_counts()), len(df_val[target].value_counts()),len(df_test[target].value_counts()))]\n",
    "print(target_classes)\n",
    "# Create a StandardScaler and fit it to the cont features\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(df_train[cont_columns])\n",
    "\n",
    "# Transform the training, test, and validation datasets\n",
    "df_train[cont_columns] = scaler.transform(df_train[cont_columns])\n",
    "df_test[cont_columns] = scaler.transform(df_test[cont_columns])\n",
    "df_val[cont_columns] = scaler.transform(df_val[cont_columns])\n",
    "\n",
    "#Wrapping in Dataset\n",
    "train_dataset = Combined_Dataset(df_train, cat_columns=cat_columns, num_columns=cont_columns, task1_column=target[0])\n",
    "val_dataset = Combined_Dataset(df_val, cat_columns=cat_columns, num_columns=cont_columns, task1_column=target[0])\n",
    "test_dataset = Combined_Dataset(df_test, cat_columns=cat_columns, num_columns=cont_columns, task1_column=target[0])\n",
    "\n",
    "batch_size = 256\n",
    "\n",
    "# Wrapping with DataLoader for easy batch extraction\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [ 1/200]  | Train: Loss 0.4820755975904749, Accuracy 0.7776770306238848       | Test: Loss 0.37563256706510273, Accuracy 0.8312101910828026      \n",
      "Epoch [ 2/200]  | Train: Loss 0.34359556658944085, Accuracy 0.840299511538799       | Test: Loss 0.3371277987957001, Accuracy 0.8478161965423112       \n",
      "Epoch [ 3/200]  | Train: Loss 0.32336301756883734, Accuracy 0.849045014478341       | Test: Loss 0.325788733788899, Accuracy 0.8525932666060054        \n",
      "Epoch [ 4/200]  | Train: Loss 0.31620153873714046, Accuracy 0.8522331744128229      | Test: Loss 0.32153202295303346, Accuracy 0.8546405823475887      \n",
      "Epoch [ 5/200]  | Train: Loss 0.3129150497157182, Accuracy 0.8551288426101963       | Test: Loss 0.3199089241879327, Accuracy 0.8577115559599636       \n",
      "Epoch [ 6/200]  | Train: Loss 0.3086722001655778, Accuracy 0.8565328029483167       | Test: Loss 0.31687322173799787, Accuracy 0.8577115559599636      \n",
      "Epoch [ 7/200]  | Train: Loss 0.3067282975387217, Accuracy 0.8577027698967504       | Test: Loss 0.31665346367018565, Accuracy 0.8585077343039127      \n",
      "Epoch [ 8/200]  | Train: Loss 0.3057514462453216, Accuracy 0.8582877533709673       | Test: Loss 0.32101868135588507, Accuracy 0.8550955414012739      \n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch [ 9/200]  | Train: Loss 0.30420298318364725, Accuracy 0.8581122583287022      | Test: Loss 0.31344728086675916, Accuracy 0.8595313921747043      \n",
      "Epoch [10/200]  | Train: Loss 0.30306577282165414, Accuracy 0.8602766971833046      | Test: Loss 0.30921292304992676, Accuracy 0.8571428571428571      \n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch [11/200]  | Train: Loss 0.30247898155183933, Accuracy 0.858521746760654       | Test: Loss 0.31200170729841503, Accuracy 0.8580527752502275      \n",
      "EarlyStopping counter: 2 out of 10\n",
      "Epoch [12/200]  | Train: Loss 0.30198412733291513, Accuracy 0.8584339992395215      | Test: Loss 0.31200482760156906, Accuracy 0.8577115559599636      \n",
      "EarlyStopping counter: 3 out of 10\n",
      "Epoch [13/200]  | Train: Loss 0.3009674245519425, Accuracy 0.8597502120565094       | Test: Loss 0.31426455293382916, Accuracy 0.857825295723385       \n",
      "EarlyStopping counter: 4 out of 10\n",
      "Epoch [14/200]  | Train: Loss 0.3017837931193523, Accuracy 0.8587849893240516       | Test: Loss 0.3104801710162844, Accuracy 0.8583939945404914       \n",
      "EarlyStopping counter: 5 out of 10\n",
      "Epoch [15/200]  | Train: Loss 0.30115539947552467, Accuracy 0.8584339992395215      | Test: Loss 0.31004548157964434, Accuracy 0.8581665150136488      \n",
      "EarlyStopping counter: 6 out of 10\n",
      "Epoch [16/200]  | Train: Loss 0.3011173604806857, Accuracy 0.8593992219719793       | Test: Loss 0.31197993968214305, Accuracy 0.8589626933575978      \n",
      "EarlyStopping counter: 7 out of 10\n",
      "Epoch [17/200]  | Train: Loss 0.3005380504834118, Accuracy 0.8593699727982684       | Test: Loss 0.3110682432140623, Accuracy 0.8565741583257507       \n",
      "EarlyStopping counter: 8 out of 10\n",
      "Epoch [18/200]  | Train: Loss 0.29910014280632363, Accuracy 0.8595747170142444      | Test: Loss 0.3100629563842501, Accuracy 0.8585077343039127       \n",
      "EarlyStopping counter: 9 out of 10\n",
      "Epoch [19/200]  | Train: Loss 0.2993018828443627, Accuracy 0.859633215361666        | Test: Loss 0.31160069874354773, Accuracy 0.856687898089172       \n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "#Lets first train a cat fully\n",
    "\n",
    "model_cat = CATTransformer(n_cont=len(cont_columns),\n",
    "                       cat_feat=cat_features,\n",
    "                       targets_classes=target_classes,\n",
    "                       get_attn=True,\n",
    "                       num_layers=10).to(device_in_use)\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params=model_cat.parameters(), lr = 0.0001)\n",
    "\n",
    "early_stopping = EarlyStopping(patience=10, verbose=True)\n",
    "\n",
    "train_losses = []\n",
    "train_accuracies_1 = [] \n",
    "test_losses = []\n",
    "test_accuracies_1 = [] \n",
    "\n",
    "epochs = 200\n",
    "\n",
    "for t in range(epochs):\n",
    "    train_loss, train_acc, attn= train(regression_on=False, \n",
    "                                  get_attn=True,\n",
    "                                   dataloader=train_dataloader, \n",
    "                                   model=model_cat, \n",
    "                                   loss_function=loss_function, \n",
    "                                   optimizer=optimizer, \n",
    "                                   device_in_use=device_in_use)\n",
    "    test_loss, test_acc, attn = test(regression_on=False,\n",
    "                               get_attn=True,\n",
    "                               dataloader=test_dataloader,\n",
    "                               model=model_cat,\n",
    "                               loss_function=loss_function,\n",
    "                               device_in_use=device_in_use)\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies_1.append(train_acc)\n",
    "    test_losses.append(test_loss)\n",
    "    test_accuracies_1.append(test_acc)\n",
    "\n",
    "    epoch_str = f\"Epoch [{t+1:2}/{epochs}]\"\n",
    "    train_metrics = f\"Train: Loss {(train_loss)}, Accuracy {(train_acc)}\"\n",
    "    test_metrics = f\"Test: Loss {(test_loss)}, Accuracy {(test_acc)}\"\n",
    "    print(f\"{epoch_str:15} | {train_metrics:65} | {test_metrics:65}\")\n",
    "\n",
    "    early_stopping(test_acc)\n",
    "    \n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [ 1/200]  | Train: Loss 0.4919486457287376, Accuracy 0.7702477405013308       | Test: Loss 0.3597981427397047, Accuracy 0.8372383985441311       \n",
      "Epoch [ 2/200]  | Train: Loss 0.3388649360902274, Accuracy 0.8404750065810641       | Test: Loss 0.33316743799618315, Accuracy 0.8489535941765242      \n",
      "Epoch [ 3/200]  | Train: Loss 0.3236610382350523, Accuracy 0.8489280177834976       | Test: Loss 0.3331290479217257, Accuracy 0.8529344858962693       \n",
      "Epoch [ 4/200]  | Train: Loss 0.3160080879704276, Accuracy 0.852379420281377        | Test: Loss 0.3176516762801579, Accuracy 0.8549818016378526       \n",
      "Epoch [ 5/200]  | Train: Loss 0.3117789940825149, Accuracy 0.8546316066571119       | Test: Loss 0.3146205587046487, Accuracy 0.857484076433121        \n",
      "Epoch [ 6/200]  | Train: Loss 0.31015400733075926, Accuracy 0.8556845769107023      | Test: Loss 0.3127829509122031, Accuracy 0.8588489535941766       \n",
      "Epoch [ 7/200]  | Train: Loss 0.30704670054699057, Accuracy 0.8568837930328468      | Test: Loss 0.3170583086354392, Accuracy 0.8557779799818016       \n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch [ 8/200]  | Train: Loss 0.30620374563914626, Accuracy 0.8575857732019071      | Test: Loss 0.3133376555783408, Accuracy 0.8565741583257507       \n",
      "EarlyStopping counter: 2 out of 10\n",
      "Epoch [ 9/200]  | Train: Loss 0.3049623767164216, Accuracy 0.8575857732019071       | Test: Loss 0.3103248664311, Accuracy 0.8583939945404914          \n",
      "EarlyStopping counter: 3 out of 10\n",
      "Epoch [10/200]  | Train: Loss 0.3039249659919027, Accuracy 0.8575565240281962       | Test: Loss 0.3126116846288953, Accuracy 0.8589626933575978       \n",
      "Epoch [11/200]  | Train: Loss 0.3038519460763504, Accuracy 0.858141507502413        | Test: Loss 0.32552018676485334, Accuracy 0.8475887170154686      \n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch [12/200]  | Train: Loss 0.3028111733607392, Accuracy 0.858521746760654        | Test: Loss 0.31107312313147956, Accuracy 0.8577115559599636      \n",
      "EarlyStopping counter: 2 out of 10\n",
      "Epoch [13/200]  | Train: Loss 0.3019448677772906, Accuracy 0.8595747170142444       | Test: Loss 0.3108206476484026, Accuracy 0.8591901728844404       \n",
      "Epoch [14/200]  | Train: Loss 0.30234809564565546, Accuracy 0.8584339992395215      | Test: Loss 0.30951758665697915, Accuracy 0.8585077343039127      \n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch [15/200]  | Train: Loss 0.3015336294227572, Accuracy 0.8584632484132323       | Test: Loss 0.3075366705656052, Accuracy 0.8577115559599636       \n",
      "EarlyStopping counter: 2 out of 10\n",
      "Epoch [16/200]  | Train: Loss 0.30087949847107504, Accuracy 0.8597209628827985      | Test: Loss 0.3085448741912842, Accuracy 0.857825295723385        \n",
      "EarlyStopping counter: 3 out of 10\n",
      "Epoch [17/200]  | Train: Loss 0.3005208309684227, Accuracy 0.8593699727982684       | Test: Loss 0.3125064756189074, Accuracy 0.856687898089172        \n",
      "EarlyStopping counter: 4 out of 10\n",
      "Epoch [18/200]  | Train: Loss 0.30082625862377793, Accuracy 0.8598087104039311      | Test: Loss 0.3097076258489064, Accuracy 0.8580527752502275       \n",
      "EarlyStopping counter: 5 out of 10\n",
      "Epoch [19/200]  | Train: Loss 0.30023235805443865, Accuracy 0.8600427037936178      | Test: Loss 0.3122581788471767, Accuracy 0.8590764331210191       \n",
      "EarlyStopping counter: 6 out of 10\n",
      "Epoch [20/200]  | Train: Loss 0.2997300977359957, Accuracy 0.8601012021410395       | Test: Loss 0.3073319354227611, Accuracy 0.8589626933575978       \n",
      "EarlyStopping counter: 7 out of 10\n",
      "Epoch [21/200]  | Train: Loss 0.2989464652404856, Accuracy 0.8594577203194009       | Test: Loss 0.3102659361703055, Accuracy 0.8587352138307552       \n",
      "EarlyStopping counter: 8 out of 10\n",
      "Epoch [22/200]  | Train: Loss 0.2987288307787767, Accuracy 0.8599842054461961       | Test: Loss 0.3104723295995167, Accuracy 0.8603275705186533       \n",
      "Epoch [23/200]  | Train: Loss 0.29847491421361466, Accuracy 0.8606861856152563      | Test: Loss 0.3138643728835242, Accuracy 0.85828025477707         \n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch [24/200]  | Train: Loss 0.29832687151076187, Accuracy 0.8610956740472082      | Test: Loss 0.309584903717041, Accuracy 0.8590764331210191        \n",
      "EarlyStopping counter: 2 out of 10\n",
      "Epoch [25/200]  | Train: Loss 0.29820225054203575, Accuracy 0.8600719529673286      | Test: Loss 0.3046631353242057, Accuracy 0.8591901728844404       \n",
      "EarlyStopping counter: 3 out of 10\n",
      "Epoch [26/200]  | Train: Loss 0.2967341898092583, Accuracy 0.8610079265260756       | Test: Loss 0.3087062499352864, Accuracy 0.859417652411283        \n",
      "EarlyStopping counter: 4 out of 10\n",
      "Epoch [27/200]  | Train: Loss 0.299124431921475, Accuracy 0.8596917137090877        | Test: Loss 0.3086256980895996, Accuracy 0.8572565969062784       \n",
      "EarlyStopping counter: 5 out of 10\n",
      "Epoch [28/200]  | Train: Loss 0.29796616088098554, Accuracy 0.8593992219719793      | Test: Loss 0.3089071661233902, Accuracy 0.8591901728844404       \n",
      "EarlyStopping counter: 6 out of 10\n",
      "Epoch [29/200]  | Train: Loss 0.29794736473417993, Accuracy 0.8597794612302202      | Test: Loss 0.30891105021749227, Accuracy 0.8586214740673339      \n",
      "EarlyStopping counter: 7 out of 10\n",
      "Epoch [30/200]  | Train: Loss 0.2968182394753641, Accuracy 0.8610371756997864       | Test: Loss 0.30798908344336917, Accuracy 0.8583939945404914      \n",
      "EarlyStopping counter: 8 out of 10\n",
      "Epoch [31/200]  | Train: Loss 0.2983532199886308, Accuracy 0.8605984380941238       | Test: Loss 0.316931585754667, Accuracy 0.8588489535941766        \n",
      "EarlyStopping counter: 9 out of 10\n",
      "Epoch [32/200]  | Train: Loss 0.2973653856498092, Accuracy 0.8610664248734973       | Test: Loss 0.3101411312818527, Accuracy 0.8580527752502275       \n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "#And a FT\n",
    "\n",
    "model_ft = MyFTTransformer(n_cont=len(cont_columns),\n",
    "                       cat_feat=cat_features,\n",
    "                       targets_classes=target_classes,\n",
    "                       get_attn=True,\n",
    "                       num_layers=10).to(device_in_use)\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params=model_ft.parameters(), lr = 0.0001)\n",
    "\n",
    "early_stopping = EarlyStopping(patience=10, verbose=True)\n",
    "\n",
    "train_losses = []\n",
    "train_accuracies_1 = [] \n",
    "test_losses = []\n",
    "test_accuracies_1 = [] \n",
    "\n",
    "epochs = 200\n",
    "\n",
    "for t in range(epochs):\n",
    "    train_loss, train_acc, attn = train(regression_on=False, \n",
    "                                  get_attn=True,\n",
    "                                   dataloader=train_dataloader, \n",
    "                                   model=model_ft, \n",
    "                                   loss_function=loss_function, \n",
    "                                   optimizer=optimizer, \n",
    "                                   device_in_use=device_in_use)\n",
    "    test_loss, test_acc, attn = test(regression_on=False,\n",
    "                               get_attn=True,\n",
    "                               dataloader=test_dataloader,\n",
    "                               model=model_ft,\n",
    "                               loss_function=loss_function,\n",
    "                               device_in_use=device_in_use)\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies_1.append(train_acc)\n",
    "    test_losses.append(test_loss)\n",
    "    test_accuracies_1.append(test_acc)\n",
    "\n",
    "    epoch_str = f\"Epoch [{t+1:2}/{epochs}]\"\n",
    "    train_metrics = f\"Train: Loss {(train_loss)}, Accuracy {(train_acc)}\"\n",
    "    test_metrics = f\"Test: Loss {(test_loss)}, Accuracy {(test_acc)}\"\n",
    "    print(f\"{epoch_str:15} | {train_metrics:65} | {test_metrics:65}\")\n",
    "\n",
    "    early_stopping(test_acc)\n",
    "    \n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your evaluation function\n",
    "def evaluate(model, dataloader, device_in_use):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    accuracies = []\n",
    "    attentions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for (cat_x, cont_x, labels) in dataloader:\n",
    "            cat_x,cont_x,labels=cat_x.to(device_in_use),cont_x.to(device_in_use),labels.to(device_in_use)\n",
    "\n",
    "            predictions, attention = model(cat_x, cont_x)\n",
    "\n",
    "            _, predicted = torch.max(predictions, 1)\n",
    "            accuracy = (predicted == labels).sum().item() / labels.size(0)\n",
    "            accuracies.append(accuracy)\n",
    "\n",
    "            attentions.append(attention.cpu().numpy()) \n",
    "\n",
    "    avg_accuracy = np.mean(accuracies)\n",
    "    all_attentions = np.concatenate(attentions, axis=0) if attentions else None\n",
    "\n",
    "    return avg_accuracy, all_attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "poor_cat_acc 0.9341155234657039\n",
      "rich cat acc 0.6151154653603919\n",
      "poor ft acc 0.9483303249097473\n",
      "rich ft acc 0.56962911126662\n",
      "\n",
      "size of the attn array I am working with:  (14,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.07298017, 0.06870462, 0.07084739, 0.07104348, 0.06818973,\n",
       "       0.0726997 , 0.07216004, 0.07220282, 0.07305511, 0.07131346,\n",
       "       0.07429491, 0.07112397, 0.06871252, 0.07267208], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we need to analyze the attention scores for the two classes\n",
    "\n",
    "# get samples from each class\n",
    "poor_samples = df_val.loc[df_val['income'] == 0]\n",
    "rich_samples = df_val.loc[df_val['income'] == 1]\n",
    "\n",
    "poor_dataset = Combined_Dataset(poor_samples, cat_columns=cat_columns, num_columns=cont_columns, task1_column=target[0])\n",
    "rich_dataset = Combined_Dataset(rich_samples, cat_columns=cat_columns, num_columns=cont_columns, task1_column=target[0])\n",
    "\n",
    "poor_dataloader = DataLoader(poor_dataset, batch_size=len(poor_dataset))\n",
    "rich_dataloader = DataLoader(rich_dataset, batch_size=len(rich_dataset))\n",
    "\n",
    "poor_acc_cat, poor_attn_cat = evaluate(model_cat, poor_dataloader, device_in_use)\n",
    "print(\"poor_cat_acc\", poor_acc_cat)\n",
    "rich_acc_cat, rich_attn_cat = evaluate(model_cat, rich_dataloader, device_in_use)\n",
    "print(\"rich cat acc\", rich_acc_cat)\n",
    "poor_acc_ft, poor_attn_ft = evaluate(model_ft, poor_dataloader, device_in_use)\n",
    "print(\"poor ft acc\", poor_acc_ft)\n",
    "rich_acc_ft, rich_attn_ft = evaluate(model_ft, rich_dataloader, device_in_use)\n",
    "print(\"rich ft acc\", rich_acc_ft)\n",
    "\n",
    "\n",
    "#Now we average over heads\n",
    "poor_attn_cat = poor_attn_cat.mean(0) #discrete distribution of attention scores\n",
    "rich_attn_cat = rich_attn_cat.mean(0)\n",
    "poor_attn_ft = poor_attn_ft.mean(0)\n",
    "rich_attn_ft = rich_attn_ft.mean(0)\n",
    "\n",
    "print(\"\\nsize of the attn array I am working with: \", poor_attn_cat.shape)\n",
    "poor_attn_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(distribution):\n",
    "    probabilities = distribution / np.sum(distribution)  # Normalize probabilities\n",
    "    entropy_val = -np.sum(probabilities * np.log2(probabilities + 1e-12))  # Add small value to avoid log(0)\n",
    "    return entropy_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CAT class 0 Entropy: 3.8069140911102295, FT class 0 Entropy: 3.806957960128784\n",
      "CAT class 1 Entropy: 3.8066823482513428, FT class 1 Entropy: 3.8065927028656006\n"
     ]
    }
   ],
   "source": [
    "poor_cat_entropy = entropy(poor_attn_cat)\n",
    "rich_cat_entropy = entropy(rich_attn_cat)\n",
    "poor_ft_entropy = entropy(poor_attn_ft)\n",
    "rich_ft_entropy = entropy(rich_attn_ft)\n",
    "\n",
    "print(f\"CAT class 0 Entropy: {poor_cat_entropy}, FT class 0 Entropy: {poor_ft_entropy}\")\n",
    "print(f\"CAT class 1 Entropy: {rich_cat_entropy}, FT class 1 Entropy: {rich_ft_entropy}\")\n",
    "\n",
    "# Well, yep it looks like cat has slightly less entropy in its averaged attention scores which means that it contains more information -> more interpretable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For a dataset:\n",
    "1. Train various layer sizes of CAT and FT\n",
    "2. Split Train/Test/Val datasets by class into seperate dataloaders\n",
    "3. Evaluate the models with these subsets and gather attn distributions and calc entropy for each\n",
    "4. store the distribution and entropies for each \n",
    "\n",
    "## {Model : {Split : {Class : {Attn : [], Entropy : val}}}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataStructure:\n",
    "    def __init__(self):\n",
    "        self.data = {}  # Initialize the data structure\n",
    "\n",
    "    def add_model(self, model_name):\n",
    "        if model_name not in self.data:\n",
    "            self.data[model_name] = {}  # Create a dictionary for the model\n",
    "\n",
    "    def add_split(self, model_name, split_name):\n",
    "        if model_name not in self.data:\n",
    "            self.add_model(model_name)\n",
    "        if split_name not in self.data[model_name]:\n",
    "            self.data[model_name][split_name] = {}  # Create a dictionary for the split\n",
    "\n",
    "    def add_class_data(self, model_name, split_name, class_name, attn_distributions, entropies):\n",
    "        if model_name not in self.data:\n",
    "            self.add_model(model_name)\n",
    "        if split_name not in self.data[model_name]:\n",
    "            self.add_split(model_name, split_name)\n",
    "        self.data[model_name][split_name][class_name] = {\n",
    "            'Attn': attn_distributions,\n",
    "            'Entropy': entropies\n",
    "        }\n",
    "\n",
    "    def get_data(self):\n",
    "        return self.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attn_entropy_get(log:DataStructure, trained_model, model_name, traindf:pd.DataFrame, testdf:pd.DataFrame, target, cat_columns, num_columns, device_in_use):\n",
    "    classes = np.unique(df_train[target])\n",
    "\n",
    "    for x in classes:\n",
    "        class_train_samples = df_train.loc[df_train[target] == x]\n",
    "        class_test_samples = df_test.loc[df_test[target] == x]\n",
    "\n",
    "        class_train_dataset = Combined_Dataset(class_train_samples, cat_columns=cat_columns, num_columns=num_columns, task1_column=target)\n",
    "        class_test_dataset = Combined_Dataset(class_test_samples, cat_columns=cat_columns, num_columns=num_columns, task1_column=target)\n",
    "\n",
    "        class_train_dataloader = DataLoader(class_train_dataset, batch_size=len(class_train_dataset))\n",
    "        class_test_dataloader = DataLoader(class_test_dataset, batch_size=len(class_test_dataset))\n",
    "             \n",
    "        train_acc, train_attn = evaluate(trained_model, class_train_dataloader, device_in_use)\n",
    "        train_attn = train_attn.mean(0)\n",
    "        test_acc, test_attn = evaluate(trained_model, class_test_dataloader, device_in_use)\n",
    "        test_attn = test_attn.mean(0)\n",
    "\n",
    "        log.add_class_data(model_name, \"train\", \"class_\"+str(x), train_attn, entropy(train_attn))\n",
    "        log.add_class_data(model_name, \"test\", \"class_\"+str(x), test_attn, entropy(test_attn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CAT': {'train': {'class_0': {'Attn': array([0.07304911, 0.06869813, 0.0708304 , 0.07112652, 0.06817421,\n",
       "           0.07270043, 0.07213499, 0.07222321, 0.07304791, 0.07126234,\n",
       "           0.07427512, 0.07110429, 0.06870748, 0.07266589], dtype=float32),\n",
       "    'Entropy': 3.8069122},\n",
       "   'class_1': {'Attn': array([0.07013594, 0.06839065, 0.07032982, 0.07563202, 0.06845   ,\n",
       "           0.07148976, 0.07168393, 0.07244731, 0.07478277, 0.07081585,\n",
       "           0.07438321, 0.0708281 , 0.06841263, 0.07221805], dtype=float32),\n",
       "    'Entropy': 3.8066483}},\n",
       "  'test': {'class_0': {'Attn': array([0.07299901, 0.06870215, 0.07083803, 0.07106112, 0.06819244,\n",
       "           0.07269025, 0.07214098, 0.07221694, 0.07305422, 0.07131784,\n",
       "           0.07430075, 0.07111096, 0.06871327, 0.07266206], dtype=float32),\n",
       "    'Entropy': 3.806914},\n",
       "   'class_1': {'Attn': array([0.07009649, 0.0683968 , 0.07033315, 0.07574085, 0.06843449,\n",
       "           0.07145561, 0.07172981, 0.07242835, 0.07471254, 0.07085246,\n",
       "           0.07437216, 0.07083561, 0.06841907, 0.07219262], dtype=float32),\n",
       "    'Entropy': 3.8066447}}}}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_log = DataStructure()\n",
    "\n",
    "model_name = 'CAT'\n",
    "target = 'income'\n",
    "\n",
    "attn_entropy_get(attn_log, model_cat, model_name, df_train, df_test, target, cat_columns=cat_columns, num_columns=cont_columns, device_in_use= device_in_use)\n",
    "\n",
    "data = attn_log.get_data()\n",
    "data\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
