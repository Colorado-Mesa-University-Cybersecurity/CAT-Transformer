{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/warin/miniconda3/envs/ml-env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/home/warin/projects/CAT-Transformer/model')\n",
    "from updatedModel import CATTransformer, Combined_Dataset, train, test\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from rff.layers import GaussianEncoding #pip install random-fourier-features-pytorch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import os\n",
    "import optuna\n",
    "from optuna.trial import TrialState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is not available, using CPU instead\n"
     ]
    }
   ],
   "source": [
    "# Run regardless if you do or do not have GPU so all tensors are moved to right location later on\n",
    "if torch.cuda.is_available():\n",
    "    device_in_use = torch.device(\"cuda\")\n",
    "    print(\"GPU is available and being used\")\n",
    "else:\n",
    "    device_in_use = torch.device(\"cpu\")\n",
    "    print(\"GPU is not available, using CPU instead\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target classes [100]\n"
     ]
    }
   ],
   "source": [
    "# df_train = pd.read_csv(r'C:\\Users\\smbm2\\projects\\CAT-Transformer\\datasets\\helena\\train.csv')\n",
    "# df_test = pd.read_csv(r'C:\\Users\\smbm2\\projects\\CAT-Transformer\\datasets\\helena\\test.csv')\n",
    "# df_val = pd.read_csv(r'C:\\Users\\smbm2\\projects\\CAT-Transformer\\datasets\\helena\\validation.csv') #READ FROM RIGHT SPOT\n",
    "\n",
    "df_train = pd.read_csv('/home/warin/projects/CAT-Transformer/datasets/helena/train.csv')\n",
    "df_test = pd.read_csv('/home/warin/projects/CAT-Transformer/datasets/helena/test.csv')\n",
    "df_val = pd.read_csv('/home/warin/projects/CAT-Transformer/datasets/helena/validation.csv')\n",
    "\n",
    "\n",
    "# df_train.columns\n",
    "cont_columns = ['V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',\n",
    "       'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20',\n",
    "       'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27']\n",
    "target = ['class']\n",
    "\n",
    "#CHECKING TO MAKE SURE YOUR LIST IS CORRECT (NO NEED TO TOUCH)\n",
    "yourlist = cont_columns + target\n",
    "yourlist.sort()\n",
    "oglist = list(df_train.columns)\n",
    "oglist.sort()\n",
    "\n",
    "assert(yourlist == oglist), \"You may of spelled feature name wrong or you forgot to put one of them in the list\"\n",
    "\n",
    "target_classes = [max(len(df_train[target].value_counts()), len(df_val[target].value_counts()),len(df_test[target].value_counts()))]\n",
    "print(\"target classes\",target_classes)\n",
    "# Create a StandardScaler and fit it to the cont features\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(df_train[cont_columns])\n",
    "\n",
    "# Transform the training, test, and validation datasets\n",
    "df_train[cont_columns] = scaler.transform(df_train[cont_columns])\n",
    "df_test[cont_columns] = scaler.transform(df_test[cont_columns])\n",
    "df_val[cont_columns] = scaler.transform(df_val[cont_columns])\n",
    "\n",
    "#Wrapping in Dataset\n",
    "train_dataset = Combined_Dataset(df_train, cat_columns=[], num_columns=cont_columns, task1_column='class')\n",
    "val_dataset = Combined_Dataset(df_val, cat_columns=[], num_columns=cont_columns, task1_column='class')\n",
    "test_dataset = Combined_Dataset(df_test, cat_columns=[], num_columns=cont_columns, task1_column='class')\n",
    "\n",
    "#This is a hyperparameter that is not tuned. Maybe mess with what makes sense here\n",
    "batch_size = 256\n",
    "\n",
    "# Wrapping with DataLoader for easy batch extraction\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUN EXPERIMENTS\n",
    "\n",
    "1. Using Optuna to optimize CAT-Transformers hyperparameters for your dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the early stopping mechanism\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5):\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.best_metric = float('-inf')\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, metric):\n",
    "        if metric > self.best_metric:\n",
    "            self.best_metric = metric\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "\n",
    "# Function to log results to a text file\n",
    "def log_to_file(filename, text):\n",
    "    with open(filename, 'a') as f:\n",
    "        f.write(text + '\\n')\n",
    "\n",
    "def objective(trial):\n",
    "    trial_number = trial.number\n",
    "\n",
    "    # Define hyperparameters to search over\n",
    "\n",
    "    alpha = trial.suggest_float('alpha', 0.1, 5, step=0.1)\n",
    "    embed_size = trial.suggest_int('embed_size', 100, 320, step=20)\n",
    "    num_layers = trial.suggest_int('num_layers', 1,2)\n",
    "    heads = trial.suggest_categorical('heads', [1,2,5,10,20])\n",
    "    forward_expansion = trial.suggest_int('forward_expansion', 2,8)\n",
    "    decoder_dropout = trial.suggest_float('decoder_dropout', 0, .5, step=.1)\n",
    "    classification_dropout = trial.suggest_float('classificiation_dropout', 0, 0.5, step=0.1)\n",
    "    pre_norm_on = trial.suggest_categorical('pre_norm_on', [True, False])\n",
    "    mlp_scale_classification = trial.suggest_int('mlp_scale_classification', 2, 8)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 0.00001, 0.1, log=True)\n",
    "\n",
    "    num_epochs = 100\n",
    "\n",
    "\n",
    "    # Create your model with the sampled hyperparameters\n",
    "    model = CATTransformer(alpha=alpha,\n",
    "                           embed_size=embed_size,\n",
    "                           n_cont=len(cont_columns),\n",
    "                           cat_feat=[],\n",
    "                           num_layers=num_layers,\n",
    "                           heads=heads,\n",
    "                           forward_expansion=forward_expansion,\n",
    "                           decoder_dropout=decoder_dropout,\n",
    "                           classification_dropout=classification_dropout,\n",
    "                           pre_norm_on=pre_norm_on,\n",
    "                           mlp_scale_classification=mlp_scale_classification,\n",
    "                           regression_on=False,\n",
    "                           targets_classes=target_classes).to(device_in_use)\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Initialize early stopping\n",
    "    early_stopping = EarlyStopping(patience=5)  # Adjust patience as needed\n",
    "\n",
    "    # Training loop with a large number of epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss, train_acc = train(regression_on=False, \n",
    "                                   dataloader=train_dataloader, \n",
    "                                   model=model, \n",
    "                                   loss_function=loss_function, \n",
    "                                   optimizer=optimizer, \n",
    "                                   device_in_use=device_in_use)\n",
    "        val_loss, val_acc = test(regression_on=False,\n",
    "                               dataloader=val_dataloader,\n",
    "                               model=model,\n",
    "                               loss_function=loss_function,\n",
    "                               device_in_use=device_in_use)\n",
    "        \n",
    "        # Check if we should early stop based on validation accuracy\n",
    "        if early_stopping(val_acc):\n",
    "            break\n",
    "    \n",
    "    # Log the final test accuracy for this trial to a shared log file\n",
    "    final_log = f\"Trial {trial_number} completed. Validation Accuracy = {val_acc:.4f}\"\n",
    "    log_to_file('all_trials_log.txt', final_log)\n",
    "\n",
    "    # Return the test accuracy as the objective to optimize\n",
    "    return val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-03 21:11:22,047] A new study created in memory with name: no-name-e28e01a7-87e3-44e7-b852-7b2df797bfef\n",
      "Best trial: 0. Best value: 0.0738317: 100%|██████████| 1/1 [00:27<00:00, 27.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2023-12-03 21:11:49,595] Trial 0 finished with value: 0.07383168013089272 and parameters: {'alpha': 4.7, 'embed_size': 320, 'num_layers': 2, 'heads': 5, 'forward_expansion': 3, 'decoder_dropout': 0.2, 'classificiation_dropout': 0.4, 'pre_norm_on': False, 'mlp_scale_classification': 7, 'learning_rate': 0.0035939638737664367}. Best is trial 0 with value: 0.07383168013089272.\n",
      "Best Hyperparameters: {'alpha': 4.7, 'embed_size': 320, 'num_layers': 2, 'heads': 5, 'forward_expansion': 3, 'decoder_dropout': 0.2, 'classificiation_dropout': 0.4, 'pre_norm_on': False, 'mlp_scale_classification': 7, 'learning_rate': 0.0035939638737664367}\n",
      "Best Validation Accuracy (at Early Stopping): 0.07383168013089272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Set the number of optimization trials\n",
    "num_trials = 50\n",
    "\n",
    "# Create an Optuna study\n",
    "study = optuna.create_study(direction='maximize')  # Maximize validation accuracy\n",
    "\n",
    "# Start the optimization process\n",
    "study.optimize(objective, n_trials=num_trials, show_progress_bar=True)\n",
    "\n",
    "# Get the best hyperparameters and the validation accuracy at the point of early stopping\n",
    "best_params = study.best_params\n",
    "best_val_accuracy = study.best_value\n",
    "\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "print(\"Best Validation Accuracy (at Early Stopping):\", best_val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [ 1/100]  | Train: Loss 4.205244916777371, Accuracy 0.07163047527225716       | Test: Loss 4.107066875849014, Accuracy 0.08517382413087934       \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/warin/projects/CAT-Transformer/experiments/helena/tuning_new.ipynb Cell 7\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/warin/projects/CAT-Transformer/experiments/helena/tuning_new.ipynb#X20sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m test_accuracies_1 \u001b[39m=\u001b[39m [] \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/warin/projects/CAT-Transformer/experiments/helena/tuning_new.ipynb#X20sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/warin/projects/CAT-Transformer/experiments/helena/tuning_new.ipynb#X20sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     train_loss, train_acc \u001b[39m=\u001b[39m train(regression_on\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/warin/projects/CAT-Transformer/experiments/helena/tuning_new.ipynb#X20sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m                                    dataloader\u001b[39m=\u001b[39;49mtrain_dataloader, \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/warin/projects/CAT-Transformer/experiments/helena/tuning_new.ipynb#X20sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m                                    model\u001b[39m=\u001b[39;49mmodel, \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/warin/projects/CAT-Transformer/experiments/helena/tuning_new.ipynb#X20sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m                                    loss_function\u001b[39m=\u001b[39;49mloss_functions, \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/warin/projects/CAT-Transformer/experiments/helena/tuning_new.ipynb#X20sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m                                    optimizer\u001b[39m=\u001b[39;49moptimizer, \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/warin/projects/CAT-Transformer/experiments/helena/tuning_new.ipynb#X20sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m                                    device_in_use\u001b[39m=\u001b[39;49mdevice_in_use)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/warin/projects/CAT-Transformer/experiments/helena/tuning_new.ipynb#X20sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m     test_loss, test_acc \u001b[39m=\u001b[39m test(regression_on\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/warin/projects/CAT-Transformer/experiments/helena/tuning_new.ipynb#X20sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m                                dataloader\u001b[39m=\u001b[39mtest_dataloader,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/warin/projects/CAT-Transformer/experiments/helena/tuning_new.ipynb#X20sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m                                model\u001b[39m=\u001b[39mmodel,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/warin/projects/CAT-Transformer/experiments/helena/tuning_new.ipynb#X20sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m                                loss_function\u001b[39m=\u001b[39mloss_functions,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/warin/projects/CAT-Transformer/experiments/helena/tuning_new.ipynb#X20sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m                                device_in_use\u001b[39m=\u001b[39mdevice_in_use)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/warin/projects/CAT-Transformer/experiments/helena/tuning_new.ipynb#X20sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m     train_losses\u001b[39m.\u001b[39mappend(train_loss)\n",
      "File \u001b[0;32m~/projects/CAT-Transformer/model/updatedModel.py:414\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(regression_on, dataloader, model, loss_function, optimizer, device_in_use)\u001b[0m\n\u001b[1;32m    411\u001b[0m all_targets_1\u001b[39m.\u001b[39mextend(labels\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy())\n\u001b[1;32m    412\u001b[0m all_predictions_1\u001b[39m.\u001b[39mextend(y_pred_labels_1\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy())\n\u001b[0;32m--> 414\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m    415\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m    416\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/miniconda3/envs/ml-env/lib/python3.11/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    493\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    494\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/ml-env/lib/python3.11/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m     tensors,\n\u001b[1;32m    253\u001b[0m     grad_tensors_,\n\u001b[1;32m    254\u001b[0m     retain_graph,\n\u001b[1;32m    255\u001b[0m     create_graph,\n\u001b[1;32m    256\u001b[0m     inputs,\n\u001b[1;32m    257\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    258\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    259\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Testing against the test dataset\n",
    "\n",
    "model = CATTransformer(alpha=best_params['alpha'],\n",
    "                           embed_size=best_params['embed_size'],\n",
    "                           n_cont=len(cont_columns),\n",
    "                           cat_feat=[],\n",
    "                           num_layers=best_params['num_layers'],\n",
    "                           heads=best_params['heads'],\n",
    "                           forward_expansion=best_params['forward_expansion'],\n",
    "                           decoder_dropout=best_params['decoder_dropout'],\n",
    "                           classification_dropout=best_params['classificiation_dropout'],\n",
    "                           pre_norm_on=best_params['pre_norm_on'],\n",
    "                           mlp_scale_classification=best_params['mlp_scale_classification'],\n",
    "                           regression_on=False,\n",
    "                           targets_classes=target_classes).to(device_in_use)\n",
    "loss_functions = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr = best_params['learning_rate']) # Maybe try messing around with optimizers. try other torch optimizers with different configurations.\n",
    "early_stopping = EarlyStopping(patience=5)\n",
    "epochs = 100 \n",
    "\n",
    "train_losses = []\n",
    "train_accuracies_1 = [] \n",
    "test_losses = []\n",
    "test_accuracies_1 = [] \n",
    "\n",
    "for t in range(epochs):\n",
    "    train_loss, train_acc = train(regression_on=False, \n",
    "                                   dataloader=train_dataloader, \n",
    "                                   model=model, \n",
    "                                   loss_function=loss_functions, \n",
    "                                   optimizer=optimizer, \n",
    "                                   device_in_use=device_in_use)\n",
    "    test_loss, test_acc = test(regression_on=False,\n",
    "                               dataloader=test_dataloader,\n",
    "                               model=model,\n",
    "                               loss_function=loss_functions,\n",
    "                               device_in_use=device_in_use)\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies_1.append(train_acc)\n",
    "    test_losses.append(test_loss)\n",
    "    test_accuracies_1.append(test_acc)\n",
    "\n",
    "    epoch_str = f\"Epoch [{t+1:2}/{epochs}]\"\n",
    "    train_metrics = f\"Train: Loss {(train_loss)}, Accuracy {(train_acc)}\"\n",
    "    test_metrics = f\"Test: Loss {(test_loss)}, Accuracy {(test_acc)}\"\n",
    "    print(f\"{epoch_str:15} | {train_metrics:65} | {test_metrics:65}\")\n",
    "\n",
    "\n",
    "# Plotting the loss curves\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, epochs+1), train_losses, label='Train Loss')\n",
    "plt.plot(range(1, epochs+1), [l for l in test_losses], label='Test Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Test Loss Curve')\n",
    "plt.legend()\n",
    "\n",
    "# Plotting the accuracy curves\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, epochs+1), train_accuracies_1, label='Train Accuracy')\n",
    "plt.plot(range(1, epochs+1), test_accuracies_1, label='Test Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Test Accuracy Curve')\n",
    "plt.legend()\n",
    "\n",
    "best_index = test_accuracies_1.index(max(test_accuracies_1))\n",
    "print(f\"Best accuracy {test_accuracies_1[best_index]}\\n\",file=open(\"log_2pi.txt\", 'a'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
