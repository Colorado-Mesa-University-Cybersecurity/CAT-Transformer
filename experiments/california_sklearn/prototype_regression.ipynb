{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from rff.layers import GaussianEncoding #pip install random-fourier-features-pytorch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import os\n",
    "import optuna\n",
    "from optuna.trial import TrialState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available and being used\n"
     ]
    }
   ],
   "source": [
    "# Run regardless if you do or do not have GPU so all tensors are moved to right location later on\n",
    "if torch.cuda.is_available():\n",
    "    device_in_use = torch.device(\"cuda\")\n",
    "    print(\"GPU is available and being used\")\n",
    "else:\n",
    "    device_in_use = torch.device(\"cpu\")\n",
    "    print(\"GPU is not available, using CPU instead\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD AND PROCESS DATA\n",
    "**EXAMPLE WITH COVTYPE DATASET**\n",
    "1. Standardize or perform quantile transformations to numerical/continuous features.\n",
    "1. Wrap with Dataset and Dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedInc</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>AveRooms</th>\n",
       "      <th>AveBedrms</th>\n",
       "      <th>Population</th>\n",
       "      <th>AveOccup</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.8631</td>\n",
       "      <td>20.0</td>\n",
       "      <td>4.401210</td>\n",
       "      <td>1.076613</td>\n",
       "      <td>999.0</td>\n",
       "      <td>2.014113</td>\n",
       "      <td>32.79</td>\n",
       "      <td>-117.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.2026</td>\n",
       "      <td>24.0</td>\n",
       "      <td>5.617544</td>\n",
       "      <td>0.989474</td>\n",
       "      <td>731.0</td>\n",
       "      <td>2.564912</td>\n",
       "      <td>34.59</td>\n",
       "      <td>-120.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.1094</td>\n",
       "      <td>14.0</td>\n",
       "      <td>5.869565</td>\n",
       "      <td>1.094203</td>\n",
       "      <td>302.0</td>\n",
       "      <td>2.188406</td>\n",
       "      <td>39.26</td>\n",
       "      <td>-121.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.3068</td>\n",
       "      <td>52.0</td>\n",
       "      <td>4.801205</td>\n",
       "      <td>1.066265</td>\n",
       "      <td>1526.0</td>\n",
       "      <td>2.298193</td>\n",
       "      <td>37.77</td>\n",
       "      <td>-122.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0791</td>\n",
       "      <td>11.0</td>\n",
       "      <td>5.878902</td>\n",
       "      <td>1.098493</td>\n",
       "      <td>4773.0</td>\n",
       "      <td>2.568891</td>\n",
       "      <td>33.17</td>\n",
       "      <td>-117.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14442</th>\n",
       "      <td>6.3700</td>\n",
       "      <td>35.0</td>\n",
       "      <td>6.129032</td>\n",
       "      <td>0.926267</td>\n",
       "      <td>658.0</td>\n",
       "      <td>3.032258</td>\n",
       "      <td>33.78</td>\n",
       "      <td>-117.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14443</th>\n",
       "      <td>3.0500</td>\n",
       "      <td>33.0</td>\n",
       "      <td>6.868597</td>\n",
       "      <td>1.269488</td>\n",
       "      <td>1753.0</td>\n",
       "      <td>3.904232</td>\n",
       "      <td>34.02</td>\n",
       "      <td>-117.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14444</th>\n",
       "      <td>2.9344</td>\n",
       "      <td>36.0</td>\n",
       "      <td>3.986717</td>\n",
       "      <td>1.079696</td>\n",
       "      <td>1756.0</td>\n",
       "      <td>3.332068</td>\n",
       "      <td>34.03</td>\n",
       "      <td>-118.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14445</th>\n",
       "      <td>5.7192</td>\n",
       "      <td>15.0</td>\n",
       "      <td>6.395349</td>\n",
       "      <td>1.067979</td>\n",
       "      <td>1777.0</td>\n",
       "      <td>3.178891</td>\n",
       "      <td>37.58</td>\n",
       "      <td>-121.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14446</th>\n",
       "      <td>2.5755</td>\n",
       "      <td>52.0</td>\n",
       "      <td>3.402576</td>\n",
       "      <td>1.058776</td>\n",
       "      <td>2619.0</td>\n",
       "      <td>2.108696</td>\n",
       "      <td>37.77</td>\n",
       "      <td>-122.42</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14447 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
       "0      2.8631      20.0  4.401210   1.076613       999.0  2.014113     32.79   \n",
       "1      4.2026      24.0  5.617544   0.989474       731.0  2.564912     34.59   \n",
       "2      3.1094      14.0  5.869565   1.094203       302.0  2.188406     39.26   \n",
       "3      3.3068      52.0  4.801205   1.066265      1526.0  2.298193     37.77   \n",
       "4      4.0791      11.0  5.878902   1.098493      4773.0  2.568891     33.17   \n",
       "...       ...       ...       ...        ...         ...       ...       ...   \n",
       "14442  6.3700      35.0  6.129032   0.926267       658.0  3.032258     33.78   \n",
       "14443  3.0500      33.0  6.868597   1.269488      1753.0  3.904232     34.02   \n",
       "14444  2.9344      36.0  3.986717   1.079696      1756.0  3.332068     34.03   \n",
       "14445  5.7192      15.0  6.395349   1.067979      1777.0  3.178891     37.58   \n",
       "14446  2.5755      52.0  3.402576   1.058776      2619.0  2.108696     37.77   \n",
       "\n",
       "       Longitude  \n",
       "0        -117.09  \n",
       "1        -120.14  \n",
       "2        -121.00  \n",
       "3        -122.45  \n",
       "4        -117.33  \n",
       "...          ...  \n",
       "14442    -117.96  \n",
       "14443    -117.43  \n",
       "14444    -118.38  \n",
       "14445    -121.96  \n",
       "14446    -122.42  \n",
       "\n",
       "[14447 rows x 8 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv('/home/cscadmin/CyberResearch/CAT-Transformer/datasets/california/train.csv')\n",
    "df_test = pd.read_csv('/home/cscadmin/CyberResearch/CAT-Transformer/datasets/california/test.csv')\n",
    "df_val = pd.read_csv('/home/cscadmin/CyberResearch/CAT-Transformer/datasets/california/validation.csv') #READ FROM RIGHT SPOT\n",
    "\n",
    "# df_train = pd.read_csv(r'C:\\Users\\smbm2\\projects\\CAT-Transformer\\datasets\\california\\train.csv')\n",
    "# df_test = pd.read_csv(r'C:\\Users\\smbm2\\projects\\CAT-Transformer\\datasets\\california\\test.csv')\n",
    "# df_val = pd.read_csv(r'C:\\Users\\smbm2\\projects\\CAT-Transformer\\datasets\\california\\validation.csv') #READ FROM RIGHT SPOT\n",
    "\n",
    "#Take a look at what the datasets look like initially to get an idea\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup',\n",
       "       'Latitude', 'Longitude'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Take a look at the feature names\n",
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_columns = ['HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup',\n",
    "       'Latitude', 'Longitude']\n",
    "target = ['MedInc']\n",
    "\n",
    "#CHECKING TO MAKE SURE YOUR LIST IS CORRECT (NO NEED TO TOUCH)\n",
    "yourlist = cont_columns + target\n",
    "yourlist.sort()\n",
    "oglist = list(df_train.columns)\n",
    "oglist.sort()\n",
    "\n",
    "assert(yourlist == oglist), \"You may of spelled feature name wrong or you forgot to put on of them in the list\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9851]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Getting the number of classes in your classification target\n",
    "target_classes = [max(len(df_train[target].value_counts()), len(df_val[target].value_counts()),len(df_test[target].value_counts()))]\n",
    "target_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a StandardScaler and fit it to the cont features\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(df_train[cont_columns])\n",
    "\n",
    "# Transform the training, test, and validation datasets\n",
    "df_train[cont_columns] = scaler.transform(df_train[cont_columns])\n",
    "df_test[cont_columns] = scaler.transform(df_test[cont_columns])\n",
    "df_val[cont_columns] = scaler.transform(df_val[cont_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleTaskDataset(Dataset):\n",
    "    def __init__(self, df : pd.DataFrame, num_columns,task1_column):\n",
    "        self.n = df.shape[0]\n",
    "        \n",
    "        self.task1_labels = df[task1_column].astype(np.float32).values\n",
    "\n",
    "        self.num = df[num_columns].astype(np.float32).values\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Retrieve features and labels from the dataframe using column names\n",
    "        num_features = self.num[idx]\n",
    "        labels_task1 = self.task1_labels[idx]\n",
    "\n",
    "        return num_features, labels_task1\n",
    "\n",
    "#Wrapping in Dataset\n",
    "train_dataset = SingleTaskDataset(df_train, cont_columns, 'MedInc')\n",
    "val_dataset = SingleTaskDataset(df_val, cont_columns, 'MedInc')\n",
    "test_dataset = SingleTaskDataset(df_test, cont_columns, 'MedInc')\n",
    "\n",
    "#This is a hyperparameter that is not tuned. Maybe mess with what makes sense here\n",
    "#Also try looking to see what other papers have done\n",
    "batch_size = 256\n",
    "\n",
    "# Wrapping with DataLoader for easy batch extraction\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL AND HELPERS\n",
    "\n",
    "1. All you should have to do is interact with Classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each task loss is scaled by its own learnable parameter, then regularization is applied \n",
    "class UncertaintyLoss(nn.Module):\n",
    "    def __init__(self, num_tasks):\n",
    "        super(UncertaintyLoss, self).__init__()\n",
    "        self.num_tasks = num_tasks\n",
    "\n",
    "        self.loss_fns = [nn.MSELoss() for x in range(num_tasks)] \n",
    "\n",
    "    def forward(self, predictions, labels_task1):\n",
    "\n",
    "        #task 1\n",
    "        target = labels_task1\n",
    "        prediction = predictions\n",
    "        loss_fn = self.loss_fns[0]\n",
    "        task_loss = loss_fn(prediction, target)\n",
    "        \n",
    "        return task_loss\n",
    "    \n",
    "#All layers of the model\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_size, heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        self.head_dim = embed_size // heads\n",
    "        assert(self.head_dim * heads == embed_size), \"Embed size needs to be div by heads\"\n",
    "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.keys =nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.fc_out = nn.Linear(heads*self.head_dim, embed_size)\n",
    "\n",
    "\n",
    "    def forward(self, values, keys, query):\n",
    "        N = query.shape[0]\n",
    "        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
    "\n",
    "        values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
    "        keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n",
    "        queries = query.reshape(N, query_len, self.heads, self.head_dim)\n",
    "\n",
    "        values = self.values(values)\n",
    "        keys = self.keys(keys)\n",
    "        queries = self.queries(queries)\n",
    "\n",
    "        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
    "\n",
    "        attention = torch.softmax(energy / (self.embed_size ** (1/2)), dim=3) #(batch_size, head_dim, #query_embeddings, #key_embeddings)\n",
    "\n",
    "        # Calculate simplified attention scores\n",
    "        avg_attention = attention.mean(dim=0)  # Average across batches\n",
    "        # print(\"batch average\", avg_attention.shape)\n",
    "        avg_attention = avg_attention.mean(dim=0).squeeze(dim=0)\n",
    "        # print(\"head average\", avg_attention.shape)\n",
    "\n",
    "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(N, query_len, self.heads*self.head_dim) #(batch_size, n_features, embed_size)\n",
    "        out = self.fc_out(out)\n",
    "\n",
    "        return out, avg_attention\n",
    "    \n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_size, heads, dropout, forward_expansion, pre_norm_on):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "\n",
    "        self.pre_norm_on = pre_norm_on\n",
    "        if self.pre_norm_on:\n",
    "            self.pre_norm = nn.LayerNorm(embed_size)\n",
    "        self.attention = MultiHeadAttention(embed_size, heads)\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "\n",
    "        self.feed_forward = nn.Sequential(nn.Linear(embed_size, forward_expansion*embed_size),\n",
    "                                          nn.ReLU(),\n",
    "                                          nn.Linear(forward_expansion*embed_size, embed_size)\n",
    "                                          )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self,value,key,query):\n",
    "        if self.pre_norm_on:\n",
    "            query = self.pre_norm(query)\n",
    "            key = self.pre_norm(key)\n",
    "            value = self.pre_norm(value)\n",
    "            \n",
    "        attention, avg_attention = self.attention(value, key, query)\n",
    "        x = self.dropout(self.norm1(attention + query))\n",
    "        forward = self.feed_forward(x)\n",
    "        out = self.dropout(self.norm2(forward + x))\n",
    "        return out, avg_attention\n",
    "    \n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, embed_size, heads, forward_expansion, dropout, pre_norm_on):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "\n",
    "        self.attention = MultiHeadAttention(embed_size, heads)\n",
    "        self.norm = nn.LayerNorm(embed_size)\n",
    "        self.transformer_block = TransformerBlock(embed_size, heads, dropout, forward_expansion, pre_norm_on)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, value, key):\n",
    "        out, avg_attention = self.transformer_block(value, key, x)\n",
    "\n",
    "        return out, avg_attention\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 embed_size,\n",
    "                 num_layers,\n",
    "                 heads,\n",
    "                 forward_expansion,\n",
    "                 decoder_dropout,\n",
    "                 pre_norm_on\n",
    "    ):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "                [\n",
    "                    DecoderBlock(\n",
    "                        embed_size,\n",
    "                        heads,\n",
    "                        dropout=decoder_dropout,\n",
    "                        forward_expansion=forward_expansion,\n",
    "                        pre_norm_on=pre_norm_on\n",
    "                    )\n",
    "                    for _ in range(num_layers)\n",
    "                ]\n",
    "            )\n",
    "        self.avg_attention = None\n",
    "\n",
    "    def forward(self, class_embed, context):\n",
    "        for layer in self.layers:\n",
    "            # x is the classification embedding (CLS Token)\n",
    "            # context are the feature embeddings that will be used as key and value\n",
    "            x, self.avg_attention = layer(class_embed, context, context)\n",
    "  \n",
    "        return x \n",
    "\n",
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, sigma, embed_size, input_size, embedding_dropout, n_features, num_target_labels, rff_on):\n",
    "        super(Embeddings, self).__init__()\n",
    "\n",
    "        self.rff_on = rff_on\n",
    "\n",
    "        if self.rff_on:\n",
    "            self.rffs = nn.ModuleList([GaussianEncoding(sigma=sigma, input_size=input_size, encoded_size=embed_size//2) for _ in range(n_features)])\n",
    "            self.dropout = nn.Dropout(embedding_dropout)\n",
    "            self.mlp_in = embed_size\n",
    "        else:\n",
    "            self.mlp_in = input_size\n",
    "\n",
    "        self.embeddings = nn.ModuleList([nn.Linear(in_features=self.mlp_in, out_features=embed_size) for _ in range(n_features)])\n",
    "\n",
    "        # Classifcation Embeddings for each target label\n",
    "        self.target_label_embeddings = nn.ModuleList([nn.Embedding(1, embed_size) for _ in range(num_target_labels)])\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(2) #(batch_size, n_features) -> (batch_size, n_features, 1)\n",
    "        rff_vectors = []\n",
    "        if self.rff_on:\n",
    "            for i, r in enumerate(self.rffs):\n",
    "                input = x[:,i,:]\n",
    "                out = r(input)\n",
    "                rff_vectors.append(out)\n",
    "        \n",
    "            x = torch.stack(rff_vectors, dim=1)\n",
    "        \n",
    "        embeddings = []\n",
    "        for i, e in enumerate(self.embeddings):\n",
    "            goin_in = x[:,i,:]\n",
    "            goin_out = e(goin_in)\n",
    "            embeddings.append(goin_out)\n",
    "\n",
    "        target_label_embeddings_ = []\n",
    "        for e in self.target_label_embeddings:\n",
    "            input = torch.tensor([0], device=x.device)\n",
    "            temp = e(input)\n",
    "            temp = temp.repeat(x.size(0), 1)\n",
    "            tmep = temp.unsqueeze(1)\n",
    "            target_label_embeddings_.append(temp)\n",
    "\n",
    "        class_embeddings = torch.stack(target_label_embeddings_, dim=1)\n",
    "\n",
    "        context = torch.stack(embeddings, dim=1)\n",
    "\n",
    "        return class_embeddings, context\n",
    "\n",
    "class classificationHead(nn.Module):\n",
    "    def __init__(self, embed_size, dropout, mlp_scale_classification, num_target_classes):\n",
    "        super(classificationHead, self).__init__()\n",
    "        \n",
    "        #flattening the embeddings out so each sample in batch is represented with a 460 dimensional vector\n",
    "        self.input = embed_size\n",
    "        self.lin1 = nn.Linear(self.input, mlp_scale_classification*self.input)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.lin2 = nn.Linear(mlp_scale_classification*self.input, mlp_scale_classification*self.input)\n",
    "        self.lin3 = nn.Linear(mlp_scale_classification*self.input, self.input)\n",
    "        self.lin4 = nn.Linear(self.input, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.initialize_weights()\n",
    "\n",
    "    def initialize_weights(self): #he_initialization.\n",
    "        torch.nn.init.kaiming_normal_(self.lin1.weight, nonlinearity='relu')\n",
    "        torch.nn.init.zeros_(self.lin1.bias)\n",
    "\n",
    "        torch.nn.init.kaiming_normal_(self.lin3.weight, nonlinearity='relu')\n",
    "        torch.nn.init.zeros_(self.lin3.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x= torch.reshape(x, (-1, self.input))\n",
    "\n",
    "        x = self.lin1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.lin2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.lin3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.lin4(x)\n",
    "  \n",
    "        return x\n",
    "    \n",
    "# class classificationHead(nn.Module):\n",
    "#     def __init__(self, embed_size, dropout, mlp_scale_classification, num_target_classes):\n",
    "#         super(classificationHead, self).__init__()\n",
    "        \n",
    "#         #flattening the embeddings out so each sample in batch is represented with a 460 dimensional vector\n",
    "#         self.input = embed_size\n",
    "#         self.lin1 = nn.Linear(self.input, mlp_scale_classification*self.input)\n",
    "#         self.drop = nn.Dropout(dropout)\n",
    "#         self.lin2 = nn.Linear(mlp_scale_classification*self.input, mlp_scale_classification*self.input)\n",
    "#         self.lin3 = nn.Linear(mlp_scale_classification*self.input, self.input)\n",
    "#         self.lin4 = nn.Linear(self.input, num_target_classes)\n",
    "#         self.relu = nn.ReLU()\n",
    "#         self.initialize_weights()\n",
    "\n",
    "#     def initialize_weights(self): #he_initialization.\n",
    "#         torch.nn.init.kaiming_normal_(self.lin1.weight, nonlinearity='relu')\n",
    "#         torch.nn.init.zeros_(self.lin1.bias)\n",
    "\n",
    "#         torch.nn.init.kaiming_normal_(self.lin3.weight, nonlinearity='relu')\n",
    "#         torch.nn.init.zeros_(self.lin3.bias)\n",
    "\n",
    "#     def forward(self, x):\n",
    "\n",
    "#         x= torch.reshape(x, (-1, self.input))\n",
    "\n",
    "#         x = self.lin1(x)\n",
    "#         x = self.relu(x)\n",
    "#         x = self.drop(x)\n",
    "#         x = self.lin2(x)\n",
    "#         x = self.relu(x)\n",
    "#         x = self.drop(x)\n",
    "#         x = self.lin3(x)\n",
    "#         x = self.relu(x)\n",
    "#         x = self.drop(x)\n",
    "#         x = self.lin4(x)\n",
    "  \n",
    "#         return x\n",
    "\n",
    "\n",
    "# DEFAULT PARAMETERS SET UP FOR VPN DATASET. BE CAREFUL AND MAKE SURE YOU SET THEM UP HOW YOU WANT.\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, \n",
    "                 rff_on = False,\n",
    "                 sigma=4,\n",
    "                 embed_size=20,\n",
    "                 input_size=1,\n",
    "                 embedding_dropout = 0,\n",
    "                 n_features=23, # YOU WILL PROBABLY NEED TO CHANGE\n",
    "                 num_layers=1,\n",
    "                 heads=1,\n",
    "                 forward_expansion=4, # Determines how wide the MLP is in the encoder. Its a scaling factor. \n",
    "                 decoder_dropout=0,\n",
    "                 classification_dropout = 0,\n",
    "                 pre_norm_on = False,\n",
    "                 mlp_scale_classification = 4,\n",
    "                 targets_classes : list=  [3,8]\n",
    "                 ):\n",
    "        super(Classifier, self).__init__()\n",
    "\n",
    "        self.embeddings = Embeddings(rff_on=rff_on, sigma=sigma, embed_size=embed_size, input_size=input_size, embedding_dropout=embedding_dropout, n_features=n_features, num_target_labels=len(targets_classes))\n",
    "        self.decoder = Decoder(embed_size=embed_size, num_layers=num_layers, heads=heads, forward_expansion=forward_expansion, decoder_dropout=decoder_dropout, pre_norm_on=pre_norm_on)\n",
    "        self.classifying_heads = nn.ModuleList([classificationHead(embed_size=embed_size, dropout=classification_dropout, mlp_scale_classification=mlp_scale_classification, num_target_classes=x) for x in targets_classes])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        class_embed, context = self.embeddings(x)\n",
    "\n",
    "        x = self.decoder(class_embed, context)\n",
    "        \n",
    "        probability_dist_raw = []\n",
    "        for i, e in enumerate(self.classifying_heads):\n",
    "            input = x[:, i,:]\n",
    "            output = e(input)\n",
    "            probability_dist_raw.append(output)\n",
    "        \n",
    "        return probability_dist_raw\n",
    "\n",
    "# Training and Testing Loops\n",
    "def train(dataloader, model, loss_function, optimizer, device_in_use):\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    total_r2_score = 0\n",
    "    root_mean_squared_error_total = 0\n",
    "\n",
    "    for (features, labels_task1) in dataloader:\n",
    "        features, labels_task1 = features.to(device_in_use), labels_task1.to(device_in_use)\n",
    "\n",
    "        task_predictions = model(features)\n",
    "\n",
    "        loss = loss_function(task_predictions[0].squeeze(1), labels_task1)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Calculate R^2 score for the regression task\n",
    "        r2 = r2_score_manual(labels_task1, task_predictions[0].squeeze(1))\n",
    "        total_r2_score += r2\n",
    "\n",
    "        # Calculate RMSE score for the regression task\n",
    "        rmse_value = rmse(labels_task1, task_predictions[0].squeeze(1))\n",
    "        root_mean_squared_error_total+=rmse_value\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    avg_r2_score = total_r2_score / len(dataloader)\n",
    "    avg_rmse_score = root_mean_squared_error_total / len(dataloader)\n",
    "\n",
    "    return avg_loss, avg_r2_score, avg_rmse_score\n",
    "\n",
    "def test(dataloader, model, loss_function, device_in_use):\n",
    "  model.eval()\n",
    "  total_loss = 0\n",
    "  \n",
    "  total_loss = 0\n",
    "  total_r2_score = 0\n",
    "  root_mean_squared_error_total = 0\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for (features,labels_task1) in dataloader:\n",
    "        features, labels_task1 = features.to(device_in_use), labels_task1.to(device_in_use)\n",
    "\n",
    "        task_predictions = model(features)\n",
    "\n",
    "        loss = loss_function(task_predictions[0].squeeze(1), labels_task1)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Calculate R^2 score for the regression task\n",
    "        r2 = r2_score_manual(labels_task1, task_predictions[0].squeeze(1))\n",
    "        total_r2_score += r2\n",
    "        \n",
    "        # Calculate RMSE score for the regression task\n",
    "        rmse_value = rmse(labels_task1, task_predictions[0].squeeze(1))\n",
    "        root_mean_squared_error_total+=rmse_value\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    avg_r2_score = total_r2_score / len(dataloader)\n",
    "    avg_rmse_score = root_mean_squared_error_total / len(dataloader)\n",
    "\n",
    "    return avg_loss, avg_r2_score, avg_rmse_score\n",
    "\n",
    "def format_metric(value): # Used to format the metrics output\n",
    "    return f\"{value:.4f}\"\n",
    "\n",
    "def r2_score_manual(y_true, y_pred):\n",
    "    # Calculate the mean of true labels\n",
    "    y_mean = torch.mean(y_true)\n",
    "\n",
    "    # Calculate the total sum of squares\n",
    "    total_ss = torch.sum((y_true - y_mean)**2)\n",
    "\n",
    "    # Calculate the residual sum of squares\n",
    "    residual_ss = torch.sum((y_true - y_pred)**2)\n",
    "\n",
    "    # Calculate R-squared\n",
    "    r2 = 1 - (residual_ss / total_ss)\n",
    "\n",
    "    return r2.item()  # Convert to a Python float\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    # Calculate the squared differences\n",
    "    squared_diff = (y_true - y_pred)**2\n",
    "\n",
    "    # Calculate the mean of the squared differences\n",
    "    mean_squared_diff = torch.mean(squared_diff)\n",
    "\n",
    "    # Calculate the square root to obtain RMSE\n",
    "    rmse = torch.sqrt(mean_squared_diff)\n",
    "\n",
    "    return rmse.item()  # Convert to a Python float"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUN EXPERIMENTS\n",
    "\n",
    "1. Using Optuna to optimize CAT-Transformers hyperparameters for your dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the early stopping mechanism\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5):\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.best_metric = float('-inf')\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, metric):\n",
    "        if metric > self.best_metric:\n",
    "            self.best_metric = metric\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "\n",
    "# Function to log results to a text file\n",
    "def log_to_file(filename, text):\n",
    "    with open(filename, 'a') as f:\n",
    "        f.write(text + '\\n')\n",
    "\n",
    "def objective(trial):\n",
    "    trial_number = trial.number\n",
    "\n",
    "    # Define hyperparameters to search over\n",
    "    sigma = trial.suggest_categorical('sigma', [.001, 0.1, 1, 2, 3, 5, 10])\n",
    "    num_layers = trial.suggest_int('num_layers', 1, 2)\n",
    "    # Ensure that embed_size is divisible by num_layers\n",
    "    embed_size = trial.suggest_categorical(\"embed_size\", [50, 60, 70, 80, 90, 100, 120, 140, 160])\n",
    "    heads = trial.suggest_categorical(\"heads\", [1, 5, 10])\n",
    "    forward_expansion = trial.suggest_int('forward_expansion', 1, 8)\n",
    "    prenorm_on = trial.suggest_categorical('prenorm_on', [True, False])\n",
    "    mlp_scale_classification = trial.suggest_int('mlp_scale_classification', 1, 8)\n",
    "    embedding_dropout = trial.suggest_categorical('embedding_dropout', [0, .1, .2, .5])\n",
    "    decoder_dropout = trial.suggest_categorical('decoder_dropout', [0,.1,.2,.5])\n",
    "    classification_dropout = trial.suggest_categorical('class_drop', [0,.1,.2,.5])\n",
    "\n",
    "    learning_rate = trial.suggest_categorical('learning_rate', [0.0001, 0.001, 0.01])\n",
    "\n",
    "    num_epochs = 100\n",
    "\n",
    "\n",
    "    # Create your model with the sampled hyperparameters\n",
    "    model = Classifier(\n",
    "        n_features=len(cont_columns),\n",
    "        targets_classes=[0],\n",
    "        rff_on=True,\n",
    "        sigma=sigma,\n",
    "        embed_size=embed_size,\n",
    "        num_layers=num_layers,\n",
    "        heads=heads,\n",
    "        forward_expansion=forward_expansion,\n",
    "        pre_norm_on=prenorm_on,\n",
    "        mlp_scale_classification=mlp_scale_classification,\n",
    "        embedding_dropout=embedding_dropout,\n",
    "        decoder_dropout=decoder_dropout,\n",
    "        classification_dropout=classification_dropout\n",
    "    ).to(device_in_use)\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    loss_function = UncertaintyLoss(1)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Initialize early stopping\n",
    "    early_stopping = EarlyStopping(patience=10)  # Adjust patience as needed\n",
    "\n",
    "    # Training loop with a large number of epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss, r2_train, rmse_train = train(train_dataloader, model, loss_function, optimizer, device_in_use)\n",
    "        \n",
    "        # Validation loop\n",
    "        test_loss, r2_test, rmse_test = test(val_dataloader, model, loss_function, device_in_use)\n",
    "        \n",
    "        # Check if we should early stop based on validation rmse\n",
    "        if early_stopping(rmse_test):\n",
    "            break\n",
    "    \n",
    "    # Log the final test accuracy for this trial to a shared log file\n",
    "    final_log = f\"Trial {trial_number} completed. Validation RMSE = {rmse_test:.4f}\"\n",
    "    log_to_file('all_trials_log.txt', final_log)\n",
    "\n",
    "    # Return the test accuracy as the objective to optimize\n",
    "    return rmse_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-25 10:14:44,746] A new study created in memory with name: no-name-286ad601-56e9-4566-b5e9-cea403e1c5ec\n",
      "Best trial: 0. Best value: 1.57955:   1%|          | 1/100 [01:43<2:50:56, 103.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2023-10-25 10:16:28,350] Trial 0 finished with value: 1.579547460262592 and parameters: {'sigma': 2, 'num_layers': 2, 'embed_size': 140, 'heads': 1, 'forward_expansion': 6, 'prenorm_on': False, 'mlp_scale_classification': 4, 'embedding_dropout': 0.2, 'decoder_dropout': 0.5, 'class_drop': 0.1, 'learning_rate': 0.001}. Best is trial 0 with value: 1.579547460262592.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 1. Best value: 0.943888:   2%|▏         | 2/100 [03:25<2:47:48, 102.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2023-10-25 10:18:10,480] Trial 1 finished with value: 0.9438884487518897 and parameters: {'sigma': 3, 'num_layers': 2, 'embed_size': 80, 'heads': 5, 'forward_expansion': 4, 'prenorm_on': False, 'mlp_scale_classification': 3, 'embedding_dropout': 0.5, 'decoder_dropout': 0, 'class_drop': 0.2, 'learning_rate': 0.001}. Best is trial 1 with value: 0.9438884487518897.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 2. Best value: 0.891359:   3%|▎         | 3/100 [05:15<2:51:08, 105.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2023-10-25 10:20:00,056] Trial 2 finished with value: 0.8913589670107915 and parameters: {'sigma': 2, 'num_layers': 2, 'embed_size': 50, 'heads': 5, 'forward_expansion': 6, 'prenorm_on': True, 'mlp_scale_classification': 7, 'embedding_dropout': 0, 'decoder_dropout': 0, 'class_drop': 0.2, 'learning_rate': 0.01}. Best is trial 2 with value: 0.8913589670107915.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 2. Best value: 0.891359:   4%|▍         | 4/100 [06:51<2:43:25, 102.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2023-10-25 10:21:36,482] Trial 3 finished with value: 1.4150457198803241 and parameters: {'sigma': 10, 'num_layers': 1, 'embed_size': 90, 'heads': 1, 'forward_expansion': 1, 'prenorm_on': True, 'mlp_scale_classification': 5, 'embedding_dropout': 0.5, 'decoder_dropout': 0.2, 'class_drop': 0, 'learning_rate': 0.0001}. Best is trial 2 with value: 0.8913589670107915.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 2. Best value: 0.891359:   5%|▌         | 5/100 [08:35<2:42:23, 102.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2023-10-25 10:23:19,804] Trial 4 finished with value: 0.9954944161268381 and parameters: {'sigma': 5, 'num_layers': 2, 'embed_size': 140, 'heads': 5, 'forward_expansion': 6, 'prenorm_on': False, 'mlp_scale_classification': 1, 'embedding_dropout': 0.1, 'decoder_dropout': 0.5, 'class_drop': 0.5, 'learning_rate': 0.001}. Best is trial 2 with value: 0.8913589670107915.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 2. Best value: 0.891359:   6%|▌         | 6/100 [10:16<2:40:11, 102.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2023-10-25 10:25:01,450] Trial 5 finished with value: 0.9526558059912461 and parameters: {'sigma': 1, 'num_layers': 2, 'embed_size': 80, 'heads': 5, 'forward_expansion': 4, 'prenorm_on': False, 'mlp_scale_classification': 2, 'embedding_dropout': 0.1, 'decoder_dropout': 0, 'class_drop': 0, 'learning_rate': 0.001}. Best is trial 2 with value: 0.8913589670107915.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 2. Best value: 0.891359:   7%|▋         | 7/100 [11:59<2:38:40, 102.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2023-10-25 10:26:44,067] Trial 6 finished with value: 2.4008707266587477 and parameters: {'sigma': 3, 'num_layers': 1, 'embed_size': 160, 'heads': 5, 'forward_expansion': 5, 'prenorm_on': True, 'mlp_scale_classification': 8, 'embedding_dropout': 0.5, 'decoder_dropout': 0.5, 'class_drop': 0.5, 'learning_rate': 0.0001}. Best is trial 2 with value: 0.8913589670107915.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 2. Best value: 0.891359:   8%|▊         | 8/100 [13:33<2:32:52, 99.70s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2023-10-25 10:28:18,040] Trial 7 finished with value: 0.9211305838364822 and parameters: {'sigma': 2, 'num_layers': 1, 'embed_size': 70, 'heads': 5, 'forward_expansion': 7, 'prenorm_on': False, 'mlp_scale_classification': 1, 'embedding_dropout': 0.2, 'decoder_dropout': 0.5, 'class_drop': 0.2, 'learning_rate': 0.01}. Best is trial 2 with value: 0.8913589670107915.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 2. Best value: 0.891359:   9%|▉         | 9/100 [15:14<2:31:41, 100.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2023-10-25 10:29:58,751] Trial 8 finished with value: 1.1274813505319448 and parameters: {'sigma': 10, 'num_layers': 2, 'embed_size': 160, 'heads': 10, 'forward_expansion': 5, 'prenorm_on': False, 'mlp_scale_classification': 8, 'embedding_dropout': 0.2, 'decoder_dropout': 0.1, 'class_drop': 0.1, 'learning_rate': 0.01}. Best is trial 2 with value: 0.8913589670107915.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 2. Best value: 0.891359:  10%|█         | 10/100 [16:46<2:26:24, 97.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2023-10-25 10:31:30,968] Trial 9 finished with value: 1.7663201827269335 and parameters: {'sigma': 5, 'num_layers': 1, 'embed_size': 140, 'heads': 1, 'forward_expansion': 8, 'prenorm_on': False, 'mlp_scale_classification': 1, 'embedding_dropout': 0, 'decoder_dropout': 0.5, 'class_drop': 0, 'learning_rate': 0.0001}. Best is trial 2 with value: 0.8913589670107915.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 2. Best value: 0.891359:  11%|█         | 11/100 [18:32<2:28:31, 100.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2023-10-25 10:33:16,824] Trial 10 finished with value: 0.8974522856565622 and parameters: {'sigma': 0.1, 'num_layers': 2, 'embed_size': 50, 'heads': 10, 'forward_expansion': 2, 'prenorm_on': True, 'mlp_scale_classification': 6, 'embedding_dropout': 0, 'decoder_dropout': 0, 'class_drop': 0.2, 'learning_rate': 0.01}. Best is trial 2 with value: 0.8913589670107915.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 2. Best value: 0.891359:  12%|█▏        | 12/100 [20:19<2:29:55, 102.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2023-10-25 10:35:03,836] Trial 11 finished with value: 0.9787112291042621 and parameters: {'sigma': 0.1, 'num_layers': 2, 'embed_size': 50, 'heads': 10, 'forward_expansion': 2, 'prenorm_on': True, 'mlp_scale_classification': 6, 'embedding_dropout': 0, 'decoder_dropout': 0, 'class_drop': 0.2, 'learning_rate': 0.01}. Best is trial 2 with value: 0.8913589670107915.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 2. Best value: 0.891359:  13%|█▎        | 13/100 [22:04<2:29:40, 103.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2023-10-25 10:36:49,368] Trial 12 finished with value: 1.0326063082768366 and parameters: {'sigma': 0.001, 'num_layers': 2, 'embed_size': 50, 'heads': 10, 'forward_expansion': 3, 'prenorm_on': True, 'mlp_scale_classification': 6, 'embedding_dropout': 0, 'decoder_dropout': 0, 'class_drop': 0.2, 'learning_rate': 0.01}. Best is trial 2 with value: 0.8913589670107915.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 13. Best value: 0.881499:  14%|█▍        | 14/100 [23:50<2:28:56, 103.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2023-10-25 10:38:34,857] Trial 13 finished with value: 0.8814985797955439 and parameters: {'sigma': 0.1, 'num_layers': 2, 'embed_size': 50, 'heads': 10, 'forward_expansion': 1, 'prenorm_on': True, 'mlp_scale_classification': 7, 'embedding_dropout': 0, 'decoder_dropout': 0, 'class_drop': 0.2, 'learning_rate': 0.01}. Best is trial 13 with value: 0.8814985797955439.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 13. Best value: 0.881499:  15%|█▌        | 15/100 [25:39<2:29:40, 105.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2023-10-25 10:40:24,536] Trial 14 finished with value: 1.0105721308634832 and parameters: {'sigma': 2, 'num_layers': 2, 'embed_size': 60, 'heads': 10, 'forward_expansion': 1, 'prenorm_on': True, 'mlp_scale_classification': 7, 'embedding_dropout': 0, 'decoder_dropout': 0.1, 'class_drop': 0.2, 'learning_rate': 0.01}. Best is trial 13 with value: 0.8814985797955439.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 15. Best value: 0.864968:  16%|█▌        | 16/100 [27:27<2:28:48, 106.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2023-10-25 10:42:12,329] Trial 15 finished with value: 0.8649675433452313 and parameters: {'sigma': 0.1, 'num_layers': 2, 'embed_size': 120, 'heads': 5, 'forward_expansion': 8, 'prenorm_on': True, 'mlp_scale_classification': 7, 'embedding_dropout': 0, 'decoder_dropout': 0.2, 'class_drop': 0.2, 'learning_rate': 0.01}. Best is trial 15 with value: 0.8649675433452313.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 15. Best value: 0.864968:  17%|█▋        | 17/100 [29:11<2:26:12, 105.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2023-10-25 10:43:56,611] Trial 16 finished with value: 0.8689053975618802 and parameters: {'sigma': 0.1, 'num_layers': 2, 'embed_size': 120, 'heads': 10, 'forward_expansion': 8, 'prenorm_on': True, 'mlp_scale_classification': 5, 'embedding_dropout': 0, 'decoder_dropout': 0.2, 'class_drop': 0.2, 'learning_rate': 0.01}. Best is trial 15 with value: 0.8649675433452313.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 15. Best value: 0.864968:  18%|█▊        | 18/100 [30:53<2:22:49, 104.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2023-10-25 10:45:38,368] Trial 17 finished with value: 0.8654124507537255 and parameters: {'sigma': 0.1, 'num_layers': 1, 'embed_size': 120, 'heads': 5, 'forward_expansion': 8, 'prenorm_on': True, 'mlp_scale_classification': 4, 'embedding_dropout': 0, 'decoder_dropout': 0.2, 'class_drop': 0.5, 'learning_rate': 0.01}. Best is trial 15 with value: 0.8649675433452313.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 15. Best value: 0.864968:  19%|█▉        | 19/100 [32:37<2:20:38, 104.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2023-10-25 10:47:21,797] Trial 18 finished with value: 1.0576759989445026 and parameters: {'sigma': 0.1, 'num_layers': 1, 'embed_size': 120, 'heads': 5, 'forward_expansion': 7, 'prenorm_on': True, 'mlp_scale_classification': 4, 'embedding_dropout': 0.1, 'decoder_dropout': 0.2, 'class_drop': 0.5, 'learning_rate': 0.01}. Best is trial 15 with value: 0.8649675433452313.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 15. Best value: 0.864968:  20%|██        | 20/100 [34:20<2:18:37, 103.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2023-10-25 10:49:05,265] Trial 19 finished with value: 1.4797649750342736 and parameters: {'sigma': 1, 'num_layers': 1, 'embed_size': 100, 'heads': 5, 'forward_expansion': 8, 'prenorm_on': True, 'mlp_scale_classification': 3, 'embedding_dropout': 0, 'decoder_dropout': 0.2, 'class_drop': 0.5, 'learning_rate': 0.0001}. Best is trial 15 with value: 0.8649675433452313.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 15. Best value: 0.864968:  21%|██        | 21/100 [36:03<2:16:22, 103.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2023-10-25 10:50:47,909] Trial 20 finished with value: 1.895534405341515 and parameters: {'sigma': 0.001, 'num_layers': 1, 'embed_size': 120, 'heads': 5, 'forward_expansion': 7, 'prenorm_on': True, 'mlp_scale_classification': 3, 'embedding_dropout': 0, 'decoder_dropout': 0.2, 'class_drop': 0.5, 'learning_rate': 0.01}. Best is trial 15 with value: 0.8649675433452313.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 21. Best value: 0.84955:  22%|██▏       | 22/100 [37:42<2:13:02, 102.35s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2023-10-25 10:52:27,395] Trial 21 finished with value: 0.8495504581011258 and parameters: {'sigma': 0.1, 'num_layers': 1, 'embed_size': 120, 'heads': 5, 'forward_expansion': 8, 'prenorm_on': True, 'mlp_scale_classification': 5, 'embedding_dropout': 0, 'decoder_dropout': 0.2, 'class_drop': 0.1, 'learning_rate': 0.01}. Best is trial 21 with value: 0.8495504581011258.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 22. Best value: 0.845318:  23%|██▎       | 23/100 [39:26<2:11:54, 102.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2023-10-25 10:54:11,199] Trial 22 finished with value: 0.8453183449231662 and parameters: {'sigma': 0.1, 'num_layers': 1, 'embed_size': 120, 'heads': 5, 'forward_expansion': 8, 'prenorm_on': True, 'mlp_scale_classification': 5, 'embedding_dropout': 0, 'decoder_dropout': 0.2, 'class_drop': 0.1, 'learning_rate': 0.01}. Best is trial 22 with value: 0.8453183449231662.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 22. Best value: 0.845318:  24%|██▍       | 24/100 [41:06<2:09:15, 102.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2023-10-25 10:55:51,516] Trial 23 finished with value: 0.8754452696213355 and parameters: {'sigma': 0.1, 'num_layers': 1, 'embed_size': 120, 'heads': 5, 'forward_expansion': 7, 'prenorm_on': True, 'mlp_scale_classification': 5, 'embedding_dropout': 0, 'decoder_dropout': 0.2, 'class_drop': 0.1, 'learning_rate': 0.01}. Best is trial 22 with value: 0.8453183449231662.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 22. Best value: 0.845318:  25%|██▌       | 25/100 [42:46<2:06:31, 101.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2023-10-25 10:57:30,832] Trial 24 finished with value: 0.8731918334960938 and parameters: {'sigma': 0.1, 'num_layers': 1, 'embed_size': 120, 'heads': 5, 'forward_expansion': 8, 'prenorm_on': True, 'mlp_scale_classification': 6, 'embedding_dropout': 0, 'decoder_dropout': 0.2, 'class_drop': 0.1, 'learning_rate': 0.01}. Best is trial 22 with value: 0.8453183449231662.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 22. Best value: 0.845318:  26%|██▌       | 26/100 [44:27<2:04:44, 101.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2023-10-25 10:59:11,771] Trial 25 finished with value: 0.8612062289164617 and parameters: {'sigma': 0.1, 'num_layers': 1, 'embed_size': 120, 'heads': 5, 'forward_expansion': 7, 'prenorm_on': True, 'mlp_scale_classification': 5, 'embedding_dropout': 0, 'decoder_dropout': 0.2, 'class_drop': 0.1, 'learning_rate': 0.01}. Best is trial 22 with value: 0.8453183449231662.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 22. Best value: 0.845318:  27%|██▋       | 27/100 [46:01<2:00:41, 99.19s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2023-10-25 11:00:46,427] Trial 26 finished with value: 1.906200069647569 and parameters: {'sigma': 0.1, 'num_layers': 1, 'embed_size': 60, 'heads': 1, 'forward_expansion': 7, 'prenorm_on': True, 'mlp_scale_classification': 5, 'embedding_dropout': 0.2, 'decoder_dropout': 0.2, 'class_drop': 0.1, 'learning_rate': 0.01}. Best is trial 22 with value: 0.8453183449231662.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 22. Best value: 0.845318:  28%|██▊       | 28/100 [47:44<2:00:30, 100.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2023-10-25 11:02:29,706] Trial 27 finished with value: 0.9158270634137667 and parameters: {'sigma': 0.1, 'num_layers': 1, 'embed_size': 70, 'heads': 5, 'forward_expansion': 6, 'prenorm_on': True, 'mlp_scale_classification': 4, 'embedding_dropout': 0.1, 'decoder_dropout': 0.1, 'class_drop': 0.1, 'learning_rate': 0.01}. Best is trial 22 with value: 0.8453183449231662.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 22. Best value: 0.845318:  29%|██▉       | 29/100 [49:27<1:59:36, 101.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2023-10-25 11:04:12,316] Trial 28 finished with value: 1.3235201560533965 and parameters: {'sigma': 10, 'num_layers': 1, 'embed_size': 90, 'heads': 5, 'forward_expansion': 7, 'prenorm_on': True, 'mlp_scale_classification': 5, 'embedding_dropout': 0.5, 'decoder_dropout': 0.2, 'class_drop': 0.1, 'learning_rate': 0.0001}. Best is trial 22 with value: 0.8453183449231662.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 22. Best value: 0.845318:  30%|███       | 30/100 [51:07<1:57:36, 100.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2023-10-25 11:05:52,507] Trial 29 finished with value: 1.1271171386425312 and parameters: {'sigma': 0.001, 'num_layers': 1, 'embed_size': 100, 'heads': 1, 'forward_expansion': 5, 'prenorm_on': True, 'mlp_scale_classification': 3, 'embedding_dropout': 0.2, 'decoder_dropout': 0.2, 'class_drop': 0.1, 'learning_rate': 0.001}. Best is trial 22 with value: 0.8453183449231662.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 22. Best value: 0.845318:  31%|███       | 31/100 [52:48<1:55:55, 100.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2023-10-25 11:07:33,315] Trial 30 finished with value: 1.0229045702860906 and parameters: {'sigma': 5, 'num_layers': 1, 'embed_size': 120, 'heads': 5, 'forward_expansion': 6, 'prenorm_on': True, 'mlp_scale_classification': 4, 'embedding_dropout': 0, 'decoder_dropout': 0.2, 'class_drop': 0.1, 'learning_rate': 0.001}. Best is trial 22 with value: 0.8453183449231662.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 22. Best value: 0.845318:  32%|███▏      | 32/100 [54:24<1:52:39, 99.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2023-10-25 11:09:09,450] Trial 31 finished with value: 0.8464146852493286 and parameters: {'sigma': 0.1, 'num_layers': 1, 'embed_size': 120, 'heads': 5, 'forward_expansion': 8, 'prenorm_on': True, 'mlp_scale_classification': 6, 'embedding_dropout': 0, 'decoder_dropout': 0.2, 'class_drop': 0.1, 'learning_rate': 0.01}. Best is trial 22 with value: 0.8453183449231662.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 22. Best value: 0.845318:  33%|███▎      | 33/100 [56:07<1:52:15, 100.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2023-10-25 11:10:52,602] Trial 32 finished with value: 0.8843777775764465 and parameters: {'sigma': 0.1, 'num_layers': 1, 'embed_size': 120, 'heads': 5, 'forward_expansion': 8, 'prenorm_on': True, 'mlp_scale_classification': 6, 'embedding_dropout': 0, 'decoder_dropout': 0.2, 'class_drop': 0.1, 'learning_rate': 0.01}. Best is trial 22 with value: 0.8453183449231662.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 22. Best value: 0.845318:  34%|███▍      | 34/100 [57:49<1:50:49, 100.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2023-10-25 11:12:33,838] Trial 33 finished with value: 0.9042230156751779 and parameters: {'sigma': 3, 'num_layers': 1, 'embed_size': 120, 'heads': 5, 'forward_expansion': 7, 'prenorm_on': True, 'mlp_scale_classification': 5, 'embedding_dropout': 0, 'decoder_dropout': 0.2, 'class_drop': 0.1, 'learning_rate': 0.01}. Best is trial 22 with value: 0.8453183449231662.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 22. Best value: 0.845318:  35%|███▌      | 35/100 [59:28<1:48:42, 100.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2023-10-25 11:14:13,269] Trial 34 finished with value: 0.9508700095690213 and parameters: {'sigma': 0.1, 'num_layers': 1, 'embed_size': 80, 'heads': 5, 'forward_expansion': 8, 'prenorm_on': True, 'mlp_scale_classification': 6, 'embedding_dropout': 0.5, 'decoder_dropout': 0.2, 'class_drop': 0.1, 'learning_rate': 0.01}. Best is trial 22 with value: 0.8453183449231662.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 22. Best value: 0.845318:  36%|███▌      | 36/100 [1:01:04<1:45:46, 99.16s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2023-10-25 11:15:49,653] Trial 35 finished with value: 0.9646786772287809 and parameters: {'sigma': 1, 'num_layers': 1, 'embed_size': 120, 'heads': 5, 'forward_expansion': 6, 'prenorm_on': False, 'mlp_scale_classification': 5, 'embedding_dropout': 0, 'decoder_dropout': 0.2, 'class_drop': 0.1, 'learning_rate': 0.01}. Best is trial 22 with value: 0.8453183449231662.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 22. Best value: 0.845318:  37%|███▋      | 37/100 [1:02:44<1:44:13, 99.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2023-10-25 11:17:29,153] Trial 36 finished with value: 0.9604285496931809 and parameters: {'sigma': 0.1, 'num_layers': 1, 'embed_size': 120, 'heads': 1, 'forward_expansion': 7, 'prenorm_on': True, 'mlp_scale_classification': 7, 'embedding_dropout': 0, 'decoder_dropout': 0.1, 'class_drop': 0.1, 'learning_rate': 0.001}. Best is trial 22 with value: 0.8453183449231662.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 22. Best value: 0.845318:  38%|███▊      | 38/100 [1:04:21<1:41:50, 98.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2023-10-25 11:19:06,059] Trial 37 finished with value: 0.9659987046168401 and parameters: {'sigma': 3, 'num_layers': 1, 'embed_size': 90, 'heads': 5, 'forward_expansion': 8, 'prenorm_on': True, 'mlp_scale_classification': 4, 'embedding_dropout': 0.1, 'decoder_dropout': 0.2, 'class_drop': 0, 'learning_rate': 0.01}. Best is trial 22 with value: 0.8453183449231662.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 22. Best value: 0.845318:  39%|███▉      | 39/100 [1:05:59<1:39:56, 98.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2023-10-25 11:20:43,766] Trial 38 finished with value: 2.076985405041621 and parameters: {'sigma': 2, 'num_layers': 1, 'embed_size': 140, 'heads': 5, 'forward_expansion': 6, 'prenorm_on': False, 'mlp_scale_classification': 5, 'embedding_dropout': 0.5, 'decoder_dropout': 0.5, 'class_drop': 0.1, 'learning_rate': 0.0001}. Best is trial 22 with value: 0.8453183449231662.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 22. Best value: 0.845318:  40%|████      | 40/100 [1:07:37<1:38:27, 98.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2023-10-25 11:22:22,567] Trial 39 finished with value: 1.1226018529671888 and parameters: {'sigma': 10, 'num_layers': 1, 'embed_size': 160, 'heads': 5, 'forward_expansion': 4, 'prenorm_on': True, 'mlp_scale_classification': 2, 'embedding_dropout': 0, 'decoder_dropout': 0.2, 'class_drop': 0.1, 'learning_rate': 0.001}. Best is trial 22 with value: 0.8453183449231662.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 22. Best value: 0.845318:  41%|████      | 41/100 [1:09:11<1:35:29, 97.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2023-10-25 11:23:56,553] Trial 40 finished with value: 0.9736755994650034 and parameters: {'sigma': 5, 'num_layers': 1, 'embed_size': 80, 'heads': 5, 'forward_expansion': 8, 'prenorm_on': False, 'mlp_scale_classification': 6, 'embedding_dropout': 0.2, 'decoder_dropout': 0.2, 'class_drop': 0.1, 'learning_rate': 0.01}. Best is trial 22 with value: 0.8453183449231662.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 22. Best value: 0.845318:  42%|████▏     | 42/100 [1:10:51<1:34:37, 97.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2023-10-25 11:25:36,225] Trial 41 finished with value: 0.8759275170472952 and parameters: {'sigma': 0.1, 'num_layers': 1, 'embed_size': 120, 'heads': 5, 'forward_expansion': 8, 'prenorm_on': True, 'mlp_scale_classification': 8, 'embedding_dropout': 0, 'decoder_dropout': 0.2, 'class_drop': 0, 'learning_rate': 0.01}. Best is trial 22 with value: 0.8453183449231662.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 22. Best value: 0.845318:  43%|████▎     | 43/100 [1:12:39<1:35:55, 100.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2023-10-25 11:27:24,440] Trial 42 finished with value: 0.8616712414301358 and parameters: {'sigma': 0.1, 'num_layers': 2, 'embed_size': 120, 'heads': 5, 'forward_expansion': 8, 'prenorm_on': True, 'mlp_scale_classification': 7, 'embedding_dropout': 0, 'decoder_dropout': 0.2, 'class_drop': 0.1, 'learning_rate': 0.01}. Best is trial 22 with value: 0.8453183449231662.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 22. Best value: 0.845318:  44%|████▍     | 44/100 [1:14:22<1:34:53, 101.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2023-10-25 11:29:07,723] Trial 43 finished with value: 0.9748206734657288 and parameters: {'sigma': 0.1, 'num_layers': 1, 'embed_size': 120, 'heads': 5, 'forward_expansion': 7, 'prenorm_on': True, 'mlp_scale_classification': 7, 'embedding_dropout': 0, 'decoder_dropout': 0.2, 'class_drop': 0.1, 'learning_rate': 0.01}. Best is trial 22 with value: 0.8453183449231662.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 22. Best value: 0.845318:  45%|████▌     | 45/100 [1:16:13<1:35:36, 104.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2023-10-25 11:30:58,182] Trial 44 finished with value: 0.9692959556212792 and parameters: {'sigma': 0.1, 'num_layers': 2, 'embed_size': 70, 'heads': 5, 'forward_expansion': 8, 'prenorm_on': True, 'mlp_scale_classification': 6, 'embedding_dropout': 0, 'decoder_dropout': 0.5, 'class_drop': 0.1, 'learning_rate': 0.01}. Best is trial 22 with value: 0.8453183449231662.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 22. Best value: 0.845318:  46%|████▌     | 46/100 [1:17:54<1:32:59, 103.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2023-10-25 11:32:39,198] Trial 45 finished with value: 0.8515145549407372 and parameters: {'sigma': 0.1, 'num_layers': 1, 'embed_size': 120, 'heads': 5, 'forward_expansion': 7, 'prenorm_on': True, 'mlp_scale_classification': 8, 'embedding_dropout': 0, 'decoder_dropout': 0.2, 'class_drop': 0.1, 'learning_rate': 0.01}. Best is trial 22 with value: 0.8453183449231662.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 22. Best value: 0.845318:  47%|████▋     | 47/100 [1:19:29<1:29:04, 100.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2023-10-25 11:34:14,251] Trial 46 finished with value: 1.8741620045441847 and parameters: {'sigma': 1, 'num_layers': 1, 'embed_size': 140, 'heads': 1, 'forward_expansion': 7, 'prenorm_on': False, 'mlp_scale_classification': 8, 'embedding_dropout': 0, 'decoder_dropout': 0.1, 'class_drop': 0.1, 'learning_rate': 0.01}. Best is trial 22 with value: 0.8453183449231662.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 47. Best value: 0.823522:  48%|████▊     | 48/100 [1:21:07<1:26:42, 100.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2023-10-25 11:35:52,474] Trial 47 finished with value: 0.823521994627439 and parameters: {'sigma': 0.1, 'num_layers': 1, 'embed_size': 160, 'heads': 5, 'forward_expansion': 6, 'prenorm_on': True, 'mlp_scale_classification': 5, 'embedding_dropout': 0.1, 'decoder_dropout': 0, 'class_drop': 0.1, 'learning_rate': 0.0001}. Best is trial 47 with value: 0.823521994627439.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 47. Best value: 0.823522:  49%|████▉     | 49/100 [1:22:45<1:24:31, 99.43s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2023-10-25 11:37:30,459] Trial 48 finished with value: 0.9937361891453083 and parameters: {'sigma': 2, 'num_layers': 1, 'embed_size': 160, 'heads': 5, 'forward_expansion': 5, 'prenorm_on': True, 'mlp_scale_classification': 6, 'embedding_dropout': 0.1, 'decoder_dropout': 0, 'class_drop': 0, 'learning_rate': 0.0001}. Best is trial 47 with value: 0.823521994627439.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 49. Best value: 0.823486:  50%|█████     | 50/100 [1:24:25<1:22:50, 99.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2023-10-25 11:39:09,798] Trial 49 finished with value: 0.8234860163468581 and parameters: {'sigma': 0.1, 'num_layers': 1, 'embed_size': 160, 'heads': 5, 'forward_expansion': 6, 'prenorm_on': True, 'mlp_scale_classification': 8, 'embedding_dropout': 0.1, 'decoder_dropout': 0, 'class_drop': 0.1, 'learning_rate': 0.0001}. Best is trial 49 with value: 0.8234860163468581.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 49. Best value: 0.823486:  51%|█████     | 51/100 [1:26:01<1:20:25, 98.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2023-10-25 11:40:46,118] Trial 50 finished with value: 0.8931739788789016 and parameters: {'sigma': 3, 'num_layers': 1, 'embed_size': 90, 'heads': 10, 'forward_expansion': 5, 'prenorm_on': True, 'mlp_scale_classification': 5, 'embedding_dropout': 0.1, 'decoder_dropout': 0, 'class_drop': 0.1, 'learning_rate': 0.0001}. Best is trial 49 with value: 0.8234860163468581.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 49. Best value: 0.823486:  52%|█████▏    | 52/100 [1:27:46<1:20:17, 100.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2023-10-25 11:42:30,911] Trial 51 finished with value: 0.8366258923824017 and parameters: {'sigma': 0.1, 'num_layers': 1, 'embed_size': 160, 'heads': 5, 'forward_expansion': 6, 'prenorm_on': True, 'mlp_scale_classification': 8, 'embedding_dropout': 0.1, 'decoder_dropout': 0, 'class_drop': 0.1, 'learning_rate': 0.0001}. Best is trial 49 with value: 0.8234860163468581.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 49. Best value: 0.823486:  53%|█████▎    | 53/100 [1:29:28<1:19:05, 100.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2023-10-25 11:44:13,265] Trial 52 finished with value: 0.8474742265848013 and parameters: {'sigma': 0.1, 'num_layers': 1, 'embed_size': 160, 'heads': 5, 'forward_expansion': 3, 'prenorm_on': True, 'mlp_scale_classification': 8, 'embedding_dropout': 0.1, 'decoder_dropout': 0, 'class_drop': 0.1, 'learning_rate': 0.0001}. Best is trial 49 with value: 0.8234860163468581.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 49. Best value: 0.823486:  54%|█████▍    | 54/100 [1:31:06<1:16:43, 100.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2023-10-25 11:45:51,250] Trial 53 finished with value: 0.8351019070698664 and parameters: {'sigma': 0.1, 'num_layers': 1, 'embed_size': 160, 'heads': 5, 'forward_expansion': 3, 'prenorm_on': True, 'mlp_scale_classification': 8, 'embedding_dropout': 0.1, 'decoder_dropout': 0, 'class_drop': 0.1, 'learning_rate': 0.0001}. Best is trial 49 with value: 0.8234860163468581.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 49. Best value: 0.823486:  55%|█████▌    | 55/100 [1:32:47<1:15:12, 100.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2023-10-25 11:47:31,974] Trial 54 finished with value: 0.8527317322217501 and parameters: {'sigma': 0.1, 'num_layers': 1, 'embed_size': 160, 'heads': 5, 'forward_expansion': 3, 'prenorm_on': True, 'mlp_scale_classification': 8, 'embedding_dropout': 0.1, 'decoder_dropout': 0, 'class_drop': 0.1, 'learning_rate': 0.0001}. Best is trial 49 with value: 0.8234860163468581.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 49. Best value: 0.823486:  56%|█████▌    | 56/100 [1:34:22<1:12:31, 98.90s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2023-10-25 11:49:07,685] Trial 55 finished with value: 1.0072326889404883 and parameters: {'sigma': 0.001, 'num_layers': 1, 'embed_size': 160, 'heads': 5, 'forward_expansion': 4, 'prenorm_on': True, 'mlp_scale_classification': 8, 'embedding_dropout': 0.1, 'decoder_dropout': 0, 'class_drop': 0.1, 'learning_rate': 0.0001}. Best is trial 49 with value: 0.8234860163468581.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 49. Best value: 0.823486:  57%|█████▋    | 57/100 [1:36:03<1:11:12, 99.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2023-10-25 11:50:48,147] Trial 56 finished with value: 0.9224386352759141 and parameters: {'sigma': 0.1, 'num_layers': 1, 'embed_size': 160, 'heads': 5, 'forward_expansion': 6, 'prenorm_on': True, 'mlp_scale_classification': 7, 'embedding_dropout': 0.1, 'decoder_dropout': 0, 'class_drop': 0.5, 'learning_rate': 0.0001}. Best is trial 49 with value: 0.8234860163468581.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 49. Best value: 0.823486:  58%|█████▊    | 58/100 [1:37:40<1:09:08, 98.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2023-10-25 11:52:25,569] Trial 57 finished with value: 0.9765268610073969 and parameters: {'sigma': 10, 'num_layers': 1, 'embed_size': 160, 'heads': 10, 'forward_expansion': 5, 'prenorm_on': False, 'mlp_scale_classification': 8, 'embedding_dropout': 0.1, 'decoder_dropout': 0, 'class_drop': 0.1, 'learning_rate': 0.0001}. Best is trial 49 with value: 0.8234860163468581.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 49. Best value: 0.823486:  59%|█████▉    | 59/100 [1:39:23<1:08:15, 99.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2023-10-25 11:54:08,041] Trial 58 finished with value: 0.8736719764195956 and parameters: {'sigma': 0.1, 'num_layers': 1, 'embed_size': 160, 'heads': 5, 'forward_expansion': 2, 'prenorm_on': True, 'mlp_scale_classification': 7, 'embedding_dropout': 0.1, 'decoder_dropout': 0, 'class_drop': 0.1, 'learning_rate': 0.0001}. Best is trial 49 with value: 0.8234860163468581.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 49. Best value: 0.823486:  60%|██████    | 60/100 [1:40:59<1:05:52, 98.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2023-10-25 11:55:44,346] Trial 59 finished with value: 1.0576425790786743 and parameters: {'sigma': 5, 'num_layers': 1, 'embed_size': 160, 'heads': 1, 'forward_expansion': 2, 'prenorm_on': True, 'mlp_scale_classification': 7, 'embedding_dropout': 0.1, 'decoder_dropout': 0, 'class_drop': 0, 'learning_rate': 0.0001}. Best is trial 49 with value: 0.8234860163468581.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 49. Best value: 0.823486:  61%|██████    | 61/100 [1:42:41<1:04:48, 99.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2023-10-25 11:57:26,168] Trial 60 finished with value: 0.8672275818311251 and parameters: {'sigma': 0.1, 'num_layers': 1, 'embed_size': 60, 'heads': 5, 'forward_expansion': 4, 'prenorm_on': True, 'mlp_scale_classification': 8, 'embedding_dropout': 0.1, 'decoder_dropout': 0, 'class_drop': 0.2, 'learning_rate': 0.0001}. Best is trial 49 with value: 0.8234860163468581.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 49. Best value: 0.823486:  62%|██████▏   | 62/100 [1:44:22<1:03:25, 100.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2023-10-25 11:59:07,338] Trial 61 finished with value: 0.8745468763204721 and parameters: {'sigma': 0.1, 'num_layers': 1, 'embed_size': 160, 'heads': 5, 'forward_expansion': 3, 'prenorm_on': True, 'mlp_scale_classification': 8, 'embedding_dropout': 0.1, 'decoder_dropout': 0, 'class_drop': 0.1, 'learning_rate': 0.0001}. Best is trial 49 with value: 0.8234860163468581.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 49. Best value: 0.823486:  63%|██████▎   | 63/100 [1:46:03<1:01:52, 100.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2023-10-25 12:00:48,092] Trial 62 finished with value: 0.8502797713646522 and parameters: {'sigma': 0.1, 'num_layers': 1, 'embed_size': 160, 'heads': 5, 'forward_expansion': 3, 'prenorm_on': True, 'mlp_scale_classification': 8, 'embedding_dropout': 0.1, 'decoder_dropout': 0, 'class_drop': 0.1, 'learning_rate': 0.0001}. Best is trial 49 with value: 0.8234860163468581.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 49. Best value: 0.823486:  64%|██████▍   | 64/100 [1:47:41<59:47, 99.66s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2023-10-25 12:02:26,166] Trial 63 finished with value: 0.8290713612849896 and parameters: {'sigma': 0.1, 'num_layers': 1, 'embed_size': 160, 'heads': 5, 'forward_expansion': 4, 'prenorm_on': True, 'mlp_scale_classification': 8, 'embedding_dropout': 0.1, 'decoder_dropout': 0, 'class_drop': 0.1, 'learning_rate': 0.0001}. Best is trial 49 with value: 0.8234860163468581.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 64. Best value: 0.818709:  65%|██████▌   | 65/100 [1:49:19<57:48, 99.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2023-10-25 12:04:03,934] Trial 64 finished with value: 0.8187088828820449 and parameters: {'sigma': 0.1, 'num_layers': 1, 'embed_size': 160, 'heads': 5, 'forward_expansion': 4, 'prenorm_on': True, 'mlp_scale_classification': 7, 'embedding_dropout': 0.1, 'decoder_dropout': 0, 'class_drop': 0.1, 'learning_rate': 0.0001}. Best is trial 64 with value: 0.8187088828820449.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 64. Best value: 0.818709:  66%|██████▌   | 66/100 [1:50:59<56:22, 99.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2023-10-25 12:05:44,367] Trial 65 finished with value: 0.8566205960053664 and parameters: {'sigma': 0.1, 'num_layers': 1, 'embed_size': 160, 'heads': 5, 'forward_expansion': 4, 'prenorm_on': True, 'mlp_scale_classification': 8, 'embedding_dropout': 0.1, 'decoder_dropout': 0, 'class_drop': 0.1, 'learning_rate': 0.0001}. Best is trial 64 with value: 0.8187088828820449.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 64. Best value: 0.818709:  67%|██████▋   | 67/100 [1:52:37<54:27, 99.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2023-10-25 12:07:22,318] Trial 66 finished with value: 0.860509624847999 and parameters: {'sigma': 0.1, 'num_layers': 1, 'embed_size': 160, 'heads': 5, 'forward_expansion': 4, 'prenorm_on': True, 'mlp_scale_classification': 7, 'embedding_dropout': 0.1, 'decoder_dropout': 0, 'class_drop': 0.1, 'learning_rate': 0.0001}. Best is trial 64 with value: 0.8187088828820449.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 64. Best value: 0.818709:  68%|██████▊   | 68/100 [1:54:17<52:53, 99.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2023-10-25 12:09:01,844] Trial 67 finished with value: 1.1163101746485784 and parameters: {'sigma': 1, 'num_layers': 1, 'embed_size': 100, 'heads': 5, 'forward_expansion': 5, 'prenorm_on': True, 'mlp_scale_classification': 8, 'embedding_dropout': 0.1, 'decoder_dropout': 0, 'class_drop': 0.5, 'learning_rate': 0.0001}. Best is trial 64 with value: 0.8187088828820449.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 64. Best value: 0.818709:  69%|██████▉   | 69/100 [1:55:58<51:31, 99.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2023-10-25 12:10:42,897] Trial 68 finished with value: 0.8859945077162522 and parameters: {'sigma': 2, 'num_layers': 1, 'embed_size': 160, 'heads': 10, 'forward_expansion': 6, 'prenorm_on': True, 'mlp_scale_classification': 7, 'embedding_dropout': 0.1, 'decoder_dropout': 0, 'class_drop': 0.1, 'learning_rate': 0.0001}. Best is trial 64 with value: 0.8187088828820449.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 64. Best value: 0.818709:  70%|███████   | 70/100 [1:57:37<49:45, 99.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2023-10-25 12:12:21,891] Trial 69 finished with value: 1.0223631171079783 and parameters: {'sigma': 0.001, 'num_layers': 1, 'embed_size': 50, 'heads': 5, 'forward_expansion': 4, 'prenorm_on': True, 'mlp_scale_classification': 8, 'embedding_dropout': 0.1, 'decoder_dropout': 0, 'class_drop': 0.1, 'learning_rate': 0.0001}. Best is trial 64 with value: 0.8187088828820449.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 64. Best value: 0.818709:  71%|███████   | 71/100 [1:59:14<47:49, 98.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2023-10-25 12:13:59,567] Trial 70 finished with value: 0.9007506737342248 and parameters: {'sigma': 0.1, 'num_layers': 1, 'embed_size': 160, 'heads': 5, 'forward_expansion': 3, 'prenorm_on': True, 'mlp_scale_classification': 2, 'embedding_dropout': 0.1, 'decoder_dropout': 0, 'class_drop': 0.2, 'learning_rate': 0.0001}. Best is trial 64 with value: 0.8187088828820449.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 64. Best value: 0.818709:  72%|███████▏  | 72/100 [2:00:53<46:07, 98.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2023-10-25 12:15:38,060] Trial 71 finished with value: 0.8410040094302251 and parameters: {'sigma': 0.1, 'num_layers': 1, 'embed_size': 160, 'heads': 5, 'forward_expansion': 6, 'prenorm_on': True, 'mlp_scale_classification': 7, 'embedding_dropout': 0.1, 'decoder_dropout': 0, 'class_drop': 0.1, 'learning_rate': 0.0001}. Best is trial 64 with value: 0.8187088828820449.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 64. Best value: 0.818709:  73%|███████▎  | 73/100 [2:02:33<44:36, 99.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2023-10-25 12:17:17,963] Trial 72 finished with value: 0.8754743475180405 and parameters: {'sigma': 0.1, 'num_layers': 1, 'embed_size': 160, 'heads': 5, 'forward_expansion': 6, 'prenorm_on': True, 'mlp_scale_classification': 7, 'embedding_dropout': 0.1, 'decoder_dropout': 0, 'class_drop': 0.1, 'learning_rate': 0.0001}. Best is trial 64 with value: 0.8187088828820449.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 73. Best value: 0.815503:  74%|███████▍  | 74/100 [2:04:16<43:27, 100.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2023-10-25 12:19:00,854] Trial 73 finished with value: 0.8155032167067895 and parameters: {'sigma': 0.1, 'num_layers': 1, 'embed_size': 160, 'heads': 5, 'forward_expansion': 6, 'prenorm_on': True, 'mlp_scale_classification': 8, 'embedding_dropout': 0.1, 'decoder_dropout': 0, 'class_drop': 0.1, 'learning_rate': 0.0001}. Best is trial 73 with value: 0.8155032167067895.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 73. Best value: 0.815503:  75%|███████▌  | 75/100 [2:05:57<41:53, 100.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2023-10-25 12:20:42,036] Trial 74 finished with value: 0.8508033248094412 and parameters: {'sigma': 0.1, 'num_layers': 1, 'embed_size': 160, 'heads': 5, 'forward_expansion': 6, 'prenorm_on': True, 'mlp_scale_classification': 8, 'embedding_dropout': 0.1, 'decoder_dropout': 0, 'class_drop': 0.1, 'learning_rate': 0.0001}. Best is trial 73 with value: 0.8155032167067895.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 73. Best value: 0.815503:  76%|███████▌  | 76/100 [2:07:37<40:09, 100.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2023-10-25 12:22:22,022] Trial 75 finished with value: 0.8590043829037592 and parameters: {'sigma': 0.1, 'num_layers': 1, 'embed_size': 160, 'heads': 5, 'forward_expansion': 5, 'prenorm_on': True, 'mlp_scale_classification': 8, 'embedding_dropout': 0.1, 'decoder_dropout': 0, 'class_drop': 0.1, 'learning_rate': 0.0001}. Best is trial 73 with value: 0.8155032167067895.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 73. Best value: 0.815503:  77%|███████▋  | 77/100 [2:09:19<38:37, 100.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2023-10-25 12:24:03,748] Trial 76 finished with value: 0.8251188168158898 and parameters: {'sigma': 0.1, 'num_layers': 1, 'embed_size': 160, 'heads': 5, 'forward_expansion': 6, 'prenorm_on': True, 'mlp_scale_classification': 7, 'embedding_dropout': 0.1, 'decoder_dropout': 0, 'class_drop': 0.1, 'learning_rate': 0.0001}. Best is trial 73 with value: 0.8155032167067895.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 73. Best value: 0.815503:  78%|███████▊  | 78/100 [2:10:54<36:22, 99.22s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2023-10-25 12:25:39,322] Trial 77 finished with value: 0.8675334866230304 and parameters: {'sigma': 0.1, 'num_layers': 1, 'embed_size': 160, 'heads': 1, 'forward_expansion': 5, 'prenorm_on': False, 'mlp_scale_classification': 8, 'embedding_dropout': 0.1, 'decoder_dropout': 0, 'class_drop': 0.1, 'learning_rate': 0.0001}. Best is trial 73 with value: 0.8155032167067895.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 73. Best value: 0.815503:  79%|███████▉  | 79/100 [2:12:34<34:45, 99.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2023-10-25 12:27:18,919] Trial 78 finished with value: 0.9015425122701205 and parameters: {'sigma': 3, 'num_layers': 1, 'embed_size': 160, 'heads': 5, 'forward_expansion': 6, 'prenorm_on': True, 'mlp_scale_classification': 8, 'embedding_dropout': 0.5, 'decoder_dropout': 0, 'class_drop': 0.1, 'learning_rate': 0.0001}. Best is trial 73 with value: 0.8155032167067895.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 73. Best value: 0.815503:  80%|████████  | 80/100 [2:14:15<33:18, 99.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2023-10-25 12:29:00,279] Trial 79 finished with value: 2.40680899986854 and parameters: {'sigma': 10, 'num_layers': 1, 'embed_size': 80, 'heads': 5, 'forward_expansion': 6, 'prenorm_on': True, 'mlp_scale_classification': 7, 'embedding_dropout': 0.2, 'decoder_dropout': 0.5, 'class_drop': 0.5, 'learning_rate': 0.0001}. Best is trial 73 with value: 0.8155032167067895.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 73. Best value: 0.815503:  81%|████████  | 81/100 [2:15:56<31:42, 100.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2023-10-25 12:30:40,897] Trial 80 finished with value: 0.9259136365010188 and parameters: {'sigma': 5, 'num_layers': 1, 'embed_size': 70, 'heads': 5, 'forward_expansion': 5, 'prenorm_on': True, 'mlp_scale_classification': 8, 'embedding_dropout': 0.1, 'decoder_dropout': 0, 'class_drop': 0.1, 'learning_rate': 0.0001}. Best is trial 73 with value: 0.8155032167067895.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 73. Best value: 0.815503:  82%|████████▏ | 82/100 [2:17:33<29:49, 99.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2023-10-25 12:32:18,607] Trial 81 finished with value: 0.8162475961905259 and parameters: {'sigma': 0.1, 'num_layers': 1, 'embed_size': 160, 'heads': 5, 'forward_expansion': 6, 'prenorm_on': True, 'mlp_scale_classification': 7, 'embedding_dropout': 0.1, 'decoder_dropout': 0, 'class_drop': 0.1, 'learning_rate': 0.0001}. Best is trial 73 with value: 0.8155032167067895.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 73. Best value: 0.815503:  83%|████████▎ | 83/100 [2:19:18<28:39, 101.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2023-10-25 12:34:03,708] Trial 82 finished with value: 0.8181197184782761 and parameters: {'sigma': 0.1, 'num_layers': 1, 'embed_size': 160, 'heads': 5, 'forward_expansion': 6, 'prenorm_on': True, 'mlp_scale_classification': 7, 'embedding_dropout': 0.1, 'decoder_dropout': 0, 'class_drop': 0.1, 'learning_rate': 0.0001}. Best is trial 73 with value: 0.8155032167067895.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 73. Best value: 0.815503:  84%|████████▍ | 84/100 [2:21:00<27:02, 101.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2023-10-25 12:35:45,735] Trial 83 finished with value: 0.8296414567874029 and parameters: {'sigma': 0.1, 'num_layers': 1, 'embed_size': 160, 'heads': 5, 'forward_expansion': 6, 'prenorm_on': True, 'mlp_scale_classification': 7, 'embedding_dropout': 0.1, 'decoder_dropout': 0, 'class_drop': 0.1, 'learning_rate': 0.0001}. Best is trial 73 with value: 0.8155032167067895.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 73. Best value: 0.815503:  85%|████████▌ | 85/100 [2:22:42<25:23, 101.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2023-10-25 12:37:27,634] Trial 84 finished with value: 0.881424761735476 and parameters: {'sigma': 0.1, 'num_layers': 1, 'embed_size': 60, 'heads': 5, 'forward_expansion': 6, 'prenorm_on': True, 'mlp_scale_classification': 7, 'embedding_dropout': 0.1, 'decoder_dropout': 0, 'class_drop': 0.1, 'learning_rate': 0.0001}. Best is trial 73 with value: 0.8155032167067895.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 73. Best value: 0.815503:  86%|████████▌ | 86/100 [2:24:22<23:32, 100.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2023-10-25 12:39:07,004] Trial 85 finished with value: 0.926463260100438 and parameters: {'sigma': 0.1, 'num_layers': 1, 'embed_size': 160, 'heads': 5, 'forward_expansion': 6, 'prenorm_on': True, 'mlp_scale_classification': 6, 'embedding_dropout': 0.1, 'decoder_dropout': 0.1, 'class_drop': 0.1, 'learning_rate': 0.001}. Best is trial 73 with value: 0.8155032167067895.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 73. Best value: 0.815503:  87%|████████▋ | 87/100 [2:26:03<21:53, 101.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2023-10-25 12:40:48,340] Trial 86 finished with value: 0.8364755098636334 and parameters: {'sigma': 0.1, 'num_layers': 1, 'embed_size': 160, 'heads': 5, 'forward_expansion': 6, 'prenorm_on': True, 'mlp_scale_classification': 7, 'embedding_dropout': 0.1, 'decoder_dropout': 0, 'class_drop': 0, 'learning_rate': 0.0001}. Best is trial 73 with value: 0.8155032167067895.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 73. Best value: 0.815503:  88%|████████▊ | 88/100 [2:27:44<20:13, 101.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2023-10-25 12:42:29,721] Trial 87 finished with value: 0.8797981830743643 and parameters: {'sigma': 0.1, 'num_layers': 1, 'embed_size': 140, 'heads': 5, 'forward_expansion': 5, 'prenorm_on': True, 'mlp_scale_classification': 7, 'embedding_dropout': 0.1, 'decoder_dropout': 0, 'class_drop': 0.1, 'learning_rate': 0.0001}. Best is trial 73 with value: 0.8155032167067895.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 73. Best value: 0.815503:  89%|████████▉ | 89/100 [2:29:25<18:31, 101.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2023-10-25 12:44:10,489] Trial 88 finished with value: 0.8360070815453162 and parameters: {'sigma': 0.1, 'num_layers': 1, 'embed_size': 100, 'heads': 10, 'forward_expansion': 6, 'prenorm_on': True, 'mlp_scale_classification': 6, 'embedding_dropout': 0.1, 'decoder_dropout': 0, 'class_drop': 0.1, 'learning_rate': 0.0001}. Best is trial 73 with value: 0.8155032167067895.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 73. Best value: 0.815503:  90%|█████████ | 90/100 [2:31:02<16:35, 99.60s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2023-10-25 12:45:46,754] Trial 89 finished with value: 0.8999874500127939 and parameters: {'sigma': 1, 'num_layers': 1, 'embed_size': 50, 'heads': 5, 'forward_expansion': 7, 'prenorm_on': False, 'mlp_scale_classification': 7, 'embedding_dropout': 0.2, 'decoder_dropout': 0, 'class_drop': 0.2, 'learning_rate': 0.0001}. Best is trial 73 with value: 0.8155032167067895.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 73. Best value: 0.815503:  91%|█████████ | 91/100 [2:32:45<15:07, 100.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2023-10-25 12:47:30,337] Trial 90 finished with value: 2.23731882755573 and parameters: {'sigma': 0.1, 'num_layers': 1, 'embed_size': 160, 'heads': 5, 'forward_expansion': 6, 'prenorm_on': True, 'mlp_scale_classification': 7, 'embedding_dropout': 0.5, 'decoder_dropout': 0.5, 'class_drop': 0.1, 'learning_rate': 0.0001}. Best is trial 73 with value: 0.8155032167067895.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 73. Best value: 0.815503:  92%|█████████▏| 92/100 [2:34:26<13:27, 100.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2023-10-25 12:49:11,476] Trial 91 finished with value: 0.8506800073843735 and parameters: {'sigma': 0.1, 'num_layers': 1, 'embed_size': 160, 'heads': 5, 'forward_expansion': 4, 'prenorm_on': True, 'mlp_scale_classification': 7, 'embedding_dropout': 0.1, 'decoder_dropout': 0, 'class_drop': 0.1, 'learning_rate': 0.0001}. Best is trial 73 with value: 0.8155032167067895.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 73. Best value: 0.815503:  93%|█████████▎| 93/100 [2:36:08<11:49, 101.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2023-10-25 12:50:53,699] Trial 92 finished with value: 0.8244591767971332 and parameters: {'sigma': 0.1, 'num_layers': 1, 'embed_size': 160, 'heads': 5, 'forward_expansion': 6, 'prenorm_on': True, 'mlp_scale_classification': 7, 'embedding_dropout': 0.1, 'decoder_dropout': 0, 'class_drop': 0.1, 'learning_rate': 0.0001}. Best is trial 73 with value: 0.8155032167067895.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 73. Best value: 0.815503:  94%|█████████▍| 94/100 [2:37:47<10:02, 100.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2023-10-25 12:52:32,267] Trial 93 finished with value: 0.869461348423591 and parameters: {'sigma': 0.1, 'num_layers': 1, 'embed_size': 160, 'heads': 5, 'forward_expansion': 6, 'prenorm_on': True, 'mlp_scale_classification': 6, 'embedding_dropout': 0.1, 'decoder_dropout': 0, 'class_drop': 0.1, 'learning_rate': 0.0001}. Best is trial 73 with value: 0.8155032167067895.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 73. Best value: 0.815503:  95%|█████████▌| 95/100 [2:39:27<08:21, 100.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2023-10-25 12:54:11,850] Trial 94 finished with value: 0.8434383364824148 and parameters: {'sigma': 0.1, 'num_layers': 1, 'embed_size': 160, 'heads': 5, 'forward_expansion': 5, 'prenorm_on': True, 'mlp_scale_classification': 7, 'embedding_dropout': 0.1, 'decoder_dropout': 0, 'class_drop': 0.1, 'learning_rate': 0.0001}. Best is trial 73 with value: 0.8155032167067895.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 73. Best value: 0.815503:  96%|█████████▌| 96/100 [2:41:11<06:45, 101.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2023-10-25 12:55:55,883] Trial 95 finished with value: 0.908696330510653 and parameters: {'sigma': 2, 'num_layers': 1, 'embed_size': 90, 'heads': 5, 'forward_expansion': 7, 'prenorm_on': True, 'mlp_scale_classification': 6, 'embedding_dropout': 0.1, 'decoder_dropout': 0, 'class_drop': 0.1, 'learning_rate': 0.001}. Best is trial 73 with value: 0.8155032167067895.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 73. Best value: 0.815503:  97%|█████████▋| 97/100 [2:42:53<05:04, 101.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2023-10-25 12:57:38,274] Trial 96 finished with value: 0.9233697102620051 and parameters: {'sigma': 0.001, 'num_layers': 1, 'embed_size': 160, 'heads': 5, 'forward_expansion': 6, 'prenorm_on': True, 'mlp_scale_classification': 7, 'embedding_dropout': 0.1, 'decoder_dropout': 0, 'class_drop': 0.1, 'learning_rate': 0.0001}. Best is trial 73 with value: 0.8155032167067895.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 73. Best value: 0.815503:  98%|█████████▊| 98/100 [2:44:32<03:21, 100.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2023-10-25 12:59:16,971] Trial 97 finished with value: 0.9997563866468576 and parameters: {'sigma': 0.1, 'num_layers': 1, 'embed_size': 160, 'heads': 1, 'forward_expansion': 6, 'prenorm_on': True, 'mlp_scale_classification': 7, 'embedding_dropout': 0.1, 'decoder_dropout': 0.1, 'class_drop': 0.1, 'learning_rate': 0.0001}. Best is trial 73 with value: 0.8155032167067895.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 73. Best value: 0.815503:  99%|█████████▉| 99/100 [2:46:12<01:40, 100.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2023-10-25 13:00:57,688] Trial 98 finished with value: 0.8172530761131873 and parameters: {'sigma': 0.1, 'num_layers': 1, 'embed_size': 160, 'heads': 5, 'forward_expansion': 7, 'prenorm_on': True, 'mlp_scale_classification': 6, 'embedding_dropout': 0.1, 'decoder_dropout': 0, 'class_drop': 0.1, 'learning_rate': 0.0001}. Best is trial 73 with value: 0.8155032167067895.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 73. Best value: 0.815503: 100%|██████████| 100/100 [2:47:49<00:00, 100.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2023-10-25 13:02:33,818] Trial 99 finished with value: 0.9102359826748188 and parameters: {'sigma': 3, 'num_layers': 1, 'embed_size': 70, 'heads': 5, 'forward_expansion': 7, 'prenorm_on': True, 'mlp_scale_classification': 6, 'embedding_dropout': 0.1, 'decoder_dropout': 0, 'class_drop': 0.1, 'learning_rate': 0.0001}. Best is trial 73 with value: 0.8155032167067895.\n",
      "Best Hyperparameters: {'sigma': 0.1, 'num_layers': 1, 'embed_size': 160, 'heads': 5, 'forward_expansion': 6, 'prenorm_on': True, 'mlp_scale_classification': 8, 'embedding_dropout': 0.1, 'decoder_dropout': 0, 'class_drop': 0.1, 'learning_rate': 0.0001}\n",
      "Best Validation RMSE (at Early Stopping): 0.8155032167067895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Set the number of optimization trials\n",
    "num_trials = 100\n",
    "\n",
    "# Create an Optuna study\n",
    "study = optuna.create_study(direction='minimize')  # Maximize validation accuracy\n",
    "\n",
    "# Start the optimization process\n",
    "study.optimize(objective, n_trials=num_trials, show_progress_bar=True)\n",
    "\n",
    "# Get the best hyperparameters and the validation accuracy at the point of early stopping\n",
    "best_params = study.best_params\n",
    "best_val_rmse = study.best_value\n",
    "\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "print(\"Best Validation RMSE (at Early Stopping):\", best_val_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [ 1/75]        | Train: Loss 4.2606, R2 -0.2035, RMSE 2.0229                       | Test: Loss 2.8355, R2 0.2110, RMSE 1.6766\n",
      "Epoch [ 2/75]        | Train: Loss 1.7403, R2 0.5148, RMSE 1.3032                        | Test: Loss 1.0974, R2 0.7017, RMSE 1.0347\n",
      "Epoch [ 3/75]        | Train: Loss 1.1830, R2 0.6705, RMSE 1.0836                        | Test: Loss 1.0229, R2 0.7089, RMSE 1.0049\n",
      "Epoch [ 4/75]        | Train: Loss 1.0712, R2 0.7003, RMSE 1.0303                        | Test: Loss 1.0608, R2 0.7231, RMSE 1.0247\n",
      "Epoch [ 5/75]        | Train: Loss 1.0355, R2 0.7122, RMSE 1.0134                        | Test: Loss 0.8938, R2 0.7495, RMSE 0.9386\n",
      "Epoch [ 6/75]        | Train: Loss 1.0010, R2 0.7181, RMSE 0.9959                        | Test: Loss 0.9026, R2 0.7598, RMSE 0.9451\n",
      "Epoch [ 7/75]        | Train: Loss 0.9662, R2 0.7291, RMSE 0.9791                        | Test: Loss 0.8788, R2 0.7648, RMSE 0.9315\n",
      "Epoch [ 8/75]        | Train: Loss 0.9486, R2 0.7324, RMSE 0.9705                        | Test: Loss 0.8552, R2 0.7668, RMSE 0.9165\n",
      "Epoch [ 9/75]        | Train: Loss 0.9430, R2 0.7349, RMSE 0.9686                        | Test: Loss 0.8856, R2 0.7607, RMSE 0.9378\n",
      "Epoch [10/75]        | Train: Loss 0.9634, R2 0.7276, RMSE 0.9777                        | Test: Loss 0.8736, R2 0.7661, RMSE 0.9310\n",
      "Epoch [11/75]        | Train: Loss 0.9492, R2 0.7360, RMSE 0.9710                        | Test: Loss 0.8691, R2 0.7734, RMSE 0.9289\n",
      "Epoch [12/75]        | Train: Loss 0.9338, R2 0.7371, RMSE 0.9628                        | Test: Loss 0.8902, R2 0.7763, RMSE 0.9335\n",
      "Epoch [13/75]        | Train: Loss 0.9422, R2 0.7374, RMSE 0.9664                        | Test: Loss 0.8581, R2 0.7557, RMSE 0.9213\n",
      "Epoch [14/75]        | Train: Loss 0.9177, R2 0.7416, RMSE 0.9539                        | Test: Loss 0.8676, R2 0.7573, RMSE 0.9282\n",
      "Epoch [15/75]        | Train: Loss 0.9071, R2 0.7477, RMSE 0.9491                        | Test: Loss 0.8251, R2 0.7786, RMSE 0.9017\n",
      "Epoch [16/75]        | Train: Loss 0.8981, R2 0.7461, RMSE 0.9446                        | Test: Loss 0.8437, R2 0.7531, RMSE 0.9122\n",
      "Epoch [17/75]        | Train: Loss 0.9177, R2 0.7436, RMSE 0.9554                        | Test: Loss 0.7907, R2 0.7764, RMSE 0.8857\n",
      "Epoch [18/75]        | Train: Loss 0.9014, R2 0.7478, RMSE 0.9453                        | Test: Loss 0.8344, R2 0.7792, RMSE 0.9090\n",
      "Epoch [19/75]        | Train: Loss 0.8915, R2 0.7510, RMSE 0.9391                        | Test: Loss 0.8922, R2 0.7579, RMSE 0.9393\n",
      "Epoch [20/75]        | Train: Loss 0.8858, R2 0.7535, RMSE 0.9369                        | Test: Loss 0.8384, R2 0.7820, RMSE 0.9100\n",
      "Epoch [21/75]        | Train: Loss 0.8936, R2 0.7489, RMSE 0.9414                        | Test: Loss 0.7799, R2 0.7582, RMSE 0.8783\n",
      "Epoch [22/75]        | Train: Loss 0.8628, R2 0.7587, RMSE 0.9257                        | Test: Loss 0.7674, R2 0.7815, RMSE 0.8719\n",
      "Epoch [23/75]        | Train: Loss 0.8822, R2 0.7555, RMSE 0.9354                        | Test: Loss 0.7691, R2 0.7951, RMSE 0.8715\n",
      "Epoch [24/75]        | Train: Loss 0.8771, R2 0.7565, RMSE 0.9316                        | Test: Loss 0.8430, R2 0.7583, RMSE 0.9114\n",
      "Epoch [25/75]        | Train: Loss 0.8682, R2 0.7579, RMSE 0.9268                        | Test: Loss 0.8087, R2 0.7683, RMSE 0.8954\n",
      "Epoch [26/75]        | Train: Loss 0.8710, R2 0.7549, RMSE 0.9300                        | Test: Loss 0.7964, R2 0.7892, RMSE 0.8889\n",
      "Epoch [27/75]        | Train: Loss 0.8566, R2 0.7616, RMSE 0.9216                        | Test: Loss 0.7594, R2 0.7928, RMSE 0.8666\n",
      "Epoch [28/75]        | Train: Loss 0.8327, R2 0.7661, RMSE 0.9095                        | Test: Loss 0.8040, R2 0.7759, RMSE 0.8935\n",
      "Epoch [29/75]        | Train: Loss 0.8645, R2 0.7584, RMSE 0.9262                        | Test: Loss 0.7984, R2 0.7856, RMSE 0.8902\n",
      "Epoch [30/75]        | Train: Loss 0.8346, R2 0.7678, RMSE 0.9087                        | Test: Loss 0.8947, R2 0.6973, RMSE 0.9317\n",
      "Epoch [31/75]        | Train: Loss 0.8437, R2 0.7619, RMSE 0.9149                        | Test: Loss 0.7886, R2 0.7867, RMSE 0.8822\n",
      "Epoch [32/75]        | Train: Loss 0.8556, R2 0.7616, RMSE 0.9202                        | Test: Loss 0.7926, R2 0.7944, RMSE 0.8832\n",
      "Epoch [33/75]        | Train: Loss 0.8305, R2 0.7677, RMSE 0.9072                        | Test: Loss 0.7571, R2 0.7928, RMSE 0.8651\n",
      "Epoch [34/75]        | Train: Loss 0.8332, R2 0.7668, RMSE 0.9089                        | Test: Loss 0.7971, R2 0.7821, RMSE 0.8905\n",
      "Epoch [35/75]        | Train: Loss 0.8279, R2 0.7672, RMSE 0.9059                        | Test: Loss 0.8126, R2 0.7746, RMSE 0.8974\n",
      "Epoch [36/75]        | Train: Loss 0.8193, R2 0.7718, RMSE 0.9011                        | Test: Loss 0.7654, R2 0.7934, RMSE 0.8729\n",
      "Epoch [37/75]        | Train: Loss 0.8303, R2 0.7669, RMSE 0.9082                        | Test: Loss 0.8009, R2 0.7849, RMSE 0.8924\n",
      "Epoch [38/75]        | Train: Loss 0.8236, R2 0.7704, RMSE 0.9028                        | Test: Loss 0.8476, R2 0.7562, RMSE 0.9178\n",
      "Epoch [39/75]        | Train: Loss 0.8200, R2 0.7692, RMSE 0.9034                        | Test: Loss 0.7382, R2 0.7945, RMSE 0.8549\n",
      "Epoch [40/75]        | Train: Loss 0.8172, R2 0.7705, RMSE 0.9008                        | Test: Loss 0.8356, R2 0.7654, RMSE 0.9029\n",
      "Epoch [41/75]        | Train: Loss 0.8308, R2 0.7688, RMSE 0.9068                        | Test: Loss 0.8953, R2 0.7359, RMSE 0.9442\n",
      "Epoch [42/75]        | Train: Loss 0.8045, R2 0.7723, RMSE 0.8928                        | Test: Loss 0.7257, R2 0.7974, RMSE 0.8480\n",
      "Epoch [43/75]        | Train: Loss 0.8106, R2 0.7747, RMSE 0.8962                        | Test: Loss 0.7533, R2 0.7754, RMSE 0.8650\n",
      "Epoch [44/75]        | Train: Loss 0.8316, R2 0.7677, RMSE 0.9090                        | Test: Loss 0.7442, R2 0.7956, RMSE 0.8587\n",
      "Epoch [45/75]        | Train: Loss 0.7995, R2 0.7767, RMSE 0.8891                        | Test: Loss 0.8693, R2 0.7694, RMSE 0.9291\n",
      "Epoch [46/75]        | Train: Loss 0.8011, R2 0.7740, RMSE 0.8918                        | Test: Loss 0.7789, R2 0.7882, RMSE 0.8794\n",
      "Epoch [47/75]        | Train: Loss 0.8106, R2 0.7734, RMSE 0.8972                        | Test: Loss 0.7355, R2 0.8008, RMSE 0.8523\n",
      "Epoch [48/75]        | Train: Loss 0.8203, R2 0.7684, RMSE 0.9014                        | Test: Loss 0.7593, R2 0.7855, RMSE 0.8686\n",
      "Epoch [49/75]        | Train: Loss 0.7829, R2 0.7804, RMSE 0.8819                        | Test: Loss 0.7640, R2 0.7785, RMSE 0.8701\n",
      "Epoch [50/75]        | Train: Loss 0.7775, R2 0.7827, RMSE 0.8788                        | Test: Loss 0.7610, R2 0.7916, RMSE 0.8643\n",
      "Epoch [51/75]        | Train: Loss 0.7838, R2 0.7814, RMSE 0.8814                        | Test: Loss 0.7213, R2 0.7970, RMSE 0.8400\n",
      "Epoch [52/75]        | Train: Loss 0.7746, R2 0.7824, RMSE 0.8769                        | Test: Loss 0.7330, R2 0.8010, RMSE 0.8521\n",
      "Epoch [53/75]        | Train: Loss 0.7803, R2 0.7821, RMSE 0.8795                        | Test: Loss 0.7437, R2 0.7909, RMSE 0.8564\n",
      "Epoch [54/75]        | Train: Loss 0.7795, R2 0.7828, RMSE 0.8779                        | Test: Loss 0.7789, R2 0.7859, RMSE 0.8748\n",
      "Epoch [55/75]        | Train: Loss 0.7677, R2 0.7840, RMSE 0.8733                        | Test: Loss 0.7292, R2 0.7969, RMSE 0.8478\n",
      "Epoch [56/75]        | Train: Loss 0.7818, R2 0.7816, RMSE 0.8797                        | Test: Loss 0.8128, R2 0.7827, RMSE 0.8964\n",
      "Epoch [57/75]        | Train: Loss 0.7721, R2 0.7836, RMSE 0.8758                        | Test: Loss 0.7459, R2 0.7916, RMSE 0.8581\n",
      "Epoch [58/75]        | Train: Loss 0.7699, R2 0.7843, RMSE 0.8737                        | Test: Loss 0.7394, R2 0.7958, RMSE 0.8522\n",
      "Epoch [59/75]        | Train: Loss 0.7755, R2 0.7835, RMSE 0.8766                        | Test: Loss 0.7306, R2 0.7949, RMSE 0.8505\n",
      "Epoch [60/75]        | Train: Loss 0.7675, R2 0.7839, RMSE 0.8724                        | Test: Loss 0.8753, R2 0.7469, RMSE 0.9320\n",
      "Epoch [61/75]        | Train: Loss 0.7579, R2 0.7866, RMSE 0.8674                        | Test: Loss 0.8777, R2 0.7165, RMSE 0.9084\n",
      "Epoch [62/75]        | Train: Loss 0.7890, R2 0.7778, RMSE 0.8845                        | Test: Loss 0.7280, R2 0.7926, RMSE 0.8475\n",
      "Epoch [63/75]        | Train: Loss 0.7570, R2 0.7879, RMSE 0.8660                        | Test: Loss 0.7577, R2 0.7899, RMSE 0.8624\n",
      "Epoch [64/75]        | Train: Loss 0.7512, R2 0.7889, RMSE 0.8629                        | Test: Loss 0.8003, R2 0.7901, RMSE 0.8906\n",
      "Epoch [65/75]        | Train: Loss 0.7705, R2 0.7831, RMSE 0.8738                        | Test: Loss 0.7638, R2 0.7893, RMSE 0.8671\n",
      "Epoch [66/75]        | Train: Loss 0.7795, R2 0.7835, RMSE 0.8793                        | Test: Loss 0.7437, R2 0.7937, RMSE 0.8571\n",
      "Epoch [67/75]        | Train: Loss 0.7706, R2 0.7842, RMSE 0.8749                        | Test: Loss 0.7878, R2 0.8024, RMSE 0.8799\n",
      "Epoch [68/75]        | Train: Loss 0.7664, R2 0.7868, RMSE 0.8716                        | Test: Loss 0.7761, R2 0.7826, RMSE 0.8763\n",
      "Epoch [69/75]        | Train: Loss 0.7349, R2 0.7944, RMSE 0.8542                        | Test: Loss 0.7163, R2 0.7958, RMSE 0.8384\n",
      "Epoch [70/75]        | Train: Loss 0.7403, R2 0.7924, RMSE 0.8576                        | Test: Loss 0.7802, R2 0.7811, RMSE 0.8749\n",
      "Epoch [71/75]        | Train: Loss 0.7430, R2 0.7917, RMSE 0.8587                        | Test: Loss 0.7214, R2 0.8041, RMSE 0.8424\n",
      "Epoch [72/75]        | Train: Loss 0.7396, R2 0.7944, RMSE 0.8560                        | Test: Loss 0.8351, R2 0.7877, RMSE 0.9065\n",
      "Epoch [73/75]        | Train: Loss 0.7270, R2 0.7951, RMSE 0.8495                        | Test: Loss 0.7719, R2 0.8007, RMSE 0.8710\n",
      "Epoch [74/75]        | Train: Loss 0.7384, R2 0.7937, RMSE 0.8565                        | Test: Loss 0.7909, R2 0.7764, RMSE 0.8806\n",
      "Epoch [75/75]        | Train: Loss 0.7597, R2 0.7858, RMSE 0.8678                        | Test: Loss 0.7581, R2 0.7682, RMSE 0.8656\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f0b4767b250>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Testing against the test dataset\n",
    "\n",
    "model = Classifier(n_features=len(cont_columns),\n",
    "                   targets_classes=[0],\n",
    "                   rff_on=True,\n",
    "                   sigma=best_params['sigma'],\n",
    "                   embed_size=best_params['embed_size'],\n",
    "                   num_layers=best_params['num_layers'],\n",
    "                   heads=best_params['heads'],\n",
    "                   forward_expansion=best_params['forward_expansion'],\n",
    "                   pre_norm_on=best_params['prenorm_on'],\n",
    "                   mlp_scale_classification=best_params['mlp_scale_classification'],\n",
    "                   embedding_dropout=best_params['embedding_dropout'],\n",
    "                   decoder_dropout=best_params['decoder_dropout'],\n",
    "                   classification_dropout=best_params['class_drop']\n",
    "                   ).to(device_in_use) # Instantiate the model\n",
    "loss_functions = UncertaintyLoss(1)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr = best_params['learning_rate']) # Maybe try messing around with optimizers. try other torch optimizers with different configurations.\n",
    "epochs = 75 #Set the number of epochs\n",
    "\n",
    "train_losses = []\n",
    "train_accuracies_1 = [] \n",
    "train_accuracies_2 = []\n",
    "train_recalls = [] \n",
    "train_f1_scores = [] \n",
    "test_losses = []\n",
    "test_accuracies_1 = []\n",
    "test_accuracies_2 = []\n",
    "test_recalls = []  \n",
    "test_f1_scores = [] \n",
    "all_attention_scores = []\n",
    "\n",
    "for t in range(epochs):\n",
    "  train_loss, r2_train, rmse_train = train(train_dataloader, model, loss_functions, optimizer, device_in_use=device_in_use)\n",
    "  test_loss, r2_test, rmse_test = test(test_dataloader, model, loss_functions, device_in_use=device_in_use)\n",
    "  train_losses.append(train_loss)\n",
    "\n",
    "  # train_accuracies_2.append(train_accuracy_2)\n",
    "  # train_recalls.append(train_recall) \n",
    "  # train_f1_scores.append(train_f1)\n",
    "  test_losses.append(test_loss)\n",
    "\n",
    "  # test_accuracies_2.append(test_accuracy_2)\n",
    "  # test_recalls.append(test_recall)\n",
    "  # test_f1_scores.append(test_f1)\n",
    "  # Formatting for easier reading\n",
    "  epoch_str = f\"Epoch [{t+1:2}/{epochs}]\"\n",
    "  train_metrics = f\"Train: Loss {format_metric(train_loss)}, R2 {format_metric(r2_train)}, RMSE {format_metric(rmse_train)}\"\n",
    "  test_metrics = f\"Test: Loss {format_metric(test_loss)}, R2 {format_metric(r2_test)}, RMSE {format_metric(rmse_test)}\"\n",
    "  print(f\"{epoch_str:20} | {train_metrics:65} | {test_metrics}\")\n",
    "\n",
    "\n",
    "torch.save(model.state_dict(), 'final_model_trained.pth')\n",
    "\n",
    "# Plotting the loss curves\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 1, 1)\n",
    "plt.plot(range(1, epochs+1), train_losses, label='Train Loss')\n",
    "plt.plot(range(1, epochs+1), [l for l in test_losses], label='Test Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Test Loss Curve')\n",
    "plt.legend()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
