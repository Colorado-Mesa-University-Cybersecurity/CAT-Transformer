{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\smbm2\\AppData\\Local\\miniconda3\\envs\\ml-env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from rff.layers import GaussianEncoding #pip install random-fourier-features-pytorch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import os\n",
    "import optuna\n",
    "from optuna.trial import TrialState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available and being used\n"
     ]
    }
   ],
   "source": [
    "# Run regardless if you do or do not have GPU so all tensors are moved to right location later on\n",
    "if torch.cuda.is_available():\n",
    "    device_in_use = torch.device(\"cuda\")\n",
    "    print(\"GPU is available and being used\")\n",
    "else:\n",
    "    device_in_use = torch.device(\"cpu\")\n",
    "    print(\"GPU is not available, using CPU instead\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD AND PROCESS DATA\n",
    "**EXAMPLE WITH COVTYPE DATASET**\n",
    "1. Standardize or perform quantile transformations to numerical/continuous features.\n",
    "1. Wrap with Dataset and Dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedInc</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>AveRooms</th>\n",
       "      <th>AveBedrms</th>\n",
       "      <th>Population</th>\n",
       "      <th>AveOccup</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.8631</td>\n",
       "      <td>20.0</td>\n",
       "      <td>4.401210</td>\n",
       "      <td>1.076613</td>\n",
       "      <td>999.0</td>\n",
       "      <td>2.014113</td>\n",
       "      <td>32.79</td>\n",
       "      <td>-117.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.2026</td>\n",
       "      <td>24.0</td>\n",
       "      <td>5.617544</td>\n",
       "      <td>0.989474</td>\n",
       "      <td>731.0</td>\n",
       "      <td>2.564912</td>\n",
       "      <td>34.59</td>\n",
       "      <td>-120.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.1094</td>\n",
       "      <td>14.0</td>\n",
       "      <td>5.869565</td>\n",
       "      <td>1.094203</td>\n",
       "      <td>302.0</td>\n",
       "      <td>2.188406</td>\n",
       "      <td>39.26</td>\n",
       "      <td>-121.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.3068</td>\n",
       "      <td>52.0</td>\n",
       "      <td>4.801205</td>\n",
       "      <td>1.066265</td>\n",
       "      <td>1526.0</td>\n",
       "      <td>2.298193</td>\n",
       "      <td>37.77</td>\n",
       "      <td>-122.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0791</td>\n",
       "      <td>11.0</td>\n",
       "      <td>5.878902</td>\n",
       "      <td>1.098493</td>\n",
       "      <td>4773.0</td>\n",
       "      <td>2.568891</td>\n",
       "      <td>33.17</td>\n",
       "      <td>-117.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14442</th>\n",
       "      <td>6.3700</td>\n",
       "      <td>35.0</td>\n",
       "      <td>6.129032</td>\n",
       "      <td>0.926267</td>\n",
       "      <td>658.0</td>\n",
       "      <td>3.032258</td>\n",
       "      <td>33.78</td>\n",
       "      <td>-117.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14443</th>\n",
       "      <td>3.0500</td>\n",
       "      <td>33.0</td>\n",
       "      <td>6.868597</td>\n",
       "      <td>1.269488</td>\n",
       "      <td>1753.0</td>\n",
       "      <td>3.904232</td>\n",
       "      <td>34.02</td>\n",
       "      <td>-117.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14444</th>\n",
       "      <td>2.9344</td>\n",
       "      <td>36.0</td>\n",
       "      <td>3.986717</td>\n",
       "      <td>1.079696</td>\n",
       "      <td>1756.0</td>\n",
       "      <td>3.332068</td>\n",
       "      <td>34.03</td>\n",
       "      <td>-118.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14445</th>\n",
       "      <td>5.7192</td>\n",
       "      <td>15.0</td>\n",
       "      <td>6.395349</td>\n",
       "      <td>1.067979</td>\n",
       "      <td>1777.0</td>\n",
       "      <td>3.178891</td>\n",
       "      <td>37.58</td>\n",
       "      <td>-121.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14446</th>\n",
       "      <td>2.5755</td>\n",
       "      <td>52.0</td>\n",
       "      <td>3.402576</td>\n",
       "      <td>1.058776</td>\n",
       "      <td>2619.0</td>\n",
       "      <td>2.108696</td>\n",
       "      <td>37.77</td>\n",
       "      <td>-122.42</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14447 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
       "0      2.8631      20.0  4.401210   1.076613       999.0  2.014113     32.79   \n",
       "1      4.2026      24.0  5.617544   0.989474       731.0  2.564912     34.59   \n",
       "2      3.1094      14.0  5.869565   1.094203       302.0  2.188406     39.26   \n",
       "3      3.3068      52.0  4.801205   1.066265      1526.0  2.298193     37.77   \n",
       "4      4.0791      11.0  5.878902   1.098493      4773.0  2.568891     33.17   \n",
       "...       ...       ...       ...        ...         ...       ...       ...   \n",
       "14442  6.3700      35.0  6.129032   0.926267       658.0  3.032258     33.78   \n",
       "14443  3.0500      33.0  6.868597   1.269488      1753.0  3.904232     34.02   \n",
       "14444  2.9344      36.0  3.986717   1.079696      1756.0  3.332068     34.03   \n",
       "14445  5.7192      15.0  6.395349   1.067979      1777.0  3.178891     37.58   \n",
       "14446  2.5755      52.0  3.402576   1.058776      2619.0  2.108696     37.77   \n",
       "\n",
       "       Longitude  \n",
       "0        -117.09  \n",
       "1        -120.14  \n",
       "2        -121.00  \n",
       "3        -122.45  \n",
       "4        -117.33  \n",
       "...          ...  \n",
       "14442    -117.96  \n",
       "14443    -117.43  \n",
       "14444    -118.38  \n",
       "14445    -121.96  \n",
       "14446    -122.42  \n",
       "\n",
       "[14447 rows x 8 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_train = pd.read_csv('/home/cscadmin/CyberResearch/CAT-Transformer/datasets/income/train.csv')\n",
    "# df_test = pd.read_csv('/home/cscadmin/CyberResearch/CAT-Transformer/datasets/income/test.csv')\n",
    "# df_val = pd.read_csv('/home/cscadmin/CyberResearch/CAT-Transformer/datasets/income/validation.csv') #READ FROM RIGHT SPOT\n",
    "\n",
    "df_train = pd.read_csv(r'C:\\Users\\smbm2\\projects\\CAT-Transformer\\datasets\\california\\train.csv')\n",
    "df_test = pd.read_csv(r'C:\\Users\\smbm2\\projects\\CAT-Transformer\\datasets\\california\\test.csv')\n",
    "df_val = pd.read_csv(r'C:\\Users\\smbm2\\projects\\CAT-Transformer\\datasets\\california\\validation.csv') #READ FROM RIGHT SPOT\n",
    "\n",
    "#Take a look at what the datasets look like initially to get an idea\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup',\n",
       "       'Latitude', 'Longitude'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Take a look at the feature names\n",
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_columns = ['HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup',\n",
    "       'Latitude', 'Longitude']\n",
    "target = ['MedInc']\n",
    "\n",
    "#CHECKING TO MAKE SURE YOUR LIST IS CORRECT (NO NEED TO TOUCH)\n",
    "yourlist = cont_columns + target\n",
    "yourlist.sort()\n",
    "oglist = list(df_train.columns)\n",
    "oglist.sort()\n",
    "\n",
    "assert(yourlist == oglist), \"You may of spelled feature name wrong or you forgot to put on of them in the list\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9603]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Getting the number of classes in your classification target\n",
    "target_classes = [max(len(df_train[target].value_counts()), len(df_val[target].value_counts()),len(df_test[target].value_counts()))]\n",
    "target_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a StandardScaler and fit it to the cont features\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(df_train[cont_columns])\n",
    "\n",
    "# Transform the training, test, and validation datasets\n",
    "df_train[cont_columns] = scaler.transform(df_train[cont_columns])\n",
    "df_test[cont_columns] = scaler.transform(df_test[cont_columns])\n",
    "df_val[cont_columns] = scaler.transform(df_val[cont_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleTaskDataset(Dataset):\n",
    "    def __init__(self, df : pd.DataFrame, num_columns,task1_column):\n",
    "        self.n = df.shape[0]\n",
    "        \n",
    "        self.task1_labels = df[task1_column].astype(np.float32).values\n",
    "\n",
    "        self.num = df[num_columns].astype(np.float32).values\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Retrieve features and labels from the dataframe using column names\n",
    "        num_features = self.num[idx]\n",
    "        labels_task1 = self.task1_labels[idx]\n",
    "\n",
    "        return num_features, labels_task1\n",
    "\n",
    "#Wrapping in Dataset\n",
    "train_dataset = SingleTaskDataset(df_train, cont_columns, 'MedInc')\n",
    "val_dataset = SingleTaskDataset(df_val, cont_columns, 'MedInc')\n",
    "test_dataset = SingleTaskDataset(df_test, cont_columns, 'MedInc')\n",
    "\n",
    "#This is a hyperparameter that is not tuned. Maybe mess with what makes sense here\n",
    "#Also try looking to see what other papers have done\n",
    "batch_size = 256\n",
    "\n",
    "# Wrapping with DataLoader for easy batch extraction\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL AND HELPERS\n",
    "\n",
    "1. All you should have to do is interact with Classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each task loss is scaled by its own learnable parameter, then regularization is applied \n",
    "class UncertaintyLoss(nn.Module):\n",
    "    def __init__(self, num_tasks):\n",
    "        super(UncertaintyLoss, self).__init__()\n",
    "        self.num_tasks = num_tasks\n",
    "\n",
    "        self.loss_fns = [nn.MSELoss() for x in range(num_tasks)] \n",
    "\n",
    "    def forward(self, predictions, labels_task1):\n",
    "\n",
    "        #task 1\n",
    "        target = labels_task1\n",
    "        prediction = predictions\n",
    "        loss_fn = self.loss_fns[0]\n",
    "        task_loss = loss_fn(prediction, target)\n",
    "        \n",
    "        return task_loss\n",
    "    \n",
    "#All layers of the model\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_size, heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        self.head_dim = embed_size // heads\n",
    "        assert(self.head_dim * heads == embed_size), \"Embed size needs to be div by heads\"\n",
    "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.keys =nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.fc_out = nn.Linear(heads*self.head_dim, embed_size)\n",
    "\n",
    "\n",
    "    def forward(self, values, keys, query):\n",
    "        N = query.shape[0]\n",
    "        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
    "\n",
    "        values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
    "        keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n",
    "        queries = query.reshape(N, query_len, self.heads, self.head_dim)\n",
    "\n",
    "        values = self.values(values)\n",
    "        keys = self.keys(keys)\n",
    "        queries = self.queries(queries)\n",
    "\n",
    "        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
    "\n",
    "        attention = torch.softmax(energy / (self.embed_size ** (1/2)), dim=3) #(batch_size, head_dim, #query_embeddings, #key_embeddings)\n",
    "\n",
    "        # Calculate simplified attention scores\n",
    "        avg_attention = attention.mean(dim=0)  # Average across batches\n",
    "        # print(\"batch average\", avg_attention.shape)\n",
    "        avg_attention = avg_attention.mean(dim=0).squeeze(dim=0)\n",
    "        # print(\"head average\", avg_attention.shape)\n",
    "\n",
    "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(N, query_len, self.heads*self.head_dim) #(batch_size, n_features, embed_size)\n",
    "        out = self.fc_out(out)\n",
    "\n",
    "        return out, avg_attention\n",
    "    \n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_size, heads, dropout, forward_expansion, pre_norm_on):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "\n",
    "        self.pre_norm_on = pre_norm_on\n",
    "        if self.pre_norm_on:\n",
    "            self.pre_norm = nn.LayerNorm(embed_size)\n",
    "        self.attention = MultiHeadAttention(embed_size, heads)\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "\n",
    "        self.feed_forward = nn.Sequential(nn.Linear(embed_size, forward_expansion*embed_size),\n",
    "                                          nn.ReLU(),\n",
    "                                          nn.Linear(forward_expansion*embed_size, embed_size)\n",
    "                                          )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self,value,key,query):\n",
    "        if self.pre_norm_on:\n",
    "            query = self.pre_norm(query)\n",
    "            key = self.pre_norm(key)\n",
    "            value = self.pre_norm(value)\n",
    "            \n",
    "        attention, avg_attention = self.attention(value, key, query)\n",
    "        x = self.dropout(self.norm1(attention + query))\n",
    "        forward = self.feed_forward(x)\n",
    "        out = self.dropout(self.norm2(forward + x))\n",
    "        return out, avg_attention\n",
    "    \n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, embed_size, heads, forward_expansion, dropout, pre_norm_on):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "\n",
    "        self.attention = MultiHeadAttention(embed_size, heads)\n",
    "        self.norm = nn.LayerNorm(embed_size)\n",
    "        self.transformer_block = TransformerBlock(embed_size, heads, dropout, forward_expansion, pre_norm_on)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, value, key):\n",
    "        out, avg_attention = self.transformer_block(value, key, x)\n",
    "\n",
    "        return out, avg_attention\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 embed_size,\n",
    "                 num_layers,\n",
    "                 heads,\n",
    "                 forward_expansion,\n",
    "                 decoder_dropout,\n",
    "                 pre_norm_on\n",
    "    ):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "                [\n",
    "                    DecoderBlock(\n",
    "                        embed_size,\n",
    "                        heads,\n",
    "                        dropout=decoder_dropout,\n",
    "                        forward_expansion=forward_expansion,\n",
    "                        pre_norm_on=pre_norm_on\n",
    "                    )\n",
    "                    for _ in range(num_layers)\n",
    "                ]\n",
    "            )\n",
    "        self.avg_attention = None\n",
    "\n",
    "    def forward(self, class_embed, context):\n",
    "        for layer in self.layers:\n",
    "            # x is the classification embedding (CLS Token)\n",
    "            # context are the feature embeddings that will be used as key and value\n",
    "            x, self.avg_attention = layer(class_embed, context, context)\n",
    "  \n",
    "        return x \n",
    "\n",
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, sigma, embed_size, input_size, embedding_dropout, n_features, num_target_labels, rff_on):\n",
    "        super(Embeddings, self).__init__()\n",
    "\n",
    "        self.rff_on = rff_on\n",
    "\n",
    "        if self.rff_on:\n",
    "            self.rffs = nn.ModuleList([GaussianEncoding(sigma=sigma, input_size=input_size, encoded_size=embed_size//2) for _ in range(n_features)])\n",
    "            self.dropout = nn.Dropout(embedding_dropout)\n",
    "            self.mlp_in = embed_size\n",
    "        else:\n",
    "            self.mlp_in = input_size\n",
    "\n",
    "        self.embeddings = nn.ModuleList([nn.Linear(in_features=self.mlp_in, out_features=embed_size) for _ in range(n_features)])\n",
    "\n",
    "        # Classifcation Embeddings for each target label\n",
    "        self.target_label_embeddings = nn.ModuleList([nn.Embedding(1, embed_size) for _ in range(num_target_labels)])\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(2) #(batch_size, n_features) -> (batch_size, n_features, 1)\n",
    "        rff_vectors = []\n",
    "        if self.rff_on:\n",
    "            for i, r in enumerate(self.rffs):\n",
    "                input = x[:,i,:]\n",
    "                out = r(input)\n",
    "                rff_vectors.append(out)\n",
    "        \n",
    "            x = torch.stack(rff_vectors, dim=1)\n",
    "        \n",
    "        embeddings = []\n",
    "        for i, e in enumerate(self.embeddings):\n",
    "            goin_in = x[:,i,:]\n",
    "            goin_out = e(goin_in)\n",
    "            embeddings.append(goin_out)\n",
    "\n",
    "        target_label_embeddings_ = []\n",
    "        for e in self.target_label_embeddings:\n",
    "            input = torch.tensor([0], device=x.device)\n",
    "            temp = e(input)\n",
    "            temp = temp.repeat(x.size(0), 1)\n",
    "            tmep = temp.unsqueeze(1)\n",
    "            target_label_embeddings_.append(temp)\n",
    "\n",
    "        class_embeddings = torch.stack(target_label_embeddings_, dim=1)\n",
    "\n",
    "        context = torch.stack(embeddings, dim=1)\n",
    "\n",
    "        return class_embeddings, context\n",
    "\n",
    "class classificationHead(nn.Module):\n",
    "    def __init__(self, embed_size, dropout, mlp_scale_classification, num_target_classes):\n",
    "        super(classificationHead, self).__init__()\n",
    "        \n",
    "        #flattening the embeddings out so each sample in batch is represented with a 460 dimensional vector\n",
    "        self.input = embed_size\n",
    "        self.lin1 = nn.Linear(self.input, mlp_scale_classification*self.input)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.lin2 = nn.Linear(mlp_scale_classification*self.input, mlp_scale_classification*self.input)\n",
    "        self.lin3 = nn.Linear(mlp_scale_classification*self.input, self.input)\n",
    "        self.lin4 = nn.Linear(self.input, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.initialize_weights()\n",
    "\n",
    "    def initialize_weights(self): #he_initialization.\n",
    "        torch.nn.init.kaiming_normal_(self.lin1.weight, nonlinearity='relu')\n",
    "        torch.nn.init.zeros_(self.lin1.bias)\n",
    "\n",
    "        torch.nn.init.kaiming_normal_(self.lin3.weight, nonlinearity='relu')\n",
    "        torch.nn.init.zeros_(self.lin3.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x= torch.reshape(x, (-1, self.input))\n",
    "\n",
    "        x = self.lin1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.lin2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.lin3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.lin4(x)\n",
    "  \n",
    "        return x\n",
    "    \n",
    "# class classificationHead(nn.Module):\n",
    "#     def __init__(self, embed_size, dropout, mlp_scale_classification, num_target_classes):\n",
    "#         super(classificationHead, self).__init__()\n",
    "        \n",
    "#         #flattening the embeddings out so each sample in batch is represented with a 460 dimensional vector\n",
    "#         self.input = embed_size\n",
    "#         self.lin1 = nn.Linear(self.input, mlp_scale_classification*self.input)\n",
    "#         self.drop = nn.Dropout(dropout)\n",
    "#         self.lin2 = nn.Linear(mlp_scale_classification*self.input, mlp_scale_classification*self.input)\n",
    "#         self.lin3 = nn.Linear(mlp_scale_classification*self.input, self.input)\n",
    "#         self.lin4 = nn.Linear(self.input, num_target_classes)\n",
    "#         self.relu = nn.ReLU()\n",
    "#         self.initialize_weights()\n",
    "\n",
    "#     def initialize_weights(self): #he_initialization.\n",
    "#         torch.nn.init.kaiming_normal_(self.lin1.weight, nonlinearity='relu')\n",
    "#         torch.nn.init.zeros_(self.lin1.bias)\n",
    "\n",
    "#         torch.nn.init.kaiming_normal_(self.lin3.weight, nonlinearity='relu')\n",
    "#         torch.nn.init.zeros_(self.lin3.bias)\n",
    "\n",
    "#     def forward(self, x):\n",
    "\n",
    "#         x= torch.reshape(x, (-1, self.input))\n",
    "\n",
    "#         x = self.lin1(x)\n",
    "#         x = self.relu(x)\n",
    "#         x = self.drop(x)\n",
    "#         x = self.lin2(x)\n",
    "#         x = self.relu(x)\n",
    "#         x = self.drop(x)\n",
    "#         x = self.lin3(x)\n",
    "#         x = self.relu(x)\n",
    "#         x = self.drop(x)\n",
    "#         x = self.lin4(x)\n",
    "  \n",
    "#         return x\n",
    "\n",
    "\n",
    "# DEFAULT PARAMETERS SET UP FOR VPN DATASET. BE CAREFUL AND MAKE SURE YOU SET THEM UP HOW YOU WANT.\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, \n",
    "                 rff_on = False,\n",
    "                 sigma=4,\n",
    "                 embed_size=20,\n",
    "                 input_size=1,\n",
    "                 embedding_dropout = 0,\n",
    "                 n_features=23, # YOU WILL PROBABLY NEED TO CHANGE\n",
    "                 num_layers=1,\n",
    "                 heads=1,\n",
    "                 forward_expansion=4, # Determines how wide the MLP is in the encoder. Its a scaling factor. \n",
    "                 decoder_dropout=0,\n",
    "                 classification_dropout = 0,\n",
    "                 pre_norm_on = False,\n",
    "                 mlp_scale_classification = 4,\n",
    "                 targets_classes : list=  [3,8]\n",
    "                 ):\n",
    "        super(Classifier, self).__init__()\n",
    "\n",
    "        self.embeddings = Embeddings(rff_on=rff_on, sigma=sigma, embed_size=embed_size, input_size=input_size, embedding_dropout=embedding_dropout, n_features=n_features, num_target_labels=len(targets_classes))\n",
    "        self.decoder = Decoder(embed_size=embed_size, num_layers=num_layers, heads=heads, forward_expansion=forward_expansion, decoder_dropout=decoder_dropout, pre_norm_on=pre_norm_on)\n",
    "        self.classifying_heads = nn.ModuleList([classificationHead(embed_size=embed_size, dropout=classification_dropout, mlp_scale_classification=mlp_scale_classification, num_target_classes=x) for x in targets_classes])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        class_embed, context = self.embeddings(x)\n",
    "\n",
    "        x = self.decoder(class_embed, context)\n",
    "        \n",
    "        probability_dist_raw = []\n",
    "        for i, e in enumerate(self.classifying_heads):\n",
    "            input = x[:, i,:]\n",
    "            output = e(input)\n",
    "            probability_dist_raw.append(output)\n",
    "        \n",
    "        return probability_dist_raw\n",
    "\n",
    "# Training and Testing Loops\n",
    "def train(dataloader, model, loss_function, optimizer, device_in_use):\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    total_r2_score = 0\n",
    "    root_mean_squared_error_total = 0\n",
    "\n",
    "    for (features, labels_task1) in dataloader:\n",
    "        features, labels_task1 = features.to(device_in_use), labels_task1.to(device_in_use)\n",
    "\n",
    "        task_predictions = model(features)\n",
    "\n",
    "        loss = loss_function(task_predictions[0].squeeze(1), labels_task1)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Calculate R^2 score for the regression task\n",
    "        r2 = r2_score_manual(labels_task1, task_predictions[0].squeeze(1))\n",
    "        total_r2_score += r2\n",
    "\n",
    "        # Calculate RMSE score for the regression task\n",
    "        rmse_value = rmse(labels_task1, task_predictions[0].squeeze(1))\n",
    "        root_mean_squared_error_total+=rmse_value\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    avg_r2_score = total_r2_score / len(dataloader)\n",
    "    avg_rmse_score = root_mean_squared_error_total / len(dataloader)\n",
    "\n",
    "    return avg_loss, avg_r2_score, avg_rmse_score\n",
    "\n",
    "def test(dataloader, model, loss_function, device_in_use):\n",
    "  model.eval()\n",
    "  total_loss = 0\n",
    "  \n",
    "  total_loss = 0\n",
    "  total_r2_score = 0\n",
    "  root_mean_squared_error_total = 0\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for (features,labels_task1) in dataloader:\n",
    "        features, labels_task1 = features.to(device_in_use), labels_task1.to(device_in_use)\n",
    "\n",
    "        task_predictions = model(features)\n",
    "\n",
    "        loss = loss_function(task_predictions[0].squeeze(1), labels_task1)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Calculate R^2 score for the regression task\n",
    "        r2 = r2_score_manual(labels_task1, task_predictions[0].squeeze(1))\n",
    "        total_r2_score += r2\n",
    "        \n",
    "        # Calculate RMSE score for the regression task\n",
    "        rmse_value = rmse(labels_task1, task_predictions[0].squeeze(1))\n",
    "        root_mean_squared_error_total+=rmse_value\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    avg_r2_score = total_r2_score / len(dataloader)\n",
    "    avg_rmse_score = root_mean_squared_error_total / len(dataloader)\n",
    "\n",
    "    return avg_loss, avg_r2_score, avg_rmse_score\n",
    "\n",
    "def format_metric(value): # Used to format the metrics output\n",
    "    return f\"{value:.4f}\"\n",
    "\n",
    "def r2_score_manual(y_true, y_pred):\n",
    "    # Calculate the mean of true labels\n",
    "    y_mean = torch.mean(y_true)\n",
    "\n",
    "    # Calculate the total sum of squares\n",
    "    total_ss = torch.sum((y_true - y_mean)**2)\n",
    "\n",
    "    # Calculate the residual sum of squares\n",
    "    residual_ss = torch.sum((y_true - y_pred)**2)\n",
    "\n",
    "    # Calculate R-squared\n",
    "    r2 = 1 - (residual_ss / total_ss)\n",
    "\n",
    "    return r2.item()  # Convert to a Python float\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    # Calculate the squared differences\n",
    "    squared_diff = (y_true - y_pred)**2\n",
    "\n",
    "    # Calculate the mean of the squared differences\n",
    "    mean_squared_diff = torch.mean(squared_diff)\n",
    "\n",
    "    # Calculate the square root to obtain RMSE\n",
    "    rmse = torch.sqrt(mean_squared_diff)\n",
    "\n",
    "    return rmse.item()  # Convert to a Python float"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUN EXPERIMENTS\n",
    "\n",
    "1. Using Optuna to optimize CAT-Transformers hyperparameters for your dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the early stopping mechanism\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5):\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.best_metric = float('-inf')\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, metric):\n",
    "        if metric > self.best_metric:\n",
    "            self.best_metric = metric\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "\n",
    "# Function to log results to a text file\n",
    "def log_to_file(filename, text):\n",
    "    with open(filename, 'a') as f:\n",
    "        f.write(text + '\\n')\n",
    "\n",
    "def objective(trial):\n",
    "    trial_number = trial.number\n",
    "\n",
    "    # Define hyperparameters to search over\n",
    "    sigma = trial.suggest_categorical('sigma', [.001, 0.1, 1, 2, 3, 5, 10])\n",
    "    num_layers = trial.suggest_int('num_layers', 1, 2)\n",
    "    # Ensure that embed_size is divisible by num_layers\n",
    "    embed_size = trial.suggest_categorical(\"embed_size\", [50, 60, 70, 80, 90, 100, 120, 140, 160])\n",
    "    heads = trial.suggest_categorical(\"heads\", [1, 5, 10])\n",
    "    forward_expansion = trial.suggest_int('forward_expansion', 1, 8)\n",
    "    prenorm_on = trial.suggest_categorical('prenorm_on', [True, False])\n",
    "    mlp_scale_classification = trial.suggest_int('mlp_scale_classification', 1, 8)\n",
    "    embedding_dropout = trial.suggest_categorical('embedding_dropout', [0, .1, .2, .5])\n",
    "    decoder_dropout = trial.suggest_categorical('decoder_dropout', [0,.1,.2,.5])\n",
    "    classification_dropout = trial.suggest_categorical('class_drop', [0,.1,.2,.5])\n",
    "\n",
    "    learning_rate = trial.suggest_categorical('learning_rate', [0.0001, 0.001, 0.01])\n",
    "\n",
    "    num_epochs = 10\n",
    "\n",
    "\n",
    "    # Create your model with the sampled hyperparameters\n",
    "    model = Classifier(\n",
    "        n_features=len(cont_columns),\n",
    "        targets_classes=[0],\n",
    "        rff_on=True,\n",
    "        sigma=sigma,\n",
    "        embed_size=embed_size,\n",
    "        num_layers=num_layers,\n",
    "        heads=heads,\n",
    "        forward_expansion=forward_expansion,\n",
    "        pre_norm_on=prenorm_on,\n",
    "        mlp_scale_classification=mlp_scale_classification,\n",
    "        embedding_dropout=embedding_dropout,\n",
    "        decoder_dropout=decoder_dropout,\n",
    "        classification_dropout=classification_dropout\n",
    "    ).to(device_in_use)\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    loss_function = UncertaintyLoss(1)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Initialize early stopping\n",
    "    early_stopping = EarlyStopping(patience=10)  # Adjust patience as needed\n",
    "\n",
    "    # Training loop with a large number of epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss, r2_train, rmse_train = train(train_dataloader, model, loss_function, optimizer, device_in_use)\n",
    "        \n",
    "        # Validation loop\n",
    "        test_loss, r2_test, rmse_test = test(val_dataloader, model, loss_function, device_in_use)\n",
    "        \n",
    "        # Check if we should early stop based on validation rmse\n",
    "        if early_stopping(rmse_test):\n",
    "            break\n",
    "    \n",
    "    # Log the final test accuracy for this trial to a shared log file\n",
    "    final_log = f\"Trial {trial_number} completed. Validation RMSE = {rmse_test:.4f}\"\n",
    "    log_to_file('all_trials_log.txt', final_log)\n",
    "\n",
    "    # Return the test accuracy as the objective to optimize\n",
    "    return rmse_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-25 10:09:50,057] A new study created in memory with name: no-name-89824049-478d-4774-8b8b-ad1c4c830033\n",
      "Best trial: 0. Best value: 1.17073: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2023-10-25 10:10:02,667] Trial 0 finished with value: 1.1707287751711333 and parameters: {'sigma': 0.1, 'num_layers': 1, 'embed_size': 60, 'heads': 5, 'forward_expansion': 1, 'prenorm_on': True, 'mlp_scale_classification': 2, 'embedding_dropout': 0.2, 'decoder_dropout': 0.2, 'class_drop': 0.2, 'learning_rate': 0.0001}. Best is trial 0 with value: 1.1707287751711333.\n",
      "Best Hyperparameters: {'sigma': 0.1, 'num_layers': 1, 'embed_size': 60, 'heads': 5, 'forward_expansion': 1, 'prenorm_on': True, 'mlp_scale_classification': 2, 'embedding_dropout': 0.2, 'decoder_dropout': 0.2, 'class_drop': 0.2, 'learning_rate': 0.0001}\n",
      "Best Validation RMSE (at Early Stopping): 1.1707287751711333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Set the number of optimization trials\n",
    "num_trials = 1\n",
    "\n",
    "# Create an Optuna study\n",
    "study = optuna.create_study(direction='minimize')  # Maximize validation accuracy\n",
    "\n",
    "# Start the optimization process\n",
    "study.optimize(objective, n_trials=num_trials, show_progress_bar=True)\n",
    "\n",
    "# Get the best hyperparameters and the validation accuracy at the point of early stopping\n",
    "best_params = study.best_params\n",
    "best_val_rmse = study.best_value\n",
    "\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "print(\"Best Validation RMSE (at Early Stopping):\", best_val_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [ 1/75]        | Train: Loss 14.1747, R2 -3.0712, RMSE 3.7363                      | Test: Loss 6.9779, R2 -0.9127, RMSE 2.6299\n",
      "Epoch [ 2/75]        | Train: Loss 4.8829, R2 -0.3455, RMSE 2.1932                       | Test: Loss 3.6212, R2 0.0101, RMSE 1.8980\n",
      "Epoch [ 3/75]        | Train: Loss 4.2006, R2 -0.1693, RMSE 2.0436                       | Test: Loss 3.6202, R2 0.0388, RMSE 1.8944\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\smbm2\\projects\\CAT-Transformer\\experiments\\california_sklearn\\prototype_regression.ipynb Cell 15\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/experiments/california_sklearn/prototype_regression.ipynb#X20sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m all_attention_scores \u001b[39m=\u001b[39m []\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/experiments/california_sklearn/prototype_regression.ipynb#X20sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/experiments/california_sklearn/prototype_regression.ipynb#X20sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m   train_loss, r2_train, rmse_train \u001b[39m=\u001b[39m train(train_dataloader, model, loss_functions, optimizer, device_in_use\u001b[39m=\u001b[39;49mdevice_in_use)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/experiments/california_sklearn/prototype_regression.ipynb#X20sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m   test_loss, r2_test, rmse_test \u001b[39m=\u001b[39m test(test_dataloader, model, loss_functions, device_in_use\u001b[39m=\u001b[39mdevice_in_use)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/experiments/california_sklearn/prototype_regression.ipynb#X20sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m   train_losses\u001b[39m.\u001b[39mappend(train_loss)\n",
      "\u001b[1;32mc:\\Users\\smbm2\\projects\\CAT-Transformer\\experiments\\california_sklearn\\prototype_regression.ipynb Cell 15\u001b[0m line \u001b[0;36m3\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/experiments/california_sklearn/prototype_regression.ipynb#X20sZmlsZQ%3D%3D?line=322'>323</a>\u001b[0m     root_mean_squared_error_total\u001b[39m+\u001b[39m\u001b[39m=\u001b[39mrmse_value\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/experiments/california_sklearn/prototype_regression.ipynb#X20sZmlsZQ%3D%3D?line=324'>325</a>\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/experiments/california_sklearn/prototype_regression.ipynb#X20sZmlsZQ%3D%3D?line=325'>326</a>\u001b[0m     optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/experiments/california_sklearn/prototype_regression.ipynb#X20sZmlsZQ%3D%3D?line=326'>327</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/experiments/california_sklearn/prototype_regression.ipynb#X20sZmlsZQ%3D%3D?line=328'>329</a>\u001b[0m avg_loss \u001b[39m=\u001b[39m total_loss \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(dataloader)\n",
      "File \u001b[1;32mc:\\Users\\smbm2\\AppData\\Local\\miniconda3\\envs\\ml-env\\Lib\\site-packages\\torch\\optim\\optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    276\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    277\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m                                \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 280\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    281\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    283\u001b[0m \u001b[39m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\smbm2\\AppData\\Local\\miniconda3\\envs\\ml-env\\Lib\\site-packages\\torch\\optim\\optimizer.py:33\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     32\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m---> 33\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     34\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     35\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[1;32mc:\\Users\\smbm2\\AppData\\Local\\miniconda3\\envs\\ml-env\\Lib\\site-packages\\torch\\optim\\adam.py:141\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    130\u001b[0m     beta1, beta2 \u001b[39m=\u001b[39m group[\u001b[39m'\u001b[39m\u001b[39mbetas\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m    132\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_group(\n\u001b[0;32m    133\u001b[0m         group,\n\u001b[0;32m    134\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    138\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    139\u001b[0m         state_steps)\n\u001b[1;32m--> 141\u001b[0m     adam(\n\u001b[0;32m    142\u001b[0m         params_with_grad,\n\u001b[0;32m    143\u001b[0m         grads,\n\u001b[0;32m    144\u001b[0m         exp_avgs,\n\u001b[0;32m    145\u001b[0m         exp_avg_sqs,\n\u001b[0;32m    146\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    147\u001b[0m         state_steps,\n\u001b[0;32m    148\u001b[0m         amsgrad\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    149\u001b[0m         beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    150\u001b[0m         beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    151\u001b[0m         lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    152\u001b[0m         weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    153\u001b[0m         eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    154\u001b[0m         maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    155\u001b[0m         foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    156\u001b[0m         capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    157\u001b[0m         differentiable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mdifferentiable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    158\u001b[0m         fused\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mfused\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    159\u001b[0m         grad_scale\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mgrad_scale\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m    160\u001b[0m         found_inf\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mfound_inf\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m    161\u001b[0m     )\n\u001b[0;32m    163\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\smbm2\\AppData\\Local\\miniconda3\\envs\\ml-env\\Lib\\site-packages\\torch\\optim\\adam.py:281\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    279\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 281\u001b[0m func(params,\n\u001b[0;32m    282\u001b[0m      grads,\n\u001b[0;32m    283\u001b[0m      exp_avgs,\n\u001b[0;32m    284\u001b[0m      exp_avg_sqs,\n\u001b[0;32m    285\u001b[0m      max_exp_avg_sqs,\n\u001b[0;32m    286\u001b[0m      state_steps,\n\u001b[0;32m    287\u001b[0m      amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[0;32m    288\u001b[0m      beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    289\u001b[0m      beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    290\u001b[0m      lr\u001b[39m=\u001b[39;49mlr,\n\u001b[0;32m    291\u001b[0m      weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[0;32m    292\u001b[0m      eps\u001b[39m=\u001b[39;49meps,\n\u001b[0;32m    293\u001b[0m      maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[0;32m    294\u001b[0m      capturable\u001b[39m=\u001b[39;49mcapturable,\n\u001b[0;32m    295\u001b[0m      differentiable\u001b[39m=\u001b[39;49mdifferentiable,\n\u001b[0;32m    296\u001b[0m      grad_scale\u001b[39m=\u001b[39;49mgrad_scale,\n\u001b[0;32m    297\u001b[0m      found_inf\u001b[39m=\u001b[39;49mfound_inf)\n",
      "File \u001b[1;32mc:\\Users\\smbm2\\AppData\\Local\\miniconda3\\envs\\ml-env\\Lib\\site-packages\\torch\\optim\\adam.py:505\u001b[0m, in \u001b[0;36m_multi_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    503\u001b[0m     denom \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39m_foreach_add(max_exp_avg_sq_sqrt, eps)\n\u001b[0;32m    504\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 505\u001b[0m     exp_avg_sq_sqrt \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49m_foreach_sqrt(device_exp_avg_sqs)\n\u001b[0;32m    506\u001b[0m     torch\u001b[39m.\u001b[39m_foreach_div_(exp_avg_sq_sqrt, bias_correction2_sqrt)\n\u001b[0;32m    507\u001b[0m     denom \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39m_foreach_add(exp_avg_sq_sqrt, eps)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Testing against the test dataset\n",
    "\n",
    "model = Classifier(n_features=len(cont_columns),\n",
    "                   targets_classes=[0],\n",
    "                   rff_on=True,\n",
    "                   sigma=best_params['sigma'],\n",
    "                   embed_size=best_params['embed_size'],\n",
    "                   num_layers=best_params['num_layers'],\n",
    "                   heads=best_params['heads'],\n",
    "                   forward_expansion=best_params['forward_expansion'],\n",
    "                   pre_norm_on=best_params['prenorm_on'],\n",
    "                   mlp_scale_classification=best_params['mlp_scale_classification'],\n",
    "                   embedding_dropout=best_params['embedding_dropout'],\n",
    "                   decoder_dropout=best_params['decoder_dropout'],\n",
    "                   classification_dropout=best_params['class_drop']\n",
    "                   ).to(device_in_use) # Instantiate the model\n",
    "loss_functions = UncertaintyLoss(1)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr = best_params['learning_rate']) # Maybe try messing around with optimizers. try other torch optimizers with different configurations.\n",
    "epochs = 75 #Set the number of epochs\n",
    "\n",
    "train_losses = []\n",
    "train_accuracies_1 = [] \n",
    "train_accuracies_2 = []\n",
    "train_recalls = [] \n",
    "train_f1_scores = [] \n",
    "test_losses = []\n",
    "test_accuracies_1 = []\n",
    "test_accuracies_2 = []\n",
    "test_recalls = []  \n",
    "test_f1_scores = [] \n",
    "all_attention_scores = []\n",
    "\n",
    "for t in range(epochs):\n",
    "  train_loss, r2_train, rmse_train = train(train_dataloader, model, loss_functions, optimizer, device_in_use=device_in_use)\n",
    "  test_loss, r2_test, rmse_test = test(test_dataloader, model, loss_functions, device_in_use=device_in_use)\n",
    "  train_losses.append(train_loss)\n",
    "\n",
    "  # train_accuracies_2.append(train_accuracy_2)\n",
    "  # train_recalls.append(train_recall) \n",
    "  # train_f1_scores.append(train_f1)\n",
    "  test_losses.append(test_loss)\n",
    "\n",
    "  # test_accuracies_2.append(test_accuracy_2)\n",
    "  # test_recalls.append(test_recall)\n",
    "  # test_f1_scores.append(test_f1)\n",
    "  # Formatting for easier reading\n",
    "  epoch_str = f\"Epoch [{t+1:2}/{epochs}]\"\n",
    "  train_metrics = f\"Train: Loss {format_metric(train_loss)}, R2 {format_metric(r2_train)}, RMSE {format_metric(rmse_train)}\"\n",
    "  test_metrics = f\"Test: Loss {format_metric(test_loss)}, R2 {format_metric(r2_test)}, RMSE {format_metric(rmse_test)}\"\n",
    "  print(f\"{epoch_str:20} | {train_metrics:65} | {test_metrics}\")\n",
    "\n",
    "\n",
    "torch.save(model.state_dict(), 'final_model_trained.pth')\n",
    "\n",
    "# Plotting the loss curves\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 1, 1)\n",
    "plt.plot(range(1, epochs+1), train_losses, label='Train Loss')\n",
    "plt.plot(range(1, epochs+1), [l for l in test_losses], label='Test Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Test Loss Curve')\n",
    "plt.legend()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
