{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available and being used\n",
      "[7]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn\n",
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from rff.layers import GaussianEncoding #pip install random-fourier-features-pytorch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sys.path.append('../../model/')\n",
    "from updatedModel import CATTransformer, train, test\n",
    "import time\n",
    "\n",
    "# Run regardless if you do or do not have GPU so all tensors are moved to right location later on\n",
    "if torch.cuda.is_available():\n",
    "    device_in_use = torch.device(\"cuda\")\n",
    "    print(\"GPU is available and being used\")\n",
    "else:\n",
    "    device_in_use = torch.device(\"cpu\")\n",
    "    print(\"GPU is not available, using CPU instead\")\n",
    "\n",
    "# df_train = pd.read_csv('/home/cscadmin/CyberResearch/CAT-Transformer/datasets/income/train.csv')\n",
    "# df_test = pd.read_csv('/home/cscadmin/CyberResearch/CAT-Transformer/datasets/income/test.csv')\n",
    "# df_val = pd.read_csv('/home/cscadmin/CyberResearch/CAT-Transformer/datasets/income/validation.csv') #READ FROM RIGHT SPOT\n",
    "\n",
    "df_train = pd.read_csv(r'C:\\Users\\smbm2\\projects\\CAT-Transformer\\datasets\\covertype\\train.csv')\n",
    "df_test = pd.read_csv(r'C:\\Users\\smbm2\\projects\\CAT-Transformer\\datasets\\covertype\\test.csv')\n",
    "df_val = pd.read_csv(r'C:\\Users\\smbm2\\projects\\CAT-Transformer\\datasets\\covertype\\validation.csv') #READ FROM RIGHT SPOT\n",
    "\n",
    "cont_columns = ['Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology',\n",
    "       'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways',\n",
    "       'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm',\n",
    "       'Horizontal_Distance_To_Fire_Points', 'Wilderness_Area1',\n",
    "       'Wilderness_Area2', 'Wilderness_Area3', 'Wilderness_Area4',\n",
    "       'Soil_Type1', 'Soil_Type2', 'Soil_Type3', 'Soil_Type4', 'Soil_Type5',\n",
    "       'Soil_Type6', 'Soil_Type7', 'Soil_Type8', 'Soil_Type9', 'Soil_Type10',\n",
    "       'Soil_Type11', 'Soil_Type12', 'Soil_Type13', 'Soil_Type14',\n",
    "       'Soil_Type15', 'Soil_Type16', 'Soil_Type17', 'Soil_Type18',\n",
    "       'Soil_Type19', 'Soil_Type20', 'Soil_Type21', 'Soil_Type22',\n",
    "       'Soil_Type23', 'Soil_Type24', 'Soil_Type25', 'Soil_Type26',\n",
    "       'Soil_Type27', 'Soil_Type28', 'Soil_Type29', 'Soil_Type30',\n",
    "       'Soil_Type31', 'Soil_Type32', 'Soil_Type33', 'Soil_Type34',\n",
    "       'Soil_Type35', 'Soil_Type36', 'Soil_Type37', 'Soil_Type38',\n",
    "       'Soil_Type39', 'Soil_Type40']\n",
    "target = ['Cover_Type']\n",
    "\n",
    "#CHECKING TO MAKE SURE YOUR LIST IS CORRECT (NO NEED TO TOUCH)\n",
    "yourlist = cont_columns + target\n",
    "yourlist.sort()\n",
    "oglist = list(df_train.columns)\n",
    "oglist.sort()\n",
    "\n",
    "assert(yourlist == oglist), \"You may of spelled feature name wrong or you forgot to put on of them in the list\"\n",
    "\n",
    "target_classes = [max(len(df_train[target].value_counts()), len(df_val[target].value_counts()),len(df_test[target].value_counts()))]\n",
    "print(target_classes)\n",
    "# Create a StandardScaler and fit it to the cont features\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(df_train[cont_columns])\n",
    "\n",
    "# Transform the training, test, and validation datasets\n",
    "df_train[cont_columns] = scaler.transform(df_train[cont_columns])\n",
    "df_test[cont_columns] = scaler.transform(df_test[cont_columns])\n",
    "df_val[cont_columns] = scaler.transform(df_val[cont_columns])\n",
    "\n",
    "class SingleTaskDataset(Dataset):\n",
    "    def __init__(self, df : pd.DataFrame, num_columns,task1_column):\n",
    "        self.n = df.shape[0]\n",
    "        \n",
    "        self.task1_labels = df[task1_column].astype(np.int64).values\n",
    "\n",
    "        self.num = df[num_columns].astype(np.float32).values\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Retrieve features and labels from the dataframe using column names\n",
    "        num_features = self.num[idx]\n",
    "        labels_task1 = self.task1_labels[idx]\n",
    "\n",
    "        return num_features, labels_task1\n",
    "\n",
    "#Wrapping in Dataset\n",
    "train_dataset = SingleTaskDataset(df_train, cont_columns, 'Cover_Type')\n",
    "val_dataset = SingleTaskDataset(df_val, cont_columns, 'Cover_Type')\n",
    "test_dataset = SingleTaskDataset(df_test, cont_columns, 'Cover_Type')\n",
    "\n",
    "#This is a hyperparameter that is not tuned. Maybe mess with what makes sense here\n",
    "#Also try looking to see what other papers have done\n",
    "batch_size = 256\n",
    "\n",
    "# Wrapping with DataLoader for easy batch extraction\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each task loss is scaled by its own learnable parameter, then regularization is applied \n",
    "class UncertaintyLoss(nn.Module):\n",
    "    def __init__(self, num_tasks):\n",
    "        super(UncertaintyLoss, self).__init__()\n",
    "        self.num_tasks = num_tasks\n",
    "\n",
    "        self.loss_fns = [nn.CrossEntropyLoss() for x in range(num_tasks)] \n",
    "\n",
    "    def forward(self, predictions, labels_task1):\n",
    "\n",
    "        #task 1\n",
    "        target = labels_task1\n",
    "        prediction = predictions[0]\n",
    "        loss_fn = self.loss_fns[0]\n",
    "        task_loss = loss_fn(prediction, target)\n",
    "        \n",
    "        return task_loss\n",
    "    \n",
    "#All layers of the model\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_size, heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        self.head_dim = embed_size // heads\n",
    "        assert(self.head_dim * heads == embed_size), \"Embed size needs to be div by heads\"\n",
    "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.keys =nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.fc_out = nn.Linear(heads*self.head_dim, embed_size)\n",
    "\n",
    "\n",
    "    def forward(self, values, keys, query):\n",
    "        N = query.shape[0]\n",
    "        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
    "\n",
    "        values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
    "        keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n",
    "        queries = query.reshape(N, query_len, self.heads, self.head_dim)\n",
    "\n",
    "        values = self.values(values)\n",
    "        keys = self.keys(keys)\n",
    "        queries = self.queries(queries)\n",
    "\n",
    "        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
    "\n",
    "        attention = torch.softmax(energy / (self.embed_size ** (1/2)), dim=3) #(batch_size, head_dim, #query_embeddings, #key_embeddings)\n",
    "\n",
    "        # Calculate simplified attention scores\n",
    "        avg_attention = attention.mean(dim=0)  # Average across batches\n",
    "        # print(\"batch average\", avg_attention.shape)\n",
    "        avg_attention = avg_attention.mean(dim=0).squeeze(dim=0)\n",
    "        # print(\"head average\", avg_attention.shape)\n",
    "\n",
    "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(N, query_len, self.heads*self.head_dim) #(batch_size, n_features, embed_size)\n",
    "        out = self.fc_out(out)\n",
    "\n",
    "        return out, avg_attention\n",
    "    \n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_size, heads, dropout, forward_expansion, pre_norm_on):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "\n",
    "        self.pre_norm_on = pre_norm_on\n",
    "        if self.pre_norm_on:\n",
    "            self.pre_norm = nn.LayerNorm(embed_size)\n",
    "        self.attention = MultiHeadAttention(embed_size, heads)\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "\n",
    "        self.feed_forward = nn.Sequential(nn.Linear(embed_size, forward_expansion*embed_size),\n",
    "                                          nn.ReLU(),\n",
    "                                          nn.Linear(forward_expansion*embed_size, embed_size)\n",
    "                                          )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self,value,key,query):\n",
    "        if self.pre_norm_on:\n",
    "            query = self.pre_norm(query)\n",
    "            key = self.pre_norm(key)\n",
    "            value = self.pre_norm(value)\n",
    "            \n",
    "        attention, avg_attention = self.attention(value, key, query)\n",
    "        x = self.dropout(self.norm1(attention + query))\n",
    "        forward = self.feed_forward(x)\n",
    "        out = self.dropout(self.norm2(forward + x))\n",
    "        return out, avg_attention\n",
    "    \n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, embed_size, heads, forward_expansion, dropout, pre_norm_on):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "\n",
    "        self.attention = MultiHeadAttention(embed_size, heads)\n",
    "        self.norm = nn.LayerNorm(embed_size)\n",
    "        self.transformer_block = TransformerBlock(embed_size, heads, dropout, forward_expansion, pre_norm_on)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, value, key):\n",
    "        out, avg_attention = self.transformer_block(value, key, x)\n",
    "\n",
    "        return out, avg_attention\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 embed_size,\n",
    "                 num_layers,\n",
    "                 heads,\n",
    "                 forward_expansion,\n",
    "                 decoder_dropout,\n",
    "                 pre_norm_on\n",
    "    ):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "                [\n",
    "                    DecoderBlock(\n",
    "                        embed_size,\n",
    "                        heads,\n",
    "                        dropout=decoder_dropout,\n",
    "                        forward_expansion=forward_expansion,\n",
    "                        pre_norm_on=pre_norm_on\n",
    "                    )\n",
    "                    for _ in range(num_layers)\n",
    "                ]\n",
    "            )\n",
    "        self.avg_attention = None\n",
    "\n",
    "    def forward(self, class_embed, context):\n",
    "        for layer in self.layers:\n",
    "            # x is the classification embedding (CLS Token)\n",
    "            # context are the feature embeddings that will be used as key and value\n",
    "            x, self.avg_attention = layer(class_embed, context, context)\n",
    "  \n",
    "        return x \n",
    "\n",
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, sigma, embed_size, input_size, embedding_dropout, n_features, num_target_labels, rff_on):\n",
    "        super(Embeddings, self).__init__()\n",
    "\n",
    "        self.rff_on = rff_on\n",
    "\n",
    "        if self.rff_on:\n",
    "            self.rffs = nn.ModuleList([GaussianEncoding(sigma=sigma, input_size=input_size, encoded_size=embed_size//2) for _ in range(n_features)])\n",
    "            self.dropout = nn.Dropout(embedding_dropout)\n",
    "            self.mlp_in = embed_size\n",
    "        else:\n",
    "            self.mlp_in = input_size\n",
    "\n",
    "        self.embeddings = nn.ModuleList([nn.Linear(in_features=self.mlp_in, out_features=embed_size) for _ in range(n_features)])\n",
    "\n",
    "        # Classifcation Embeddings for each target label\n",
    "        self.target_label_embeddings = nn.ModuleList([nn.Embedding(1, embed_size) for _ in range(num_target_labels)])\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(2) #(batch_size, n_features) -> (batch_size, n_features, 1)\n",
    "        rff_vectors = []\n",
    "        if self.rff_on:\n",
    "            for i, r in enumerate(self.rffs):\n",
    "                input = x[:,i,:]\n",
    "                out = r(input)\n",
    "                rff_vectors.append(out)\n",
    "        \n",
    "            x = torch.stack(rff_vectors, dim=1)\n",
    "        \n",
    "        embeddings = []\n",
    "        for i, e in enumerate(self.embeddings):\n",
    "            goin_in = x[:,i,:]\n",
    "            goin_out = e(goin_in)\n",
    "            embeddings.append(goin_out)\n",
    "\n",
    "        target_label_embeddings_ = []\n",
    "        for e in self.target_label_embeddings:\n",
    "            input = torch.tensor([0], device=x.device)\n",
    "            temp = e(input)\n",
    "            temp = temp.repeat(x.size(0), 1)\n",
    "            tmep = temp.unsqueeze(1)\n",
    "            target_label_embeddings_.append(temp)\n",
    "\n",
    "        class_embeddings = torch.stack(target_label_embeddings_, dim=1)\n",
    "\n",
    "        context = torch.stack(embeddings, dim=1)\n",
    "\n",
    "        return class_embeddings, context\n",
    "\n",
    "class classificationHead(nn.Module):\n",
    "    def __init__(self, embed_size, dropout, mlp_scale_classification, num_target_classes):\n",
    "        super(classificationHead, self).__init__()\n",
    "        \n",
    "        #flattening the embeddings out so each sample in batch is represented with a 460 dimensional vector\n",
    "        self.input = embed_size\n",
    "        self.lin1 = nn.Linear(self.input, mlp_scale_classification*self.input)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.lin2 = nn.Linear(mlp_scale_classification*self.input, mlp_scale_classification*self.input)\n",
    "        self.lin3 = nn.Linear(mlp_scale_classification*self.input, self.input)\n",
    "        self.lin4 = nn.Linear(self.input, num_target_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.initialize_weights()\n",
    "\n",
    "    def initialize_weights(self): #he_initialization.\n",
    "        torch.nn.init.kaiming_normal_(self.lin1.weight, nonlinearity='relu')\n",
    "        torch.nn.init.zeros_(self.lin1.bias)\n",
    "\n",
    "        torch.nn.init.kaiming_normal_(self.lin3.weight, nonlinearity='relu')\n",
    "        torch.nn.init.zeros_(self.lin3.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x= torch.reshape(x, (-1, self.input))\n",
    "\n",
    "        x = self.lin1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.lin2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.lin3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.lin4(x)\n",
    "  \n",
    "        return x\n",
    "\n",
    "\n",
    "# DEFAULT PARAMETERS SET UP FOR VPN DATASET. BE CAREFUL AND MAKE SURE YOU SET THEM UP HOW YOU WANT.\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, \n",
    "                 rff_on = False,\n",
    "                 sigma=4,\n",
    "                 embed_size=20,\n",
    "                 input_size=1,\n",
    "                 embedding_dropout = 0,\n",
    "                 n_features=23, # YOU WILL PROBABLY NEED TO CHANGE\n",
    "                 num_layers=1,\n",
    "                 heads=1,\n",
    "                 forward_expansion=4, # Determines how wide the MLP is in the encoder. Its a scaling factor. \n",
    "                 decoder_dropout=0,\n",
    "                 classification_dropout = 0,\n",
    "                 pre_norm_on = False,\n",
    "                 mlp_scale_classification = 4,\n",
    "                 targets_classes : list=  [3,8]\n",
    "                 ):\n",
    "        super(Classifier, self).__init__()\n",
    "\n",
    "        self.embeddings = Embeddings(rff_on=rff_on, sigma=sigma, embed_size=embed_size, input_size=input_size, embedding_dropout=embedding_dropout, n_features=n_features, num_target_labels=len(targets_classes))\n",
    "        self.decoder = Decoder(embed_size=embed_size, num_layers=num_layers, heads=heads, forward_expansion=forward_expansion, decoder_dropout=decoder_dropout, pre_norm_on=pre_norm_on)\n",
    "        self.classifying_heads = nn.ModuleList([classificationHead(embed_size=embed_size, dropout=classification_dropout, mlp_scale_classification=mlp_scale_classification, num_target_classes=x) for x in targets_classes])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        class_embed, context = self.embeddings(x)\n",
    "\n",
    "        x = self.decoder(class_embed, context)\n",
    "        \n",
    "        probability_dist_raw = []\n",
    "        for i, e in enumerate(self.classifying_heads):\n",
    "            input = x[:, i,:]\n",
    "            output = e(input)\n",
    "            probability_dist_raw.append(output)\n",
    "        \n",
    "        return probability_dist_raw\n",
    "\n",
    "# Training and Testing Loops\n",
    "def train(dataloader, model, loss_function, optimizer, device_in_use):\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "\n",
    "    total_correct_1 = 0\n",
    "    total_samples_1 = 0\n",
    "    all_targets_1 = []\n",
    "    all_predictions_1 = []\n",
    "\n",
    "    total_correct_2 = 0\n",
    "    total_samples_2 = 0\n",
    "    all_targets_2 = []\n",
    "    all_predictions_2 = []\n",
    "\n",
    "    for (features,labels_task1) in dataloader:\n",
    "        features,labels_task1 = features.to(device_in_use),labels_task1.to(device_in_use)\n",
    "\n",
    "\n",
    "        task_predictions = model(features) #contains a list of the tensor outputs for each task\n",
    "\n",
    "        loss = loss_function(task_predictions, labels_task1)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        #computing accuracy for first target\n",
    "        y_pred_softmax_1 = torch.softmax(task_predictions[0], dim=1)\n",
    "        _, y_pred_labels_1 = torch.max(y_pred_softmax_1, dim=1)\n",
    "        total_correct_1 += (y_pred_labels_1 == labels_task1).sum().item()\n",
    "        total_samples_1 += labels_task1.size(0)\n",
    "        all_targets_1.extend(labels_task1.cpu().numpy())\n",
    "        all_predictions_1.extend(y_pred_labels_1.cpu().numpy())\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    avg_loss = total_loss/len(dataloader)\n",
    "    accuracy_1 = total_correct_1 / total_samples_1\n",
    "    # accuracy_2 = total_correct_2 / total_samples_2\n",
    "\n",
    "    # # precision = precision_score(all_targets, all_predictions, average='weighted')\n",
    "    # recall = recall_score(all_targets, all_predictions, average='weighted')\n",
    "    # f1 = f1_score(all_targets, all_predictions, average='weighted')\n",
    "\n",
    "    return avg_loss, accuracy_1\n",
    "\n",
    "def test(dataloader, model, loss_function, device_in_use):\n",
    "  model.eval()\n",
    "  total_loss = 0\n",
    "  \n",
    "  total_correct_1 = 0\n",
    "  total_samples_1 = 0\n",
    "  all_targets_1 = []\n",
    "  all_predictions_1 = []\n",
    "\n",
    "  total_correct_2 = 0\n",
    "  total_samples_2 = 0\n",
    "  all_targets_2 = []\n",
    "  all_predictions_2 = []\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for (features,labels_task1) in dataloader:\n",
    "      features,labels_task1 = features.to(device_in_use),labels_task1.to(device_in_use)\n",
    "\n",
    "      #compute prediction error\n",
    "      task_predictions = model(features) #contains a list of the tensor outputs for each task\n",
    "\n",
    "      loss = loss_function(task_predictions, labels_task1)\n",
    "      total_loss += loss.item()\n",
    "\n",
    "      #computing accuracy for first target\n",
    "      y_pred_softmax_1 = torch.softmax(task_predictions[0], dim=1)\n",
    "      _, y_pred_labels_1 = torch.max(y_pred_softmax_1, dim=1)\n",
    "      total_correct_1 += (y_pred_labels_1 == labels_task1).sum().item()\n",
    "      total_samples_1 += labels_task1.size(0)\n",
    "      all_targets_1.extend(labels_task1.cpu().numpy())\n",
    "      all_predictions_1.extend(y_pred_labels_1.cpu().numpy())\n",
    "\n",
    "  avg = total_loss/len(dataloader)\n",
    "  accuracy_1 = total_correct_1 / total_samples_1\n",
    "  # accuracy_2 = total_correct_2 / total_samples_2\n",
    "  # recall = recall_score(all_targets, all_predictions, average='weighted')\n",
    "  f1_1 = f1_score(all_targets_1, all_predictions_1, average='weighted')\n",
    "  # f1_2 = f1_score(all_targets_2, all_predictions_2, average=\"weighted\")\n",
    "\n",
    "  return avg, accuracy_1, all_predictions_1, all_targets_1, f1_1\n",
    "\n",
    "def format_metric(value): # Used to format the metrics output\n",
    "    return f\"{value:.4f}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [ 1/100]       | Train: Loss 0.7400, Accuracy 0.6882                               | Test: Loss 0.6410, Accuracy 0.7306, F1 0.7159\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\smbm2\\projects\\CAT-Transformer\\experiments\\covertype\\CAT_final_test.ipynb Cell 3\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/experiments/covertype/CAT_final_test.ipynb#W1sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mprocess_time()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/experiments/covertype/CAT_final_test.ipynb#W1sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/experiments/covertype/CAT_final_test.ipynb#W1sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m   train_loss, train_accuracy_1 \u001b[39m=\u001b[39m train(train_dataloader, model, loss_functions, optimizer, device_in_use\u001b[39m=\u001b[39;49mdevice_in_use)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/experiments/covertype/CAT_final_test.ipynb#W1sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m   test_loss, test_accuracy_1, all_predictions_1, all_targets_1, f1_1 \u001b[39m=\u001b[39m test(test_dataloader, model, loss_functions, device_in_use\u001b[39m=\u001b[39mdevice_in_use)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/experiments/covertype/CAT_final_test.ipynb#W1sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m   train_losses\u001b[39m.\u001b[39mappend(train_loss)\n",
      "\u001b[1;32mc:\\Users\\smbm2\\projects\\CAT-Transformer\\experiments\\covertype\\CAT_final_test.ipynb Cell 3\u001b[0m line \u001b[0;36m2\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/experiments/covertype/CAT_final_test.ipynb#W1sZmlsZQ%3D%3D?line=278'>279</a>\u001b[0m \u001b[39mfor\u001b[39;00m (features,labels_task1) \u001b[39min\u001b[39;00m dataloader:\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/experiments/covertype/CAT_final_test.ipynb#W1sZmlsZQ%3D%3D?line=279'>280</a>\u001b[0m     features,labels_task1 \u001b[39m=\u001b[39m features\u001b[39m.\u001b[39mto(device_in_use),labels_task1\u001b[39m.\u001b[39mto(device_in_use)\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/experiments/covertype/CAT_final_test.ipynb#W1sZmlsZQ%3D%3D?line=282'>283</a>\u001b[0m     task_predictions \u001b[39m=\u001b[39m model(features) \u001b[39m#contains a list of the tensor outputs for each task\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/experiments/covertype/CAT_final_test.ipynb#W1sZmlsZQ%3D%3D?line=284'>285</a>\u001b[0m     loss \u001b[39m=\u001b[39m loss_function(task_predictions, labels_task1)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/experiments/covertype/CAT_final_test.ipynb#W1sZmlsZQ%3D%3D?line=285'>286</a>\u001b[0m     total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\smbm2\\AppData\\Local\\miniconda3\\envs\\ml-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mc:\\Users\\smbm2\\projects\\CAT-Transformer\\experiments\\covertype\\CAT_final_test.ipynb Cell 3\u001b[0m line \u001b[0;36m2\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/experiments/covertype/CAT_final_test.ipynb#W1sZmlsZQ%3D%3D?line=249'>250</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/experiments/covertype/CAT_final_test.ipynb#W1sZmlsZQ%3D%3D?line=250'>251</a>\u001b[0m     class_embed, context \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membeddings(x)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/experiments/covertype/CAT_final_test.ipynb#W1sZmlsZQ%3D%3D?line=252'>253</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder(class_embed, context)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/experiments/covertype/CAT_final_test.ipynb#W1sZmlsZQ%3D%3D?line=254'>255</a>\u001b[0m     probability_dist_raw \u001b[39m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\smbm2\\AppData\\Local\\miniconda3\\envs\\ml-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mc:\\Users\\smbm2\\projects\\CAT-Transformer\\experiments\\covertype\\CAT_final_test.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/experiments/covertype/CAT_final_test.ipynb#W1sZmlsZQ%3D%3D?line=159'>160</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, r \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrffs):\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/experiments/covertype/CAT_final_test.ipynb#W1sZmlsZQ%3D%3D?line=160'>161</a>\u001b[0m     \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m x[:,i,:]\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/experiments/covertype/CAT_final_test.ipynb#W1sZmlsZQ%3D%3D?line=161'>162</a>\u001b[0m     out \u001b[39m=\u001b[39m r(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/experiments/covertype/CAT_final_test.ipynb#W1sZmlsZQ%3D%3D?line=162'>163</a>\u001b[0m     rff_vectors\u001b[39m.\u001b[39mappend(out)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/experiments/covertype/CAT_final_test.ipynb#W1sZmlsZQ%3D%3D?line=164'>165</a>\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstack(rff_vectors, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\smbm2\\AppData\\Local\\miniconda3\\envs\\ml-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\smbm2\\AppData\\Local\\miniconda3\\envs\\ml-env\\Lib\\site-packages\\rff\\layers.py:47\u001b[0m, in \u001b[0;36mGaussianEncoding.forward\u001b[1;34m(self, v)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, v: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m     39\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Computes :math:`\\gamma(\\mathbf{v}) = (\\cos{2 \\pi \\mathbf{B} \\mathbf{v}} , \\sin{2 \\pi \\mathbf{B} \\mathbf{v}})`\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \n\u001b[0;32m     41\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[39m        Tensor: Tensor mapping using random fourier features of shape :math:`(N, *, 2 \\cdot \\text{encoded_size})`\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 47\u001b[0m     \u001b[39mreturn\u001b[39;00m rff\u001b[39m.\u001b[39;49mfunctional\u001b[39m.\u001b[39;49mgaussian_encoding(v, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mb)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = Classifier(\n",
    "                    n_features=len(cont_columns),\n",
    "                    targets_classes=target_classes,\n",
    "                    rff_on=True,\n",
    "                   sigma=2,\n",
    "                   embed_size=160,\n",
    "                   num_layers=1,\n",
    "                   heads=5,\n",
    "                   forward_expansion=8,\n",
    "                   pre_norm_on=False,\n",
    "                   mlp_scale_classification=8,\n",
    "                   embedding_dropout=0.1,\n",
    "                   decoder_dropout=0,\n",
    "                   classification_dropout=0.1\n",
    "                   ).to(device_in_use) # Instantiate the model\n",
    "loss_functions = UncertaintyLoss(1)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr = 0.0001) \n",
    "epochs = 100 #Set the number of epochs\n",
    "\n",
    "train_losses = []\n",
    "train_accuracies_1 = [] \n",
    "test_losses = []\n",
    "test_accuracies_1 = [] \n",
    "test_f1_scores = [] \n",
    "\n",
    "\n",
    "#Time will be recorded for all 100 epochs - This means the results will not be comparable to Xgboost but that is ok, we will only compare between transformer models who will also train for 100 epochs\n",
    "start_time = time.process_time()\n",
    "\n",
    "for t in range(epochs):\n",
    "  train_loss, train_accuracy_1 = train(train_dataloader, model, loss_functions, optimizer, device_in_use=device_in_use)\n",
    "  test_loss, test_accuracy_1, all_predictions_1, all_targets_1, f1_1 = test(test_dataloader, model, loss_functions, device_in_use=device_in_use)\n",
    "  train_losses.append(train_loss)\n",
    "  train_accuracies_1.append(train_accuracy_1)\n",
    "  # train_accuracies_2.append(train_accuracy_2)\n",
    "  # train_recalls.append(train_recall) \n",
    "  # train_f1_scores.append(train_f1)\n",
    "  test_losses.append(test_loss)\n",
    "  test_accuracies_1.append(test_accuracy_1)\n",
    "  # test_accuracies_2.append(test_accuracy_2)\n",
    "  # test_recalls.append(test_recall)\n",
    "  test_f1_scores.append(f1_1)\n",
    "  # Formatting for easier reading\n",
    "  epoch_str = f\"Epoch [{t+1:2}/{epochs}]\"\n",
    "  train_metrics = f\"Train: Loss {format_metric(train_loss)}, Accuracy {format_metric(train_accuracy_1)}\"\n",
    "  test_metrics = f\"Test: Loss {format_metric(test_loss)}, Accuracy {format_metric(test_accuracy_1)}, F1 {format_metric(f1_1)}\"\n",
    "  print(f\"{epoch_str:20} | {train_metrics:65} | {test_metrics}\")\n",
    "\n",
    "total_time = time.process_time() - start_time\n",
    "\n",
    "# Plotting the loss curves\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, epochs+1), train_losses, label='Train Loss')\n",
    "plt.plot(range(1, epochs+1), [l for l in test_losses], label='Test Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Test Loss Curve')\n",
    "plt.legend()\n",
    "\n",
    "# Plotting the accuracy curves\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, epochs+1), train_accuracies_1, label='Train Accuracy')\n",
    "plt.plot(range(1, epochs+1), test_accuracies_1, label='Test Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Test Accuracy Curve')\n",
    "plt.legend()\n",
    "\n",
    "# Display confusion matrix for the first task (Traffic Type) on test data\n",
    "conf_matrix_1 = confusion_matrix(all_targets_1, all_predictions_1)\n",
    "print(\"Confusion Matrix for income\")\n",
    "print(conf_matrix_1)\n",
    "\n",
    "best_index = test_accuracies_1.index(max(test_accuracies_1))\n",
    "print(f\"Best accuracy {test_accuracies_1[best_index]}\")\n",
    "print(f\"Best F1 {test_f1_scores[best_index]}\")\n",
    "print(f\"100 epochs of training and evaluation took, {total_time}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
