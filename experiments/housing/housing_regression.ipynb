{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\prime\\anaconda3\\envs\\DL\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from rff.layers import GaussianEncoding #pip install random-fourier-features-pytorch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import os\n",
    "import optuna\n",
    "from optuna.trial import TrialState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is not available, using CPU instead\n"
     ]
    }
   ],
   "source": [
    "# Run regardless if you do or do not have GPU so all tensors are moved to right location later on\n",
    "if torch.cuda.is_available():\n",
    "    device_in_use = torch.device(\"cuda\")\n",
    "    print(\"GPU is available and being used\")\n",
    "else:\n",
    "    device_in_use = torch.device(\"cpu\")\n",
    "    print(\"GPU is not available, using CPU instead\")\n",
    "\n",
    "device_in_use='cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD AND PROCESS DATA\n",
    "**EXAMPLE WITH COVTYPE DATASET**\n",
    "1. Standardize or perform quantile transformations to numerical/continuous features.\n",
    "1. Wrap with Dataset and Dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>housing_median_age</th>\n",
       "      <th>total_rooms</th>\n",
       "      <th>total_bedrooms</th>\n",
       "      <th>population</th>\n",
       "      <th>households</th>\n",
       "      <th>median_income</th>\n",
       "      <th>ocean_proximity</th>\n",
       "      <th>median_house_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.261189</td>\n",
       "      <td>-0.764261</td>\n",
       "      <td>-0.358478</td>\n",
       "      <td>0.584211</td>\n",
       "      <td>0.643412</td>\n",
       "      <td>0.431828</td>\n",
       "      <td>0.688976</td>\n",
       "      <td>-0.627034</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.786025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.906340</td>\n",
       "      <td>1.384381</td>\n",
       "      <td>0.355019</td>\n",
       "      <td>0.401362</td>\n",
       "      <td>0.681233</td>\n",
       "      <td>0.077837</td>\n",
       "      <td>0.741252</td>\n",
       "      <td>-0.993331</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.764461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.425748</td>\n",
       "      <td>1.000528</td>\n",
       "      <td>1.861291</td>\n",
       "      <td>-0.415056</td>\n",
       "      <td>-0.361193</td>\n",
       "      <td>-0.698186</td>\n",
       "      <td>-0.372218</td>\n",
       "      <td>-0.104586</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.523613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.581963</td>\n",
       "      <td>-0.689363</td>\n",
       "      <td>0.196464</td>\n",
       "      <td>0.115662</td>\n",
       "      <td>0.258117</td>\n",
       "      <td>0.173441</td>\n",
       "      <td>0.362254</td>\n",
       "      <td>-0.108109</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.104704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.936559</td>\n",
       "      <td>-0.736174</td>\n",
       "      <td>0.037909</td>\n",
       "      <td>-0.620761</td>\n",
       "      <td>-0.599934</td>\n",
       "      <td>-0.179689</td>\n",
       "      <td>-0.476769</td>\n",
       "      <td>-0.674510</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.630765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12254</th>\n",
       "      <td>0.806707</td>\n",
       "      <td>-0.904695</td>\n",
       "      <td>-0.358478</td>\n",
       "      <td>0.163202</td>\n",
       "      <td>-0.086995</td>\n",
       "      <td>0.010657</td>\n",
       "      <td>-0.118681</td>\n",
       "      <td>0.814126</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.569055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12255</th>\n",
       "      <td>1.021462</td>\n",
       "      <td>-0.885971</td>\n",
       "      <td>-1.864750</td>\n",
       "      <td>1.468283</td>\n",
       "      <td>1.177626</td>\n",
       "      <td>1.579069</td>\n",
       "      <td>1.313670</td>\n",
       "      <td>0.481794</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.402187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12256</th>\n",
       "      <td>0.581963</td>\n",
       "      <td>-0.768942</td>\n",
       "      <td>1.068516</td>\n",
       "      <td>-0.475396</td>\n",
       "      <td>-0.396649</td>\n",
       "      <td>-0.407070</td>\n",
       "      <td>-0.356535</td>\n",
       "      <td>-0.410684</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.103841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12257</th>\n",
       "      <td>-1.225976</td>\n",
       "      <td>0.897543</td>\n",
       "      <td>-1.309808</td>\n",
       "      <td>1.410229</td>\n",
       "      <td>1.246175</td>\n",
       "      <td>1.732379</td>\n",
       "      <td>1.460041</td>\n",
       "      <td>0.740152</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.205055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12258</th>\n",
       "      <td>-1.420754</td>\n",
       "      <td>0.972441</td>\n",
       "      <td>1.861291</td>\n",
       "      <td>0.442503</td>\n",
       "      <td>0.475584</td>\n",
       "      <td>0.728113</td>\n",
       "      <td>0.388392</td>\n",
       "      <td>0.006980</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.382742</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12259 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
       "0       1.261189 -0.764261           -0.358478     0.584211        0.643412   \n",
       "1      -0.906340  1.384381            0.355019     0.401362        0.681233   \n",
       "2      -1.425748  1.000528            1.861291    -0.415056       -0.361193   \n",
       "3       0.581963 -0.689363            0.196464     0.115662        0.258117   \n",
       "4       0.936559 -0.736174            0.037909    -0.620761       -0.599934   \n",
       "...          ...       ...                 ...          ...             ...   \n",
       "12254   0.806707 -0.904695           -0.358478     0.163202       -0.086995   \n",
       "12255   1.021462 -0.885971           -1.864750     1.468283        1.177626   \n",
       "12256   0.581963 -0.768942            1.068516    -0.475396       -0.396649   \n",
       "12257  -1.225976  0.897543           -1.309808     1.410229        1.246175   \n",
       "12258  -1.420754  0.972441            1.861291     0.442503        0.475584   \n",
       "\n",
       "       population  households  median_income  ocean_proximity  \\\n",
       "0        0.431828    0.688976      -0.627034              1.0   \n",
       "1        0.077837    0.741252      -0.993331              1.0   \n",
       "2       -0.698186   -0.372218      -0.104586              3.0   \n",
       "3        0.173441    0.362254      -0.108109              0.0   \n",
       "4       -0.179689   -0.476769      -0.674510              1.0   \n",
       "...           ...         ...            ...              ...   \n",
       "12254    0.010657   -0.118681       0.814126              0.0   \n",
       "12255    1.579069    1.313670       0.481794              0.0   \n",
       "12256   -0.407070   -0.356535      -0.410684              0.0   \n",
       "12257    1.732379    1.460041       0.740152              3.0   \n",
       "12258    0.728113    0.388392       0.006980              3.0   \n",
       "\n",
       "       median_house_value  \n",
       "0               -0.786025  \n",
       "1               -0.764461  \n",
       "2                2.523613  \n",
       "3                1.104704  \n",
       "4               -0.630765  \n",
       "...                   ...  \n",
       "12254            0.569055  \n",
       "12255           -0.402187  \n",
       "12256            1.103841  \n",
       "12257            0.205055  \n",
       "12258            0.382742  \n",
       "\n",
       "[12259 rows x 10 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_train = pd.read_csv('/home/cscadmin/CyberResearch/CAT-Transformer/datasets/income/train.csv')\n",
    "# df_test = pd.read_csv('/home/cscadmin/CyberResearch/CAT-Transformer/datasets/income/test.csv')\n",
    "# df_val = pd.read_csv('/home/cscadmin/CyberResearch/CAT-Transformer/datasets/income/validation.csv') #READ FROM RIGHT SPOT\n",
    "\n",
    "df_train = pd.read_csv('./data/train.csv')\n",
    "df_test = pd.read_csv('./data/test.csv')\n",
    "df_val = pd.read_csv('./data/validation.csv') #READ FROM RIGHT SPOT\n",
    "\n",
    "#Take a look at what the datasets look like initially to get an idea\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['longitude', 'latitude', 'housing_median_age', 'total_rooms',\n",
       "       'total_bedrooms', 'population', 'households', 'median_income',\n",
       "       'ocean_proximity', 'median_house_value'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Take a look at the feature names\n",
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_columns = ['longitude', 'latitude', 'housing_median_age', 'total_rooms',\n",
    "       'total_bedrooms', 'population', 'households', 'median_income']\n",
    "target = ['median_house_value']\n",
    "\n",
    "#CHECKING TO MAKE SURE YOUR LIST IS CORRECT (NO NEED TO TOUCH)\n",
    "yourlist = cont_columns + target\n",
    "yourlist.sort()\n",
    "oglist = list(df_train.columns)\n",
    "oglist.sort()\n",
    "\n",
    "# assert(yourlist == oglist), \"You may of spelled feature name wrong or you forgot to put on of them in the list\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3420]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Getting the number of classes in your classification target\n",
    "target_classes = [max(len(df_train[target].value_counts()), len(df_val[target].value_counts()),len(df_test[target].value_counts()))]\n",
    "target_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a StandardScaler and fit it to the cont features\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(df_train[cont_columns])\n",
    "\n",
    "# Transform the training, test, and validation datasets\n",
    "df_train[cont_columns] = scaler.transform(df_train[cont_columns])\n",
    "df_test[cont_columns] = scaler.transform(df_test[cont_columns])\n",
    "df_val[cont_columns] = scaler.transform(df_val[cont_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleTaskDataset(Dataset):\n",
    "    def __init__(self, df : pd.DataFrame, num_columns,task1_column):\n",
    "        self.n = df.shape[0]\n",
    "        \n",
    "        self.task1_labels = df[task1_column].astype(np.float32).values\n",
    "\n",
    "        self.num = df[num_columns].astype(np.float32).values\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Retrieve features and labels from the dataframe using column names\n",
    "        num_features = self.num[idx]\n",
    "        labels_task1 = self.task1_labels[idx]\n",
    "\n",
    "        return num_features, labels_task1\n",
    "\n",
    "#Wrapping in Dataset\n",
    "train_dataset = SingleTaskDataset(df_train, cont_columns, 'median_house_value')\n",
    "val_dataset = SingleTaskDataset(df_val, cont_columns, 'median_house_value')\n",
    "test_dataset = SingleTaskDataset(df_test, cont_columns, 'median_house_value')\n",
    "\n",
    "#This is a hyperparameter that is not tuned. Maybe mess with what makes sense here\n",
    "#Also try looking to see what other papers have done\n",
    "batch_size = 256\n",
    "\n",
    "# Wrapping with DataLoader for easy batch extraction\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL AND HELPERS\n",
    "\n",
    "1. All you should have to do is interact with Classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each task loss is scaled by its own learnable parameter, then regularization is applied \n",
    "class UncertaintyLoss(nn.Module):\n",
    "    def __init__(self, num_tasks):\n",
    "        super(UncertaintyLoss, self).__init__()\n",
    "        self.num_tasks = num_tasks\n",
    "\n",
    "        self.loss_fns = [nn.MSELoss() for x in range(num_tasks)] \n",
    "\n",
    "    def forward(self, predictions, labels_task1):\n",
    "\n",
    "        #task 1\n",
    "        target = labels_task1\n",
    "        prediction = predictions\n",
    "        loss_fn = self.loss_fns[0]\n",
    "        task_loss = loss_fn(prediction, target)\n",
    "        \n",
    "        return task_loss\n",
    "    \n",
    "#All layers of the model\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_size, heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        self.head_dim = embed_size // heads\n",
    "        assert(self.head_dim * heads == embed_size), \"Embed size needs to be div by heads\"\n",
    "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.keys =nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.fc_out = nn.Linear(heads*self.head_dim, embed_size)\n",
    "\n",
    "\n",
    "    def forward(self, values, keys, query):\n",
    "        N = query.shape[0]\n",
    "        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
    "\n",
    "        values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
    "        keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n",
    "        queries = query.reshape(N, query_len, self.heads, self.head_dim)\n",
    "\n",
    "        values = self.values(values)\n",
    "        keys = self.keys(keys)\n",
    "        queries = self.queries(queries)\n",
    "\n",
    "        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
    "\n",
    "        attention = torch.softmax(energy / (self.embed_size ** (1/2)), dim=3) #(batch_size, head_dim, #query_embeddings, #key_embeddings)\n",
    "\n",
    "        # Calculate simplified attention scores\n",
    "        avg_attention = attention.mean(dim=0)  # Average across batches\n",
    "        # print(\"batch average\", avg_attention.shape)\n",
    "        avg_attention = avg_attention.mean(dim=0).squeeze(dim=0)\n",
    "        # print(\"head average\", avg_attention.shape)\n",
    "\n",
    "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(N, query_len, self.heads*self.head_dim) #(batch_size, n_features, embed_size)\n",
    "        out = self.fc_out(out)\n",
    "\n",
    "        return out, avg_attention\n",
    "    \n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_size, heads, dropout, forward_expansion, pre_norm_on):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "\n",
    "        self.pre_norm_on = pre_norm_on\n",
    "        if self.pre_norm_on:\n",
    "            self.pre_norm = nn.LayerNorm(embed_size)\n",
    "        self.attention = MultiHeadAttention(embed_size, heads)\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "\n",
    "        self.feed_forward = nn.Sequential(nn.Linear(embed_size, forward_expansion*embed_size),\n",
    "                                          nn.ReLU(),\n",
    "                                          nn.Linear(forward_expansion*embed_size, embed_size)\n",
    "                                          )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self,value,key,query):\n",
    "        if self.pre_norm_on:\n",
    "            query = self.pre_norm(query)\n",
    "            key = self.pre_norm(key)\n",
    "            value = self.pre_norm(value)\n",
    "            \n",
    "        attention, avg_attention = self.attention(value, key, query)\n",
    "        x = self.dropout(self.norm1(attention + query))\n",
    "        forward = self.feed_forward(x)\n",
    "        out = self.dropout(self.norm2(forward + x))\n",
    "        return out, avg_attention\n",
    "    \n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, embed_size, heads, forward_expansion, dropout, pre_norm_on):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "\n",
    "        self.attention = MultiHeadAttention(embed_size, heads)\n",
    "        self.norm = nn.LayerNorm(embed_size)\n",
    "        self.transformer_block = TransformerBlock(embed_size, heads, dropout, forward_expansion, pre_norm_on)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, value, key):\n",
    "        out, avg_attention = self.transformer_block(value, key, x)\n",
    "\n",
    "        return out, avg_attention\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 embed_size,\n",
    "                 num_layers,\n",
    "                 heads,\n",
    "                 forward_expansion,\n",
    "                 decoder_dropout,\n",
    "                 pre_norm_on\n",
    "    ):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "                [\n",
    "                    DecoderBlock(\n",
    "                        embed_size,\n",
    "                        heads,\n",
    "                        dropout=decoder_dropout,\n",
    "                        forward_expansion=forward_expansion,\n",
    "                        pre_norm_on=pre_norm_on\n",
    "                    )\n",
    "                    for _ in range(num_layers)\n",
    "                ]\n",
    "            )\n",
    "        self.avg_attention = None\n",
    "\n",
    "    def forward(self, class_embed, context):\n",
    "        for layer in self.layers:\n",
    "            # x is the classification embedding (CLS Token)\n",
    "            # context are the feature embeddings that will be used as key and value\n",
    "            x, self.avg_attention = layer(class_embed, context, context)\n",
    "  \n",
    "        return x \n",
    "\n",
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, sigma, embed_size, input_size, embedding_dropout, n_features, num_target_labels, rff_on):\n",
    "        super(Embeddings, self).__init__()\n",
    "\n",
    "        self.rff_on = rff_on\n",
    "\n",
    "        if self.rff_on:\n",
    "            self.rffs = nn.ModuleList([GaussianEncoding(sigma=sigma, input_size=input_size, encoded_size=embed_size//2) for _ in range(n_features)])\n",
    "            self.dropout = nn.Dropout(embedding_dropout)\n",
    "            self.mlp_in = embed_size\n",
    "        else:\n",
    "            self.mlp_in = input_size\n",
    "\n",
    "        self.embeddings = nn.ModuleList([nn.Linear(in_features=self.mlp_in, out_features=embed_size) for _ in range(n_features)])\n",
    "\n",
    "        # Classifcation Embeddings for each target label\n",
    "        self.target_label_embeddings = nn.ModuleList([nn.Embedding(1, embed_size) for _ in range(num_target_labels)])\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(2) #(batch_size, n_features) -> (batch_size, n_features, 1)\n",
    "        rff_vectors = []\n",
    "        if self.rff_on:\n",
    "            for i, r in enumerate(self.rffs):\n",
    "                input = x[:,i,:]\n",
    "                out = r(input)\n",
    "                rff_vectors.append(out)\n",
    "        \n",
    "            x = torch.stack(rff_vectors, dim=1)\n",
    "        \n",
    "        embeddings = []\n",
    "        for i, e in enumerate(self.embeddings):\n",
    "            goin_in = x[:,i,:]\n",
    "            goin_out = e(goin_in)\n",
    "            embeddings.append(goin_out)\n",
    "\n",
    "        target_label_embeddings_ = []\n",
    "        for e in self.target_label_embeddings:\n",
    "            input = torch.tensor([0], device=x.device)\n",
    "            temp = e(input)\n",
    "            temp = temp.repeat(x.size(0), 1)\n",
    "            tmep = temp.unsqueeze(1)\n",
    "            target_label_embeddings_.append(temp)\n",
    "\n",
    "        class_embeddings = torch.stack(target_label_embeddings_, dim=1)\n",
    "\n",
    "        print(len(embeddings))\n",
    "\n",
    "        context = torch.stack(embeddings, dim=1)\n",
    "\n",
    "        return class_embeddings, context\n",
    "\n",
    "class classificationHead(nn.Module):\n",
    "    def __init__(self, embed_size, dropout, mlp_scale_classification, num_target_classes):\n",
    "        super(classificationHead, self).__init__()\n",
    "        \n",
    "        #flattening the embeddings out so each sample in batch is represented with a 460 dimensional vector\n",
    "        self.input = embed_size\n",
    "        self.lin1 = nn.Linear(self.input, mlp_scale_classification*self.input)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.lin2 = nn.Linear(mlp_scale_classification*self.input, mlp_scale_classification*self.input)\n",
    "        self.lin3 = nn.Linear(mlp_scale_classification*self.input, self.input)\n",
    "        self.lin4 = nn.Linear(self.input, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.initialize_weights()\n",
    "\n",
    "    def initialize_weights(self): #he_initialization.\n",
    "        torch.nn.init.kaiming_normal_(self.lin1.weight, nonlinearity='relu')\n",
    "        torch.nn.init.zeros_(self.lin1.bias)\n",
    "\n",
    "        torch.nn.init.kaiming_normal_(self.lin3.weight, nonlinearity='relu')\n",
    "        torch.nn.init.zeros_(self.lin3.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x= torch.reshape(x, (-1, self.input))\n",
    "\n",
    "        x = self.lin1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.lin2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.lin3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.lin4(x)\n",
    "  \n",
    "        return x\n",
    "    \n",
    "# class classificationHead(nn.Module):\n",
    "#     def __init__(self, embed_size, dropout, mlp_scale_classification, num_target_classes):\n",
    "#         super(classificationHead, self).__init__()\n",
    "        \n",
    "#         #flattening the embeddings out so each sample in batch is represented with a 460 dimensional vector\n",
    "#         self.input = embed_size\n",
    "#         self.lin1 = nn.Linear(self.input, mlp_scale_classification*self.input)\n",
    "#         self.drop = nn.Dropout(dropout)\n",
    "#         self.lin2 = nn.Linear(mlp_scale_classification*self.input, mlp_scale_classification*self.input)\n",
    "#         self.lin3 = nn.Linear(mlp_scale_classification*self.input, self.input)\n",
    "#         self.lin4 = nn.Linear(self.input, num_target_classes)\n",
    "#         self.relu = nn.ReLU()\n",
    "#         self.initialize_weights()\n",
    "\n",
    "#     def initialize_weights(self): #he_initialization.\n",
    "#         torch.nn.init.kaiming_normal_(self.lin1.weight, nonlinearity='relu')\n",
    "#         torch.nn.init.zeros_(self.lin1.bias)\n",
    "\n",
    "#         torch.nn.init.kaiming_normal_(self.lin3.weight, nonlinearity='relu')\n",
    "#         torch.nn.init.zeros_(self.lin3.bias)\n",
    "\n",
    "#     def forward(self, x):\n",
    "\n",
    "#         x= torch.reshape(x, (-1, self.input))\n",
    "\n",
    "#         x = self.lin1(x)\n",
    "#         x = self.relu(x)\n",
    "#         x = self.drop(x)\n",
    "#         x = self.lin2(x)\n",
    "#         x = self.relu(x)\n",
    "#         x = self.drop(x)\n",
    "#         x = self.lin3(x)\n",
    "#         x = self.relu(x)\n",
    "#         x = self.drop(x)\n",
    "#         x = self.lin4(x)\n",
    "  \n",
    "#         return x\n",
    "\n",
    "\n",
    "# DEFAULT PARAMETERS SET UP FOR VPN DATASET. BE CAREFUL AND MAKE SURE YOU SET THEM UP HOW YOU WANT.\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, \n",
    "                 rff_on = False,\n",
    "                 sigma=4,\n",
    "                 embed_size=20,\n",
    "                 input_size=1,\n",
    "                 embedding_dropout = 0,\n",
    "                 n_features=23, # YOU WILL PROBABLY NEED TO CHANGE\n",
    "                 num_layers=1,\n",
    "                 heads=1,\n",
    "                 forward_expansion=4, # Determines how wide the MLP is in the encoder. Its a scaling factor. \n",
    "                 decoder_dropout=0,\n",
    "                 classification_dropout = 0,\n",
    "                 pre_norm_on = False,\n",
    "                 mlp_scale_classification = 4,\n",
    "                 targets_classes : list=  [3,8]\n",
    "                 ):\n",
    "        super(Classifier, self).__init__()\n",
    "\n",
    "        self.embeddings = Embeddings(rff_on=rff_on, sigma=sigma, embed_size=embed_size, input_size=input_size, embedding_dropout=embedding_dropout, n_features=n_features, num_target_labels=len(targets_classes))\n",
    "        self.decoder = Decoder(embed_size=embed_size, num_layers=num_layers, heads=heads, forward_expansion=forward_expansion, decoder_dropout=decoder_dropout, pre_norm_on=pre_norm_on)\n",
    "        self.classifying_heads = nn.ModuleList([classificationHead(embed_size=embed_size, dropout=classification_dropout, mlp_scale_classification=mlp_scale_classification, num_target_classes=x) for x in targets_classes])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        class_embed, context = self.embeddings(x)\n",
    "\n",
    "        x = self.decoder(class_embed, context)\n",
    "        \n",
    "        probability_dist_raw = []\n",
    "        for i, e in enumerate(self.classifying_heads):\n",
    "            input = x[:, i,:]\n",
    "            output = e(input)\n",
    "            probability_dist_raw.append(output)\n",
    "        \n",
    "        return probability_dist_raw\n",
    "\n",
    "# Training and Testing Loops\n",
    "def train(dataloader, model, loss_function, optimizer, device_in_use):\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    total_r2_score = 0\n",
    "    root_mean_squared_error_total = 0\n",
    "\n",
    "    for (features, labels_task1) in dataloader:\n",
    "        features, labels_task1 = features.to(device_in_use), labels_task1.to(device_in_use)\n",
    "\n",
    "        task_predictions = model(features)\n",
    "\n",
    "        loss = loss_function(task_predictions[0].squeeze(1), labels_task1)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Calculate R^2 score for the regression task\n",
    "        r2 = r2_score_manual(labels_task1, task_predictions[0].squeeze(1))\n",
    "        total_r2_score += r2\n",
    "\n",
    "        # Calculate RMSE score for the regression task\n",
    "        rmse_value = rmse(labels_task1, task_predictions[0].squeeze(1))\n",
    "        root_mean_squared_error_total+=rmse_value\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    avg_r2_score = total_r2_score / len(dataloader)\n",
    "    avg_rmse_score = root_mean_squared_error_total / len(dataloader)\n",
    "\n",
    "    return avg_loss, avg_r2_score, avg_rmse_score\n",
    "\n",
    "def test(dataloader, model, loss_function, device_in_use):\n",
    "  model.eval()\n",
    "  total_loss = 0\n",
    "  \n",
    "  total_loss = 0\n",
    "  total_r2_score = 0\n",
    "  root_mean_squared_error_total = 0\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for (features,labels_task1) in dataloader:\n",
    "        features, labels_task1 = features.to(device_in_use), labels_task1.to(device_in_use)\n",
    "\n",
    "        task_predictions = model(features)\n",
    "\n",
    "        loss = loss_function(task_predictions[0].squeeze(1), labels_task1)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Calculate R^2 score for the regression task\n",
    "        r2 = r2_score_manual(labels_task1, task_predictions[0].squeeze(1))\n",
    "        total_r2_score += r2\n",
    "        \n",
    "        # Calculate RMSE score for the regression task\n",
    "        rmse_value = rmse(labels_task1, task_predictions[0].squeeze(1))\n",
    "        root_mean_squared_error_total+=rmse_value\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    avg_r2_score = total_r2_score / len(dataloader)\n",
    "    avg_rmse_score = root_mean_squared_error_total / len(dataloader)\n",
    "\n",
    "    return avg_loss, avg_r2_score, avg_rmse_score\n",
    "\n",
    "def format_metric(value): # Used to format the metrics output\n",
    "    return f\"{value:.4f}\"\n",
    "\n",
    "def r2_score_manual(y_true, y_pred):\n",
    "    # Calculate the mean of true labels\n",
    "    y_mean = torch.mean(y_true)\n",
    "\n",
    "    # Calculate the total sum of squares\n",
    "    total_ss = torch.sum((y_true - y_mean)**2)\n",
    "\n",
    "    # Calculate the residual sum of squares\n",
    "    residual_ss = torch.sum((y_true - y_pred)**2)\n",
    "\n",
    "    # Calculate R-squared\n",
    "    r2 = 1 - (residual_ss / total_ss)\n",
    "\n",
    "    return r2.item()  # Convert to a Python float\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    # Calculate the squared differences\n",
    "    squared_diff = (y_true - y_pred)**2\n",
    "\n",
    "    # Calculate the mean of the squared differences\n",
    "    mean_squared_diff = torch.mean(squared_diff)\n",
    "\n",
    "    # Calculate the square root to obtain RMSE\n",
    "    rmse = torch.sqrt(mean_squared_diff)\n",
    "\n",
    "    return rmse.item()  # Convert to a Python float"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUN EXPERIMENTS\n",
    "\n",
    "1. Using Optuna to optimize CAT-Transformers hyperparameters for your dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the early stopping mechanism\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5):\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.best_metric = float('-inf')\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, metric):\n",
    "        if metric > self.best_metric:\n",
    "            self.best_metric = metric\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "\n",
    "# Function to log results to a text file\n",
    "def log_to_file(filename, text):\n",
    "    with open(filename, 'a') as f:\n",
    "        f.write(text + '\\n')\n",
    "\n",
    "def objective(trial):\n",
    "    trial_number = trial.number\n",
    "\n",
    "    # Define hyperparameters to search over\n",
    "    sigma = trial.suggest_categorical('sigma', [.001, 0.1, 1, 2, 3, 5, 10])\n",
    "    num_layers = trial.suggest_int('num_layers', 1, 2)\n",
    "    # Ensure that embed_size is divisible by num_layers\n",
    "    embed_size = trial.suggest_categorical(\"embed_size\", [50, 60, 70, 80, 90, 100, 120, 140, 160])\n",
    "    heads = trial.suggest_categorical(\"heads\", [1, 5, 10])\n",
    "    forward_expansion = trial.suggest_int('forward_expansion', 1, 8)\n",
    "    prenorm_on = trial.suggest_categorical('prenorm_on', [True, False])\n",
    "    mlp_scale_classification = trial.suggest_int('mlp_scale_classification', 1, 8)\n",
    "    embedding_dropout = trial.suggest_categorical('embedding_dropout', [0, .1, .2, .5])\n",
    "    decoder_dropout = trial.suggest_categorical('decoder_dropout', [0,.1,.2,.5])\n",
    "    classification_dropout = trial.suggest_categorical('class_drop', [0,.1,.2,.5])\n",
    "\n",
    "    learning_rate = trial.suggest_categorical('learning_rate', [0.0001, 0.001, 0.01])\n",
    "\n",
    "    num_epochs = 75\n",
    "\n",
    "\n",
    "    # Create your model with the sampled hyperparameters\n",
    "    model = Classifier(\n",
    "        n_features=len(cont_columns),\n",
    "        targets_classes=[0],\n",
    "        rff_on=True,\n",
    "        sigma=sigma,\n",
    "        embed_size=embed_size,\n",
    "        num_layers=num_layers,\n",
    "        heads=heads,\n",
    "        forward_expansion=forward_expansion,\n",
    "        pre_norm_on=prenorm_on,\n",
    "        mlp_scale_classification=mlp_scale_classification,\n",
    "        embedding_dropout=embedding_dropout,\n",
    "        decoder_dropout=decoder_dropout,\n",
    "        classification_dropout=classification_dropout\n",
    "    ).to(device_in_use)\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    loss_function = UncertaintyLoss(1)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Initialize early stopping\n",
    "    early_stopping = EarlyStopping(patience=3)  # Adjust patience as needed\n",
    "\n",
    "    # Training loop with a large number of epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss, train_accuracy, _ = train(train_dataloader, model, loss_function, optimizer, device_in_use)\n",
    "        \n",
    "        # Validation loop\n",
    "        val_loss, val_accuracy, _ = test(val_dataloader, model, loss_function, device_in_use)\n",
    "        \n",
    "        # Check if we should early stop based on validation accuracy\n",
    "        if early_stopping(val_accuracy):\n",
    "            break\n",
    "    \n",
    "    # Log the final test accuracy for this trial to a shared log file\n",
    "    final_log = f\"Trial {trial_number} completed. Validation Accuracy = {val_accuracy:.4f}\"\n",
    "    log_to_file('all_trials_log.txt', final_log)\n",
    "\n",
    "    # Return the test accuracy as the objective to optimize\n",
    "    return val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-27 11:14:37,674] A new study created in memory with name: no-name-b480e4d6-44cb-4be9-81f2-a761a76a2198\n",
      "  0%|          | 0/100 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2023-10-27 11:14:39,580] Trial 0 failed with parameters: {'sigma': 0.1, 'num_layers': 2, 'embed_size': 160, 'heads': 5, 'forward_expansion': 4, 'prenorm_on': False, 'mlp_scale_classification': 3, 'embedding_dropout': 0.5, 'decoder_dropout': 0.1, 'class_drop': 0, 'learning_rate': 0.001} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\prime\\anaconda3\\envs\\DL\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"C:\\Users\\prime\\AppData\\Local\\Temp\\ipykernel_15260\\2334515759.py\", line 70, in objective\n",
      "    train_loss, train_accuracy, _ = train(train_dataloader, model, loss_function, optimizer, device_in_use)\n",
      "  File \"C:\\Users\\prime\\AppData\\Local\\Temp\\ipykernel_15260\\3670286400.py\", line 326, in train\n",
      "    optimizer.step()\n",
      "  File \"c:\\Users\\prime\\anaconda3\\envs\\DL\\lib\\site-packages\\torch\\optim\\optimizer.py\", line 280, in wrapper\n",
      "    out = func(*args, **kwargs)\n",
      "  File \"c:\\Users\\prime\\anaconda3\\envs\\DL\\lib\\site-packages\\torch\\optim\\optimizer.py\", line 33, in _use_grad\n",
      "    ret = func(self, *args, **kwargs)\n",
      "  File \"c:\\Users\\prime\\anaconda3\\envs\\DL\\lib\\site-packages\\torch\\optim\\adam.py\", line 141, in step\n",
      "    adam(\n",
      "  File \"c:\\Users\\prime\\anaconda3\\envs\\DL\\lib\\site-packages\\torch\\optim\\adam.py\", line 281, in adam\n",
      "    func(params,\n",
      "  File \"c:\\Users\\prime\\anaconda3\\envs\\DL\\lib\\site-packages\\torch\\optim\\adam.py\", line 391, in _single_tensor_adam\n",
      "    denom = (exp_avg_sq.sqrt() / bias_correction2_sqrt).add_(eps)\n",
      "KeyboardInterrupt\n",
      "[W 2023-10-27 11:14:39,587] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\prime\\OneDrive\\Documents\\CMU\\research\\CAT-Transformer\\experiments\\housing\\housing_regression.ipynb Cell 14\u001b[0m line \u001b[0;36m8\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/prime/OneDrive/Documents/CMU/research/CAT-Transformer/experiments/housing/housing_regression.ipynb#X16sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m study \u001b[39m=\u001b[39m optuna\u001b[39m.\u001b[39mcreate_study(direction\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmaximize\u001b[39m\u001b[39m'\u001b[39m)  \u001b[39m# Maximize validation accuracy\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/prime/OneDrive/Documents/CMU/research/CAT-Transformer/experiments/housing/housing_regression.ipynb#X16sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# Start the optimization process\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/prime/OneDrive/Documents/CMU/research/CAT-Transformer/experiments/housing/housing_regression.ipynb#X16sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m study\u001b[39m.\u001b[39;49moptimize(objective, n_trials\u001b[39m=\u001b[39;49mnum_trials, show_progress_bar\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/prime/OneDrive/Documents/CMU/research/CAT-Transformer/experiments/housing/housing_regression.ipynb#X16sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m# Get the best hyperparameters and the validation accuracy at the point of early stopping\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/prime/OneDrive/Documents/CMU/research/CAT-Transformer/experiments/housing/housing_regression.ipynb#X16sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m best_params \u001b[39m=\u001b[39m study\u001b[39m.\u001b[39mbest_params\n",
      "File \u001b[1;32mc:\\Users\\prime\\anaconda3\\envs\\DL\\lib\\site-packages\\optuna\\study\\study.py:451\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moptimize\u001b[39m(\n\u001b[0;32m    349\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    350\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    357\u001b[0m     show_progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    358\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    359\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    360\u001b[0m \n\u001b[0;32m    361\u001b[0m \u001b[39m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    449\u001b[0m \u001b[39m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    450\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 451\u001b[0m     _optimize(\n\u001b[0;32m    452\u001b[0m         study\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m    453\u001b[0m         func\u001b[39m=\u001b[39;49mfunc,\n\u001b[0;32m    454\u001b[0m         n_trials\u001b[39m=\u001b[39;49mn_trials,\n\u001b[0;32m    455\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[0;32m    456\u001b[0m         n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[0;32m    457\u001b[0m         catch\u001b[39m=\u001b[39;49m\u001b[39mtuple\u001b[39;49m(catch) \u001b[39mif\u001b[39;49;00m \u001b[39misinstance\u001b[39;49m(catch, Iterable) \u001b[39melse\u001b[39;49;00m (catch,),\n\u001b[0;32m    458\u001b[0m         callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[0;32m    459\u001b[0m         gc_after_trial\u001b[39m=\u001b[39;49mgc_after_trial,\n\u001b[0;32m    460\u001b[0m         show_progress_bar\u001b[39m=\u001b[39;49mshow_progress_bar,\n\u001b[0;32m    461\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\prime\\anaconda3\\envs\\DL\\lib\\site-packages\\optuna\\study\\_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     65\u001b[0m     \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m---> 66\u001b[0m         _optimize_sequential(\n\u001b[0;32m     67\u001b[0m             study,\n\u001b[0;32m     68\u001b[0m             func,\n\u001b[0;32m     69\u001b[0m             n_trials,\n\u001b[0;32m     70\u001b[0m             timeout,\n\u001b[0;32m     71\u001b[0m             catch,\n\u001b[0;32m     72\u001b[0m             callbacks,\n\u001b[0;32m     73\u001b[0m             gc_after_trial,\n\u001b[0;32m     74\u001b[0m             reseed_sampler_rng\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m     75\u001b[0m             time_start\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m     76\u001b[0m             progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[0;32m     77\u001b[0m         )\n\u001b[0;32m     78\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     79\u001b[0m         \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\prime\\anaconda3\\envs\\DL\\lib\\site-packages\\optuna\\study\\_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    160\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 163\u001b[0m     frozen_trial \u001b[39m=\u001b[39m _run_trial(study, func, catch)\n\u001b[0;32m    164\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    165\u001b[0m     \u001b[39m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[39m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    167\u001b[0m     \u001b[39m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    168\u001b[0m     \u001b[39m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    169\u001b[0m     \u001b[39mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32mc:\\Users\\prime\\anaconda3\\envs\\DL\\lib\\site-packages\\optuna\\study\\_optimize.py:251\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    244\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mFalse\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mShould not reach.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    246\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m    247\u001b[0m     frozen_trial\u001b[39m.\u001b[39mstate \u001b[39m==\u001b[39m TrialState\u001b[39m.\u001b[39mFAIL\n\u001b[0;32m    248\u001b[0m     \u001b[39mand\u001b[39;00m func_err \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    249\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    250\u001b[0m ):\n\u001b[1;32m--> 251\u001b[0m     \u001b[39mraise\u001b[39;00m func_err\n\u001b[0;32m    252\u001b[0m \u001b[39mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32mc:\\Users\\prime\\anaconda3\\envs\\DL\\lib\\site-packages\\optuna\\study\\_optimize.py:200\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[39mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[39m.\u001b[39m_trial_id, study\u001b[39m.\u001b[39m_storage):\n\u001b[0;32m    199\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 200\u001b[0m         value_or_values \u001b[39m=\u001b[39m func(trial)\n\u001b[0;32m    201\u001b[0m     \u001b[39mexcept\u001b[39;00m exceptions\u001b[39m.\u001b[39mTrialPruned \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    202\u001b[0m         \u001b[39m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    203\u001b[0m         state \u001b[39m=\u001b[39m TrialState\u001b[39m.\u001b[39mPRUNED\n",
      "\u001b[1;32mc:\\Users\\prime\\OneDrive\\Documents\\CMU\\research\\CAT-Transformer\\experiments\\housing\\housing_regression.ipynb Cell 14\u001b[0m line \u001b[0;36m7\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/prime/OneDrive/Documents/CMU/research/CAT-Transformer/experiments/housing/housing_regression.ipynb#X16sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m \u001b[39m# Training loop with a large number of epochs\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/prime/OneDrive/Documents/CMU/research/CAT-Transformer/experiments/housing/housing_regression.ipynb#X16sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/prime/OneDrive/Documents/CMU/research/CAT-Transformer/experiments/housing/housing_regression.ipynb#X16sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m     train_loss, train_accuracy, _ \u001b[39m=\u001b[39m train(train_dataloader, model, loss_function, optimizer, device_in_use)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/prime/OneDrive/Documents/CMU/research/CAT-Transformer/experiments/housing/housing_regression.ipynb#X16sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m     \u001b[39m# Validation loop\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/prime/OneDrive/Documents/CMU/research/CAT-Transformer/experiments/housing/housing_regression.ipynb#X16sZmlsZQ%3D%3D?line=72'>73</a>\u001b[0m     val_loss, val_accuracy, _ \u001b[39m=\u001b[39m test(val_dataloader, model, loss_function, device_in_use)\n",
      "\u001b[1;32mc:\\Users\\prime\\OneDrive\\Documents\\CMU\\research\\CAT-Transformer\\experiments\\housing\\housing_regression.ipynb Cell 14\u001b[0m line \u001b[0;36m3\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/prime/OneDrive/Documents/CMU/research/CAT-Transformer/experiments/housing/housing_regression.ipynb#X16sZmlsZQ%3D%3D?line=322'>323</a>\u001b[0m     root_mean_squared_error_total\u001b[39m+\u001b[39m\u001b[39m=\u001b[39mrmse_value\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/prime/OneDrive/Documents/CMU/research/CAT-Transformer/experiments/housing/housing_regression.ipynb#X16sZmlsZQ%3D%3D?line=324'>325</a>\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/prime/OneDrive/Documents/CMU/research/CAT-Transformer/experiments/housing/housing_regression.ipynb#X16sZmlsZQ%3D%3D?line=325'>326</a>\u001b[0m     optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/prime/OneDrive/Documents/CMU/research/CAT-Transformer/experiments/housing/housing_regression.ipynb#X16sZmlsZQ%3D%3D?line=326'>327</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/prime/OneDrive/Documents/CMU/research/CAT-Transformer/experiments/housing/housing_regression.ipynb#X16sZmlsZQ%3D%3D?line=328'>329</a>\u001b[0m avg_loss \u001b[39m=\u001b[39m total_loss \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(dataloader)\n",
      "File \u001b[1;32mc:\\Users\\prime\\anaconda3\\envs\\DL\\lib\\site-packages\\torch\\optim\\optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    276\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    277\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m                                \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 280\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    281\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    283\u001b[0m \u001b[39m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\prime\\anaconda3\\envs\\DL\\lib\\site-packages\\torch\\optim\\optimizer.py:33\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     32\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m---> 33\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     34\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     35\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[1;32mc:\\Users\\prime\\anaconda3\\envs\\DL\\lib\\site-packages\\torch\\optim\\adam.py:141\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    130\u001b[0m     beta1, beta2 \u001b[39m=\u001b[39m group[\u001b[39m'\u001b[39m\u001b[39mbetas\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m    132\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_group(\n\u001b[0;32m    133\u001b[0m         group,\n\u001b[0;32m    134\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    138\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    139\u001b[0m         state_steps)\n\u001b[1;32m--> 141\u001b[0m     adam(\n\u001b[0;32m    142\u001b[0m         params_with_grad,\n\u001b[0;32m    143\u001b[0m         grads,\n\u001b[0;32m    144\u001b[0m         exp_avgs,\n\u001b[0;32m    145\u001b[0m         exp_avg_sqs,\n\u001b[0;32m    146\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    147\u001b[0m         state_steps,\n\u001b[0;32m    148\u001b[0m         amsgrad\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    149\u001b[0m         beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    150\u001b[0m         beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    151\u001b[0m         lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    152\u001b[0m         weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    153\u001b[0m         eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    154\u001b[0m         maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    155\u001b[0m         foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    156\u001b[0m         capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    157\u001b[0m         differentiable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mdifferentiable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    158\u001b[0m         fused\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mfused\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    159\u001b[0m         grad_scale\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mgrad_scale\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m    160\u001b[0m         found_inf\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mfound_inf\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m    161\u001b[0m     )\n\u001b[0;32m    163\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\prime\\anaconda3\\envs\\DL\\lib\\site-packages\\torch\\optim\\adam.py:281\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    279\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 281\u001b[0m func(params,\n\u001b[0;32m    282\u001b[0m      grads,\n\u001b[0;32m    283\u001b[0m      exp_avgs,\n\u001b[0;32m    284\u001b[0m      exp_avg_sqs,\n\u001b[0;32m    285\u001b[0m      max_exp_avg_sqs,\n\u001b[0;32m    286\u001b[0m      state_steps,\n\u001b[0;32m    287\u001b[0m      amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[0;32m    288\u001b[0m      beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    289\u001b[0m      beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    290\u001b[0m      lr\u001b[39m=\u001b[39;49mlr,\n\u001b[0;32m    291\u001b[0m      weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[0;32m    292\u001b[0m      eps\u001b[39m=\u001b[39;49meps,\n\u001b[0;32m    293\u001b[0m      maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[0;32m    294\u001b[0m      capturable\u001b[39m=\u001b[39;49mcapturable,\n\u001b[0;32m    295\u001b[0m      differentiable\u001b[39m=\u001b[39;49mdifferentiable,\n\u001b[0;32m    296\u001b[0m      grad_scale\u001b[39m=\u001b[39;49mgrad_scale,\n\u001b[0;32m    297\u001b[0m      found_inf\u001b[39m=\u001b[39;49mfound_inf)\n",
      "File \u001b[1;32mc:\\Users\\prime\\anaconda3\\envs\\DL\\lib\\site-packages\\torch\\optim\\adam.py:391\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    389\u001b[0m     denom \u001b[39m=\u001b[39m (max_exp_avg_sqs[i]\u001b[39m.\u001b[39msqrt() \u001b[39m/\u001b[39m bias_correction2_sqrt)\u001b[39m.\u001b[39madd_(eps)\n\u001b[0;32m    390\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 391\u001b[0m     denom \u001b[39m=\u001b[39m (exp_avg_sq\u001b[39m.\u001b[39;49msqrt() \u001b[39m/\u001b[39m bias_correction2_sqrt)\u001b[39m.\u001b[39madd_(eps)\n\u001b[0;32m    393\u001b[0m param\u001b[39m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[39m=\u001b[39m\u001b[39m-\u001b[39mstep_size)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Set the number of optimization trials\n",
    "num_trials = 100\n",
    "\n",
    "# Create an Optuna study\n",
    "study = optuna.create_study(direction='maximize')  # Maximize validation accuracy\n",
    "\n",
    "# Start the optimization process\n",
    "study.optimize(objective, n_trials=num_trials, show_progress_bar=True)\n",
    "\n",
    "# Get the best hyperparameters and the validation accuracy at the point of early stopping\n",
    "best_params = study.best_params\n",
    "best_val_accuracy = study.best_value\n",
    "\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "print(\"Best Validation Accuracy (at Early Stopping):\", best_val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [ 1/60]        | Train: Loss 3.5118, R2 0.0462, RMSE 1.7889                        | Test: Loss 1.6328, R2 0.5634, RMSE 1.2730\n",
      "Epoch [ 2/60]        | Train: Loss 1.2996, R2 0.6405, RMSE 1.1313                        | Test: Loss 1.0101, R2 0.7232, RMSE 1.0007\n",
      "Epoch [ 3/60]        | Train: Loss 0.9925, R2 0.7225, RMSE 0.9912                        | Test: Loss 0.9198, R2 0.7524, RMSE 0.9527\n",
      "Epoch [ 4/60]        | Train: Loss 0.9007, R2 0.7474, RMSE 0.9453                        | Test: Loss 0.8637, R2 0.7646, RMSE 0.9248\n",
      "Epoch [ 5/60]        | Train: Loss 0.8550, R2 0.7604, RMSE 0.9208                        | Test: Loss 0.9374, R2 0.7429, RMSE 0.9620\n",
      "Epoch [ 6/60]        | Train: Loss 0.8170, R2 0.7722, RMSE 0.8997                        | Test: Loss 0.8845, R2 0.7616, RMSE 0.9338\n",
      "Epoch [ 7/60]        | Train: Loss 0.7986, R2 0.7777, RMSE 0.8910                        | Test: Loss 0.9264, R2 0.7472, RMSE 0.9557\n",
      "Epoch [ 8/60]        | Train: Loss 0.8087, R2 0.7732, RMSE 0.8961                        | Test: Loss 0.8731, R2 0.7659, RMSE 0.9282\n",
      "Epoch [ 9/60]        | Train: Loss 0.7832, R2 0.7802, RMSE 0.8824                        | Test: Loss 0.8990, R2 0.7531, RMSE 0.9418\n",
      "Epoch [10/60]        | Train: Loss 0.7745, R2 0.7840, RMSE 0.8757                        | Test: Loss 1.1237, R2 0.6973, RMSE 1.0538\n",
      "Epoch [11/60]        | Train: Loss 0.7583, R2 0.7857, RMSE 0.8678                        | Test: Loss 0.8671, R2 0.7661, RMSE 0.9247\n",
      "Epoch [12/60]        | Train: Loss 0.7483, R2 0.7915, RMSE 0.8625                        | Test: Loss 0.8887, R2 0.7635, RMSE 0.9357\n",
      "Epoch [13/60]        | Train: Loss 0.7477, R2 0.7881, RMSE 0.8617                        | Test: Loss 0.9442, R2 0.7439, RMSE 0.9672\n",
      "Epoch [14/60]        | Train: Loss 0.7278, R2 0.7954, RMSE 0.8506                        | Test: Loss 0.8941, R2 0.7607, RMSE 0.9344\n",
      "Epoch [15/60]        | Train: Loss 0.7096, R2 0.8014, RMSE 0.8390                        | Test: Loss 0.8709, R2 0.7670, RMSE 0.9258\n",
      "Epoch [16/60]        | Train: Loss 0.6906, R2 0.8068, RMSE 0.8284                        | Test: Loss 0.8448, R2 0.7719, RMSE 0.9124\n",
      "Epoch [17/60]        | Train: Loss 0.6874, R2 0.8064, RMSE 0.8268                        | Test: Loss 0.9191, R2 0.7522, RMSE 0.9535\n",
      "Epoch [18/60]        | Train: Loss 0.7014, R2 0.8035, RMSE 0.8359                        | Test: Loss 0.8904, R2 0.7584, RMSE 0.9365\n",
      "Epoch [19/60]        | Train: Loss 0.6787, R2 0.8092, RMSE 0.8204                        | Test: Loss 0.8259, R2 0.7778, RMSE 0.8994\n",
      "Epoch [20/60]        | Train: Loss 0.6601, R2 0.8154, RMSE 0.8098                        | Test: Loss 0.8411, R2 0.7733, RMSE 0.9132\n",
      "Epoch [21/60]        | Train: Loss 0.6550, R2 0.8155, RMSE 0.8077                        | Test: Loss 0.8387, R2 0.7758, RMSE 0.9084\n",
      "Epoch [22/60]        | Train: Loss 0.6634, R2 0.8128, RMSE 0.8125                        | Test: Loss 0.8321, R2 0.7796, RMSE 0.8979\n",
      "Epoch [23/60]        | Train: Loss 0.6763, R2 0.8092, RMSE 0.8200                        | Test: Loss 0.8133, R2 0.7819, RMSE 0.8928\n",
      "Epoch [24/60]        | Train: Loss 0.6518, R2 0.8162, RMSE 0.8054                        | Test: Loss 0.8547, R2 0.7681, RMSE 0.9152\n",
      "Epoch [25/60]        | Train: Loss 0.6472, R2 0.8184, RMSE 0.8025                        | Test: Loss 0.9560, R2 0.7388, RMSE 0.9723\n",
      "Epoch [26/60]        | Train: Loss 0.6552, R2 0.8160, RMSE 0.8067                        | Test: Loss 0.8610, R2 0.7697, RMSE 0.9223\n",
      "Epoch [27/60]        | Train: Loss 0.6553, R2 0.8147, RMSE 0.8073                        | Test: Loss 0.8561, R2 0.7732, RMSE 0.9195\n",
      "Epoch [28/60]        | Train: Loss 0.6111, R2 0.8273, RMSE 0.7797                        | Test: Loss 0.8192, R2 0.7806, RMSE 0.8975\n",
      "Epoch [29/60]        | Train: Loss 0.6250, R2 0.8229, RMSE 0.7889                        | Test: Loss 0.8332, R2 0.7765, RMSE 0.9069\n",
      "Epoch [30/60]        | Train: Loss 0.6030, R2 0.8290, RMSE 0.7743                        | Test: Loss 0.8572, R2 0.7671, RMSE 0.9202\n",
      "Epoch [31/60]        | Train: Loss 0.6039, R2 0.8302, RMSE 0.7751                        | Test: Loss 0.8757, R2 0.7677, RMSE 0.9265\n",
      "Epoch [32/60]        | Train: Loss 0.5997, R2 0.8317, RMSE 0.7713                        | Test: Loss 0.8604, R2 0.7674, RMSE 0.9230\n",
      "Epoch [33/60]        | Train: Loss 0.6113, R2 0.8282, RMSE 0.7796                        | Test: Loss 0.8126, R2 0.7781, RMSE 0.8975\n",
      "Epoch [34/60]        | Train: Loss 0.5751, R2 0.8383, RMSE 0.7566                        | Test: Loss 0.8205, R2 0.7786, RMSE 0.8994\n",
      "Epoch [35/60]        | Train: Loss 0.5746, R2 0.8373, RMSE 0.7563                        | Test: Loss 0.8157, R2 0.7795, RMSE 0.8973\n",
      "Epoch [36/60]        | Train: Loss 0.5689, R2 0.8401, RMSE 0.7513                        | Test: Loss 0.8673, R2 0.7647, RMSE 0.9238\n",
      "Epoch [37/60]        | Train: Loss 0.5628, R2 0.8413, RMSE 0.7482                        | Test: Loss 0.8314, R2 0.7788, RMSE 0.9024\n",
      "Epoch [38/60]        | Train: Loss 0.5665, R2 0.8389, RMSE 0.7506                        | Test: Loss 0.8029, R2 0.7838, RMSE 0.8888\n",
      "Epoch [39/60]        | Train: Loss 0.5657, R2 0.8407, RMSE 0.7499                        | Test: Loss 0.8511, R2 0.7724, RMSE 0.9162\n",
      "Epoch [40/60]        | Train: Loss 0.5567, R2 0.8429, RMSE 0.7443                        | Test: Loss 0.8360, R2 0.7740, RMSE 0.9076\n",
      "Epoch [41/60]        | Train: Loss 0.5558, R2 0.8443, RMSE 0.7440                        | Test: Loss 0.8193, R2 0.7806, RMSE 0.8996\n",
      "Epoch [42/60]        | Train: Loss 0.5367, R2 0.8481, RMSE 0.7313                        | Test: Loss 0.8401, R2 0.7720, RMSE 0.9078\n",
      "Epoch [43/60]        | Train: Loss 0.5417, R2 0.8470, RMSE 0.7343                        | Test: Loss 0.8386, R2 0.7734, RMSE 0.9105\n",
      "Epoch [44/60]        | Train: Loss 0.5376, R2 0.8482, RMSE 0.7312                        | Test: Loss 0.8458, R2 0.7729, RMSE 0.9112\n",
      "Epoch [45/60]        | Train: Loss 0.5293, R2 0.8505, RMSE 0.7260                        | Test: Loss 0.9223, R2 0.7461, RMSE 0.9503\n",
      "Epoch [46/60]        | Train: Loss 0.5302, R2 0.8487, RMSE 0.7266                        | Test: Loss 0.8734, R2 0.7645, RMSE 0.9243\n",
      "Epoch [47/60]        | Train: Loss 0.5353, R2 0.8501, RMSE 0.7301                        | Test: Loss 0.8813, R2 0.7639, RMSE 0.9335\n",
      "Epoch [48/60]        | Train: Loss 0.5108, R2 0.8553, RMSE 0.7130                        | Test: Loss 0.9032, R2 0.7553, RMSE 0.9441\n",
      "Epoch [49/60]        | Train: Loss 0.5015, R2 0.8577, RMSE 0.7063                        | Test: Loss 0.8749, R2 0.7627, RMSE 0.9250\n",
      "Epoch [50/60]        | Train: Loss 0.4900, R2 0.8622, RMSE 0.6979                        | Test: Loss 0.8722, R2 0.7580, RMSE 0.9236\n",
      "Epoch [51/60]        | Train: Loss 0.4871, R2 0.8618, RMSE 0.6965                        | Test: Loss 0.9516, R2 0.7444, RMSE 0.9635\n",
      "Epoch [52/60]        | Train: Loss 0.4847, R2 0.8633, RMSE 0.6944                        | Test: Loss 0.9007, R2 0.7598, RMSE 0.9324\n",
      "Epoch [53/60]        | Train: Loss 0.4806, R2 0.8628, RMSE 0.6921                        | Test: Loss 0.8728, R2 0.7663, RMSE 0.9235\n",
      "Epoch [54/60]        | Train: Loss 0.4615, R2 0.8694, RMSE 0.6781                        | Test: Loss 0.9039, R2 0.7534, RMSE 0.9456\n",
      "Epoch [55/60]        | Train: Loss 0.4711, R2 0.8671, RMSE 0.6847                        | Test: Loss 0.9008, R2 0.7571, RMSE 0.9431\n",
      "Epoch [56/60]        | Train: Loss 0.4778, R2 0.8653, RMSE 0.6895                        | Test: Loss 0.8765, R2 0.7640, RMSE 0.9273\n",
      "Epoch [57/60]        | Train: Loss 0.4976, R2 0.8595, RMSE 0.7039                        | Test: Loss 0.9155, R2 0.7530, RMSE 0.9463\n",
      "Epoch [58/60]        | Train: Loss 0.4548, R2 0.8708, RMSE 0.6728                        | Test: Loss 0.8730, R2 0.7693, RMSE 0.9227\n",
      "Epoch [59/60]        | Train: Loss 0.4463, R2 0.8738, RMSE 0.6664                        | Test: Loss 0.8875, R2 0.7598, RMSE 0.9354\n",
      "Epoch [60/60]        | Train: Loss 0.4368, R2 0.8763, RMSE 0.6594                        | Test: Loss 0.9474, R2 0.7462, RMSE 0.9630\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x2a71f9423d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Testing against the test dataset\n",
    "\n",
    "model = Classifier(n_features=len(cont_columns),\n",
    "                   targets_classes=[0],\n",
    "                   rff_on=True,\n",
    "                   sigma=best_params['sigma'],\n",
    "                   embed_size=best_params['embed_size'],\n",
    "                   num_layers=best_params['num_layers'],\n",
    "                   heads=best_params['heads'],\n",
    "                   forward_expansion=best_params['forward_expansion'],\n",
    "                   pre_norm_on=best_params['prenorm_on'],\n",
    "                   mlp_scale_classification=best_params['mlp_scale_classification'],\n",
    "                   embedding_dropout=best_params['embedding_dropout'],\n",
    "                   decoder_dropout=best_params['decoder_dropout'],\n",
    "                   classification_dropout=best_params['class_drop']\n",
    "                   ).to(device_in_use) # Instantiate the model\n",
    "loss_functions = UncertaintyLoss(1)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr = best_params['learning_rate']) # Maybe try messing around with optimizers. try other torch optimizers with different configurations.\n",
    "epochs = 75 #Set the number of epochs\n",
    "\n",
    "train_losses = []\n",
    "train_accuracies_1 = [] \n",
    "train_accuracies_2 = []\n",
    "train_recalls = [] \n",
    "train_f1_scores = [] \n",
    "test_losses = []\n",
    "test_accuracies_1 = []\n",
    "test_accuracies_2 = []\n",
    "test_recalls = []  \n",
    "test_f1_scores = [] \n",
    "all_attention_scores = []\n",
    "\n",
    "for t in range(epochs):\n",
    "  train_loss, r2_train, rmse_train = train(train_dataloader, model, loss_functions, optimizer, device_in_use=device_in_use)\n",
    "  test_loss, r2_test, rmse_test = test(test_dataloader, model, loss_functions, device_in_use=device_in_use)\n",
    "  train_losses.append(train_loss)\n",
    "\n",
    "  # train_accuracies_2.append(train_accuracy_2)\n",
    "  # train_recalls.append(train_recall) \n",
    "  # train_f1_scores.append(train_f1)\n",
    "  test_losses.append(test_loss)\n",
    "\n",
    "  # test_accuracies_2.append(test_accuracy_2)\n",
    "  # test_recalls.append(test_recall)\n",
    "  # test_f1_scores.append(test_f1)\n",
    "  # Formatting for easier reading\n",
    "  epoch_str = f\"Epoch [{t+1:2}/{epochs}]\"\n",
    "  train_metrics = f\"Train: Loss {format_metric(train_loss)}, R2 {format_metric(r2_train)}, RMSE {format_metric(rmse_train)}\"\n",
    "  test_metrics = f\"Test: Loss {format_metric(test_loss)}, R2 {format_metric(r2_test)}, RMSE {format_metric(rmse_test)}\"\n",
    "  print(f\"{epoch_str:20} | {train_metrics:65} | {test_metrics}\")\n",
    "\n",
    "\n",
    "torch.save(model.state_dict(), 'final_model_trained.pth')\n",
    "\n",
    "# Plotting the loss curves\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 1, 1)\n",
    "plt.plot(range(1, epochs+1), train_losses, label='Train Loss')\n",
    "plt.plot(range(1, epochs+1), [l for l in test_losses], label='Test Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Test Loss Curve')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "Epoch [ 1/75]        | Train: Loss 0.7568, R2 0.2324, RMSE 0.8531                        | Test: Loss 0.4621, R2 0.5326, RMSE 0.6793\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\prime\\OneDrive\\Documents\\CMU\\research\\CAT-Transformer\\experiments\\housing\\housing_regression.ipynb Cell 16\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/prime/OneDrive/Documents/CMU/research/CAT-Transformer/experiments/housing/housing_regression.ipynb#X21sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/prime/OneDrive/Documents/CMU/research/CAT-Transformer/experiments/housing/housing_regression.ipynb#X21sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m   train_loss, r2_train, rmse_train \u001b[39m=\u001b[39m train(train_dataloader, model, loss_functions, optimizer, device_in_use\u001b[39m=\u001b[39mdevice_in_use)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/prime/OneDrive/Documents/CMU/research/CAT-Transformer/experiments/housing/housing_regression.ipynb#X21sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m   test_loss, r2_test, rmse_test \u001b[39m=\u001b[39m test(test_dataloader, model, loss_functions, device_in_use\u001b[39m=\u001b[39;49mdevice_in_use)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/prime/OneDrive/Documents/CMU/research/CAT-Transformer/experiments/housing/housing_regression.ipynb#X21sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m   train_losses\u001b[39m.\u001b[39mappend(train_loss)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/prime/OneDrive/Documents/CMU/research/CAT-Transformer/experiments/housing/housing_regression.ipynb#X21sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m   \u001b[39m# train_accuracies_2.append(train_accuracy_2)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/prime/OneDrive/Documents/CMU/research/CAT-Transformer/experiments/housing/housing_regression.ipynb#X21sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m   \u001b[39m# train_recalls.append(train_recall) \u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/prime/OneDrive/Documents/CMU/research/CAT-Transformer/experiments/housing/housing_regression.ipynb#X21sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m   \u001b[39m# train_f1_scores.append(train_f1)\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\prime\\OneDrive\\Documents\\CMU\\research\\CAT-Transformer\\experiments\\housing\\housing_regression.ipynb Cell 16\u001b[0m line \u001b[0;36m3\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/prime/OneDrive/Documents/CMU/research/CAT-Transformer/experiments/housing/housing_regression.ipynb#X21sZmlsZQ%3D%3D?line=345'>346</a>\u001b[0m \u001b[39mfor\u001b[39;00m (features,labels_task1) \u001b[39min\u001b[39;00m dataloader:\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/prime/OneDrive/Documents/CMU/research/CAT-Transformer/experiments/housing/housing_regression.ipynb#X21sZmlsZQ%3D%3D?line=346'>347</a>\u001b[0m     features, labels_task1 \u001b[39m=\u001b[39m features\u001b[39m.\u001b[39mto(device_in_use), labels_task1\u001b[39m.\u001b[39mto(device_in_use)\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/prime/OneDrive/Documents/CMU/research/CAT-Transformer/experiments/housing/housing_regression.ipynb#X21sZmlsZQ%3D%3D?line=348'>349</a>\u001b[0m     task_predictions \u001b[39m=\u001b[39m model(features)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/prime/OneDrive/Documents/CMU/research/CAT-Transformer/experiments/housing/housing_regression.ipynb#X21sZmlsZQ%3D%3D?line=350'>351</a>\u001b[0m     loss \u001b[39m=\u001b[39m loss_function(task_predictions[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39msqueeze(\u001b[39m1\u001b[39m), labels_task1)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/prime/OneDrive/Documents/CMU/research/CAT-Transformer/experiments/housing/housing_regression.ipynb#X21sZmlsZQ%3D%3D?line=351'>352</a>\u001b[0m     total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\prime\\anaconda3\\envs\\DL\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mc:\\Users\\prime\\OneDrive\\Documents\\CMU\\research\\CAT-Transformer\\experiments\\housing\\housing_regression.ipynb Cell 16\u001b[0m line \u001b[0;36m2\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/prime/OneDrive/Documents/CMU/research/CAT-Transformer/experiments/housing/housing_regression.ipynb#X21sZmlsZQ%3D%3D?line=289'>290</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/prime/OneDrive/Documents/CMU/research/CAT-Transformer/experiments/housing/housing_regression.ipynb#X21sZmlsZQ%3D%3D?line=290'>291</a>\u001b[0m     class_embed, context \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(x)\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/prime/OneDrive/Documents/CMU/research/CAT-Transformer/experiments/housing/housing_regression.ipynb#X21sZmlsZQ%3D%3D?line=292'>293</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecoder(class_embed, context)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/prime/OneDrive/Documents/CMU/research/CAT-Transformer/experiments/housing/housing_regression.ipynb#X21sZmlsZQ%3D%3D?line=294'>295</a>\u001b[0m     probability_dist_raw \u001b[39m=\u001b[39m []\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/prime/OneDrive/Documents/CMU/research/CAT-Transformer/experiments/housing/housing_regression.ipynb#X21sZmlsZQ%3D%3D?line=295'>296</a>\u001b[0m     \u001b[39mfor\u001b[39;00m i, e \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclassifying_heads):\n",
      "File \u001b[1;32mc:\\Users\\prime\\anaconda3\\envs\\DL\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mc:\\Users\\prime\\OneDrive\\Documents\\CMU\\research\\CAT-Transformer\\experiments\\housing\\housing_regression.ipynb Cell 16\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/prime/OneDrive/Documents/CMU/research/CAT-Transformer/experiments/housing/housing_regression.ipynb#X21sZmlsZQ%3D%3D?line=128'>129</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, class_embed, context):\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/prime/OneDrive/Documents/CMU/research/CAT-Transformer/experiments/housing/housing_regression.ipynb#X21sZmlsZQ%3D%3D?line=129'>130</a>\u001b[0m     \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers:\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/prime/OneDrive/Documents/CMU/research/CAT-Transformer/experiments/housing/housing_regression.ipynb#X21sZmlsZQ%3D%3D?line=130'>131</a>\u001b[0m         \u001b[39m# x is the classification embedding (CLS Token)\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/prime/OneDrive/Documents/CMU/research/CAT-Transformer/experiments/housing/housing_regression.ipynb#X21sZmlsZQ%3D%3D?line=131'>132</a>\u001b[0m         \u001b[39m# context are the feature embeddings that will be used as key and value\u001b[39;00m\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/prime/OneDrive/Documents/CMU/research/CAT-Transformer/experiments/housing/housing_regression.ipynb#X21sZmlsZQ%3D%3D?line=132'>133</a>\u001b[0m         x, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mavg_attention \u001b[39m=\u001b[39m layer(class_embed, context, context)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/prime/OneDrive/Documents/CMU/research/CAT-Transformer/experiments/housing/housing_regression.ipynb#X21sZmlsZQ%3D%3D?line=134'>135</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\prime\\anaconda3\\envs\\DL\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mc:\\Users\\prime\\OneDrive\\Documents\\CMU\\research\\CAT-Transformer\\experiments\\housing\\housing_regression.ipynb Cell 16\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/prime/OneDrive/Documents/CMU/research/CAT-Transformer/experiments/housing/housing_regression.ipynb#X21sZmlsZQ%3D%3D?line=98'>99</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, value, key):\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/prime/OneDrive/Documents/CMU/research/CAT-Transformer/experiments/housing/housing_regression.ipynb#X21sZmlsZQ%3D%3D?line=99'>100</a>\u001b[0m     out, avg_attention \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer_block(value, key, x)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/prime/OneDrive/Documents/CMU/research/CAT-Transformer/experiments/housing/housing_regression.ipynb#X21sZmlsZQ%3D%3D?line=101'>102</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m out, avg_attention\n",
      "File \u001b[1;32mc:\\Users\\prime\\anaconda3\\envs\\DL\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mc:\\Users\\prime\\OneDrive\\Documents\\CMU\\research\\CAT-Transformer\\experiments\\housing\\housing_regression.ipynb Cell 16\u001b[0m line \u001b[0;36m8\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/prime/OneDrive/Documents/CMU/research/CAT-Transformer/experiments/housing/housing_regression.ipynb#X21sZmlsZQ%3D%3D?line=80'>81</a>\u001b[0m     key \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpre_norm(key)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/prime/OneDrive/Documents/CMU/research/CAT-Transformer/experiments/housing/housing_regression.ipynb#X21sZmlsZQ%3D%3D?line=81'>82</a>\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpre_norm(value)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/prime/OneDrive/Documents/CMU/research/CAT-Transformer/experiments/housing/housing_regression.ipynb#X21sZmlsZQ%3D%3D?line=83'>84</a>\u001b[0m attention, avg_attention \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(value, key, query)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/prime/OneDrive/Documents/CMU/research/CAT-Transformer/experiments/housing/housing_regression.ipynb#X21sZmlsZQ%3D%3D?line=84'>85</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm1(attention \u001b[39m+\u001b[39m query))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/prime/OneDrive/Documents/CMU/research/CAT-Transformer/experiments/housing/housing_regression.ipynb#X21sZmlsZQ%3D%3D?line=85'>86</a>\u001b[0m forward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeed_forward(x)\n",
      "File \u001b[1;32mc:\\Users\\prime\\anaconda3\\envs\\DL\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mc:\\Users\\prime\\OneDrive\\Documents\\CMU\\research\\CAT-Transformer\\experiments\\housing\\housing_regression.ipynb Cell 16\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/prime/OneDrive/Documents/CMU/research/CAT-Transformer/experiments/housing/housing_regression.ipynb#X21sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m keys \u001b[39m=\u001b[39m keys\u001b[39m.\u001b[39mreshape(N, key_len, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mheads, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhead_dim)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/prime/OneDrive/Documents/CMU/research/CAT-Transformer/experiments/housing/housing_regression.ipynb#X21sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m queries \u001b[39m=\u001b[39m query\u001b[39m.\u001b[39mreshape(N, query_len, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mheads, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhead_dim)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/prime/OneDrive/Documents/CMU/research/CAT-Transformer/experiments/housing/housing_regression.ipynb#X21sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m values \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvalues(values)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/prime/OneDrive/Documents/CMU/research/CAT-Transformer/experiments/housing/housing_regression.ipynb#X21sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m keys \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkeys(keys)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/prime/OneDrive/Documents/CMU/research/CAT-Transformer/experiments/housing/housing_regression.ipynb#X21sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m queries \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mqueries(queries)\n",
      "File \u001b[1;32mc:\\Users\\prime\\anaconda3\\envs\\DL\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\prime\\anaconda3\\envs\\DL\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Testing against the test dataset\n",
    "\n",
    "model = Classifier(n_features=len(cont_columns),\n",
    "                   targets_classes=[0],\n",
    "                   rff_on=True,\n",
    "                   sigma=4,\n",
    "                   embed_size=100,\n",
    "                   num_layers=1,\n",
    "                   heads=1,\n",
    "                   forward_expansion=1,\n",
    "                   pre_norm_on=True,\n",
    "                   mlp_scale_classification=8,\n",
    "                   embedding_dropout=0,\n",
    "                   decoder_dropout=0,\n",
    "                   classification_dropout=0,\n",
    "                   ).to(device_in_use) # Instantiate the model\n",
    "loss_functions = UncertaintyLoss(1)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr = .001) # Maybe try messing around with optimizers. try other torch optimizers with different configurations.\n",
    "epochs = 75 #Set the number of epochs\n",
    "\n",
    "train_losses = []\n",
    "train_accuracies_1 = [] \n",
    "train_accuracies_2 = []\n",
    "train_recalls = [] \n",
    "train_f1_scores = [] \n",
    "test_losses = []\n",
    "test_accuracies_1 = []\n",
    "test_accuracies_2 = []\n",
    "test_recalls = []  \n",
    "test_f1_scores = [] \n",
    "all_attention_scores = []\n",
    "\n",
    "for t in range(epochs):\n",
    "  train_loss, r2_train, rmse_train = train(train_dataloader, model, loss_functions, optimizer, device_in_use=device_in_use)\n",
    "  test_loss, r2_test, rmse_test = test(test_dataloader, model, loss_functions, device_in_use=device_in_use)\n",
    "  train_losses.append(train_loss)\n",
    "\n",
    "  # train_accuracies_2.append(train_accuracy_2)\n",
    "  # train_recalls.append(train_recall) \n",
    "  # train_f1_scores.append(train_f1)\n",
    "  test_losses.append(test_loss)\n",
    "\n",
    "  # test_accuracies_2.append(test_accuracy_2)\n",
    "  # test_recalls.append(test_recall)\n",
    "  # test_f1_scores.append(test_f1)\n",
    "  # Formatting for easier reading\n",
    "  epoch_str = f\"Epoch [{t+1:2}/{epochs}]\"\n",
    "  train_metrics = f\"Train: Loss {format_metric(train_loss)}, R2 {format_metric(r2_train)}, RMSE {format_metric(rmse_train)}\"\n",
    "  test_metrics = f\"Test: Loss {format_metric(test_loss)}, R2 {format_metric(r2_test)}, RMSE {format_metric(rmse_test)}\"\n",
    "  print(f\"{epoch_str:20} | {train_metrics:65} | {test_metrics}\")\n",
    "\n",
    "\n",
    "torch.save(model.state_dict(), 'final_model_trained.pth')\n",
    "\n",
    "# Plotting the loss curves\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 1, 1)\n",
    "plt.plot(range(1, epochs+1), train_losses, label='Train Loss')\n",
    "plt.plot(range(1, epochs+1), [l for l in test_losses], label='Test Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Test Loss Curve')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
