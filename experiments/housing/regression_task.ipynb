{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from rff.layers import GaussianEncoding #pip install random-fourier-features-pytorch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "import sys\n",
    "import importlib\n",
    "sys.path.append('../../helpers/')\n",
    "import helper\n",
    "# sys.path.append('../../model/')\n",
    "# from modified_emb_model import Classifier, train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is not available, using CPU instead\n"
     ]
    }
   ],
   "source": [
    "# Run regardless if you do or do not have GPU so all tensors are moved to right location later on\n",
    "if torch.cuda.is_available():\n",
    "    device_in_use = torch.device(\"cuda\")\n",
    "    print(\"GPU is available and being used\")\n",
    "else:\n",
    "    device_in_use = torch.device(\"cpu\")\n",
    "    print(\"GPU is not available, using CPU instead\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('./data/train.csv')\n",
    "df_test = pd.read_csv('./data/test.csv')\n",
    "df_val = pd.read_csv('./data/validation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['longitude', 'latitude', 'housing_median_age', 'total_rooms',\n",
       "       'total_bedrooms', 'population', 'households', 'median_income',\n",
       "       'ocean_proximity', 'median_house_value'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(df_train.columns))\n",
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ocean_proximity']\n",
      "['longitude', 'latitude', 'housing_median_age', 'total_rooms', 'total_bedrooms', 'population', 'households', 'median_income', 'median_house_value']\n",
      "5\n",
      "[5]\n"
     ]
    }
   ],
   "source": [
    "def categorize_columns(dataframe):\n",
    "    categorical_columns = []\n",
    "    continuous_columns = []\n",
    "    unique_classes_per_column = []  # To hold the number of unique classes for each categorical column\n",
    "\n",
    "    for column in dataframe.columns:\n",
    "        if dataframe[column].dtype == 'object' or len(dataframe[column].unique()) <= 10:\n",
    "            # If the column's data type is 'object' or it has 10 or fewer unique values, consider it categorical.\n",
    "            categorical_columns.append(column)\n",
    "            unique_classes_per_column.append(dataframe[column].nunique())  # Store the number of unique classes\n",
    "        else:\n",
    "            # Otherwise, consider it continuous.\n",
    "            continuous_columns.append(column)\n",
    "\n",
    "    # Calculate the total number of unique classes across all categorical columns.\n",
    "    total_unique_classes = sum(dataframe[col].nunique() for col in categorical_columns)\n",
    "\n",
    "    return categorical_columns, continuous_columns, total_unique_classes, unique_classes_per_column\n",
    "\n",
    "\n",
    "cat_cols, cont_cols, total_unique, unique_classes_per_column = categorize_columns(df_train)\n",
    "print(cat_cols)\n",
    "print(cont_cols)\n",
    "print(total_unique)\n",
    "print(unique_classes_per_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ocean_proximity\n",
       "0.0    5423\n",
       "1.0    3898\n",
       "4.0    1558\n",
       "3.0    1378\n",
       "2.0       2\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['ocean_proximity'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleTaskDataset(Dataset):\n",
    "    def __init__(self, df : pd.DataFrame, task1_column):\n",
    "        self.n = df.shape[0]\n",
    "        \n",
    "        self.task1_labels = df[task1_column].astype(np.int64).values\n",
    "\n",
    "        self.x = df.drop(task1_column, axis=1).astype(np.float32).values\n",
    "\n",
    "   \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Retrieve features and labels from the dataframe using column names\n",
    "        features = self.x[idx]\n",
    "        labels_task1 = self.task1_labels[idx]\n",
    "\n",
    "        return features, labels_task1\n",
    "        # return self.x[index], self.task1_labels[index], self.task2_labels[index]\n",
    "\n",
    "train_dataset = SingleTaskDataset(df_train, 'median_house_value')\n",
    "val_dataset = SingleTaskDataset(df_val, 'median_house_value')\n",
    "test_dataset = SingleTaskDataset(df_test, 'median_house_value')\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "# Wrapping with DataLoader for easy batch extraction\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CatNumTaskDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, task1_column, categorical_columns, numerical_columns):\n",
    "        self.n = df.shape[0]\n",
    "        self.task1_labels = df[task1_column].astype(np.int64).values\n",
    "        \n",
    "        # Extract categorical and numerical features\n",
    "        self.x_categ = df[categorical_columns].astype(np.int64).values\n",
    "        self.x_numer = df[numerical_columns].astype(np.float32).values\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Retrieve features and labels\n",
    "        categ_features = self.x_categ[idx]\n",
    "        numer_features = self.x_numer[idx]\n",
    "        labels_task1 = self.task1_labels[idx]\n",
    "\n",
    "        return categ_features, numer_features, labels_task1\n",
    "    \n",
    "# determine cont and cat columns\n",
    "cat_cols, cont_cols, num_unique_cats, unique_classes_per_column = categorize_columns(df_train)\n",
    "    \n",
    "train_dataset = CatNumTaskDataset(df_train, 'median_house_value', cat_cols, cont_cols)\n",
    "val_dataset = CatNumTaskDataset(df_val, 'median_house_value', cat_cols, cont_cols)\n",
    "test_dataset = CatNumTaskDataset(df_test, 'median_house_value', cat_cols, cont_cols)\n",
    "\n",
    "batch_size = 1064\n",
    "\n",
    "# Wrapping with DataLoader for easy batch extraction\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: 9\n",
      "Targets: 1\n",
      "Classes per Target: [0]\n"
     ]
    }
   ],
   "source": [
    "num_features =  9\n",
    "num_targets =   1\n",
    "classes_per_target = [0] # 0 for single target regression task\n",
    "\n",
    "print(f\"Features: {num_features}\")\n",
    "print(f\"Targets: {num_targets}\")\n",
    "print(f\"Classes per Target: {classes_per_target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_size, heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        self.head_dim = embed_size // heads\n",
    "        assert(self.head_dim * heads == embed_size), \"Embed size needs to be div by heads\"\n",
    "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.keys =nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.fc_out = nn.Linear(heads*self.head_dim, embed_size)\n",
    "\n",
    "\n",
    "    def forward(self, values, keys, query):\n",
    "        N = query.shape[0]\n",
    "        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
    "\n",
    "        values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
    "        keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n",
    "        queries = query.reshape(N, query_len, self.heads, self.head_dim)\n",
    "\n",
    "        values = self.values(values)\n",
    "        keys = self.keys(keys)\n",
    "        queries = self.queries(queries)\n",
    "\n",
    "        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
    "\n",
    "        attention = torch.softmax(energy / (self.embed_size ** (1/2)), dim=3) #(batch_size, head_dim, #query_embeddings, #key_embeddings)\n",
    "\n",
    "        # Calculate simplified attention scores\n",
    "        avg_attention = attention.mean(dim=0)  # Average across batches\n",
    "        # print(\"batch average\", avg_attention.shape)\n",
    "        avg_attention = avg_attention.mean(dim=0).squeeze(dim=0)\n",
    "        # print(\"head average\", avg_attention.shape)\n",
    "\n",
    "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(N, query_len, self.heads*self.head_dim) #(batch_size, n_features, embed_size)\n",
    "        out = self.fc_out(out)\n",
    "\n",
    "        return out, avg_attention\n",
    "    \n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_size, heads, dropout, forward_expansion, pre_norm_on):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "\n",
    "        self.pre_norm_on = pre_norm_on\n",
    "        if self.pre_norm_on:\n",
    "            self.pre_norm = nn.LayerNorm(embed_size)\n",
    "        self.attention = MultiHeadAttention(embed_size, heads)\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "\n",
    "        self.feed_forward = nn.Sequential(nn.Linear(embed_size, forward_expansion*embed_size),\n",
    "                                          nn.ReLU(),\n",
    "                                          nn.Linear(forward_expansion*embed_size, embed_size)\n",
    "                                          )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self,value,key,query):\n",
    "        if self.pre_norm_on:\n",
    "            query = self.pre_norm(query)\n",
    "            key = self.pre_norm(key)\n",
    "            value = self.pre_norm(value)\n",
    "            \n",
    "        attention, avg_attention = self.attention(value, key, query)\n",
    "        x = self.dropout(self.norm1(attention + query))\n",
    "        forward = self.feed_forward(x)\n",
    "        out = self.dropout(self.norm2(forward + x))\n",
    "        return out, avg_attention\n",
    "    \n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, embed_size, heads, forward_expansion, dropout, pre_norm_on):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "\n",
    "        self.attention = MultiHeadAttention(embed_size, heads)\n",
    "        self.norm = nn.LayerNorm(embed_size)\n",
    "        self.transformer_block = TransformerBlock(embed_size, heads, dropout, forward_expansion, pre_norm_on)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, value, key):\n",
    "        out, avg_attention = self.transformer_block(value, key, x)\n",
    "\n",
    "        return out, avg_attention\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 embed_size,\n",
    "                 num_layers,\n",
    "                 heads,\n",
    "                 forward_expansion,\n",
    "                 decoder_dropout,\n",
    "                 pre_norm_on\n",
    "    ):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "                [\n",
    "                    DecoderBlock(\n",
    "                        embed_size,\n",
    "                        heads,\n",
    "                        dropout=decoder_dropout,\n",
    "                        forward_expansion=forward_expansion,\n",
    "                        pre_norm_on=pre_norm_on\n",
    "                    )\n",
    "                    for _ in range(num_layers)\n",
    "                ]\n",
    "            )\n",
    "        self.avg_attention = None\n",
    "\n",
    "    def forward(self, class_embed, context):\n",
    "        for layer in self.layers:\n",
    "            # x is the classification embedding (CLS Token)\n",
    "            # context are the feature embeddings that will be used as key and value\n",
    "            x, self.avg_attention = layer(class_embed, context, context)\n",
    "  \n",
    "        return x \n",
    "\n",
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, sigma, embed_size, input_size, embedding_dropout, n_cat_features, n_cont_features, num_target_labels, rff_on):\n",
    "        super(Embeddings, self).__init__()\n",
    "\n",
    "        self.rff_on = rff_on\n",
    "\n",
    "        if self.rff_on:\n",
    "            self.rffs = nn.ModuleList([GaussianEncoding(sigma=sigma, input_size=input_size, encoded_size=embed_size//2) for _ in range(n_cont_features)])\n",
    "            self.dropout = nn.Dropout(embedding_dropout)\n",
    "            self.mlp_in = embed_size\n",
    "        else:\n",
    "            self.mlp_in = input_size\n",
    "\n",
    "        \n",
    "        \n",
    "        self.cont_embeddings = nn.ModuleList([nn.Linear(in_features=self.mlp_in, out_features=embed_size) for _ in range(n_cont_features)])\n",
    "\n",
    "        self.n_cat_features = n_cat_features\n",
    "        self.cat_embeddings = nn.ModuleList(nn.Embedding(num_embeddings=n_cat_features[_], embedding_dim=embed_size) for _ in range(len(n_cat_features)))\n",
    "\n",
    "        # Classifcation Embeddings for each target label\n",
    "        self.target_label_embeddings = nn.ModuleList([nn.Embedding(1, embed_size) for _ in range(num_target_labels)])\n",
    "\n",
    "\n",
    "    def forward(self, x_cat, x_cont):\n",
    "        # print(x_cat)\n",
    "        # print(x_cat.unsqueeze(2))\n",
    "        # print(x_cont)\n",
    "        # print(x_cont.unsqueeze(2))\n",
    "        x_cat = x_cat.unsqueeze(2) #(batch_size, n_features) -> (batch_size, n_features, 1)\n",
    "        x_cont = x_cont.unsqueeze(2)\n",
    "\n",
    "        rff_vectors = []\n",
    "        if self.rff_on:\n",
    "            for i, r in enumerate(self.rffs):\n",
    "                input = x_cont[:,i,:]\n",
    "                out = r(input)\n",
    "                rff_vectors.append(out)\n",
    "        \n",
    "            x_cont = torch.stack(rff_vectors, dim=1)\n",
    "        \n",
    "        embeddings = []\n",
    "        for i, e in enumerate(self.cont_embeddings):\n",
    "            # print(x_cont.shape)\n",
    "            goin_in = x_cont[:,i,:]\n",
    "            goin_out = e(goin_in)\n",
    "            # print(f'cont going out : {goin_out.shape}')\n",
    "            embeddings.append(goin_out)\n",
    "\n",
    "        # for i, e in enumerate(self.cat_embeddings):\n",
    "        #     # print(x_cat.shape)\n",
    "        #     goin_in = x_cat[:,i,:]\n",
    "        #     # print(f' in: {goin_in.shape}')\n",
    "\n",
    "        #     # the nn.Embeding module returns shape [bs, i, emb_dim] to allow accessing these embeddigns by an index\n",
    "        #     # I am assuming that we don't need to indecies, just the embeddings so I flatten\n",
    "\n",
    "        #     goin_out = e(goin_in) \n",
    "        #     # print(f'cat going out : {goin_out}')\n",
    "        #     embeddings.append(goin_out.squeeze(1))\n",
    "        for i, e in enumerate(self.cat_embeddings):\n",
    "            goin_in = x_cat[:, i, :]\n",
    "            goin_out = e(goin_in)\n",
    "            # print(goin_out.shape)\n",
    "            embeddings.append(goin_out.squeeze(1))\n",
    "\n",
    "        target_label_embeddings_ = []\n",
    "        for e in self.target_label_embeddings:\n",
    "            input = torch.tensor([0], device=x_cat.device)\n",
    "            temp = e(input)\n",
    "            temp = temp.repeat(x_cat.size(0), 1)\n",
    "            temp = temp.unsqueeze(1)\n",
    "            target_label_embeddings_.append(temp)\n",
    "\n",
    "        class_embeddings = torch.stack(target_label_embeddings_, dim=1)\n",
    "        \n",
    "        # class_embed = self.classification_embedding(torch.tensor([0], device=x.device))  # use index 0 for the classification embedding\n",
    "        # class_embed = class_embed.repeat(x.size(0), 1) # -> (batch_size, embed_size)\n",
    "        # class_embed = class_embed.unsqueeze(1)\n",
    "\n",
    "        context = torch.stack(embeddings, dim=1)\n",
    "\n",
    "        return class_embeddings, context\n",
    "\n",
    "class classificationHead(nn.Module):\n",
    "    def __init__(self, embed_size, dropout, mlp_scale_classification, num_target_classes):\n",
    "        super(classificationHead, self).__init__()\n",
    "        \n",
    "        #flattening the embeddings out so each sample in batch is represented with a 460 dimensional vector\n",
    "        self.input = embed_size\n",
    "        self.lin1 = nn.Linear(self.input, mlp_scale_classification*self.input)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        # self.lin2 = nn.Linear(2*self.input, 2*self.input)\n",
    "        self.lin3 = nn.Linear(mlp_scale_classification*self.input, self.input)\n",
    "        self.lin4 = nn.Linear(self.input, num_target_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.initialize_weights()\n",
    "\n",
    "    def initialize_weights(self): #he_initialization.\n",
    "        torch.nn.init.kaiming_normal_(self.lin1.weight, nonlinearity='relu')\n",
    "        torch.nn.init.zeros_(self.lin1.bias)\n",
    "\n",
    "        torch.nn.init.kaiming_normal_(self.lin3.weight, nonlinearity='relu')\n",
    "        torch.nn.init.zeros_(self.lin3.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x= torch.reshape(x, (-1, self.input))\n",
    "\n",
    "        x = self.lin1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.lin3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.lin4(x)\n",
    "  \n",
    "        return x\n",
    "    \n",
    "class regressionHead(nn.Module):\n",
    "    def __init__(self, embed_size, dropout, mlp_scale_regression):\n",
    "        super(regressionHead, self).__init__()\n",
    "        \n",
    "        self.input = embed_size\n",
    "        self.lin1 = nn.Linear(self.input, mlp_scale_regression * self.input)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.lin2 = nn.Linear(mlp_scale_regression * self.input, self.input)\n",
    "        self.lin3 = nn.Linear(self.input, 1)  # Output layer for regression (single output)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.initialize_weights()\n",
    "\n",
    "    def initialize_weights(self): # the initialization\n",
    "        torch.nn.init.kaiming_normal_(self.lin1.weight, nonlinearity='relu')\n",
    "        torch.nn.init.zeros_(self.lin1.bias)\n",
    "\n",
    "        torch.nn.init.kaiming_normal_(self.lin2.weight, nonlinearity='relu')\n",
    "        torch.nn.init.zeros_(self.lin2.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.reshape(x, (-1, self.input))\n",
    "\n",
    "        x = self.lin1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.lin2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.lin3(x) \n",
    "        return x    \n",
    "\n",
    "\n",
    "# DEFAULT PARAMETERS SET UP FOR MY DATASET. BE CAREFUL AND MAKE SURE YOU SET THEM UP HOW YOU WANT.\n",
    "# All dropout is initially turned off\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, \n",
    "                 rff_on = False,\n",
    "                 sigma=4,\n",
    "                 embed_size=20,\n",
    "                 input_size=1,\n",
    "                 embedding_dropout = 0,\n",
    "                 n_cat_features=66, # YOU WILL PROBABLY NEED TO CHANGE\n",
    "                 n_cont_features=2, # YOU WILL PROBABLY NEED TO CHANGE\n",
    "                 num_layers=1,\n",
    "                 heads=1,\n",
    "                 forward_expansion=4, # Determines how wide the MLP is in the encoder. Its a scaling factor. \n",
    "                 decoder_dropout=0,\n",
    "                 classification_dropout = 0,\n",
    "                 pre_norm_on = False,\n",
    "                 mlp_scale_classification = 4, #widens the mlp in the classification heads\n",
    "                 targets_classes : list= [0] #[12,2] #put the number of classes in each target variable. traffic type = 3 classes, application type = 8 classes\n",
    "                 ):\n",
    "        super(Classifier, self).__init__()\n",
    "\n",
    "        self.embeddings = Embeddings(rff_on=rff_on, sigma=sigma, embed_size=embed_size, input_size=input_size, embedding_dropout=embedding_dropout, n_cat_features=n_cat_features, n_cont_features=n_cont_features, num_target_labels=len(targets_classes))\n",
    "        self.decoder = Decoder(embed_size=embed_size, num_layers=num_layers, heads=heads, forward_expansion=forward_expansion, decoder_dropout=decoder_dropout, pre_norm_on=pre_norm_on)\n",
    "        self.classifying_heads = nn.ModuleList([regressionHead(embed_size=embed_size, dropout=classification_dropout, mlp_scale_regression=mlp_scale_classification)])\n",
    "        \n",
    "    def forward(self, x_cat, x_num):\n",
    "        \n",
    "        class_embed, context = self.embeddings(x_cat, x_num)\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        ########################\n",
    "        \n",
    "        # class_embed, context = self.embeddings(x)\n",
    "\n",
    "        x = self.decoder(class_embed, context)\n",
    "        \n",
    "        probability_dist_raw = []\n",
    "        for i, e in enumerate(self.classifying_heads):\n",
    "            input = x[:, i,:]\n",
    "            output = e(input)\n",
    "            probability_dist_raw.append(output)\n",
    "        \n",
    "        return probability_dist_raw\n",
    "\n",
    "\n",
    "# Training and Testing Loops\n",
    "# Should not need modification\n",
    "def train(dataloader, model, loss_functions : list, optimizer, device_in_use):\n",
    "  total_loss = 0\n",
    "  target_count = len(loss_functions)\n",
    "\n",
    "  total_correct = [0] * target_count\n",
    "  total_samples = [0] * target_count\n",
    "  all_targets   = [[] for _ in range(target_count)]\n",
    "  all_predictions = [[] for _ in range(target_count)]\n",
    "  y_pred_softmax = [0] * target_count\n",
    "  y_pred_labels = [0] * target_count\n",
    "  accuracy = []\n",
    "\n",
    "  model.train()\n",
    "\n",
    "  for (x_num, x_cat, targets) in dataloader:\n",
    "      \n",
    "    x_cat, x_num, targets = x_cat.to(device_in_use), x_num.to(device_in_use), targets.float().to(device_in_use)\n",
    "    task_predictions = model(x_num, x_cat) #contains a list of the tensor outputs for each task\n",
    "\n",
    "    print(task_predictions)\n",
    "    loss = 0\n",
    "    # print(len(loss_functions))\n",
    "    for i in range(len(loss_functions)):\n",
    "    #   print('yp')\n",
    "    #   print(i)\n",
    "    #   print(loss_functions[0])\n",
    "    #   print(task_predictions[0].dtype)\n",
    "    #   print(targets.dtype)\n",
    "      loss += loss_functions[0](task_predictions[0], targets)\n",
    "    loss = loss/len(loss_functions)\n",
    "    total_loss+= loss.item() #just summing the two losses\n",
    "    \n",
    "    mse_1 = ((task_predictions[0] - targets)**2).mean()\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "  #Loss\n",
    "  avg_loss = total_loss/len(dataloader)\n",
    "  #Accuracies\n",
    "  \n",
    "\n",
    "\n",
    "  return avg_loss, mse_1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def test(dataloader, model, loss_functions : list, device_in_use):\n",
    "  total_loss = 0\n",
    "  target_count = len(loss_functions)\n",
    "  total_correct = [0] * target_count\n",
    "  total_samples = [0] * target_count\n",
    "  all_targets   = [[] for _ in range(target_count)]\n",
    "  all_predictions = [[] for _ in range(target_count)]\n",
    "  y_pred_softmax = [0] * target_count\n",
    "  y_pred_labels = [0] * target_count\n",
    "  accuracy = []\n",
    "  f1       = []\n",
    "\n",
    "  model.eval()\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for (x_num, x_cat, targets) in dataloader:\n",
    "      x_cat, x_num, targets = x_cat.to(device_in_use), x_num.to(device_in_use), targets.float().to(device_in_use)\n",
    "\n",
    "      task_predictions = model(x_num, x_cat) #contains a list of the tensor outputs for each task\n",
    "      \n",
    "\n",
    "      loss = 0\n",
    "      for i in range(len(loss_functions)):\n",
    "        loss += loss_functions[0](task_predictions[0], targets)\n",
    "    #   loss = loss/len(loss_functions)\n",
    "    #   print(loss)\n",
    "      total_loss+= loss.item()\n",
    "\n",
    "      mse_1 = ((task_predictions[0] - targets)**2).mean()\n",
    "\n",
    "  #Loss\n",
    "  avgloss = total_loss/len(dataloader)\n",
    " \n",
    "  #Recall\n",
    "  # recall = recall_score(all_targets, all_predictions, average='weighted')\n",
    "\n",
    "\n",
    "  return avgloss, mse_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 9\n"
     ]
    }
   ],
   "source": [
    "print(len(cat_cols), len(cont_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Updated with f1\n",
    "model = Classifier(n_cat_features=unique_classes_per_column,\n",
    "                   n_cont_features=len(cont_cols)-1,\n",
    "                                   pre_norm_on=True, \n",
    "                                   rff_on=True, \n",
    "                                   forward_expansion=1, \n",
    "                                   mlp_scale_classification=2, \n",
    "                                   targets_classes=classes_per_target\n",
    "                                   ).to(device_in_use) # Instantiate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_functions = []\n",
    "loss_functions.append(nn.MSELoss())\n",
    "\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr = .001) # Maybe try messing around with optimizers. try other torch optimizers with different configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_metric(value): # Used to format the metrics output\n",
    "    return f\"{value:.5f}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [ 1/20]        | Train: Loss 0.50631,  MSE 0.51875                                 | Test: Loss 0.49928,  MSE  0.46826\n",
      "Epoch [ 2/20]        | Train: Loss 0.50464,  MSE 0.47226                                 | Test: Loss 0.49001,  MSE  0.52902\n",
      "Epoch [ 3/20]        | Train: Loss 0.49801,  MSE 0.49767                                 | Test: Loss 0.48708,  MSE  0.44522\n",
      "Epoch [ 4/20]        | Train: Loss 0.49340,  MSE 0.43619                                 | Test: Loss 0.48792,  MSE  0.52895\n",
      "Epoch [ 5/20]        | Train: Loss 0.49370,  MSE 0.47367                                 | Test: Loss 0.48633,  MSE  0.47843\n",
      "Epoch [ 6/20]        | Train: Loss 0.49546,  MSE 0.48808                                 | Test: Loss 0.48495,  MSE  0.48746\n",
      "Epoch [ 7/20]        | Train: Loss 0.49546,  MSE 0.51403                                 | Test: Loss 0.48427,  MSE  0.47678\n",
      "Epoch [ 8/20]        | Train: Loss 0.49563,  MSE 0.53047                                 | Test: Loss 0.48535,  MSE  0.50502\n",
      "Epoch [ 9/20]        | Train: Loss 0.49231,  MSE 0.44441                                 | Test: Loss 0.48640,  MSE  0.50006\n",
      "Epoch [10/20]        | Train: Loss 0.49539,  MSE 0.50356                                 | Test: Loss 0.48489,  MSE  0.47713\n",
      "Epoch [11/20]        | Train: Loss 0.49416,  MSE 0.49906                                 | Test: Loss 0.48479,  MSE  0.46622\n",
      "Epoch [12/20]        | Train: Loss 0.49624,  MSE 0.53248                                 | Test: Loss 0.48432,  MSE  0.47223\n",
      "Epoch [13/20]        | Train: Loss 0.49369,  MSE 0.48476                                 | Test: Loss 0.48715,  MSE  0.53074\n",
      "Epoch [14/20]        | Train: Loss 0.49597,  MSE 0.53367                                 | Test: Loss 0.48242,  MSE  0.40992\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "train_accuracies = []\n",
    "test_accuracies =  []\n",
    "\n",
    "for t in range(epochs):\n",
    "  train_loss, train_mse= train(train_dataloader, model, loss_functions, optimizer, device_in_use=device_in_use)\n",
    "  test_loss, test_mse = test(test_dataloader, model, loss_functions, device_in_use=device_in_use)\n",
    "\n",
    "  # Training metrics \n",
    "  train_losses.append(train_loss)\n",
    "  train_accuracies.append(train_mse.item())\n",
    "\n",
    "  # Test metrics \n",
    "  test_losses.append(test_loss)\n",
    "  test_accuracies.append(test_mse)\n",
    "\n",
    "  epoch_str = f\"Epoch [{t+1:2}/{epochs}]\"\n",
    "  # print(train_accuracies[0])\n",
    "  train_metrics = f\"Train: Loss {format_metric(train_loss)},  MSE {format_metric(train_mse)}\"\n",
    "  test_metrics = f\"Test: Loss {format_metric(test_loss)},  MSE  {format_metric(test_mse)}\"\n",
    "  print(f\"{epoch_str:20} | {train_metrics:65} | {test_metrics}\")\n",
    "\n",
    "\n",
    "\n",
    "# Plotting the loss curves\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(range(1, epochs+1), train_losses, label='Train Loss')\n",
    "plt.plot(range(1, epochs+1), [l for l in test_losses], label='Test Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Test Loss Curve')\n",
    "plt.legend()\n",
    "\n",
    "# Plotting the accuracy curves\n",
    "plt.subplot(1, 3, 2)\n",
    "# print(train_accuracies_1.dtype)\n",
    "plt.plot(range(1, epochs+1), train_accuracies, label='Train MSE')\n",
    "plt.plot(range(1, epochs+1), test_accuracies, label='Test MSE')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Test MSE Curve for Flow Bytes')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
