{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/aavila/CAT-Transformer/experiments/malware\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from rff.layers import GaussianEncoding #pip install random-fourier-features-pytorch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as snss\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import importlib\n",
    "sys.path.append('../../helpers/')\n",
    "import helper\n",
    "sys.path.append('../../model/')\n",
    "from ourModel import Classifier, train, test\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is not available, using CPU instead\n"
     ]
    }
   ],
   "source": [
    "# Run regardless if you do or do not have GPU so all tensors are moved to right location later on\n",
    "if torch.cuda.is_available():\n",
    "    device_in_use = torch.device(\"cuda\")\n",
    "    print(\"GPU is available and being used\")\n",
    "else:\n",
    "    device_in_use = torch.device(\"cpu\")\n",
    "    print(\"GPU is not available, using CPU instead\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(helper)\n",
    "df = pd.read_csv('../../datasets/MalwareData.csv', sep=\"|\")\n",
    "\n",
    "# df.isna().any().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)\n",
    "print(df.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_columns = [\"Name\", \"md5\"]\n",
    "\n",
    "# legit = df[0:41323].drop(columns=remove_columns, axis=1)\n",
    "# mal = df[41323::].drop(columns=remove_columns, axis=1)\n",
    "df = df[~df.isin([np.nan, np.inf, -np.inf]).any(axis=1)]\n",
    "# df\n",
    "df = df[0::].drop(columns=remove_columns, axis=1)\n",
    "df\n",
    "# df.drop(columns=remove_columns, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = ['legitimate']\n",
    "# target = \n",
    "for x in df:\n",
    "    print(df[x].value_counts())\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode the labels of the targets and create decoding dict\n",
    "label_encoders = []\n",
    "encoded_to_labels = []\n",
    "for x in range(len(target)):\n",
    "    label_encoders.append(LabelEncoder())\n",
    "    df[target[x]] = label_encoders[x].fit_transform(df[target[x]])\n",
    "    encoded_to_labels.append({encoded: label for label, encoded in zip(label_encoders[x].classes_, label_encoders[x].transform(label_encoders[x].classes_))})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(encoded_to_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset wrapper for the dataframe. Just a way to structure the data that pytorch likes and needs to then wrap with dataloader.\n",
    "\n",
    "class DatasetWrapper(Dataset):\n",
    "    def __init__(self, df : pd.DataFrame, target : str):\n",
    "        self.n = df.shape[0]\n",
    "        \n",
    "        self.y = df[target].astype(np.int64).values\n",
    "\n",
    "        self.scalar = StandardScaler()\n",
    "        self.x = self.scalar.fit_transform(df.drop(columns=target)).astype(np.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "    \n",
    "exp_dataset = DatasetWrapper(df, target=target) # UPDATE for your experiments\n",
    "\n",
    "# Split training and testing samples\n",
    "train_size = int(0.8*len(exp_dataset)) # Use a different ratio if you want\n",
    "test_size = len(exp_dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(exp_dataset, [train_size, test_size])\n",
    "\n",
    "print(f\"Train length: {len(train_dataset)}, Test length: {len(test_dataset)}\") # See the ratios\n",
    "\n",
    "batch_size = 256 #Hyperparameter that you should try messing with depending on the size of your dataset. The smaller it is, the more stochastic and chaotic the training gets.\n",
    "\n",
    "# Wrapping with dataloader so that its easy to extract batches from the train and test subsets\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True) \n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features =  len(train_dataset[0][0])\n",
    "num_targets =   len(train_dataset[0][1])\n",
    "classes_per_target = [len(df[t].unique()) for t in target ]\n",
    "\n",
    "print(f\"Features: {num_features}\")\n",
    "print(f\"Targets: {num_targets}\")\n",
    "print(f\"Classes per Target: {classes_per_target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Updated with f1\n",
    "model = Classifier(n_features=num_features, \n",
    "                                   pre_norm_on=True, \n",
    "                                   rff_on=True, \n",
    "                                   forward_expansion=1, \n",
    "                                   mlp_scale_classification=2, \n",
    "                                   targets_classes=classes_per_target\n",
    "                                   ).to(device_in_use) # Instantiate the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_functions = []\n",
    "#This loop could easily be adapted to us BinaryCrossEntropy for binary class and Cross for multi class\n",
    "for x in target:\n",
    "    loss_functions.append(nn.CrossEntropyLoss())\n",
    "\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr = 0.001) # Maybe try messing around with optimizers. try other torch optimizers with different configurations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 40 #Set the number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "test_losses = []\n",
    "train_accuracies = [[] for _ in range(num_targets)]\n",
    "test_accuracies =  [[] for _ in range(num_targets)]\n",
    "\n",
    "for t in range(epochs):\n",
    "  train_loss, train_accuracy= train(train_dataloader, model, loss_functions, optimizer, device_in_use=device_in_use)\n",
    "  test_loss, test_accuracy, all_predictions, all_targets, f1 = test(test_dataloader, model, loss_functions, device_in_use=device_in_use)\n",
    "\n",
    "  #Losses\n",
    "  train_losses.append(train_loss)\n",
    "  test_losses.append(test_loss)\n",
    "  #Accuracies\n",
    "  for i in range(num_targets):\n",
    "    train_accuracies[i].append(train_accuracy[i])\n",
    "    test_accuracies[i].append(test_accuracy[i])\n",
    " \n",
    "  # Formatting for easier reading\n",
    "  epoch_str = f\"Epoch [{t+1:2}/{epochs}]\"\n",
    "\n",
    "  train_metrics = f\"Train: Loss {train_loss:.2f}\"\n",
    "  for target, Acc in zip(target, train_accuracy):\n",
    "    train_metrics += f\", {target} Accuracy: {Acc:.2f}\"\n",
    "  \n",
    "  test_metrics = f\"Test: Loss {test_loss:.2f}\"\n",
    "  for target, Acc, fone in zip(target, test_accuracy, f1):\n",
    "    test_metrics += f\", {target} Accuracy: {Acc:.2f}, {target} F1: {fone:.2f}\"\n",
    "\n",
    "  print(f\"{epoch_str:15} | {train_metrics} | {test_metrics}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_num = 1\n",
    "# Plotting the loss curves\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 3, plot_num)\n",
    "plt.plot(range(1, epochs+1), train_losses, label='Train Loss')\n",
    "plt.plot(range(1, epochs+1), [l for l in test_losses], label='Test Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Test Loss Curve')\n",
    "plt.legend()\n",
    "plot_num+=1\n",
    "\n",
    "for i in range(num_targets):\n",
    "    plt.subplot(1, 3, plot_num)\n",
    "    plt.plot(range(1, epochs+1), train_accuracies[i], label='Train Accuracy')\n",
    "    plt.plot(range(1, epochs+1), test_accuracies[i], label='Test Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title(f'Training and Test Accuracy Curve for {target[i]}')\n",
    "    plt.legend()\n",
    "    plot_num+=1\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CAT-Former",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
