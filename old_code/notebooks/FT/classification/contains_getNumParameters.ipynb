{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\smbm2\\AppData\\Local\\miniconda3\\envs\\ml-env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from rff.layers import GaussianEncoding #pip install random-fourier-features-pytorch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import os\n",
    "import optuna\n",
    "from optuna.trial import TrialState\n",
    "from tab_transformer_pytorch import FTTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available and being used\n"
     ]
    }
   ],
   "source": [
    "# Run regardless if you do or do not have GPU so all tensors are moved to right location later on\n",
    "if torch.cuda.is_available():\n",
    "    device_in_use = torch.device(\"cuda\")\n",
    "    print(\"GPU is available and being used\")\n",
    "else:\n",
    "    device_in_use = torch.device(\"cpu\")\n",
    "    print(\"GPU is not available, using CPU instead\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD AND PROCESS DATA\n",
    "**EXAMPLE WITH Income**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['age', 'workclass', 'fnlwgt', 'education', 'education-num',\n",
       "       'marital-status', 'occupation', 'relationship', 'race', 'sex',\n",
       "       'capital-gain', 'capital-loss', 'hours-per-week', 'native-country',\n",
       "       'income'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_train = pd.read_csv('/home/cscadmin/CyberResearch/CAT-Transformer/datasets/income/train.csv')\n",
    "# df_test = pd.read_csv('/home/cscadmin/CyberResearch/CAT-Transformer/datasets/income/test.csv')\n",
    "# df_val = pd.read_csv('/home/cscadmin/CyberResearch/CAT-Transformer/datasets/income/validation.csv') #READ FROM RIGHT SPOT\n",
    "\n",
    "df_train = pd.read_csv(r'C:\\Users\\smbm2\\projects\\CAT-Transformer\\datasets\\income\\train.csv')\n",
    "df_test = pd.read_csv(r'C:\\Users\\smbm2\\projects\\CAT-Transformer\\datasets\\income\\test.csv')\n",
    "df_val = pd.read_csv(r'C:\\Users\\smbm2\\projects\\CAT-Transformer\\datasets\\income\\validation.csv') #READ FROM RIGHT SPOT\n",
    "\n",
    "#Take a look at what the datasets look like initially to get an idea\n",
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_columns = ['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss',\n",
    "       'hours-per-week']\n",
    "cat_columns = ['workclass', 'education', 'marital-status', 'occupation',\n",
    "       'relationship', 'race', 'sex', 'native-country']\n",
    "target = ['income']\n",
    "\n",
    "#CHECKING TO MAKE SURE YOUR LIST IS CORRECT (NO NEED TO TOUCH)\n",
    "yourlist = cont_columns + cat_columns+target\n",
    "yourlist.sort()\n",
    "oglist = list(df_train.columns)\n",
    "oglist.sort()\n",
    "\n",
    "assert(yourlist == oglist), \"You may of spelled feature name wrong or you forgot to put on of them in the list\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "16\n",
      "7\n",
      "16\n",
      "6\n",
      "5\n",
      "2\n",
      "43\n"
     ]
    }
   ],
   "source": [
    "#Get class counts and store in a list below\n",
    "\n",
    "for x in cat_columns:\n",
    "    print(max(len(df_train[x].value_counts()), len(df_val[x].value_counts()), len(df_test[x].value_counts())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually enter class counts in the same order\n",
    "cat_features = (10,16,7,16,6,5,2,43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the number of classes in your classification target\n",
    "target_classes = [max(len(df_train[target].value_counts()), len(df_val[target].value_counts()),len(df_test[target].value_counts()))]\n",
    "target_classes = target_classes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a StandardScaler and fit it to the cont features\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(df_train[cont_columns])\n",
    "\n",
    "# Transform the training, test, and validation datasets\n",
    "df_train[cont_columns] = scaler.transform(df_train[cont_columns])\n",
    "df_test[cont_columns] = scaler.transform(df_test[cont_columns])\n",
    "df_val[cont_columns] = scaler.transform(df_val[cont_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleTaskDataset(Dataset):\n",
    "    def __init__(self, df : pd.DataFrame, cat_columns, num_columns,task1_column):\n",
    "        self.n = df.shape[0]\n",
    "        \n",
    "        self.task1_labels = df[task1_column].astype(np.float32).values\n",
    "\n",
    "        self.cate = df[cat_columns].astype(np.int64).values\n",
    "        self.num = df[num_columns].astype(np.float32).values\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Retrieve features and labels from the dataframe using column names\n",
    "        cat_features = self.cate[idx]\n",
    "        num_features = self.num[idx]\n",
    "        labels_task1 = self.task1_labels[idx]\n",
    "\n",
    "        return cat_features, num_features, labels_task1\n",
    "\n",
    "#Wrapping in Dataset\n",
    "train_dataset = SingleTaskDataset(df_train, cat_columns, cont_columns, 'income')\n",
    "val_dataset = SingleTaskDataset(df_val, cat_columns, cont_columns, 'income')\n",
    "test_dataset = SingleTaskDataset(df_test, cat_columns, cont_columns, 'income')\n",
    "\n",
    "#This is a hyperparameter that is not tuned. Maybe mess with what makes sense here\n",
    "#Also try looking to see what other papers have done\n",
    "batch_size = 256\n",
    "\n",
    "# Wrapping with DataLoader for easy batch extraction\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 8])\n",
      "torch.Size([256, 6])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 8])\n",
      "torch.Size([256, 6])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 8])\n",
      "torch.Size([256, 6])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 8])\n",
      "torch.Size([256, 6])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 8])\n",
      "torch.Size([256, 6])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 8])\n",
      "torch.Size([256, 6])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 8])\n",
      "torch.Size([256, 6])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 8])\n",
      "torch.Size([256, 6])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 8])\n",
      "torch.Size([256, 6])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 8])\n",
      "torch.Size([256, 6])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 8])\n",
      "torch.Size([256, 6])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 8])\n",
      "torch.Size([256, 6])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 8])\n",
      "torch.Size([256, 6])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 8])\n",
      "torch.Size([256, 6])\n",
      "torch.Size([256])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 8])\n",
      "torch.Size([256, 6])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 8])\n",
      "torch.Size([256, 6])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 8])\n",
      "torch.Size([256, 6])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 8])\n",
      "torch.Size([256, 6])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 8])\n",
      "torch.Size([256, 6])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 8])\n",
      "torch.Size([256, 6])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 8])\n",
      "torch.Size([256, 6])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 8])\n",
      "torch.Size([256, 6])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 8])\n",
      "torch.Size([256, 6])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 8])\n",
      "torch.Size([256, 6])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 8])\n",
      "torch.Size([256, 6])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 8])\n",
      "torch.Size([256, 6])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 8])\n",
      "torch.Size([256, 6])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 8])\n",
      "torch.Size([256, 6])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 8])\n",
      "torch.Size([256, 6])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 8])\n",
      "torch.Size([256, 6])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 8])\n",
      "torch.Size([256, 6])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 8])\n",
      "torch.Size([256, 6])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 8])\n",
      "torch.Size([256, 6])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 8])\n",
      "torch.Size([256, 6])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 8])\n",
      "torch.Size([256, 6])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 8])\n",
      "torch.Size([256, 6])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 8])\n",
      "torch.Size([256, 6])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 8])\n",
      "torch.Size([256, 6])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 8])\n",
      "torch.Size([256, 6])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 8])\n",
      "torch.Size([256, 6])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 8])\n",
      "torch.Size([256, 6])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 8])\n",
      "torch.Size([256, 6])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 8])\n",
      "torch.Size([256, 6])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 8])\n",
      "torch.Size([256, 6])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 8])\n",
      "torch.Size([256, 6])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 8])\n",
      "torch.Size([256, 6])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 8])\n",
      "torch.Size([256, 6])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 8])\n",
      "torch.Size([256, 6])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 8])\n",
      "torch.Size([256, 6])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 8])\n",
      "torch.Size([256, 6])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 8])\n",
      "torch.Size([256, 6])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 8])\n",
      "torch.Size([256, 6])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 8])\n",
      "torch.Size([256, 6])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 8])\n",
      "torch.Size([256, 6])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 8])\n",
      "torch.Size([256, 6])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 8])\n",
      "torch.Size([256, 6])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 8])\n",
      "torch.Size([256, 6])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 8])\n",
      "torch.Size([256, 6])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 8])\n",
      "torch.Size([256, 6])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 8])\n",
      "torch.Size([256, 6])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 8])\n",
      "torch.Size([256, 6])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 8])\n",
      "torch.Size([256, 6])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 8])\n",
      "torch.Size([256, 6])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 8])\n",
      "torch.Size([256, 6])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 8])\n",
      "torch.Size([256, 6])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 8])\n",
      "torch.Size([256, 6])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 8])\n",
      "torch.Size([256, 6])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 8])\n",
      "torch.Size([256, 6])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 8])\n",
      "torch.Size([256, 6])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 8])\n",
      "torch.Size([256, 6])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 8])\n",
      "torch.Size([256, 6])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 8])\n",
      "torch.Size([256, 6])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 8])\n",
      "torch.Size([256, 6])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 8])\n",
      "torch.Size([256, 6])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 8])\n",
      "torch.Size([256, 6])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 8])\n",
      "torch.Size([256, 6])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 8])\n",
      "torch.Size([256, 6])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 8])\n",
      "torch.Size([256, 6])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 8])\n",
      "torch.Size([256, 6])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 8])\n",
      "torch.Size([256, 6])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 8])\n",
      "torch.Size([256, 6])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 8])\n",
      "torch.Size([256, 6])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 8])\n",
      "torch.Size([256, 6])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 8])\n",
      "torch.Size([256, 6])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 8])\n",
      "torch.Size([256, 6])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 8])\n",
      "torch.Size([256, 6])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 8])\n",
      "torch.Size([256, 6])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 8])\n",
      "torch.Size([256, 6])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 8])\n",
      "torch.Size([256, 6])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 8])\n",
      "torch.Size([256, 6])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 8])\n",
      "torch.Size([256, 6])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 8])\n",
      "torch.Size([256, 6])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 8])\n",
      "torch.Size([256, 6])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 8])\n",
      "torch.Size([256, 6])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 8])\n",
      "torch.Size([256, 6])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 8])\n",
      "torch.Size([256, 6])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 8])\n",
      "torch.Size([256, 6])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 8])\n",
      "torch.Size([256, 6])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 8])\n",
      "torch.Size([256, 6])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 8])\n",
      "torch.Size([256, 6])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 8])\n",
      "torch.Size([256, 6])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 8])\n",
      "torch.Size([256, 6])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 8])\n",
      "torch.Size([256, 6])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 8])\n",
      "torch.Size([256, 6])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 8])\n",
      "torch.Size([256, 6])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 8])\n",
      "torch.Size([256, 6])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 8])\n",
      "torch.Size([256, 6])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 8])\n",
      "torch.Size([256, 6])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 8])\n",
      "torch.Size([256, 6])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 8])\n",
      "torch.Size([256, 6])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 8])\n",
      "torch.Size([256, 6])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 8])\n",
      "torch.Size([256, 6])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 8])\n",
      "torch.Size([256, 6])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 8])\n",
      "torch.Size([256, 6])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 8])\n",
      "torch.Size([256, 6])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 8])\n",
      "torch.Size([256, 6])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 8])\n",
      "torch.Size([256, 6])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 8])\n",
      "torch.Size([256, 6])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 8])\n",
      "torch.Size([256, 6])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 8])\n",
      "torch.Size([256, 6])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 8])\n",
      "torch.Size([256, 6])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 8])\n",
      "torch.Size([256, 6])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 8])\n",
      "torch.Size([256, 6])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 8])\n",
      "torch.Size([256, 6])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 8])\n",
      "torch.Size([256, 6])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 8])\n",
      "torch.Size([256, 6])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 8])\n",
      "torch.Size([256, 6])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 8])\n",
      "torch.Size([256, 6])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 8])\n",
      "torch.Size([256, 6])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 8])\n",
      "torch.Size([256, 6])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 8])\n",
      "torch.Size([256, 6])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 8])\n",
      "torch.Size([256, 6])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 8])\n",
      "torch.Size([256, 6])\n",
      "torch.Size([256])\n",
      "torch.Size([141, 8])\n",
      "torch.Size([141, 6])\n",
      "torch.Size([141])\n"
     ]
    }
   ],
   "source": [
    "for cat, cont, label in train_dataloader:\n",
    "    print(cat.shape)\n",
    "    print(cont.shape)\n",
    "    print(label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each task loss is scaled by its own learnable parameter, then regularization is applied \n",
    "class LossFunctions(nn.Module):\n",
    "    def __init__(self, num_tasks):\n",
    "        super(LossFunctions, self).__init__()\n",
    "        self.num_tasks = num_tasks\n",
    "\n",
    "        self.loss_fns = [nn.CrossEntropyLoss() for x in range(num_tasks)] \n",
    "\n",
    "    def forward(self, predictions, labels_task1):\n",
    "\n",
    "        #task 1\n",
    "        target = labels_task1.long()\n",
    "        prediction = predictions\n",
    "        loss_fn = self.loss_fns[0]\n",
    "        task_loss = loss_fn(prediction, target)\n",
    "        \n",
    "        return task_loss\n",
    "    \n",
    "# Training and Testing Loops\n",
    "def train(dataloader, model, loss_function, optimizer, device_in_use):\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    total_correct_1 = 0\n",
    "    total_samples_1 = 0\n",
    "    all_targets_1 = []\n",
    "    all_predictions_1 = []\n",
    "\n",
    "    for (categorical, numerical, labels_task1) in dataloader:\n",
    "        categorical, numerical, labels_task1 = categorical.to(device_in_use), numerical.to(device_in_use), labels_task1.to(device_in_use)\n",
    "        \n",
    "        #running them through model and modifying the shape slightly for the loss function\n",
    "        task_predictions = model(categorical, numerical)\n",
    "        task_predictions = task_predictions.squeeze(1)\n",
    "        \n",
    "        loss = loss_function(task_predictions, labels_task1)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        #computing accuracy for first target\n",
    "        y_pred_softmax_1 = torch.softmax(task_predictions, dim=1)\n",
    "        _, y_pred_labels_1 = torch.max(y_pred_softmax_1, dim=1)\n",
    "        total_correct_1 += (y_pred_labels_1 == labels_task1).sum().item()\n",
    "        total_samples_1 += labels_task1.size(0)\n",
    "        all_targets_1.extend(labels_task1.cpu().numpy())\n",
    "        all_predictions_1.extend(y_pred_labels_1.cpu().numpy())\n",
    "\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    avg_loss = total_loss/len(dataloader)\n",
    "    accuracy_1 = total_correct_1 / total_samples_1\n",
    "\n",
    "    return avg_loss, accuracy_1\n",
    "\n",
    "def test(dataloader, model, loss_function, device_in_use):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    total_loss = 0\n",
    "    total_correct_1 = 0\n",
    "    total_samples_1 = 0\n",
    "    all_targets_1 = []\n",
    "    all_predictions_1 = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for (categorical, numerical, labels_task1) in dataloader:\n",
    "            categorical, numerical, labels_task1 = categorical.to(device_in_use), numerical.to(device_in_use), labels_task1.to(device_in_use)\n",
    "            \n",
    "            #running them through model and modifying the shape slightly for the loss function\n",
    "            task_predictions = model(categorical, numerical)\n",
    "            task_predictions = task_predictions.squeeze(1)\n",
    "            loss = loss_function(task_predictions, labels_task1)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            #computing accuracy for first target\n",
    "            y_pred_softmax_1 = torch.softmax(task_predictions, dim=1)\n",
    "            _, y_pred_labels_1 = torch.max(y_pred_softmax_1, dim=1)\n",
    "            total_correct_1 += (y_pred_labels_1 == labels_task1).sum().item()\n",
    "            total_samples_1 += labels_task1.size(0)\n",
    "            all_targets_1.extend(labels_task1.cpu().numpy())\n",
    "            all_predictions_1.extend(y_pred_labels_1.cpu().numpy())\n",
    "\n",
    "    avg = total_loss/len(dataloader)\n",
    "    accuracy_1 = total_correct_1 / total_samples_1\n",
    "    f1_1 = f1_score(all_targets_1, all_predictions_1, average='weighted')\n",
    "\n",
    "    return avg, accuracy_1, all_predictions_1, all_targets_1, f1_1\n",
    "\n",
    "def format_metric(value): # Used to format the metrics output\n",
    "    return f\"{value:.4f}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1653314"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1 = FTTransformer(categories=cat_features,\n",
    "                          num_continuous=len(cont_columns),\n",
    "                          dim=192,\n",
    "                          depth=3,\n",
    "                          heads=8,\n",
    "                          dim_head=16,\n",
    "                          dim_out=target_classes,\n",
    "                          attn_dropout=0.2,\n",
    "                          ff_dropout=0.1)\n",
    "    \n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "count_parameters(model1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUN EXPERIMENTS\n",
    "\n",
    "1. Using Optuna to optimize FT-Transformers hyperparameters for your dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the early stopping mechanism\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5):\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.best_metric = float('-inf')\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, metric):\n",
    "        if metric > self.best_metric:\n",
    "            self.best_metric = metric\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "\n",
    "# Function to log results to a text file\n",
    "def log_to_file(filename, text):\n",
    "    with open(filename, 'a') as f:\n",
    "        f.write(text + '\\n')\n",
    "\n",
    "def objective(trial):\n",
    "    trial_number = trial.number\n",
    "    '''\n",
    "        attn_dropout = 0.,\n",
    "        ff_dropout = 0.\n",
    "    '''\n",
    "\n",
    "    # Define hyperparameters to search over\n",
    "    depth = trial.suggest_int('depth', 1, 2)\n",
    "    # Ensure that embed_size is divisible by num_layers\n",
    "    dim = trial.suggest_categorical(\"dim\", [50, 60, 70, 80, 90, 100, 120, 140, 160])\n",
    "    heads = trial.suggest_categorical(\"heads\", [1, 5, 10])\n",
    "    dim_head = trial.suggest_categorical(\"dim_head\", [5,10,15,20,25,30])\n",
    "    attn_dropout = trial.suggest_categorical('attn_dropout', [0,.1,.2,.5])\n",
    "    ff_dropout = trial.suggest_categorical('ff_dropout', [0,.1,.2,.5])\n",
    "\n",
    "    learning_rate = trial.suggest_categorical('learning_rate', [0.0001, 0.001, 0.01])\n",
    "\n",
    "    num_epochs = 100\n",
    "\n",
    "    # Create your model with the sampled hyperparameters\n",
    "    model = FTTransformer(categories=cat_features,\n",
    "                          num_continuous=len(cont_columns),\n",
    "                          dim=dim,\n",
    "                          depth=depth,\n",
    "                          heads=heads,\n",
    "                          dim_head=dim_head,\n",
    "                          dim_out=target_classes,\n",
    "                          attn_dropout=attn_dropout,\n",
    "                          ff_dropout=ff_dropout).to(device_in_use)\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    loss_function = LossFunctions(1)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Initialize early stopping\n",
    "    early_stopping = EarlyStopping(patience=10)  # Adjust patience as needed\n",
    "\n",
    "    # Training loop with a large number of epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss, train_accuracy = train(train_dataloader, model, loss_function, optimizer, device_in_use)\n",
    "        \n",
    "        # Validation loop\n",
    "        val_loss, val_accuracy, _, _, _ = test(val_dataloader, model, loss_function, device_in_use)\n",
    "        \n",
    "        # Check if we should early stop based on validation accuracy\n",
    "        if early_stopping(val_accuracy):\n",
    "            break\n",
    "\n",
    "    \n",
    "    # Log the final test accuracy for this trial to a shared log file\n",
    "    final_log = f\"Trial {trial_number} completed. Validation Accuracy = {val_accuracy:.4f}\"\n",
    "    log_to_file('all_trials_log.txt', final_log)\n",
    "\n",
    "    # Return the test accuracy as the objective to optimize\n",
    "    return val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-04 16:43:05,822] A new study created in memory with name: no-name-ec87992c-fe47-4435-b56e-33a29362de9b\n",
      "  0%|          | 0/50 [00:05<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2023-11-04 16:43:11,252] Trial 0 failed with parameters: {'depth': 2, 'dim': 60, 'heads': 10, 'dim_head': 20, 'attn_dropout': 0.5, 'ff_dropout': 0.2, 'learning_rate': 0.01} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\smbm2\\AppData\\Local\\miniconda3\\envs\\ml-env\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\smbm2\\AppData\\Local\\Temp\\ipykernel_20892\\3221945157.py\", line 63, in objective\n",
      "    train_loss, train_accuracy = train(train_dataloader, model, loss_function, optimizer, device_in_use)\n",
      "                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\smbm2\\AppData\\Local\\Temp\\ipykernel_20892\\3054126438.py\", line 33, in train\n",
      "    task_predictions = model(categorical, numerical)\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\smbm2\\AppData\\Local\\miniconda3\\envs\\ml-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\smbm2\\AppData\\Local\\miniconda3\\envs\\ml-env\\Lib\\site-packages\\tab_transformer_pytorch\\ft_transformer.py\", line 211, in forward\n",
      "    x, attns = self.transformer(x, return_attn = True)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\smbm2\\AppData\\Local\\miniconda3\\envs\\ml-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\smbm2\\AppData\\Local\\miniconda3\\envs\\ml-env\\Lib\\site-packages\\tab_transformer_pytorch\\ft_transformer.py\", line 88, in forward\n",
      "    attn_out, post_softmax_attn = attn(x)\n",
      "                                  ^^^^^^^\n",
      "  File \"c:\\Users\\smbm2\\AppData\\Local\\miniconda3\\envs\\ml-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\smbm2\\AppData\\Local\\miniconda3\\envs\\ml-env\\Lib\\site-packages\\tab_transformer_pytorch\\ft_transformer.py\", line 48, in forward\n",
      "    q, k, v = self.to_qkv(x).chunk(3, dim = -1)\n",
      "              ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\smbm2\\AppData\\Local\\miniconda3\\envs\\ml-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\smbm2\\AppData\\Local\\miniconda3\\envs\\ml-env\\Lib\\site-packages\\torch\\nn\\modules\\linear.py\", line 114, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "[W 2023-11-04 16:43:11,320] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\smbm2\\projects\\CAT-Transformer\\notebooks\\FT\\classification\\cont_and_cat_classification copy.ipynb Cell 16\u001b[0m line \u001b[0;36m8\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/notebooks/FT/classification/cont_and_cat_classification%20copy.ipynb#X20sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m study \u001b[39m=\u001b[39m optuna\u001b[39m.\u001b[39mcreate_study(direction\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmaximize\u001b[39m\u001b[39m'\u001b[39m)  \u001b[39m# Maximize validation accuracy\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/notebooks/FT/classification/cont_and_cat_classification%20copy.ipynb#X20sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# Start the optimization process\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/notebooks/FT/classification/cont_and_cat_classification%20copy.ipynb#X20sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m study\u001b[39m.\u001b[39;49moptimize(objective, n_trials\u001b[39m=\u001b[39;49mnum_trials, show_progress_bar\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/notebooks/FT/classification/cont_and_cat_classification%20copy.ipynb#X20sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m# Get the best hyperparameters and the validation accuracy at the point of early stopping\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/notebooks/FT/classification/cont_and_cat_classification%20copy.ipynb#X20sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m best_params \u001b[39m=\u001b[39m study\u001b[39m.\u001b[39mbest_params\n",
      "File \u001b[1;32mc:\\Users\\smbm2\\AppData\\Local\\miniconda3\\envs\\ml-env\\Lib\\site-packages\\optuna\\study\\study.py:442\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    339\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moptimize\u001b[39m(\n\u001b[0;32m    340\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    341\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    348\u001b[0m     show_progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    349\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    350\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    351\u001b[0m \n\u001b[0;32m    352\u001b[0m \u001b[39m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    440\u001b[0m \u001b[39m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    441\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 442\u001b[0m     _optimize(\n\u001b[0;32m    443\u001b[0m         study\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m    444\u001b[0m         func\u001b[39m=\u001b[39;49mfunc,\n\u001b[0;32m    445\u001b[0m         n_trials\u001b[39m=\u001b[39;49mn_trials,\n\u001b[0;32m    446\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[0;32m    447\u001b[0m         n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[0;32m    448\u001b[0m         catch\u001b[39m=\u001b[39;49m\u001b[39mtuple\u001b[39;49m(catch) \u001b[39mif\u001b[39;49;00m \u001b[39misinstance\u001b[39;49m(catch, Iterable) \u001b[39melse\u001b[39;49;00m (catch,),\n\u001b[0;32m    449\u001b[0m         callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[0;32m    450\u001b[0m         gc_after_trial\u001b[39m=\u001b[39;49mgc_after_trial,\n\u001b[0;32m    451\u001b[0m         show_progress_bar\u001b[39m=\u001b[39;49mshow_progress_bar,\n\u001b[0;32m    452\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\smbm2\\AppData\\Local\\miniconda3\\envs\\ml-env\\Lib\\site-packages\\optuna\\study\\_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     65\u001b[0m     \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m---> 66\u001b[0m         _optimize_sequential(\n\u001b[0;32m     67\u001b[0m             study,\n\u001b[0;32m     68\u001b[0m             func,\n\u001b[0;32m     69\u001b[0m             n_trials,\n\u001b[0;32m     70\u001b[0m             timeout,\n\u001b[0;32m     71\u001b[0m             catch,\n\u001b[0;32m     72\u001b[0m             callbacks,\n\u001b[0;32m     73\u001b[0m             gc_after_trial,\n\u001b[0;32m     74\u001b[0m             reseed_sampler_rng\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m     75\u001b[0m             time_start\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m     76\u001b[0m             progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[0;32m     77\u001b[0m         )\n\u001b[0;32m     78\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     79\u001b[0m         \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\smbm2\\AppData\\Local\\miniconda3\\envs\\ml-env\\Lib\\site-packages\\optuna\\study\\_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    160\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 163\u001b[0m     frozen_trial \u001b[39m=\u001b[39m _run_trial(study, func, catch)\n\u001b[0;32m    164\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    165\u001b[0m     \u001b[39m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[39m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    167\u001b[0m     \u001b[39m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    168\u001b[0m     \u001b[39m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    169\u001b[0m     \u001b[39mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32mc:\\Users\\smbm2\\AppData\\Local\\miniconda3\\envs\\ml-env\\Lib\\site-packages\\optuna\\study\\_optimize.py:251\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    244\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mFalse\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mShould not reach.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    246\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m    247\u001b[0m     frozen_trial\u001b[39m.\u001b[39mstate \u001b[39m==\u001b[39m TrialState\u001b[39m.\u001b[39mFAIL\n\u001b[0;32m    248\u001b[0m     \u001b[39mand\u001b[39;00m func_err \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    249\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    250\u001b[0m ):\n\u001b[1;32m--> 251\u001b[0m     \u001b[39mraise\u001b[39;00m func_err\n\u001b[0;32m    252\u001b[0m \u001b[39mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32mc:\\Users\\smbm2\\AppData\\Local\\miniconda3\\envs\\ml-env\\Lib\\site-packages\\optuna\\study\\_optimize.py:200\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[39mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[39m.\u001b[39m_trial_id, study\u001b[39m.\u001b[39m_storage):\n\u001b[0;32m    199\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 200\u001b[0m         value_or_values \u001b[39m=\u001b[39m func(trial)\n\u001b[0;32m    201\u001b[0m     \u001b[39mexcept\u001b[39;00m exceptions\u001b[39m.\u001b[39mTrialPruned \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    202\u001b[0m         \u001b[39m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    203\u001b[0m         state \u001b[39m=\u001b[39m TrialState\u001b[39m.\u001b[39mPRUNED\n",
      "\u001b[1;32mc:\\Users\\smbm2\\projects\\CAT-Transformer\\notebooks\\FT\\classification\\cont_and_cat_classification copy.ipynb Cell 16\u001b[0m line \u001b[0;36m6\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/notebooks/FT/classification/cont_and_cat_classification%20copy.ipynb#X20sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m \u001b[39m# Training loop with a large number of epochs\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/notebooks/FT/classification/cont_and_cat_classification%20copy.ipynb#X20sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/notebooks/FT/classification/cont_and_cat_classification%20copy.ipynb#X20sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m     train_loss, train_accuracy \u001b[39m=\u001b[39m train(train_dataloader, model, loss_function, optimizer, device_in_use)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/notebooks/FT/classification/cont_and_cat_classification%20copy.ipynb#X20sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m     \u001b[39m# Validation loop\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/notebooks/FT/classification/cont_and_cat_classification%20copy.ipynb#X20sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m     val_loss, val_accuracy, _, _, _ \u001b[39m=\u001b[39m test(val_dataloader, model, loss_function, device_in_use)\n",
      "\u001b[1;32mc:\\Users\\smbm2\\projects\\CAT-Transformer\\notebooks\\FT\\classification\\cont_and_cat_classification copy.ipynb Cell 16\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/notebooks/FT/classification/cont_and_cat_classification%20copy.ipynb#X20sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m categorical, numerical, labels_task1 \u001b[39m=\u001b[39m categorical\u001b[39m.\u001b[39mto(device_in_use), numerical\u001b[39m.\u001b[39mto(device_in_use), labels_task1\u001b[39m.\u001b[39mto(device_in_use)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/notebooks/FT/classification/cont_and_cat_classification%20copy.ipynb#X20sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39m#running them through model and modifying the shape slightly for the loss function\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/notebooks/FT/classification/cont_and_cat_classification%20copy.ipynb#X20sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m task_predictions \u001b[39m=\u001b[39m model(categorical, numerical)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/notebooks/FT/classification/cont_and_cat_classification%20copy.ipynb#X20sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m task_predictions \u001b[39m=\u001b[39m task_predictions\u001b[39m.\u001b[39msqueeze(\u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/notebooks/FT/classification/cont_and_cat_classification%20copy.ipynb#X20sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m loss \u001b[39m=\u001b[39m loss_function(task_predictions, labels_task1)\n",
      "File \u001b[1;32mc:\\Users\\smbm2\\AppData\\Local\\miniconda3\\envs\\ml-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\smbm2\\AppData\\Local\\miniconda3\\envs\\ml-env\\Lib\\site-packages\\tab_transformer_pytorch\\ft_transformer.py:211\u001b[0m, in \u001b[0;36mFTTransformer.forward\u001b[1;34m(self, x_categ, x_numer, return_attn)\u001b[0m\n\u001b[0;32m    207\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((cls_tokens, x), dim \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[0;32m    209\u001b[0m \u001b[39m# attend\u001b[39;00m\n\u001b[1;32m--> 211\u001b[0m x, attns \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(x, return_attn \u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m    213\u001b[0m \u001b[39m# get cls token\u001b[39;00m\n\u001b[0;32m    215\u001b[0m x \u001b[39m=\u001b[39m x[:, \u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\smbm2\\AppData\\Local\\miniconda3\\envs\\ml-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\smbm2\\AppData\\Local\\miniconda3\\envs\\ml-env\\Lib\\site-packages\\tab_transformer_pytorch\\ft_transformer.py:88\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, x, return_attn)\u001b[0m\n\u001b[0;32m     85\u001b[0m post_softmax_attns \u001b[39m=\u001b[39m []\n\u001b[0;32m     87\u001b[0m \u001b[39mfor\u001b[39;00m attn, ff \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers:\n\u001b[1;32m---> 88\u001b[0m     attn_out, post_softmax_attn \u001b[39m=\u001b[39m attn(x)\n\u001b[0;32m     89\u001b[0m     post_softmax_attns\u001b[39m.\u001b[39mappend(post_softmax_attn)\n\u001b[0;32m     91\u001b[0m     x \u001b[39m=\u001b[39m attn_out \u001b[39m+\u001b[39m x\n",
      "File \u001b[1;32mc:\\Users\\smbm2\\AppData\\Local\\miniconda3\\envs\\ml-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\smbm2\\AppData\\Local\\miniconda3\\envs\\ml-env\\Lib\\site-packages\\tab_transformer_pytorch\\ft_transformer.py:48\u001b[0m, in \u001b[0;36mAttention.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     44\u001b[0m h \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mheads\n\u001b[0;32m     46\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm(x)\n\u001b[1;32m---> 48\u001b[0m q, k, v \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mto_qkv(x)\u001b[39m.\u001b[39mchunk(\u001b[39m3\u001b[39m, dim \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     49\u001b[0m q, k, v \u001b[39m=\u001b[39m \u001b[39mmap\u001b[39m(\u001b[39mlambda\u001b[39;00m t: rearrange(t, \u001b[39m'\u001b[39m\u001b[39mb n (h d) -> b h n d\u001b[39m\u001b[39m'\u001b[39m, h \u001b[39m=\u001b[39m h), (q, k, v))\n\u001b[0;32m     50\u001b[0m q \u001b[39m=\u001b[39m q \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscale\n",
      "File \u001b[1;32mc:\\Users\\smbm2\\AppData\\Local\\miniconda3\\envs\\ml-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\smbm2\\AppData\\Local\\miniconda3\\envs\\ml-env\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Set the number of optimization trials\n",
    "num_trials = 50\n",
    "\n",
    "# Create an Optuna study\n",
    "study = optuna.create_study(direction='maximize')  # Maximize validation accuracy\n",
    "\n",
    "# Start the optimization process\n",
    "study.optimize(objective, n_trials=num_trials, show_progress_bar=True)\n",
    "\n",
    "# Get the best hyperparameters and the validation accuracy at the point of early stopping\n",
    "best_params = study.best_params\n",
    "best_val_accuracy = study.best_value\n",
    "\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "print(\"Best Validation Accuracy (at Early Stopping):\", best_val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [ 1/75]        | Train: Loss 0.5173, Accuracy 0.7501                               | Test: Loss 0.3896, Accuracy 0.8288, F1 0.8175\n",
      "Epoch [ 2/75]        | Train: Loss 0.3476, Accuracy 0.8405                               | Test: Loss 0.3413, Accuracy 0.8427, F1 0.8438\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\smbm2\\projects\\CAT-Transformer\\notebooks\\FT\\cont_and_cat_classification.ipynb Cell 16\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/notebooks/FT/cont_and_cat_classification.ipynb#X14sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m all_attention_scores \u001b[39m=\u001b[39m []\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/notebooks/FT/cont_and_cat_classification.ipynb#X14sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/notebooks/FT/cont_and_cat_classification.ipynb#X14sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m   train_loss, train_accuracy_1 \u001b[39m=\u001b[39m train(train_dataloader, model, loss_functions, optimizer, device_in_use\u001b[39m=\u001b[39;49mdevice_in_use)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/notebooks/FT/cont_and_cat_classification.ipynb#X14sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m   test_loss, test_accuracy_1, all_predictions_1, all_targets_1, f1_1 \u001b[39m=\u001b[39m test(test_dataloader, model, loss_functions, device_in_use\u001b[39m=\u001b[39mdevice_in_use)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/notebooks/FT/cont_and_cat_classification.ipynb#X14sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m   train_losses\u001b[39m.\u001b[39mappend(train_loss)\n",
      "\u001b[1;32mc:\\Users\\smbm2\\projects\\CAT-Transformer\\notebooks\\FT\\cont_and_cat_classification.ipynb Cell 16\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/notebooks/FT/cont_and_cat_classification.ipynb#X14sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m all_targets_1\u001b[39m.\u001b[39mextend(labels_task1\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy())\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/notebooks/FT/cont_and_cat_classification.ipynb#X14sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m all_predictions_1\u001b[39m.\u001b[39mextend(y_pred_labels_1\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy())\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/notebooks/FT/cont_and_cat_classification.ipynb#X14sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/notebooks/FT/cont_and_cat_classification.ipynb#X14sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/notebooks/FT/cont_and_cat_classification.ipynb#X14sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\Users\\smbm2\\AppData\\Local\\miniconda3\\envs\\ml-env\\Lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    489\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\smbm2\\AppData\\Local\\miniconda3\\envs\\ml-env\\Lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Testing against the test dataset\n",
    "model = FTTransformer(categories=cat_features,\n",
    "                          num_continuous=len(cont_columns),\n",
    "                          dim=best_params['dim'],\n",
    "                          depth=best_params['depth'],\n",
    "                          heads=best_params['heads'],\n",
    "                          dim_head=best_params['dim_head'],\n",
    "                          dim_out=target_classes,\n",
    "                          attn_dropout=best_params['attn_dropout'],\n",
    "                          ff_dropout=best_params['ff_dropout']).to(device_in_use)\n",
    "loss_functions = LossFunctions(1)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr = best_params['learning_rate']) # Maybe try messing around with optimizers. try other torch optimizers with different configurations.\n",
    "epochs = 100 #Set the number of epochs\n",
    "early_stopping = EarlyStopping(patience=5)\n",
    "\n",
    "train_losses = []\n",
    "train_accuracies_1 = [] \n",
    "train_accuracies_2 = []\n",
    "train_recalls = [] \n",
    "train_f1_scores = [] \n",
    "test_losses = []\n",
    "test_accuracies_1 = []\n",
    "test_accuracies_2 = []\n",
    "test_recalls = []  \n",
    "test_f1_scores = [] \n",
    "all_attention_scores = []\n",
    "\n",
    "for t in range(epochs):\n",
    "  train_loss, train_accuracy_1 = train(train_dataloader, model, loss_functions, optimizer, device_in_use=device_in_use)\n",
    "  test_loss, test_accuracy_1, all_predictions_1, all_targets_1, f1_1 = test(test_dataloader, model, loss_functions, device_in_use=device_in_use)\n",
    "  train_losses.append(train_loss)\n",
    "  train_accuracies_1.append(train_accuracy_1)\n",
    "  # train_accuracies_2.append(train_accuracy_2)\n",
    "  # train_recalls.append(train_recall) \n",
    "  # train_f1_scores.append(train_f1)\n",
    "  test_losses.append(test_loss)\n",
    "  test_accuracies_1.append(test_accuracy_1)\n",
    "  # test_accuracies_2.append(test_accuracy_2)\n",
    "  # test_recalls.append(test_recall)\n",
    "  # test_f1_scores.append(test_f1)\n",
    "  # Formatting for easier reading\n",
    "  epoch_str = f\"Epoch [{t+1:2}/{epochs}]\"\n",
    "  train_metrics = f\"Train: Loss {format_metric(train_loss)}, Accuracy {format_metric(train_accuracy_1)}\"\n",
    "  test_metrics = f\"Test: Loss {format_metric(test_loss)}, Accuracy {format_metric(test_accuracy_1)}, F1 {format_metric(f1_1)}\"\n",
    "  print(f\"{epoch_str:20} | {train_metrics:65} | {test_metrics}\")\n",
    "\n",
    "  if early_stopping(test_accuracy_1):\n",
    "    break\n",
    "\n",
    "torch.save(model.state_dict(), 'final_model_trained.pth')\n",
    "\n",
    "# Plotting the loss curves\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 1, 1)\n",
    "plt.plot(range(1, epochs+1), train_losses, label='Train Loss')\n",
    "plt.plot(range(1, epochs+1), [l for l in test_losses], label='Test Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Test Loss Curve')\n",
    "plt.legend()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
