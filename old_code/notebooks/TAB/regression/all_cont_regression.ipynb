{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\smbm2\\AppData\\Local\\miniconda3\\envs\\ml-env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from rff.layers import GaussianEncoding #pip install random-fourier-features-pytorch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import os\n",
    "import optuna\n",
    "from optuna.trial import TrialState\n",
    "from tab_transformer_pytorch import TabTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available and being used\n"
     ]
    }
   ],
   "source": [
    "# Run regardless if you do or do not have GPU so all tensors are moved to right location later on\n",
    "if torch.cuda.is_available():\n",
    "    device_in_use = torch.device(\"cuda\")\n",
    "    print(\"GPU is available and being used\")\n",
    "else:\n",
    "    device_in_use = torch.device(\"cpu\")\n",
    "    print(\"GPU is not available, using CPU instead\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD AND PROCESS DATA\n",
    "**EXAMPLE WITH California Sklearn DATASET**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedInc</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>AveRooms</th>\n",
       "      <th>AveBedrms</th>\n",
       "      <th>Population</th>\n",
       "      <th>AveOccup</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.8631</td>\n",
       "      <td>20.0</td>\n",
       "      <td>4.401210</td>\n",
       "      <td>1.076613</td>\n",
       "      <td>999.0</td>\n",
       "      <td>2.014113</td>\n",
       "      <td>32.79</td>\n",
       "      <td>-117.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.2026</td>\n",
       "      <td>24.0</td>\n",
       "      <td>5.617544</td>\n",
       "      <td>0.989474</td>\n",
       "      <td>731.0</td>\n",
       "      <td>2.564912</td>\n",
       "      <td>34.59</td>\n",
       "      <td>-120.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.1094</td>\n",
       "      <td>14.0</td>\n",
       "      <td>5.869565</td>\n",
       "      <td>1.094203</td>\n",
       "      <td>302.0</td>\n",
       "      <td>2.188406</td>\n",
       "      <td>39.26</td>\n",
       "      <td>-121.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.3068</td>\n",
       "      <td>52.0</td>\n",
       "      <td>4.801205</td>\n",
       "      <td>1.066265</td>\n",
       "      <td>1526.0</td>\n",
       "      <td>2.298193</td>\n",
       "      <td>37.77</td>\n",
       "      <td>-122.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0791</td>\n",
       "      <td>11.0</td>\n",
       "      <td>5.878902</td>\n",
       "      <td>1.098493</td>\n",
       "      <td>4773.0</td>\n",
       "      <td>2.568891</td>\n",
       "      <td>33.17</td>\n",
       "      <td>-117.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14442</th>\n",
       "      <td>6.3700</td>\n",
       "      <td>35.0</td>\n",
       "      <td>6.129032</td>\n",
       "      <td>0.926267</td>\n",
       "      <td>658.0</td>\n",
       "      <td>3.032258</td>\n",
       "      <td>33.78</td>\n",
       "      <td>-117.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14443</th>\n",
       "      <td>3.0500</td>\n",
       "      <td>33.0</td>\n",
       "      <td>6.868597</td>\n",
       "      <td>1.269488</td>\n",
       "      <td>1753.0</td>\n",
       "      <td>3.904232</td>\n",
       "      <td>34.02</td>\n",
       "      <td>-117.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14444</th>\n",
       "      <td>2.9344</td>\n",
       "      <td>36.0</td>\n",
       "      <td>3.986717</td>\n",
       "      <td>1.079696</td>\n",
       "      <td>1756.0</td>\n",
       "      <td>3.332068</td>\n",
       "      <td>34.03</td>\n",
       "      <td>-118.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14445</th>\n",
       "      <td>5.7192</td>\n",
       "      <td>15.0</td>\n",
       "      <td>6.395349</td>\n",
       "      <td>1.067979</td>\n",
       "      <td>1777.0</td>\n",
       "      <td>3.178891</td>\n",
       "      <td>37.58</td>\n",
       "      <td>-121.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14446</th>\n",
       "      <td>2.5755</td>\n",
       "      <td>52.0</td>\n",
       "      <td>3.402576</td>\n",
       "      <td>1.058776</td>\n",
       "      <td>2619.0</td>\n",
       "      <td>2.108696</td>\n",
       "      <td>37.77</td>\n",
       "      <td>-122.42</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14447 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
       "0      2.8631      20.0  4.401210   1.076613       999.0  2.014113     32.79   \n",
       "1      4.2026      24.0  5.617544   0.989474       731.0  2.564912     34.59   \n",
       "2      3.1094      14.0  5.869565   1.094203       302.0  2.188406     39.26   \n",
       "3      3.3068      52.0  4.801205   1.066265      1526.0  2.298193     37.77   \n",
       "4      4.0791      11.0  5.878902   1.098493      4773.0  2.568891     33.17   \n",
       "...       ...       ...       ...        ...         ...       ...       ...   \n",
       "14442  6.3700      35.0  6.129032   0.926267       658.0  3.032258     33.78   \n",
       "14443  3.0500      33.0  6.868597   1.269488      1753.0  3.904232     34.02   \n",
       "14444  2.9344      36.0  3.986717   1.079696      1756.0  3.332068     34.03   \n",
       "14445  5.7192      15.0  6.395349   1.067979      1777.0  3.178891     37.58   \n",
       "14446  2.5755      52.0  3.402576   1.058776      2619.0  2.108696     37.77   \n",
       "\n",
       "       Longitude  \n",
       "0        -117.09  \n",
       "1        -120.14  \n",
       "2        -121.00  \n",
       "3        -122.45  \n",
       "4        -117.33  \n",
       "...          ...  \n",
       "14442    -117.96  \n",
       "14443    -117.43  \n",
       "14444    -118.38  \n",
       "14445    -121.96  \n",
       "14446    -122.42  \n",
       "\n",
       "[14447 rows x 8 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_train = pd.read_csv('/home/cscadmin/CyberResearch/CAT-Transformer/datasets/california/train.csv')\n",
    "# df_test = pd.read_csv('/home/cscadmin/CyberResearch/CAT-Transformer/datasets/california/test.csv')\n",
    "# df_val = pd.read_csv('/home/cscadmin/CyberResearch/CAT-Transformer/datasets/california/validation.csv') #READ FROM RIGHT SPOT\n",
    "\n",
    "df_train = pd.read_csv(r'C:\\Users\\smbm2\\projects\\CAT-Transformer\\datasets\\california\\train.csv')\n",
    "df_test = pd.read_csv(r'C:\\Users\\smbm2\\projects\\CAT-Transformer\\datasets\\california\\test.csv')\n",
    "df_val = pd.read_csv(r'C:\\Users\\smbm2\\projects\\CAT-Transformer\\datasets\\california\\validation.csv') #READ FROM RIGHT SPOT\n",
    "\n",
    "#Take a look at what the datasets look like initially to get an idea\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_columns = ['HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup',\n",
    "       'Latitude', 'Longitude']\n",
    "target = ['MedInc']\n",
    "\n",
    "#CHECKING TO MAKE SURE YOUR LIST IS CORRECT (NO NEED TO TOUCH)\n",
    "yourlist = cont_columns + target\n",
    "yourlist.sort()\n",
    "oglist = list(df_train.columns)\n",
    "oglist.sort()\n",
    "\n",
    "assert(yourlist == oglist), \"You may of spelled feature name wrong or you forgot to put on of them in the list\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a StandardScaler and fit it to the cont features\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(df_train[cont_columns])\n",
    "\n",
    "# Transform the training, test, and validation datasets\n",
    "df_train[cont_columns] = scaler.transform(df_train[cont_columns])\n",
    "df_test[cont_columns] = scaler.transform(df_test[cont_columns])\n",
    "df_val[cont_columns] = scaler.transform(df_val[cont_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleTaskDataset(Dataset):\n",
    "    def __init__(self, df : pd.DataFrame, num_columns,task1_column):\n",
    "        self.n = df.shape[0]\n",
    "        \n",
    "        self.task1_labels = df[task1_column].astype(np.float32).values\n",
    "\n",
    "        self.num = df[num_columns].astype(np.float32).values\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Retrieve features and labels from the dataframe using column names\n",
    "        num_features = self.num[idx]\n",
    "        labels_task1 = self.task1_labels[idx]\n",
    "\n",
    "        return num_features, labels_task1\n",
    "\n",
    "#Wrapping in Dataset\n",
    "train_dataset = SingleTaskDataset(df_train, cont_columns, 'MedInc')\n",
    "val_dataset = SingleTaskDataset(df_val, cont_columns, 'MedInc')\n",
    "test_dataset = SingleTaskDataset(df_test, cont_columns, 'MedInc')\n",
    "\n",
    "#This is a hyperparameter that is not tuned. Maybe mess with what makes sense here\n",
    "#Also try looking to see what other papers have done\n",
    "batch_size = 256\n",
    "\n",
    "# Wrapping with DataLoader for easy batch extraction\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each task loss is scaled by its own learnable parameter, then regularization is applied \n",
    "class LossFunctions(nn.Module):\n",
    "    def __init__(self, num_tasks):\n",
    "        super(LossFunctions, self).__init__()\n",
    "        self.num_tasks = num_tasks\n",
    "\n",
    "        self.loss_fns = [nn.MSELoss() for x in range(num_tasks)] \n",
    "\n",
    "    def forward(self, predictions, labels_task1):\n",
    "\n",
    "        #task 1\n",
    "        target = labels_task1\n",
    "        prediction = predictions\n",
    "        loss_fn = self.loss_fns[0]\n",
    "        task_loss = loss_fn(prediction, target)\n",
    "        \n",
    "        return task_loss\n",
    "    \n",
    "# Training and Testing Loops\n",
    "def train(dataloader, model, loss_function, optimizer, device_in_use):\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    total_r2_score = 0\n",
    "    root_mean_squared_error_total = 0\n",
    "\n",
    "    for (numerical, labels_task1) in dataloader:\n",
    "        numerical, labels_task1 = numerical.to(device_in_use), labels_task1.to(device_in_use)\n",
    "        #FT requires categorical, so must pass empty tensor since there are no cat features in this dataset\n",
    "        categorical = torch.tensor([]).to(device_in_use)\n",
    "        #running them through model and modifying the shape slightly for the loss function\n",
    "        task_predictions = model(categorical, numerical)\n",
    "        task_predictions = task_predictions.squeeze(1)\n",
    "        \n",
    "        loss = loss_function(task_predictions, labels_task1)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Calculate R^2 score for the regression task\n",
    "        r2 = r2_score_manual(labels_task1, task_predictions)\n",
    "        total_r2_score += r2\n",
    "\n",
    "        # Calculate RMSE score for the regression task\n",
    "        rmse_value = rmse(labels_task1, task_predictions)\n",
    "        root_mean_squared_error_total+=rmse_value\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    avg_r2_score = total_r2_score / len(dataloader)\n",
    "    avg_rmse_score = root_mean_squared_error_total / len(dataloader)\n",
    "\n",
    "    return avg_loss, avg_r2_score, avg_rmse_score\n",
    "\n",
    "def test(dataloader, model, loss_function, device_in_use):\n",
    "  model.eval()\n",
    "  total_loss = 0\n",
    "  \n",
    "  total_loss = 0\n",
    "  total_r2_score = 0\n",
    "  root_mean_squared_error_total = 0\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for (numerical, labels_task1) in dataloader:\n",
    "        numerical, labels_task1 = numerical.to(device_in_use), labels_task1.to(device_in_use)\n",
    "\n",
    "        categorical = torch.tensor([]).to(device_in_use)\n",
    "        task_predictions = model(categorical, numerical)\n",
    "        task_predictions = task_predictions.squeeze(1)\n",
    "        loss = loss_function(task_predictions, labels_task1)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Calculate R^2 score for the regression task\n",
    "        r2 = r2_score_manual(labels_task1, task_predictions)\n",
    "        total_r2_score += r2\n",
    "        \n",
    "        # Calculate RMSE score for the regression task\n",
    "        rmse_value = rmse(labels_task1, task_predictions)\n",
    "        root_mean_squared_error_total+=rmse_value\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    avg_r2_score = total_r2_score / len(dataloader)\n",
    "    avg_rmse_score = root_mean_squared_error_total / len(dataloader)\n",
    "\n",
    "    return avg_loss, avg_r2_score, avg_rmse_score\n",
    "\n",
    "def format_metric(value): # Used to format the metrics output\n",
    "    return f\"{value:.4f}\"\n",
    "\n",
    "def r2_score_manual(y_true, y_pred):\n",
    "    # Calculate the mean of true labels\n",
    "    y_mean = torch.mean(y_true)\n",
    "\n",
    "    # Calculate the total sum of squares\n",
    "    total_ss = torch.sum((y_true - y_mean)**2)\n",
    "\n",
    "    # Calculate the residual sum of squares\n",
    "    residual_ss = torch.sum((y_true - y_pred)**2)\n",
    "\n",
    "    # Calculate R-squared\n",
    "    r2 = 1 - (residual_ss / total_ss)\n",
    "\n",
    "    return r2.item()  # Convert to a Python float\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    # Calculate the squared differences\n",
    "    squared_diff = (y_true - y_pred)**2\n",
    "\n",
    "    # Calculate the mean of the squared differences\n",
    "    mean_squared_diff = torch.mean(squared_diff)\n",
    "\n",
    "    # Calculate the square root to obtain RMSE\n",
    "    rmse = torch.sqrt(mean_squared_diff)\n",
    "\n",
    "    return rmse.item()  # Convert to a Python float"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUN EXPERIMENTS\n",
    "\n",
    "1. Using Optuna to optimize FT-Transformers hyperparameters for your dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the early stopping mechanism\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5):\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.best_metric = float('-inf')\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, metric):\n",
    "        if metric > self.best_metric:\n",
    "            self.best_metric = metric\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "\n",
    "# Function to log results to a text file\n",
    "def log_to_file(filename, text):\n",
    "    with open(filename, 'a') as f:\n",
    "        f.write(text + '\\n')\n",
    "\n",
    "def objective(trial):\n",
    "    trial_number = trial.number\n",
    "    '''\n",
    "        attn_dropout = 0.,\n",
    "        ff_dropout = 0.\n",
    "    '''\n",
    "\n",
    "    # Define hyperparameters to search over\n",
    "    depth = trial.suggest_int('depth', 1, 2,3,4,5,6,7,8)\n",
    "    # Ensure that embed_size is divisible by num_layers\n",
    "    dim = trial.suggest_categorical(\"dim\", [50, 60, 70, 80, 90, 100, 120, 140, 160])\n",
    "    heads = trial.suggest_categorical(\"heads\", [1, 5, 10])\n",
    "    dim_head = trial.suggest_categorical(\"dim_head\", [5,10,15,20,25,30])\n",
    "    attn_dropout = trial.suggest_categorical('attn_dropout', [0,.1,.2,.5])\n",
    "    ff_dropout = trial.suggest_categorical('ff_dropout', [0,.1,.2,.5])\n",
    "    mlp_act = trial.suggest_categorical('mlp_act', [])\n",
    "\n",
    "    learning_rate = trial.suggest_categorical('learning_rate', [0.0001, 0.001, 0.01])\n",
    "\n",
    "    num_epochs = 2\n",
    "\n",
    "\n",
    "    # Create your model with the sampled hyperparameters\n",
    "    model = TabTransformer(categories=(),\n",
    "                           num_continuous=len(cont_columns),\n",
    "                           dim=dim,\n",
    "                           depth=depth,\n",
    "                           heads=heads,\n",
    "                           dim_head=dim_head,\n",
    "                           dim_out=1,\n",
    "                           mlp_hidden_mults=,\n",
    "                            mlp_act: Any | None = None,\n",
    "                            num_special_tokens: int = 2,\n",
    "                            continuous_mean_std: Any | None = None,\n",
    "                            attn_dropout: float = 0,\n",
    "                            ff_dropout: float = 0\n",
    "                           )\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    loss_function = LossFunctions(1)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Initialize early stopping\n",
    "    early_stopping = EarlyStopping(patience=10)  # Adjust patience as needed\n",
    "\n",
    "    # Training loop with a large number of epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss, r2_train, rmse_train = train(train_dataloader, model, loss_function, optimizer, device_in_use)\n",
    "        \n",
    "        # Validation loop\n",
    "        test_loss, r2_test, rmse_test = test(val_dataloader, model, loss_function, device_in_use)\n",
    "        \n",
    "        # Check if we should early stop based on validation rmse\n",
    "        if early_stopping(rmse_test):\n",
    "            break\n",
    "    \n",
    "    # Log the final test accuracy for this trial to a shared log file\n",
    "    final_log = f\"Trial {trial_number} completed. Validation RMSE = {rmse_test:.4f}\"\n",
    "    log_to_file('all_trials_log.txt', final_log)\n",
    "\n",
    "    # Return the test accuracy as the objective to optimize\n",
    "    return rmse_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-30 17:17:32,695] A new study created in memory with name: no-name-7d0d0827-4ac9-42bd-9699-146be859bef4\n",
      "Best trial: 0. Best value: 1.85345:  33%|███▎      | 1/3 [00:00<00:01,  1.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2023-10-30 17:17:33,695] Trial 0 finished with value: 1.8534508485060472 and parameters: {'depth': 1, 'dim': 160, 'heads': 5, 'dim_head': 15, 'attn_dropout': 0, 'ff_dropout': 0.2, 'learning_rate': 0.01}. Best is trial 0 with value: 1.8534508485060472.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 1.85345:  67%|██████▋   | 2/3 [00:02<00:01,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2023-10-30 17:17:34,782] Trial 1 finished with value: 2.040074797777029 and parameters: {'depth': 2, 'dim': 80, 'heads': 1, 'dim_head': 10, 'attn_dropout': 0.5, 'ff_dropout': 0.2, 'learning_rate': 0.0001}. Best is trial 0 with value: 1.8534508485060472.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 2. Best value: 1.30279: 100%|██████████| 3/3 [00:03<00:00,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2023-10-30 17:17:35,980] Trial 2 finished with value: 1.3027858092234685 and parameters: {'depth': 2, 'dim': 80, 'heads': 10, 'dim_head': 25, 'attn_dropout': 0, 'ff_dropout': 0.5, 'learning_rate': 0.001}. Best is trial 2 with value: 1.3027858092234685.\n",
      "Best Hyperparameters: {'depth': 2, 'dim': 80, 'heads': 10, 'dim_head': 25, 'attn_dropout': 0, 'ff_dropout': 0.5, 'learning_rate': 0.001}\n",
      "Best Validation RMSE (at Early Stopping): 1.3027858092234685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Set the number of optimization trials\n",
    "num_trials = 3\n",
    "\n",
    "# Create an Optuna study\n",
    "study = optuna.create_study(direction='minimize')  # Maximize validation accuracy\n",
    "\n",
    "# Start the optimization process\n",
    "study.optimize(objective, n_trials=num_trials, show_progress_bar=True)\n",
    "\n",
    "# Get the best hyperparameters and the validation accuracy at the point of early stopping\n",
    "best_params = study.best_params\n",
    "best_val_rmse = study.best_value\n",
    "\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "print(\"Best Validation RMSE (at Early Stopping):\", best_val_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [ 1/75]        | Train: Loss 3.9265, R2 -0.0887, RMSE 1.9477                       | Test: Loss 2.5701, R2 0.2847, RMSE 1.5962\n",
      "Epoch [ 2/75]        | Train: Loss 1.9876, R2 0.4561, RMSE 1.3922                        | Test: Loss 1.5746, R2 0.5792, RMSE 1.2507\n",
      "Epoch [ 3/75]        | Train: Loss 1.2633, R2 0.6523, RMSE 1.1154                        | Test: Loss 1.1124, R2 0.6911, RMSE 1.0498\n",
      "Epoch [ 4/75]        | Train: Loss 1.0668, R2 0.7062, RMSE 1.0273                        | Test: Loss 1.2599, R2 0.7102, RMSE 1.0760\n",
      "Epoch [ 5/75]        | Train: Loss 0.9745, R2 0.7318, RMSE 0.9786                        | Test: Loss 0.9202, R2 0.7518, RMSE 0.9544\n",
      "Epoch [ 6/75]        | Train: Loss 0.9247, R2 0.7437, RMSE 0.9560                        | Test: Loss 0.9212, R2 0.7473, RMSE 0.9536\n",
      "Epoch [ 7/75]        | Train: Loss 0.8987, R2 0.7496, RMSE 0.9439                        | Test: Loss 0.8648, R2 0.7584, RMSE 0.9247\n",
      "Epoch [ 8/75]        | Train: Loss 0.8615, R2 0.7620, RMSE 0.9234                        | Test: Loss 0.8589, R2 0.7607, RMSE 0.9208\n",
      "Epoch [ 9/75]        | Train: Loss 0.8537, R2 0.7624, RMSE 0.9207                        | Test: Loss 0.9203, R2 0.7415, RMSE 0.9576\n",
      "Epoch [10/75]        | Train: Loss 0.8426, R2 0.7671, RMSE 0.9140                        | Test: Loss 0.7849, R2 0.7773, RMSE 0.8794\n",
      "Epoch [11/75]        | Train: Loss 0.8398, R2 0.7677, RMSE 0.9120                        | Test: Loss 0.8681, R2 0.7726, RMSE 0.9247\n",
      "Epoch [12/75]        | Train: Loss 0.7874, R2 0.7804, RMSE 0.8834                        | Test: Loss 0.7604, R2 0.7877, RMSE 0.8670\n",
      "Epoch [13/75]        | Train: Loss 0.7790, R2 0.7839, RMSE 0.8784                        | Test: Loss 0.8039, R2 0.7832, RMSE 0.8894\n",
      "Epoch [14/75]        | Train: Loss 0.7771, R2 0.7841, RMSE 0.8772                        | Test: Loss 0.8198, R2 0.7573, RMSE 0.9016\n",
      "Epoch [15/75]        | Train: Loss 0.7926, R2 0.7796, RMSE 0.8855                        | Test: Loss 0.8317, R2 0.7770, RMSE 0.9088\n",
      "Epoch [16/75]        | Train: Loss 0.7770, R2 0.7833, RMSE 0.8779                        | Test: Loss 0.7817, R2 0.7811, RMSE 0.8775\n",
      "Epoch [17/75]        | Train: Loss 0.7477, R2 0.7909, RMSE 0.8615                        | Test: Loss 0.8353, R2 0.7877, RMSE 0.9014\n",
      "Epoch [18/75]        | Train: Loss 0.7705, R2 0.7861, RMSE 0.8740                        | Test: Loss 0.9657, R2 0.7039, RMSE 0.9750\n",
      "Epoch [19/75]        | Train: Loss 0.7570, R2 0.7884, RMSE 0.8667                        | Test: Loss 0.7917, R2 0.7901, RMSE 0.8833\n",
      "Epoch [20/75]        | Train: Loss 0.7359, R2 0.7932, RMSE 0.8546                        | Test: Loss 0.8080, R2 0.7762, RMSE 0.8938\n",
      "Epoch [21/75]        | Train: Loss 0.7417, R2 0.7939, RMSE 0.8577                        | Test: Loss 0.7708, R2 0.7930, RMSE 0.8734\n",
      "Epoch [22/75]        | Train: Loss 0.7269, R2 0.7961, RMSE 0.8492                        | Test: Loss 0.7442, R2 0.7949, RMSE 0.8523\n",
      "Epoch [23/75]        | Train: Loss 0.7008, R2 0.8031, RMSE 0.8340                        | Test: Loss 0.6992, R2 0.8067, RMSE 0.8278\n",
      "Epoch [24/75]        | Train: Loss 0.6995, R2 0.8051, RMSE 0.8318                        | Test: Loss 0.7613, R2 0.7836, RMSE 0.8677\n",
      "Epoch [25/75]        | Train: Loss 0.7049, R2 0.8033, RMSE 0.8365                        | Test: Loss 1.0030, R2 0.7834, RMSE 0.9347\n",
      "Epoch [26/75]        | Train: Loss 0.7141, R2 0.8005, RMSE 0.8418                        | Test: Loss 0.7495, R2 0.7982, RMSE 0.8606\n",
      "Epoch [27/75]        | Train: Loss 0.6848, R2 0.8098, RMSE 0.8234                        | Test: Loss 0.7269, R2 0.8068, RMSE 0.8467\n",
      "Epoch [28/75]        | Train: Loss 0.6740, R2 0.8108, RMSE 0.8174                        | Test: Loss 0.8195, R2 0.7721, RMSE 0.9000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\smbm2\\projects\\CAT-Transformer\\notebooks\\FT\\all_cont_regression.ipynb Cell 12\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/notebooks/FT/all_cont_regression.ipynb#X15sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m all_attention_scores \u001b[39m=\u001b[39m []\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/notebooks/FT/all_cont_regression.ipynb#X15sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/notebooks/FT/all_cont_regression.ipynb#X15sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m   train_loss, r2_train, rmse_train \u001b[39m=\u001b[39m train(train_dataloader, model, loss_functions, optimizer, device_in_use\u001b[39m=\u001b[39;49mdevice_in_use)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/notebooks/FT/all_cont_regression.ipynb#X15sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m   test_loss, r2_test, rmse_test \u001b[39m=\u001b[39m test(test_dataloader, model, loss_functions, device_in_use\u001b[39m=\u001b[39mdevice_in_use)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/notebooks/FT/all_cont_regression.ipynb#X15sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m   train_losses\u001b[39m.\u001b[39mappend(train_loss)\n",
      "\u001b[1;32mc:\\Users\\smbm2\\projects\\CAT-Transformer\\notebooks\\FT\\all_cont_regression.ipynb Cell 12\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/notebooks/FT/all_cont_regression.ipynb#X15sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m rmse_value \u001b[39m=\u001b[39m rmse(labels_task1, task_predictions)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/notebooks/FT/all_cont_regression.ipynb#X15sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m root_mean_squared_error_total\u001b[39m+\u001b[39m\u001b[39m=\u001b[39mrmse_value\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/notebooks/FT/all_cont_regression.ipynb#X15sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/notebooks/FT/all_cont_regression.ipynb#X15sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/notebooks/FT/all_cont_regression.ipynb#X15sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\Users\\smbm2\\AppData\\Local\\miniconda3\\envs\\ml-env\\Lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    489\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\smbm2\\AppData\\Local\\miniconda3\\envs\\ml-env\\Lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Testing against the test dataset\n",
    "model = FTTransformer(categories=(),\n",
    "                          num_continuous=len(cont_columns),\n",
    "                          dim=best_params['dim'],\n",
    "                          depth=best_params['depth'],\n",
    "                          heads=best_params['heads'],\n",
    "                          dim_head=best_params['dim_head'],\n",
    "                          dim_out=1,\n",
    "                          attn_dropout=best_params['attn_dropout'],\n",
    "                          ff_dropout=best_params['ff_dropout']).to(device_in_use)\n",
    "loss_functions = LossFunctions(1)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr = best_params['learning_rate']) # Maybe try messing around with optimizers. try other torch optimizers with different configurations.\n",
    "epochs = 75 #Set the number of epochs\n",
    "\n",
    "train_losses = []\n",
    "train_accuracies_1 = [] \n",
    "train_accuracies_2 = []\n",
    "train_recalls = [] \n",
    "train_f1_scores = [] \n",
    "test_losses = []\n",
    "test_accuracies_1 = []\n",
    "test_accuracies_2 = []\n",
    "test_recalls = []  \n",
    "test_f1_scores = [] \n",
    "all_attention_scores = []\n",
    "\n",
    "for t in range(epochs):\n",
    "  train_loss, r2_train, rmse_train = train(train_dataloader, model, loss_functions, optimizer, device_in_use=device_in_use)\n",
    "  test_loss, r2_test, rmse_test = test(test_dataloader, model, loss_functions, device_in_use=device_in_use)\n",
    "  train_losses.append(train_loss)\n",
    "\n",
    "  # train_accuracies_2.append(train_accuracy_2)\n",
    "  # train_recalls.append(train_recall) \n",
    "  # train_f1_scores.append(train_f1)\n",
    "  test_losses.append(test_loss)\n",
    "\n",
    "  # test_accuracies_2.append(test_accuracy_2)\n",
    "  # test_recalls.append(test_recall)\n",
    "  # test_f1_scores.append(test_f1)\n",
    "  # Formatting for easier reading\n",
    "  epoch_str = f\"Epoch [{t+1:2}/{epochs}]\"\n",
    "  train_metrics = f\"Train: Loss {format_metric(train_loss)}, R2 {format_metric(r2_train)}, RMSE {format_metric(rmse_train)}\"\n",
    "  test_metrics = f\"Test: Loss {format_metric(test_loss)}, R2 {format_metric(r2_test)}, RMSE {format_metric(rmse_test)}\"\n",
    "  print(f\"{epoch_str:20} | {train_metrics:65} | {test_metrics}\")\n",
    "\n",
    "\n",
    "torch.save(model.state_dict(), 'final_model_trained.pth')\n",
    "\n",
    "# Plotting the loss curves\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 1, 1)\n",
    "plt.plot(range(1, epochs+1), train_losses, label='Train Loss')\n",
    "plt.plot(range(1, epochs+1), [l for l in test_losses], label='Test Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Test Loss Curve')\n",
    "plt.legend()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
