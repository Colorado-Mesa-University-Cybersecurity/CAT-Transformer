{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\prime\\anaconda3\\envs\\DL\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from rff.layers import GaussianEncoding #pip install random-fourier-features-pytorch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import os\n",
    "import optuna\n",
    "from optuna.trial import TrialState\n",
    "from tab_transformer_pytorch import TabTransformer\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is not available, using CPU instead\n"
     ]
    }
   ],
   "source": [
    "# Run regardless if you do or do not have GPU so all tensors are moved to right location later on\n",
    "if torch.cuda.is_available():\n",
    "    device_in_use = torch.device(\"cuda\")\n",
    "    print(\"GPU is available and being used\")\n",
    "else:\n",
    "    device_in_use = torch.device(\"cpu\")\n",
    "    print(\"GPU is not available, using CPU instead\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD AND PROCESS DATA\n",
    "**EXAMPLE WITH Income**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['class', 'lepton_pT', 'lepton_eta', 'lepton_phi',\n",
       "       'missing_energy_magnitude', 'missing_energy_phi', 'jet1pt', 'jet1eta',\n",
       "       'jet1phi', 'jet1b-tag', 'jet2pt', 'jet2eta', 'jet2phi', 'jet2b-tag',\n",
       "       'jet3pt', 'jet3eta', 'jet3phi', 'jet3b-tag', 'jet4pt', 'jet4eta',\n",
       "       'jet4phi', 'jet4b-tag', 'm_jj', 'm_jjj', 'm_lv', 'm_jlv', 'm_bb',\n",
       "       'm_wbb', 'm_wwbb'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv('../../datasets/higgs/train.csv')\n",
    "df_test = pd.read_csv('../../datasets/higgs/test.csv')\n",
    "df_val = pd.read_csv('../../datasets/higgs/validation.csv') #READ FROM RIGHT SPOT\n",
    "\n",
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_columns(dataframe, target):\n",
    "    categorical_columns = []\n",
    "    continuous_columns = []\n",
    "    unique_classes_per_column = []  # To hold the number of unique classes for each categorical column\n",
    "\n",
    "    for column in dataframe.columns:\n",
    "        if dataframe[column].dtype == 'object' or len(dataframe[column].unique()) <= 10:\n",
    "            # If the column's data type is 'object' or it has 10 or fewer unique values, consider it categorical.\n",
    "            if column != target:\n",
    "                categorical_columns.append(column)\n",
    "                unique_classes_per_column.append(dataframe[column].nunique())  # Store the number of unique classes\n",
    "        else:\n",
    "            # Otherwise, consider it continuous.\n",
    "            continuous_columns.append(column)\n",
    "\n",
    "    return categorical_columns, continuous_columns, unique_classes_per_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target: class\n",
      "Categorical:  ['jet1b-tag', 'jet2b-tag', 'jet3b-tag', 'jet4b-tag']\n",
      "Continous:  ['lepton_pT', 'lepton_eta', 'lepton_phi', 'missing_energy_magnitude', 'missing_energy_phi', 'jet1pt', 'jet1eta', 'jet1phi', 'jet2pt', 'jet2eta', 'jet2phi', 'jet3pt', 'jet3eta', 'jet3phi', 'jet4pt', 'jet4eta', 'jet4phi', 'm_jj', 'm_jjj', 'm_lv', 'm_jlv', 'm_bb', 'm_wbb', 'm_wwbb']\n",
      "Unique Classes per Column:  [3, 3, 3, 3]\n",
      "[2]\n",
      "jet1b-tag\n",
      "jet2b-tag\n",
      "jet3b-tag\n",
      "jet4b-tag\n",
      "jet1b-tag\n",
      "0    33704\n",
      "2    28139\n",
      "1     6791\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "target = 'class'\n",
    "cat_columns, cont_columns, unique_classes_per_column = categorize_columns(df_train, target)\n",
    "\n",
    "if target in cont_columns:\n",
    "    cont_columns.remove(target)\n",
    "elif target in cat_columns:\n",
    "    print(\"in here\")\n",
    "    cat_columns.remove(target)\n",
    "\n",
    "print(\"Target:\", target)\n",
    "print(\"Categorical: \", cat_columns)\n",
    "print(\"Continous: \", cont_columns)\n",
    "print(\"Unique Classes per Column: \", unique_classes_per_column)\n",
    "\n",
    "#CHECKING TO MAKE SURE YOUR LIST IS CORRECT (NO NEED TO TOUCH)\n",
    "yourlist = cat_columns + cont_columns + [target]\n",
    "yourlist.sort()\n",
    "oglist = list(df_train.columns)\n",
    "oglist.sort()\n",
    "\n",
    "assert(yourlist == oglist), \"You may of spelled feature name wrong or you forgot to put on of them in the list\"\n",
    "\n",
    "target_classes = [max(len(df_train[target].value_counts()), len(df_val[target].value_counts()),len(df_test[target].value_counts()))]\n",
    "print(target_classes)\n",
    "# Create a StandardScaler and fit it to the cont features\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(df_train[cont_columns])\n",
    "\n",
    "# Transform the training, test, and validation datasets\n",
    "df_train[cont_columns] = scaler.transform(df_train[cont_columns])\n",
    "df_test[cont_columns] = scaler.transform(df_test[cont_columns])\n",
    "df_val[cont_columns] = scaler.transform(df_val[cont_columns])\n",
    "\n",
    "# label encode target...\n",
    "le = LabelEncoder()\n",
    "df_train[target] = le.fit_transform(df_train[target])\n",
    "df_test[target] = le.fit_transform(df_test[target])\n",
    "df_val[target] = le.fit_transform(df_val[target])\n",
    "\n",
    "# ...and categorical features\n",
    "for feature in cat_columns:\n",
    "    print(feature)\n",
    "    df_train[feature] = le.fit_transform(df_train[feature])\n",
    "    df_test[feature] = le.fit_transform(df_test[feature])\n",
    "    df_val[feature] = le.fit_transform(df_val[feature])\n",
    "\n",
    "print(df_train['jet1b-tag'].value_counts())\n",
    "\n",
    "class SingleTaskDataset(Dataset):\n",
    "    def __init__(self, df : pd.DataFrame, cat_columns, num_columns,task1_column):\n",
    "        self.n = df.shape[0]\n",
    "        \n",
    "        self.task1_labels = df[task1_column].astype(np.float32).values\n",
    "\n",
    "        self.cate = df[cat_columns].astype(np.int64).values\n",
    "        self.num = df[num_columns].astype(np.float32).values\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Retrieve features and labels from the dataframe using column names\n",
    "        cat_features = self.cate[idx]\n",
    "        num_features = self.num[idx]\n",
    "        labels_task1 = self.task1_labels[idx]\n",
    "\n",
    "        return cat_features, num_features, labels_task1\n",
    "\n",
    "#Wrapping in Dataset\n",
    "train_dataset = SingleTaskDataset(df_train, cat_columns, cont_columns, 'class')\n",
    "val_dataset = SingleTaskDataset(df_val, cat_columns, cont_columns, 'class')\n",
    "test_dataset = SingleTaskDataset(df_test, cat_columns, cont_columns, 'class')\n",
    "\n",
    "#This is a hyperparameter that is not tuned. Maybe mess with what makes sense here\n",
    "#Also try looking to see what other papers have done\n",
    "batch_size = 256\n",
    "\n",
    "# Wrapping with DataLoader for easy batch extraction\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each task loss is scaled by its own learnable parameter, then regularization is applied \n",
    "class LossFunctions(nn.Module):\n",
    "    def __init__(self, num_tasks):\n",
    "        super(LossFunctions, self).__init__()\n",
    "        self.num_tasks = num_tasks\n",
    "\n",
    "        self.loss_fns = [nn.CrossEntropyLoss() for x in range(num_tasks)] \n",
    "\n",
    "    def forward(self, predictions, labels_task1):\n",
    "\n",
    "        #task 1\n",
    "        target = labels_task1.long()\n",
    "        prediction = predictions\n",
    "        loss_fn = self.loss_fns[0]\n",
    "        task_loss = loss_fn(prediction, target)\n",
    "        \n",
    "        return task_loss\n",
    "    \n",
    "# Training and Testing Loops\n",
    "def train(dataloader, model, loss_function, optimizer, device_in_use):\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    total_correct_1 = 0\n",
    "    total_samples_1 = 0\n",
    "    all_targets_1 = []\n",
    "    all_predictions_1 = []\n",
    "\n",
    "    for (categorical, numerical, labels_task1) in dataloader:\n",
    "        categorical, numerical, labels_task1 = categorical.to(device_in_use), numerical.to(device_in_use), labels_task1.to(device_in_use)\n",
    "        \n",
    "        #running them through model and modifying the shape slightly for the loss function\n",
    "        task_predictions = model(categorical, numerical)\n",
    "        task_predictions = task_predictions.squeeze(1)\n",
    "        \n",
    "        loss = loss_function(task_predictions, labels_task1)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        #computing accuracy for first target\n",
    "        y_pred_softmax_1 = torch.softmax(task_predictions, dim=1)\n",
    "        _, y_pred_labels_1 = torch.max(y_pred_softmax_1, dim=1)\n",
    "        total_correct_1 += (y_pred_labels_1 == labels_task1).sum().item()\n",
    "        total_samples_1 += labels_task1.size(0)\n",
    "        all_targets_1.extend(labels_task1.cpu().numpy())\n",
    "        all_predictions_1.extend(y_pred_labels_1.cpu().numpy())\n",
    "\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    avg_loss = total_loss/len(dataloader)\n",
    "    accuracy_1 = total_correct_1 / total_samples_1\n",
    "\n",
    "    return avg_loss, accuracy_1\n",
    "\n",
    "def test(dataloader, model, loss_function, device_in_use):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    total_loss = 0\n",
    "    total_correct_1 = 0\n",
    "    total_samples_1 = 0\n",
    "    all_targets_1 = []\n",
    "    all_predictions_1 = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for (categorical, numerical, labels_task1) in dataloader:\n",
    "            categorical, numerical, labels_task1 = categorical.to(device_in_use), numerical.to(device_in_use), labels_task1.to(device_in_use)\n",
    "            \n",
    "            #running them through model and modifying the shape slightly for the loss function\n",
    "            task_predictions = model(categorical, numerical)\n",
    "            task_predictions = task_predictions.squeeze(1)\n",
    "            loss = loss_function(task_predictions, labels_task1)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            #computing accuracy for first target\n",
    "            y_pred_softmax_1 = torch.softmax(task_predictions, dim=1)\n",
    "            _, y_pred_labels_1 = torch.max(y_pred_softmax_1, dim=1)\n",
    "            total_correct_1 += (y_pred_labels_1 == labels_task1).sum().item()\n",
    "            total_samples_1 += labels_task1.size(0)\n",
    "            all_targets_1.extend(labels_task1.cpu().numpy())\n",
    "            all_predictions_1.extend(y_pred_labels_1.cpu().numpy())\n",
    "\n",
    "    avg = total_loss/len(dataloader)\n",
    "    accuracy_1 = total_correct_1 / total_samples_1\n",
    "    f1_1 = f1_score(all_targets_1, all_predictions_1, average='weighted')\n",
    "\n",
    "    return avg, accuracy_1, all_predictions_1, all_targets_1, f1_1\n",
    "\n",
    "def format_metric(value): # Used to format the metrics output\n",
    "    return f\"{value:.4f}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [ 1/100]       | Train: Loss 0.6786, Accuracy 0.5682                               | Test: Loss 0.6716, Accuracy 0.5897, F1 0.5890\n",
      "Epoch [ 2/100]       | Train: Loss 0.6581, Accuracy 0.6120                               | Test: Loss 0.6577, Accuracy 0.6061, F1 0.5797\n",
      "Epoch [ 3/100]       | Train: Loss 0.6434, Accuracy 0.6291                               | Test: Loss 0.6445, Accuracy 0.6269, F1 0.6143\n",
      "Epoch [ 4/100]       | Train: Loss 0.6363, Accuracy 0.6367                               | Test: Loss 0.6380, Accuracy 0.6350, F1 0.6348\n",
      "Epoch [ 5/100]       | Train: Loss 0.6314, Accuracy 0.6432                               | Test: Loss 0.6327, Accuracy 0.6414, F1 0.6365\n",
      "Epoch [ 6/100]       | Train: Loss 0.6286, Accuracy 0.6476                               | Test: Loss 0.6292, Accuracy 0.6447, F1 0.6433\n",
      "Epoch [ 7/100]       | Train: Loss 0.6243, Accuracy 0.6524                               | Test: Loss 0.6269, Accuracy 0.6481, F1 0.6408\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\prime\\OneDrive\\Documents\\CMU\\research\\CAT-Transformer\\experiments\\higgs\\TAB_final_test.ipynb Cell 8\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/prime/OneDrive/Documents/CMU/research/CAT-Transformer/experiments/higgs/TAB_final_test.ipynb#X10sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mprocess_time()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/prime/OneDrive/Documents/CMU/research/CAT-Transformer/experiments/higgs/TAB_final_test.ipynb#X10sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/prime/OneDrive/Documents/CMU/research/CAT-Transformer/experiments/higgs/TAB_final_test.ipynb#X10sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m   train_loss, train_accuracy_1 \u001b[39m=\u001b[39m train(train_dataloader, model, loss_functions, optimizer, device_in_use\u001b[39m=\u001b[39;49mdevice_in_use)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/prime/OneDrive/Documents/CMU/research/CAT-Transformer/experiments/higgs/TAB_final_test.ipynb#X10sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m   test_loss, test_accuracy_1, all_predictions_1, all_targets_1, f1_1 \u001b[39m=\u001b[39m test(test_dataloader, model, loss_functions, device_in_use\u001b[39m=\u001b[39mdevice_in_use)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/prime/OneDrive/Documents/CMU/research/CAT-Transformer/experiments/higgs/TAB_final_test.ipynb#X10sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m   train_losses\u001b[39m.\u001b[39mappend(train_loss)\n",
      "\u001b[1;32mc:\\Users\\prime\\OneDrive\\Documents\\CMU\\research\\CAT-Transformer\\experiments\\higgs\\TAB_final_test.ipynb Cell 8\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/prime/OneDrive/Documents/CMU/research/CAT-Transformer/experiments/higgs/TAB_final_test.ipynb#X10sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m categorical, numerical, labels_task1 \u001b[39m=\u001b[39m categorical\u001b[39m.\u001b[39mto(device_in_use), numerical\u001b[39m.\u001b[39mto(device_in_use), labels_task1\u001b[39m.\u001b[39mto(device_in_use)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/prime/OneDrive/Documents/CMU/research/CAT-Transformer/experiments/higgs/TAB_final_test.ipynb#X10sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39m#running them through model and modifying the shape slightly for the loss function\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/prime/OneDrive/Documents/CMU/research/CAT-Transformer/experiments/higgs/TAB_final_test.ipynb#X10sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m task_predictions \u001b[39m=\u001b[39m model(categorical, numerical)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/prime/OneDrive/Documents/CMU/research/CAT-Transformer/experiments/higgs/TAB_final_test.ipynb#X10sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m task_predictions \u001b[39m=\u001b[39m task_predictions\u001b[39m.\u001b[39msqueeze(\u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/prime/OneDrive/Documents/CMU/research/CAT-Transformer/experiments/higgs/TAB_final_test.ipynb#X10sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m loss \u001b[39m=\u001b[39m loss_function(task_predictions, labels_task1)\n",
      "File \u001b[1;32mc:\\Users\\prime\\anaconda3\\envs\\DL\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\prime\\anaconda3\\envs\\DL\\lib\\site-packages\\tab_transformer_pytorch\\tab_transformer_pytorch.py:220\u001b[0m, in \u001b[0;36mTabTransformer.forward\u001b[1;34m(self, x_categ, x_cont, return_attn)\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_unique_categories \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    218\u001b[0m     x_categ \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcategories_offset\n\u001b[1;32m--> 220\u001b[0m     x, attns \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(x_categ, return_attn \u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m    222\u001b[0m     flat_categ \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mflatten(\u001b[39m1\u001b[39m)\n\u001b[0;32m    223\u001b[0m     xs\u001b[39m.\u001b[39mappend(flat_categ)\n",
      "File \u001b[1;32mc:\\Users\\prime\\anaconda3\\envs\\DL\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\prime\\anaconda3\\envs\\DL\\lib\\site-packages\\tab_transformer_pytorch\\tab_transformer_pytorch.py:105\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, x, return_attn)\u001b[0m\n\u001b[0;32m    102\u001b[0m post_softmax_attns \u001b[39m=\u001b[39m []\n\u001b[0;32m    104\u001b[0m \u001b[39mfor\u001b[39;00m attn, ff \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers:\n\u001b[1;32m--> 105\u001b[0m     attn_out, post_softmax_attn \u001b[39m=\u001b[39m attn(x)\n\u001b[0;32m    106\u001b[0m     post_softmax_attns\u001b[39m.\u001b[39mappend(post_softmax_attn)\n\u001b[0;32m    108\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m attn_out\n",
      "File \u001b[1;32mc:\\Users\\prime\\anaconda3\\envs\\DL\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\prime\\anaconda3\\envs\\DL\\lib\\site-packages\\tab_transformer_pytorch\\tab_transformer_pytorch.py:32\u001b[0m, in \u001b[0;36mPreNorm.forward\u001b[1;34m(self, x, **kwargs)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m---> 32\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfn(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm(x), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\prime\\anaconda3\\envs\\DL\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\prime\\anaconda3\\envs\\DL\\lib\\site-packages\\tab_transformer_pytorch\\tab_transformer_pytorch.py:81\u001b[0m, in \u001b[0;36mAttention.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     78\u001b[0m attn \u001b[39m=\u001b[39m sim\u001b[39m.\u001b[39msoftmax(dim \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     79\u001b[0m dropped_attn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(attn)\n\u001b[1;32m---> 81\u001b[0m out \u001b[39m=\u001b[39m einsum(\u001b[39m'\u001b[39;49m\u001b[39mb h i j, b h j d -> b h i d\u001b[39;49m\u001b[39m'\u001b[39;49m, dropped_attn, v)\n\u001b[0;32m     82\u001b[0m out \u001b[39m=\u001b[39m rearrange(out, \u001b[39m'\u001b[39m\u001b[39mb h n d -> b n (h d)\u001b[39m\u001b[39m'\u001b[39m, h \u001b[39m=\u001b[39m h)\n\u001b[0;32m     83\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mto_out(out), attn\n",
      "File \u001b[1;32mc:\\Users\\prime\\anaconda3\\envs\\DL\\lib\\site-packages\\torch\\functional.py:378\u001b[0m, in \u001b[0;36meinsum\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m    373\u001b[0m     \u001b[39mreturn\u001b[39;00m einsum(equation, \u001b[39m*\u001b[39m_operands)\n\u001b[0;32m    375\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(operands) \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m2\u001b[39m \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m opt_einsum\u001b[39m.\u001b[39menabled:\n\u001b[0;32m    376\u001b[0m     \u001b[39m# the path for contracting 0 or 1 time(s) is already optimized\u001b[39;00m\n\u001b[0;32m    377\u001b[0m     \u001b[39m# or the user has disabled using opt_einsum\u001b[39;00m\n\u001b[1;32m--> 378\u001b[0m     \u001b[39mreturn\u001b[39;00m _VF\u001b[39m.\u001b[39;49meinsum(equation, operands)  \u001b[39m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    380\u001b[0m path \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    381\u001b[0m \u001b[39mif\u001b[39;00m opt_einsum\u001b[39m.\u001b[39mis_available():\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Testing against the test dataset\n",
    "\n",
    "# hidden (embedding)\n",
    "#dimension, the number of layers and the number of \n",
    "#attention heads are fixed to 32, 6, and 8 respectively. The MLP\n",
    "#layer sizes are set to {4 × l, 2 × l}, where l is the size of its input.\n",
    "model = TabTransformer(categories=tuple(unique_classes_per_column),\n",
    "                       num_continuous=len(cont_columns),\n",
    "                       dim = 32,                           # dimension, paper set at 32\n",
    "                        dim_out = target_classes[0],                        # binary prediction, but could be anything\n",
    "                        depth = 6,                          # depth, paper recommended 6\n",
    "                        heads = 8,                          # heads, paper recommends 8\n",
    "                        attn_dropout = 0.1,                 # post-attention dropout\n",
    "                        ff_dropout = 0.1,                   # feed forward dropout\n",
    "                        mlp_hidden_mults = (4, 2),          # relative multiples of each hidden dimension of the last mlp to logits\n",
    "                        mlp_act = nn.ReLU(),    \n",
    "                       ).to(device_in_use)\n",
    "loss_functions = LossFunctions(1)\n",
    "optimizer = torch.optim.AdamW(params=model.parameters(), lr = 0.0001, weight_decay=0.000001)\n",
    "epochs = 100\n",
    "\n",
    "train_losses = []\n",
    "train_accuracies_1 = [] \n",
    "\n",
    "test_losses = []\n",
    "test_accuracies_1 = []\n",
    "  \n",
    "test_f1_scores = [] \n",
    "all_attention_scores = []\n",
    "\n",
    "#Time will be recorded for all 100 epochs - This means the results will not be comparable to Xgboost but that is ok, we will only compare between transformer models who will also train for 100 epochs\n",
    "start_time = time.process_time()\n",
    "\n",
    "for t in range(epochs):\n",
    "  train_loss, train_accuracy_1 = train(train_dataloader, model, loss_functions, optimizer, device_in_use=device_in_use)\n",
    "  test_loss, test_accuracy_1, all_predictions_1, all_targets_1, f1_1 = test(test_dataloader, model, loss_functions, device_in_use=device_in_use)\n",
    "  train_losses.append(train_loss)\n",
    "  train_accuracies_1.append(train_accuracy_1)\n",
    "  # train_accuracies_2.append(train_accuracy_2)\n",
    "  # train_recalls.append(train_recall) \n",
    "  # train_f1_scores.append(train_f1)\n",
    "  test_losses.append(test_loss)\n",
    "  test_accuracies_1.append(test_accuracy_1)\n",
    "  # test_accuracies_2.append(test_accuracy_2)\n",
    "  # test_recalls.append(test_recall)\n",
    "  test_f1_scores.append(f1_1)\n",
    "  # Formatting for easier reading\n",
    "  epoch_str = f\"Epoch [{t+1:2}/{epochs}]\"\n",
    "  train_metrics = f\"Train: Loss {format_metric(train_loss)}, Accuracy {format_metric(train_accuracy_1)}\"\n",
    "  test_metrics = f\"Test: Loss {format_metric(test_loss)}, Accuracy {format_metric(test_accuracy_1)}, F1 {format_metric(f1_1)}\"\n",
    "  print(f\"{epoch_str:20} | {train_metrics:65} | {test_metrics}\")\n",
    "\n",
    "total_time = time.process_time() - start_time\n",
    "\n",
    "# Plotting the loss curves\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 1, 1)\n",
    "plt.plot(range(1, epochs+1), train_losses, label='Train Loss')\n",
    "plt.plot(range(1, epochs+1), [l for l in test_losses], label='Test Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Test Loss Curve')\n",
    "plt.legend()\n",
    "\n",
    "# Display confusion matrix for the first task (Traffic Type) on test data\n",
    "conf_matrix_1 = confusion_matrix(all_targets_1, all_predictions_1)\n",
    "print(\"Confusion Matrix\")\n",
    "print(conf_matrix_1)\n",
    "\n",
    "best_index = test_accuracies_1.index(max(test_accuracies_1))\n",
    "print(f\"Best accuracy {test_accuracies_1[best_index]}\")\n",
    "print(f\"Best F1 {test_f1_scores[best_index]}\")\n",
    "print(f\"100 epochs of training and evaluation took, {total_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
