{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from rff.layers import GaussianEncoding #pip install random-fourier-features-pytorch\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import os\n",
    "from tab_transformer_pytorch import FTTransformer, TabTransformer\n",
    "import sys\n",
    "import time\n",
    "from torch import Tensor\n",
    "from typing import Literal\n",
    "\n",
    "device_in_use='cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train = pd.read_csv('/home/cscadmin/CyberResearch/CAT-Transformer/datasets/higgs/train.csv')\n",
    "# df_test = pd.read_csv('/home/cscadmin/CyberResearch/CAT-Transformer/datasets/higgs/test.csv')\n",
    "# df_val = pd.read_csv('/home/cscadmin/CyberResearch/CAT-Transformer/datasets/higgs/validation.csv') #READ FROM RIGHT SPOT\n",
    "\n",
    "# df_train = pd.read_csv(r'C:\\Users\\smbm2\\projects\\CAT-Transformer\\datasets\\california\\train.csv')\n",
    "# df_test = pd.read_csv(r'C:\\Users\\smbm2\\projects\\CAT-Transformer\\datasets\\california\\test.csv')\n",
    "# df_val = pd.read_csv(r'C:\\Users\\smbm2\\projects\\CAT-Transformer\\datasets\\california\\validation.csv') #READ FROM RIGHT SPOT\n",
    "\n",
    "df_train = pd.read_csv('/home/warin/projects/CAT-Transformer/datasets/california_sklearn/train.csv')\n",
    "df_test = pd.read_csv('/home/warin/projects/CAT-Transformer/datasets/california_sklearn/test.csv')\n",
    "df_val = pd.read_csv('/home/warin/projects/CAT-Transformer/datasets/california_sklearn/validation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9851]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cont_columns = [ 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup',\n",
    "       'Latitude', 'Longitude']\n",
    "target = ['MedInc']\n",
    "\n",
    "#CHECKING TO MAKE SURE YOUR LIST IS CORRECT (NO NEED TO TOUCH)\n",
    "yourlist = cont_columns + target\n",
    "yourlist.sort()\n",
    "oglist = list(df_train.columns)\n",
    "oglist.sort()\n",
    "\n",
    "assert(yourlist == oglist), \"You may of spelled feature name wrong or you forgot to put on of them in the list\"\n",
    "\n",
    "target_classes = [max(len(df_train[target].value_counts()), len(df_val[target].value_counts()),len(df_test[target].value_counts()))]\n",
    "print(target_classes)\n",
    "# Create a StandardScaler and fit it to the cont features\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(df_train[cont_columns])\n",
    "\n",
    "# Transform the training, test, and validation datasets\n",
    "df_train[cont_columns] = scaler.transform(df_train[cont_columns])\n",
    "df_test[cont_columns] = scaler.transform(df_test[cont_columns])\n",
    "df_val[cont_columns] = scaler.transform(df_val[cont_columns])\n",
    "\n",
    "class SingleTaskDataset(Dataset):\n",
    "    def __init__(self, df : pd.DataFrame, num_columns,task1_column):\n",
    "        self.n = df.shape[0]\n",
    "        \n",
    "        self.task1_labels = df[task1_column].astype(np.float32).values\n",
    "\n",
    "        self.num = df[num_columns].astype(np.float32).values\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Retrieve features and labels from the dataframe using column names\n",
    "        num_features = self.num[idx]\n",
    "        labels_task1 = self.task1_labels[idx]\n",
    "\n",
    "        return num_features, labels_task1\n",
    "\n",
    "#Wrapping in Dataset\n",
    "train_dataset = SingleTaskDataset(df_train, cont_columns, 'MedInc')\n",
    "val_dataset = SingleTaskDataset(df_val, cont_columns, 'MedInc')\n",
    "test_dataset = SingleTaskDataset(df_test, cont_columns, 'MedInc')\n",
    "\n",
    "#This is a hyperparameter that is not tuned. Maybe mess with what makes sense here\n",
    "#Also try looking to see what other papers have done\n",
    "batch_size = 256\n",
    "\n",
    "# Wrapping with DataLoader for easy batch extraction\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_size, heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        self.head_dim = embed_size // heads\n",
    "        assert(self.head_dim * heads == embed_size), \"Embed size needs to be div by heads\"\n",
    "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.keys =nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.fc_out = nn.Linear(heads*self.head_dim, embed_size)\n",
    "\n",
    "\n",
    "    def forward(self, values, keys, query):\n",
    "        N = query.shape[0]\n",
    "        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
    "\n",
    "        values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
    "        keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n",
    "        queries = query.reshape(N, query_len, self.heads, self.head_dim)\n",
    "\n",
    "        values = self.values(values)\n",
    "        keys = self.keys(keys)\n",
    "        queries = self.queries(queries)\n",
    "\n",
    "        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
    "\n",
    "        attention = torch.softmax(energy / (self.embed_size ** (1/2)), dim=3) #(batch_size, head_dim, #query_embeddings, #key_embeddings)\n",
    "\n",
    "        # Calculate simplified attention scores\n",
    "        avg_attention = attention.mean(dim=0)  # Average across batches\n",
    "        # print(\"batch average\", avg_attention.shape)\n",
    "        avg_attention = avg_attention.mean(dim=0).squeeze(dim=0)\n",
    "        # print(\"head average\", avg_attention.shape)\n",
    "\n",
    "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(N, query_len, self.heads*self.head_dim) #(batch_size, n_features, embed_size)\n",
    "        out = self.fc_out(out)\n",
    "\n",
    "        return out, avg_attention\n",
    "    \n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_size, heads, dropout, forward_expansion, pre_norm_on):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "\n",
    "        self.pre_norm_on = pre_norm_on\n",
    "        if self.pre_norm_on:\n",
    "            self.pre_norm = nn.LayerNorm(embed_size)\n",
    "        self.attention = MultiHeadAttention(embed_size, heads)\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "\n",
    "        self.feed_forward = nn.Sequential(nn.Linear(embed_size, forward_expansion*embed_size),\n",
    "                                          nn.ReLU(),\n",
    "                                          nn.Linear(forward_expansion*embed_size, embed_size)\n",
    "                                          )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self,value,key,query):\n",
    "        if self.pre_norm_on:\n",
    "            query = self.pre_norm(query)\n",
    "            key = self.pre_norm(key)\n",
    "            value = self.pre_norm(value)\n",
    "            \n",
    "        attention, avg_attention = self.attention(value, key, query)\n",
    "        x = self.dropout(self.norm1(attention + query))\n",
    "        forward = self.feed_forward(x)\n",
    "        out = self.dropout(self.norm2(forward + x))\n",
    "        return out, avg_attention\n",
    "    \n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, embed_size, heads, forward_expansion, dropout, pre_norm_on):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "\n",
    "        self.attention = MultiHeadAttention(embed_size, heads)\n",
    "        self.norm = nn.LayerNorm(embed_size)\n",
    "        self.transformer_block = TransformerBlock(embed_size, heads, dropout, forward_expansion, pre_norm_on)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, value, key):\n",
    "        out, avg_attention = self.transformer_block(value, key, x)\n",
    "\n",
    "        return out, avg_attention\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 embed_size,\n",
    "                 num_layers,\n",
    "                 heads,\n",
    "                 forward_expansion,\n",
    "                 decoder_dropout,\n",
    "                 pre_norm_on\n",
    "    ):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "                [\n",
    "                    DecoderBlock(\n",
    "                        embed_size,\n",
    "                        heads,\n",
    "                        dropout=decoder_dropout,\n",
    "                        forward_expansion=forward_expansion,\n",
    "                        pre_norm_on=pre_norm_on\n",
    "                    )\n",
    "                    for _ in range(num_layers)\n",
    "                ]\n",
    "            )\n",
    "        self.avg_attention = None\n",
    "\n",
    "    def forward(self, class_embed, context):\n",
    "        for layer in self.layers:\n",
    "            # x is the classification embedding (CLS Token)\n",
    "            # context are the feature embeddings that will be used as key and value\n",
    "            x, self.avg_attention = layer(class_embed, context, context)\n",
    "  \n",
    "        return x \n",
    "\n",
    "class EmbeddingsRFFforIndividualFeatures(nn.Module):\n",
    "    def __init__(self, sigma, embed_size, input_size, n_cont,  num_target_labels, rff_on, linear_on:bool):\n",
    "        super(EmbeddingsRFFforIndividualFeatures, self).__init__()\n",
    "\n",
    "        self.rff_on = rff_on\n",
    "\n",
    "        self.linear_on = linear_on\n",
    "\n",
    "        if self.rff_on:\n",
    "            self.rffs = nn.ModuleList([GaussianEncoding(sigma=sigma, input_size=input_size, encoded_size=embed_size//2) for _ in range(n_cont)])\n",
    "            self.mlp_in = embed_size\n",
    "        else:\n",
    "            self.mlp_in = input_size\n",
    "\n",
    "        if self.linear_on:\n",
    "            self.cont_embeddings = nn.ModuleList([nn.Linear(in_features=self.mlp_in, out_features=embed_size) for _ in range(n_cont)])\n",
    "\n",
    "        # Classifcation Embeddings for each target label\n",
    "        self.target_label_embeddings = nn.ModuleList([nn.Embedding(1, embed_size) for _ in range(num_target_labels)])\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(2) #(batch_size, n_features) -> (batch_size, n_features, 1)\n",
    "        rff_vectors = []\n",
    "        if self.rff_on:\n",
    "            for i, r in enumerate(self.rffs):\n",
    "                input = x[:,i,:]\n",
    "                out = r(input)\n",
    "                rff_vectors.append(out)\n",
    "\n",
    "        embeddings = []\n",
    "        if self.linear_on:\n",
    "            x = torch.stack(rff_vectors, dim=1)\n",
    "            for i, e in enumerate(self.cont_embeddings):\n",
    "                goin_in = x[:,i,:]\n",
    "                goin_out = e(goin_in)\n",
    "                embeddings.append(goin_out)\n",
    "        else:\n",
    "            embeddings = rff_vectors\n",
    "\n",
    "        target_label_embeddings_ = []\n",
    "        for e in self.target_label_embeddings:\n",
    "            input = torch.tensor([0], device=x.device)\n",
    "            temp = e(input)\n",
    "            temp = temp.repeat(x.size(0), 1)\n",
    "            tmep = temp.unsqueeze(1)\n",
    "            target_label_embeddings_.append(temp)\n",
    "\n",
    "        class_embeddings = torch.stack(target_label_embeddings_, dim=1)\n",
    "\n",
    "        context = torch.stack(embeddings, dim=1)\n",
    "\n",
    "\n",
    "        return class_embeddings, context\n",
    "\n",
    "class EmbeddingsRFFforAllFeatures(nn.Module):\n",
    "    def __init__(self, sigma, embed_size, input_size, n_cont, num_target_labels, rff_on, linear_on:bool):\n",
    "        super(EmbeddingsRFFforAllFeatures, self).__init__()\n",
    "\n",
    "        self.rff_on = rff_on\n",
    "        self.n_cont = n_cont\n",
    "        self.linear_on=linear_on\n",
    "\n",
    "        if self.rff_on:\n",
    "            self.rff = GaussianEncoding(sigma=sigma, input_size=input_size, encoded_size=embed_size//2) \n",
    "            self.mlp_in = embed_size\n",
    "        else:\n",
    "            self.mlp_in = input_size\n",
    "\n",
    "        if self.linear_on:\n",
    "            self.cont_embeddings = nn.ModuleList([nn.Linear(in_features=self.mlp_in, out_features=embed_size) for _ in range(n_cont)])\n",
    "\n",
    "        \n",
    "        # Classifcation Embeddings for each target label\n",
    "        self.target_label_embeddings = nn.ModuleList([nn.Embedding(1, embed_size) for _ in range(num_target_labels)])\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(2) #(batch_size, n_features) -> (batch_size, n_features, 1)\n",
    "        rff_vectors = []\n",
    "        if self.rff_on:\n",
    "            for i in range(self.n_cont):\n",
    "                input = x[:,i,:]\n",
    "                out = self.rff(input)\n",
    "                rff_vectors.append(out)\n",
    "        \n",
    "        embeddings = []\n",
    "        if self.linear_on:\n",
    "            x = torch.stack(rff_vectors, dim=1)\n",
    "            for i, e in enumerate(self.cont_embeddings):\n",
    "                goin_in = x[:,i,:]\n",
    "                goin_out = e(goin_in)\n",
    "                embeddings.append(goin_out)\n",
    "        else:\n",
    "            embeddings = rff_vectors\n",
    "\n",
    "        target_label_embeddings_ = []\n",
    "        for e in self.target_label_embeddings:\n",
    "            input = torch.tensor([0], device=x.device)\n",
    "            temp = e(input)\n",
    "            temp = temp.repeat(x.size(0), 1)\n",
    "            tmep = temp.unsqueeze(1)\n",
    "            target_label_embeddings_.append(temp)\n",
    "\n",
    "        class_embeddings = torch.stack(target_label_embeddings_, dim=1)\n",
    "\n",
    "        context = torch.stack(embeddings, dim=1)\n",
    "\n",
    "        return class_embeddings, context\n",
    "    \n",
    "class PeriodicActivation(nn.Module):\n",
    "    def __init__(self, sigma, embed_size, n_cont, num_target_labels,trainable: bool, initialization: str, linear_on:bool):\n",
    "        super(PeriodicActivation, self).__init__()\n",
    "\n",
    "        self.n = embed_size\n",
    "        self.sigma = sigma\n",
    "        self.trainable = trainable\n",
    "        self.initialization = initialization\n",
    "        self.linear_on = linear_on\n",
    "        self.n_cont = n_cont\n",
    "\n",
    "        if self.initialization == 'log-linear':\n",
    "            coefficients = self.sigma ** (torch.arange(self.n//2) / self.n) # sigma matters here but more in the way of >1 or <1\n",
    "            coefficients = coefficients[None] #same as unsqueeze(0)\n",
    "        else:\n",
    "            assert self.initialization == 'normal'\n",
    "            coefficients = torch.normal(0.0, self.sigma, (1, self.n//2))\n",
    "\n",
    "        if self.trainable:\n",
    "            self.coefficients = nn.Parameter(coefficients)\n",
    "        else:\n",
    "            self.register_buffer('coefficients', coefficients)\n",
    "\n",
    "        if self.linear_on:\n",
    "            self.cont_embeddings = nn.ModuleList([nn.Linear(in_features=embed_size, out_features=embed_size) for _ in range(n_cont)])\n",
    "\n",
    "        \n",
    "        # Classifcation Embeddings for each target label\n",
    "        self.target_label_embeddings = nn.ModuleList([nn.Embedding(1, embed_size) for _ in range(num_target_labels)])\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(2) #(batch_size, n_features) -> (batch_size, n_features, 1)\n",
    "\n",
    "        temp = []\n",
    "        for i in range(self.n_cont):\n",
    "            input = x[:,i,:]\n",
    "            out = torch.cat([torch.cos(self.coefficients * input), torch.sin(self.coefficients * input)], dim=-1)\n",
    "            temp.append(out)\n",
    "        \n",
    "        embeddings = []\n",
    "        if self.linear_on:\n",
    "            x = torch.stack(temp, dim=1)\n",
    "            for i, e in enumerate(self.cont_embeddings):\n",
    "                goin_in = x[:,i,:]\n",
    "                goin_out = e(goin_in)\n",
    "                embeddings.append(goin_out)\n",
    "        else:\n",
    "            embeddings = temp\n",
    "\n",
    "        target_label_embeddings_ = []\n",
    "        for e in self.target_label_embeddings:\n",
    "            input = torch.tensor([0], device=x.device)\n",
    "            temp = e(input)\n",
    "            temp = temp.repeat(x.size(0), 1)\n",
    "            tmep = temp.unsqueeze(1)\n",
    "            target_label_embeddings_.append(temp)\n",
    "\n",
    "        class_embeddings = torch.stack(target_label_embeddings_, dim=1)\n",
    "\n",
    "        context = torch.stack(embeddings, dim=1)\n",
    "\n",
    "        return class_embeddings, context\n",
    "\n",
    "\n",
    "class classificationHead(nn.Module):\n",
    "    def __init__(self, embed_size, dropout, mlp_scale_classification, num_target_classes):\n",
    "        super(classificationHead, self).__init__()\n",
    "        \n",
    "        #flattening the embeddings out so each sample in batch is represented with a 460 dimensional vector\n",
    "        self.input = embed_size\n",
    "        self.lin1 = nn.Linear(self.input, mlp_scale_classification*self.input)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.lin2 = nn.Linear(mlp_scale_classification*self.input, mlp_scale_classification*self.input)\n",
    "        self.lin3 = nn.Linear(mlp_scale_classification*self.input, self.input)\n",
    "        self.lin4 = nn.Linear(self.input, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.initialize_weights()\n",
    "\n",
    "    def initialize_weights(self): #he_initialization.\n",
    "        torch.nn.init.kaiming_normal_(self.lin1.weight, nonlinearity='relu')\n",
    "        torch.nn.init.zeros_(self.lin1.bias)\n",
    "\n",
    "        torch.nn.init.kaiming_normal_(self.lin3.weight, nonlinearity='relu')\n",
    "        torch.nn.init.zeros_(self.lin3.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x= torch.reshape(x, (-1, self.input))\n",
    "\n",
    "        x = self.lin1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.lin2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.lin3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.lin4(x)\n",
    "  \n",
    "        return x\n",
    "\n",
    "class CATTransformer(nn.Module):\n",
    "    def __init__(self, \n",
    "                 rff_on = True,\n",
    "                 sigma=4,\n",
    "                 embed_size=20,\n",
    "                 input_size=1,\n",
    "                 n_cont = 0,\n",
    "                 num_layers=1,\n",
    "                 heads=1,\n",
    "                 forward_expansion=4, # Determines how wide the MLP is in the encoder. Its a scaling factor. \n",
    "                 decoder_dropout=0,\n",
    "                 classification_dropout = 0,\n",
    "                 pre_norm_on = False,\n",
    "                 mlp_scale_classification = 4,\n",
    "                 targets_classes : list=  [3,8],\n",
    "                embedding_scheme =\"rff_unique\",\n",
    "                trainable=True,\n",
    "                linear_on=True\n",
    "                 ):\n",
    "        super(CATTransformer, self).__init__()\n",
    "\n",
    "        assert(embedding_scheme in ['rff_unique', 'rff', 'log-linear_periodic', 'normal_periodic']), \"wrong embedding_scheme\"\n",
    "\n",
    "        if embedding_scheme == 'rff_unique':\n",
    "            self.embeddings = EmbeddingsRFFforIndividualFeatures(rff_on=rff_on, sigma=sigma, embed_size=embed_size, input_size=input_size, \n",
    "                                     n_cont=n_cont, num_target_labels=len(targets_classes), linear_on=linear_on)\n",
    "        elif embedding_scheme == 'log-linear_periodic':\n",
    "            self.embeddings = PeriodicActivation(sigma=sigma, embed_size=embed_size, n_cont=n_cont, \n",
    "                                                 num_target_labels=len(target_classes),trainable=trainable,initialization='log-linear', linear_on=linear_on)\n",
    "        elif embedding_scheme == 'normal_periodic':\n",
    "            self.embeddings = PeriodicActivation(sigma=sigma, embed_size=embed_size, n_cont=n_cont, \n",
    "                                                 num_target_labels=len(target_classes),trainable=trainable,initialization='normal',linear_on=linear_on)\n",
    "        else:\n",
    "            self.embeddings = EmbeddingsRFFforAllFeatures(rff_on=rff_on, sigma=sigma, embed_size=embed_size, input_size=input_size, \n",
    "                                     n_cont=n_cont, num_target_labels=len(targets_classes),linear_on=linear_on)\n",
    "            \n",
    "        self.decoder = Decoder(embed_size=embed_size, num_layers=num_layers, heads=heads, forward_expansion=forward_expansion, \n",
    "                               decoder_dropout=decoder_dropout, pre_norm_on=pre_norm_on)\n",
    "        self.classifying_heads = nn.ModuleList([classificationHead(embed_size=embed_size, dropout=classification_dropout, \n",
    "                                                                   mlp_scale_classification=mlp_scale_classification, \n",
    "                                                                   num_target_classes=x) for x in targets_classes])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        class_embed, context = self.embeddings(x)\n",
    "\n",
    "        x = self.decoder(class_embed, context)\n",
    "        \n",
    "        probability_dist_raw = []\n",
    "        for i, e in enumerate(self.classifying_heads):\n",
    "            input = x[:, i,:]\n",
    "            output = e(input)\n",
    "            probability_dist_raw.append(output)\n",
    "        \n",
    "        return probability_dist_raw\n",
    "    \n",
    "    # Training and Testing Loops\n",
    "def train(dataloader, model, loss_function, optimizer, device_in_use):\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "  \n",
    "    total_loss = 0\n",
    "    total_r2_score = 0\n",
    "    root_mean_squared_error_total = 0\n",
    "\n",
    "    for (features,labels_task1) in dataloader:\n",
    "        features,labels_task1 = features.to(device_in_use),labels_task1.to(device_in_use)\n",
    "\n",
    "        task_predictions = model(features) #contains a list of the tensor outputs for each task\n",
    "\n",
    "        loss = loss_function(task_predictions[0].squeeze(1), labels_task1)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Calculate R^2 score for the regression task\n",
    "        r2 = r2_score_manual(labels_task1, task_predictions[0].squeeze(1))\n",
    "        total_r2_score += r2\n",
    "\n",
    "        # Calculate RMSE score for the regression task\n",
    "        rmse_value = rmse(labels_task1, task_predictions[0].squeeze(1))\n",
    "        root_mean_squared_error_total+=rmse_value\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    avg_r2_score = total_r2_score / len(dataloader)\n",
    "    avg_rmse_score = root_mean_squared_error_total / len(dataloader)\n",
    "\n",
    "    return avg_loss, avg_r2_score, avg_rmse_score\n",
    "\n",
    "def test(dataloader, model, loss_function, device_in_use):\n",
    "  model.eval()\n",
    "  total_loss = 0\n",
    "  \n",
    "  total_loss = 0\n",
    "  total_r2_score = 0\n",
    "  root_mean_squared_error_total = 0\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for (features,labels_task1) in dataloader:\n",
    "        features,labels_task1 = features.to(device_in_use),labels_task1.to(device_in_use)\n",
    "\n",
    "        #compute prediction error\n",
    "        task_predictions = model(features) #contains a list of the tensor outputs for each task\n",
    "\n",
    "        loss = loss_function(task_predictions[0].squeeze(1), labels_task1)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Calculate R^2 score for the regression task\n",
    "        r2 = r2_score_manual(labels_task1, task_predictions[0].squeeze(1))\n",
    "        total_r2_score += r2\n",
    "        \n",
    "        # Calculate RMSE score for the regression task\n",
    "        rmse_value = rmse(labels_task1, task_predictions[0].squeeze(1))\n",
    "        root_mean_squared_error_total+=rmse_value\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    avg_r2_score = total_r2_score / len(dataloader)\n",
    "    avg_rmse_score = root_mean_squared_error_total / len(dataloader)\n",
    "\n",
    "    return avg_loss, avg_r2_score, avg_rmse_score\n",
    "\n",
    "def format_metric(value): # Used to format the metrics output\n",
    "    return f\"{value:.4f}\"\n",
    "\n",
    "def r2_score_manual(y_true, y_pred):\n",
    "    # Calculate the mean of true labels\n",
    "    y_mean = torch.mean(y_true)\n",
    "\n",
    "    # Calculate the total sum of squares\n",
    "    total_ss = torch.sum((y_true - y_mean)**2)\n",
    "\n",
    "    # Calculate the residual sum of squares\n",
    "    residual_ss = torch.sum((y_true - y_pred)**2)\n",
    "\n",
    "    # Calculate R-squared\n",
    "    r2 = 1 - (residual_ss / total_ss)\n",
    "\n",
    "    return r2.item()  # Convert to a Python float\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    # Calculate the squared differences\n",
    "    squared_diff = (y_true - y_pred)**2\n",
    "\n",
    "    # Calculate the mean of the squared differences\n",
    "    mean_squared_diff = torch.mean(squared_diff)\n",
    "\n",
    "    # Calculate the square root to obtain RMSE\n",
    "    rmse = torch.sqrt(mean_squared_diff)\n",
    "\n",
    "    return rmse.item()  # Convert to a Python float"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log-Linear RFF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\sigma = -2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [ 1/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [ 2/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [ 3/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [ 4/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [ 5/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [ 6/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [ 7/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [ 8/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [ 9/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [10/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [11/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [12/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [13/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [14/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [15/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [16/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [17/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [18/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [19/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [20/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [21/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [22/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [23/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [24/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [25/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [26/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [27/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [28/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [29/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [30/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [31/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [32/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [33/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [34/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [35/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [36/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [37/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [38/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [39/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [40/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [41/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [42/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [43/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [44/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [45/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [46/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [47/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [48/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [49/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [50/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [51/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [52/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [53/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [54/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [55/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [56/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [57/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [58/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [59/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [60/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [61/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [62/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [63/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [64/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [65/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [66/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [67/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [68/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [69/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [70/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [71/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [72/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [73/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [74/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [75/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [76/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [77/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [78/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [79/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [80/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [81/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [82/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [83/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [84/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [85/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [86/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [87/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [88/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [89/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [90/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [91/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [92/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [93/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [94/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [95/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [96/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [97/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [98/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [99/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [100/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [101/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [102/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [103/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [104/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [105/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [106/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [107/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [108/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [109/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [110/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [111/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [112/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [113/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [114/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [115/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [116/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [117/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [118/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [119/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [120/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [121/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [122/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [123/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [124/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [125/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [126/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [127/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [128/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [129/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [130/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [131/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [132/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [133/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [134/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [135/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [136/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [137/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [138/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [139/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [140/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [141/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [142/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [143/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [144/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [145/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [146/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [147/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [148/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [149/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [150/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [151/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [152/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [153/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [154/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [155/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [156/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [157/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [158/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [159/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [160/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [161/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [162/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [163/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [164/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [165/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [166/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [167/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [168/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [169/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [170/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [171/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [172/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [173/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [174/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [175/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [176/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [177/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [178/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [179/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [180/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [181/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [182/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [183/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [184/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [185/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [186/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [187/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [188/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [189/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [190/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [191/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [192/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [193/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [194/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [195/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [196/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [197/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [198/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [199/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [200/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Confusion Matrix for income\n",
      "[[42204     0     0     0     0     0     0]\n",
      " [56912     0     0     0     0     0     0]\n",
      " [ 7094     0     0     0     0     0     0]\n",
      " [  533     0     0     0     0     0     0]\n",
      " [ 1962     0     0     0     0     0     0]\n",
      " [ 3460     0     0     0     0     0     0]\n",
      " [ 4038     0     0     0     0     0     0]]\n",
      "Best accuracy 0.36319200020653514\n",
      "Best F1 0.19352876042998868\n",
      "100 epochs of training and evaluation took, 744.359375\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class LossFunction(nn.Module):\n",
    "    def __init__(self, num_tasks):\n",
    "        super(LossFunction, self).__init__()\n",
    "        self.num_tasks = num_tasks\n",
    "\n",
    "        self.loss_fns = [nn.CrossEntropyLoss() for x in range(num_tasks)] \n",
    "\n",
    "    def forward(self, predictions, labels_task1):\n",
    "\n",
    "        #task 1\n",
    "        target = labels_task1.long()\n",
    "        prediction = predictions[0]\n",
    "        loss_fn = self.loss_fns[0]\n",
    "\n",
    "        task_loss = loss_fn(prediction, target)\n",
    "        \n",
    "        return task_loss\n",
    "    \n",
    "model = CATTransformer(rff_on=True,\n",
    "                       sigma=-2,\n",
    "                       embed_size=160,\n",
    "                       heads=5,\n",
    "                       forward_expansion=8,\n",
    "                       pre_norm_on=False,\n",
    "                       mlp_scale_classification=8,\n",
    "                   \n",
    "                       decoder_dropout=0,\n",
    "                       classification_dropout=0.1,\n",
    "                       targets_classes=target_classes,\n",
    "                       n_cont=len(cont_columns),\n",
    "              \n",
    "                       embedding_scheme='log-linear_periodic',\n",
    "                       trainable=False,\n",
    "                       linear_on=True).to(device_in_use)\n",
    "\n",
    "def format_metric(value): # Used to format the metrics output\n",
    "    return f\"{value:.5f}\"\n",
    "\n",
    "loss_functions = LossFunction(1)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr = 0.0001) # Maybe try messing around with optimizers. try other torch optimizers with different configurations.\n",
    "epochs = 200 #Set the number of epochs\n",
    "\n",
    "train_losses = []\n",
    "train_accuracies_1 = [] \n",
    "test_losses = []\n",
    "test_accuracies_1 = [] \n",
    "test_f1_scores = [] \n",
    "\n",
    "\n",
    "#Time will be recorded for all 100 epochs - This means the results will not be comparable to Xgboost but that is ok, we will only compare between transformer models who will also train for 100 epochs\n",
    "start_time = time.process_time()\n",
    "\n",
    "for t in range(epochs):\n",
    "  train_loss, train_accuracy_1 = train(train_dataloader, model, loss_functions, optimizer, device_in_use=device_in_use)\n",
    "  test_loss, test_accuracy_1, all_predictions_1, all_targets_1, f1_1 = test(test_dataloader, model, loss_functions, device_in_use=device_in_use)\n",
    "  train_losses.append(train_loss)\n",
    "  train_accuracies_1.append(train_accuracy_1)\n",
    "  # train_accuracies_2.append(train_accuracy_2)\n",
    "  # train_recalls.append(train_recall) \n",
    "  # train_f1_scores.append(train_f1)\n",
    "  test_losses.append(test_loss)\n",
    "  test_accuracies_1.append(test_accuracy_1)\n",
    "  # test_accuracies_2.append(test_accuracy_2)\n",
    "  # test_recalls.append(test_recall)\n",
    "  test_f1_scores.append(f1_1)\n",
    "  # Formatting for easier reading\n",
    "  epoch_str = f\"Epoch [{t+1:2}/{epochs}]\"\n",
    "  train_metrics = f\"Train: Loss {format_metric(train_loss)}, Accuracy {format_metric(train_accuracy_1)}\"\n",
    "  test_metrics = f\"Test: Loss {format_metric(test_loss)}, Accuracy {format_metric(test_accuracy_1)}, F1 {format_metric(f1_1)}\"\n",
    "  print(f\"{epoch_str:20} | {train_metrics:65} | {test_metrics}\")\n",
    "\n",
    "total_time = time.process_time() - start_time\n",
    "\n",
    "# Plotting the loss curves\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, epochs+1), train_losses, label='Train Loss')\n",
    "plt.plot(range(1, epochs+1), [l for l in test_losses], label='Test Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Test Loss Curve')\n",
    "plt.legend()\n",
    "\n",
    "# Plotting the accuracy curves\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, epochs+1), train_accuracies_1, label='Train Accuracy')\n",
    "plt.plot(range(1, epochs+1), test_accuracies_1, label='Test Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Test Accuracy Curve')\n",
    "plt.legend()\n",
    "\n",
    "# Display confusion matrix for the first task (Traffic Type) on test data\n",
    "conf_matrix_1 = confusion_matrix(all_targets_1, all_predictions_1)\n",
    "print(\"Confusion Matrix for income\")\n",
    "print(conf_matrix_1)\n",
    "\n",
    "best_index = test_accuracies_1.index(max(test_accuracies_1))\n",
    "print(f\"Best accuracy {test_accuracies_1[best_index]}\")\n",
    "print(f\"Best F1 {test_f1_scores[best_index]}\")\n",
    "print(f\"100 epochs of training and evaluation took, {total_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\sigma = -1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [ 1/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\smbm2\\projects\\CAT-Transformer\\rff_experiments\\covertype_loglinear.ipynb Cell 9\u001b[0m line \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/rff_experiments/covertype_loglinear.ipynb#X45sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mprocess_time()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/rff_experiments/covertype_loglinear.ipynb#X45sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/rff_experiments/covertype_loglinear.ipynb#X45sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m   train_loss, train_accuracy_1 \u001b[39m=\u001b[39m train(train_dataloader, model, loss_functions, optimizer, device_in_use\u001b[39m=\u001b[39;49mdevice_in_use)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/rff_experiments/covertype_loglinear.ipynb#X45sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m   test_loss, test_accuracy_1, all_predictions_1, all_targets_1, f1_1 \u001b[39m=\u001b[39m test(test_dataloader, model, loss_functions, device_in_use\u001b[39m=\u001b[39mdevice_in_use)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/rff_experiments/covertype_loglinear.ipynb#X45sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m   train_losses\u001b[39m.\u001b[39mappend(train_loss)\n",
      "\u001b[1;32mc:\\Users\\smbm2\\projects\\CAT-Transformer\\rff_experiments\\covertype_loglinear.ipynb Cell 9\u001b[0m line \u001b[0;36m4\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/rff_experiments/covertype_loglinear.ipynb#X45sZmlsZQ%3D%3D?line=404'>405</a>\u001b[0m \u001b[39mfor\u001b[39;00m (features,labels_task1) \u001b[39min\u001b[39;00m dataloader:\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/rff_experiments/covertype_loglinear.ipynb#X45sZmlsZQ%3D%3D?line=405'>406</a>\u001b[0m     features,labels_task1 \u001b[39m=\u001b[39m features\u001b[39m.\u001b[39mto(device_in_use),labels_task1\u001b[39m.\u001b[39mto(device_in_use)\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/rff_experiments/covertype_loglinear.ipynb#X45sZmlsZQ%3D%3D?line=408'>409</a>\u001b[0m     task_predictions \u001b[39m=\u001b[39m model(features) \u001b[39m#contains a list of the tensor outputs for each task\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/rff_experiments/covertype_loglinear.ipynb#X45sZmlsZQ%3D%3D?line=410'>411</a>\u001b[0m     loss \u001b[39m=\u001b[39m loss_function(task_predictions, labels_task1)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/rff_experiments/covertype_loglinear.ipynb#X45sZmlsZQ%3D%3D?line=411'>412</a>\u001b[0m     total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\smbm2\\AppData\\Local\\miniconda3\\envs\\ml-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mc:\\Users\\smbm2\\projects\\CAT-Transformer\\rff_experiments\\covertype_loglinear.ipynb Cell 9\u001b[0m line \u001b[0;36m3\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/rff_experiments/covertype_loglinear.ipynb#X45sZmlsZQ%3D%3D?line=375'>376</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/rff_experiments/covertype_loglinear.ipynb#X45sZmlsZQ%3D%3D?line=376'>377</a>\u001b[0m     class_embed, context \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membeddings(x)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/rff_experiments/covertype_loglinear.ipynb#X45sZmlsZQ%3D%3D?line=378'>379</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder(class_embed, context)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/rff_experiments/covertype_loglinear.ipynb#X45sZmlsZQ%3D%3D?line=380'>381</a>\u001b[0m     probability_dist_raw \u001b[39m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\smbm2\\AppData\\Local\\miniconda3\\envs\\ml-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mc:\\Users\\smbm2\\projects\\CAT-Transformer\\rff_experiments\\covertype_loglinear.ipynb Cell 9\u001b[0m line \u001b[0;36m2\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/rff_experiments/covertype_loglinear.ipynb#X45sZmlsZQ%3D%3D?line=265'>266</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_cont):\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/rff_experiments/covertype_loglinear.ipynb#X45sZmlsZQ%3D%3D?line=266'>267</a>\u001b[0m     \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m x[:,i,:]\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/rff_experiments/covertype_loglinear.ipynb#X45sZmlsZQ%3D%3D?line=267'>268</a>\u001b[0m     out \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mcat([torch\u001b[39m.\u001b[39;49mcos(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcoefficients \u001b[39m*\u001b[39;49m \u001b[39minput\u001b[39;49m), torch\u001b[39m.\u001b[39;49msin(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcoefficients \u001b[39m*\u001b[39;49m \u001b[39minput\u001b[39;49m)], dim\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/rff_experiments/covertype_loglinear.ipynb#X45sZmlsZQ%3D%3D?line=268'>269</a>\u001b[0m     temp\u001b[39m.\u001b[39mappend(out)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/rff_experiments/covertype_loglinear.ipynb#X45sZmlsZQ%3D%3D?line=270'>271</a>\u001b[0m embeddings \u001b[39m=\u001b[39m []\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class LossFunction(nn.Module):\n",
    "    def __init__(self, num_tasks):\n",
    "        super(LossFunction, self).__init__()\n",
    "        self.num_tasks = num_tasks\n",
    "\n",
    "        self.loss_fns = [nn.CrossEntropyLoss() for x in range(num_tasks)] \n",
    "\n",
    "    def forward(self, predictions, labels_task1):\n",
    "\n",
    "        #task 1\n",
    "        target = labels_task1.long()\n",
    "        prediction = predictions[0]\n",
    "        loss_fn = self.loss_fns[0]\n",
    "\n",
    "        task_loss = loss_fn(prediction, target)\n",
    "        \n",
    "        return task_loss\n",
    "    \n",
    "model = CATTransformer(rff_on=True,\n",
    "                       sigma=-1,\n",
    "                       embed_size=160,\n",
    "                       heads=5,\n",
    "                       forward_expansion=8,\n",
    "                       pre_norm_on=False,\n",
    "                       mlp_scale_classification=8,\n",
    "                   \n",
    "                       decoder_dropout=0,\n",
    "                       classification_dropout=0.1,\n",
    "                       targets_classes=target_classes,\n",
    "                       n_cont=len(cont_columns),\n",
    "              \n",
    "                       embedding_scheme='log-linear_periodic',\n",
    "                       trainable=False,\n",
    "                       linear_on=True).to(device_in_use)\n",
    "\n",
    "def format_metric(value): # Used to format the metrics output\n",
    "    return f\"{value:.5f}\"\n",
    "\n",
    "loss_functions = LossFunction(1)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr = 0.0001) # Maybe try messing around with optimizers. try other torch optimizers with different configurations.\n",
    "epochs = 200 #Set the number of epochs\n",
    "\n",
    "train_losses = []\n",
    "train_accuracies_1 = [] \n",
    "test_losses = []\n",
    "test_accuracies_1 = [] \n",
    "test_f1_scores = [] \n",
    "\n",
    "\n",
    "#Time will be recorded for all 100 epochs - This means the results will not be comparable to Xgboost but that is ok, we will only compare between transformer models who will also train for 100 epochs\n",
    "start_time = time.process_time()\n",
    "\n",
    "for t in range(epochs):\n",
    "  train_loss, train_accuracy_1 = train(train_dataloader, model, loss_functions, optimizer, device_in_use=device_in_use)\n",
    "  test_loss, test_accuracy_1, all_predictions_1, all_targets_1, f1_1 = test(test_dataloader, model, loss_functions, device_in_use=device_in_use)\n",
    "  train_losses.append(train_loss)\n",
    "  train_accuracies_1.append(train_accuracy_1)\n",
    "  # train_accuracies_2.append(train_accuracy_2)\n",
    "  # train_recalls.append(train_recall) \n",
    "  # train_f1_scores.append(train_f1)\n",
    "  test_losses.append(test_loss)\n",
    "  test_accuracies_1.append(test_accuracy_1)\n",
    "  # test_accuracies_2.append(test_accuracy_2)\n",
    "  # test_recalls.append(test_recall)\n",
    "  test_f1_scores.append(f1_1)\n",
    "  # Formatting for easier reading\n",
    "  epoch_str = f\"Epoch [{t+1:2}/{epochs}]\"\n",
    "  train_metrics = f\"Train: Loss {format_metric(train_loss)}, Accuracy {format_metric(train_accuracy_1)}\"\n",
    "  test_metrics = f\"Test: Loss {format_metric(test_loss)}, Accuracy {format_metric(test_accuracy_1)}, F1 {format_metric(f1_1)}\"\n",
    "  print(f\"{epoch_str:20} | {train_metrics:65} | {test_metrics}\")\n",
    "\n",
    "total_time = time.process_time() - start_time\n",
    "\n",
    "# Plotting the loss curves\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, epochs+1), train_losses, label='Train Loss')\n",
    "plt.plot(range(1, epochs+1), [l for l in test_losses], label='Test Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Test Loss Curve')\n",
    "plt.legend()\n",
    "\n",
    "# Plotting the accuracy curves\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, epochs+1), train_accuracies_1, label='Train Accuracy')\n",
    "plt.plot(range(1, epochs+1), test_accuracies_1, label='Test Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Test Accuracy Curve')\n",
    "plt.legend()\n",
    "\n",
    "# Display confusion matrix for the first task (Traffic Type) on test data\n",
    "conf_matrix_1 = confusion_matrix(all_targets_1, all_predictions_1)\n",
    "print(\"Confusion Matrix for income\")\n",
    "print(conf_matrix_1)\n",
    "\n",
    "best_index = test_accuracies_1.index(max(test_accuracies_1))\n",
    "print(f\"Best accuracy {test_accuracies_1[best_index]}\")\n",
    "print(f\"Best F1 {test_f1_scores[best_index]}\")\n",
    "print(f\"100 epochs of training and evaluation took, {total_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\sigma = -0.5$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossFunction(nn.Module):\n",
    "    def __init__(self, num_tasks):\n",
    "        super(LossFunction, self).__init__()\n",
    "        self.num_tasks = num_tasks\n",
    "\n",
    "        self.loss_fns = [nn.CrossEntropyLoss() for x in range(num_tasks)] \n",
    "\n",
    "    def forward(self, predictions, labels_task1):\n",
    "\n",
    "        #task 1\n",
    "        target = labels_task1.long()\n",
    "        prediction = predictions[0]\n",
    "        loss_fn = self.loss_fns[0]\n",
    "\n",
    "        task_loss = loss_fn(prediction, target)\n",
    "        \n",
    "        return task_loss\n",
    "    \n",
    "model = CATTransformer(rff_on=True,\n",
    "                       sigma=-0.5,\n",
    "                       embed_size=160,\n",
    "                       heads=5,\n",
    "                       forward_expansion=8,\n",
    "                       pre_norm_on=False,\n",
    "                       mlp_scale_classification=8,\n",
    "                   \n",
    "                       decoder_dropout=0,\n",
    "                       classification_dropout=0.1,\n",
    "                       targets_classes=target_classes,\n",
    "                       n_cont=len(cont_columns),\n",
    "              \n",
    "                       embedding_scheme='log-linear_periodic',\n",
    "                       trainable=False,\n",
    "                       linear_on=True).to(device_in_use)\n",
    "\n",
    "def format_metric(value): # Used to format the metrics output\n",
    "    return f\"{value:.5f}\"\n",
    "\n",
    "loss_functions = LossFunction(1)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr = 0.0001) # Maybe try messing around with optimizers. try other torch optimizers with different configurations.\n",
    "epochs = 200 #Set the number of epochs\n",
    "\n",
    "train_losses = []\n",
    "train_accuracies_1 = [] \n",
    "test_losses = []\n",
    "test_accuracies_1 = [] \n",
    "test_f1_scores = [] \n",
    "\n",
    "\n",
    "#Time will be recorded for all 100 epochs - This means the results will not be comparable to Xgboost but that is ok, we will only compare between transformer models who will also train for 100 epochs\n",
    "start_time = time.process_time()\n",
    "\n",
    "for t in range(epochs):\n",
    "  train_loss, train_accuracy_1 = train(train_dataloader, model, loss_functions, optimizer, device_in_use=device_in_use)\n",
    "  test_loss, test_accuracy_1, all_predictions_1, all_targets_1, f1_1 = test(test_dataloader, model, loss_functions, device_in_use=device_in_use)\n",
    "  train_losses.append(train_loss)\n",
    "  train_accuracies_1.append(train_accuracy_1)\n",
    "  # train_accuracies_2.append(train_accuracy_2)\n",
    "  # train_recalls.append(train_recall) \n",
    "  # train_f1_scores.append(train_f1)\n",
    "  test_losses.append(test_loss)\n",
    "  test_accuracies_1.append(test_accuracy_1)\n",
    "  # test_accuracies_2.append(test_accuracy_2)\n",
    "  # test_recalls.append(test_recall)\n",
    "  test_f1_scores.append(f1_1)\n",
    "  # Formatting for easier reading\n",
    "  epoch_str = f\"Epoch [{t+1:2}/{epochs}]\"\n",
    "  train_metrics = f\"Train: Loss {format_metric(train_loss)}, Accuracy {format_metric(train_accuracy_1)}\"\n",
    "  test_metrics = f\"Test: Loss {format_metric(test_loss)}, Accuracy {format_metric(test_accuracy_1)}, F1 {format_metric(f1_1)}\"\n",
    "  print(f\"{epoch_str:20} | {train_metrics:65} | {test_metrics}\")\n",
    "\n",
    "total_time = time.process_time() - start_time\n",
    "\n",
    "# Plotting the loss curves\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, epochs+1), train_losses, label='Train Loss')\n",
    "plt.plot(range(1, epochs+1), [l for l in test_losses], label='Test Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Test Loss Curve')\n",
    "plt.legend()\n",
    "\n",
    "# Plotting the accuracy curves\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, epochs+1), train_accuracies_1, label='Train Accuracy')\n",
    "plt.plot(range(1, epochs+1), test_accuracies_1, label='Test Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Test Accuracy Curve')\n",
    "plt.legend()\n",
    "\n",
    "# Display confusion matrix for the first task (Traffic Type) on test data\n",
    "conf_matrix_1 = confusion_matrix(all_targets_1, all_predictions_1)\n",
    "print(\"Confusion Matrix for income\")\n",
    "print(conf_matrix_1)\n",
    "\n",
    "best_index = test_accuracies_1.index(max(test_accuracies_1))\n",
    "print(f\"Best accuracy {test_accuracies_1[best_index]}\")\n",
    "print(f\"Best F1 {test_f1_scores[best_index]}\")\n",
    "print(f\"100 epochs of training and evaluation took, {total_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\sigma = 0.5$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [ 1/500]       | Train: Loss 4.61455, R2 -0.28788, RMSE 2.08795                    | Test: Loss 3.87617, R2 0.02866, RMSE 1.94974\n",
      "Epoch [ 2/500]       | Train: Loss 2.50285, R2 0.30260, RMSE 1.55418                     | Test: Loss 1.20046, R2 0.66059, RMSE 1.08656\n",
      "Epoch [ 3/500]       | Train: Loss 1.29807, R2 0.63898, RMSE 1.13357                     | Test: Loss 1.12699, R2 0.68919, RMSE 1.05453\n",
      "Epoch [ 4/500]       | Train: Loss 1.20136, R2 0.66520, RMSE 1.09047                     | Test: Loss 1.07541, R2 0.71101, RMSE 1.02860\n",
      "Epoch [ 5/500]       | Train: Loss 1.20332, R2 0.66753, RMSE 1.08969                     | Test: Loss 1.06065, R2 0.70308, RMSE 1.02009\n",
      "Epoch [ 6/500]       | Train: Loss 1.13216, R2 0.68385, RMSE 1.06043                     | Test: Loss 1.07146, R2 0.66251, RMSE 1.02810\n",
      "Epoch [ 7/500]       | Train: Loss 1.11113, R2 0.68861, RMSE 1.05000                     | Test: Loss 1.00103, R2 0.71917, RMSE 0.98900\n",
      "Epoch [ 8/500]       | Train: Loss 1.07803, R2 0.70061, RMSE 1.03396                     | Test: Loss 0.99729, R2 0.70802, RMSE 0.98808\n",
      "Epoch [ 9/500]       | Train: Loss 1.05946, R2 0.70312, RMSE 1.02591                     | Test: Loss 0.99385, R2 0.70213, RMSE 0.98353\n",
      "Epoch [10/500]       | Train: Loss 1.05091, R2 0.70477, RMSE 1.02001                     | Test: Loss 1.14033, R2 0.69145, RMSE 1.05957\n",
      "Epoch [11/500]       | Train: Loss 1.05211, R2 0.70475, RMSE 1.02153                     | Test: Loss 1.01635, R2 0.71741, RMSE 0.99274\n",
      "Epoch [12/500]       | Train: Loss 1.00299, R2 0.71902, RMSE 0.99779                     | Test: Loss 1.02995, R2 0.70972, RMSE 0.99304\n",
      "Epoch [13/500]       | Train: Loss 1.01603, R2 0.71598, RMSE 1.00297                     | Test: Loss 1.07220, R2 0.71735, RMSE 1.01967\n",
      "Epoch [14/500]       | Train: Loss 0.97686, R2 0.72703, RMSE 0.98430                     | Test: Loss 1.05043, R2 0.71396, RMSE 1.00347\n",
      "Epoch [15/500]       | Train: Loss 0.99290, R2 0.72284, RMSE 0.99170                     | Test: Loss 1.02660, R2 0.71696, RMSE 0.98908\n",
      "Epoch [16/500]       | Train: Loss 0.96863, R2 0.73071, RMSE 0.97978                     | Test: Loss 1.07957, R2 0.69224, RMSE 1.00661\n",
      "Epoch [17/500]       | Train: Loss 0.95510, R2 0.73058, RMSE 0.97402                     | Test: Loss 1.05420, R2 0.70676, RMSE 1.00424\n",
      "Epoch [18/500]       | Train: Loss 0.97647, R2 0.72874, RMSE 0.98272                     | Test: Loss 1.13464, R2 0.70027, RMSE 1.03185\n",
      "Epoch [19/500]       | Train: Loss 0.94758, R2 0.73266, RMSE 0.96994                     | Test: Loss 1.20474, R2 0.69366, RMSE 1.06491\n",
      "Epoch [20/500]       | Train: Loss 0.94908, R2 0.73325, RMSE 0.97028                     | Test: Loss 1.31609, R2 0.65571, RMSE 1.11845\n",
      "Epoch [21/500]       | Train: Loss 0.95740, R2 0.73149, RMSE 0.97587                     | Test: Loss 1.15919, R2 0.68428, RMSE 1.04834\n",
      "Epoch [22/500]       | Train: Loss 0.97109, R2 0.72854, RMSE 0.98143                     | Test: Loss 1.05560, R2 0.71278, RMSE 0.99726\n",
      "Epoch [23/500]       | Train: Loss 0.92780, R2 0.74177, RMSE 0.95869                     | Test: Loss 1.09353, R2 0.70572, RMSE 1.00359\n",
      "Epoch [24/500]       | Train: Loss 0.91111, R2 0.74591, RMSE 0.95006                     | Test: Loss 1.13451, R2 0.70870, RMSE 1.02960\n",
      "Epoch [25/500]       | Train: Loss 0.91025, R2 0.74420, RMSE 0.95100                     | Test: Loss 1.13861, R2 0.68295, RMSE 1.03422\n",
      "Epoch [26/500]       | Train: Loss 0.89929, R2 0.74798, RMSE 0.94615                     | Test: Loss 1.12670, R2 0.68358, RMSE 1.02480\n",
      "Epoch [27/500]       | Train: Loss 0.89095, R2 0.74882, RMSE 0.93952                     | Test: Loss 1.16472, R2 0.69490, RMSE 1.03775\n",
      "Epoch [28/500]       | Train: Loss 0.88247, R2 0.75145, RMSE 0.93526                     | Test: Loss 1.16238, R2 0.67521, RMSE 1.04052\n",
      "Epoch [29/500]       | Train: Loss 0.87751, R2 0.75496, RMSE 0.93354                     | Test: Loss 1.13940, R2 0.68205, RMSE 1.02342\n",
      "Epoch [30/500]       | Train: Loss 0.87855, R2 0.75425, RMSE 0.93289                     | Test: Loss 1.07735, R2 0.70468, RMSE 0.99742\n",
      "Epoch [31/500]       | Train: Loss 0.88724, R2 0.75080, RMSE 0.93835                     | Test: Loss 1.23773, R2 0.68569, RMSE 1.06602\n",
      "Epoch [32/500]       | Train: Loss 0.86992, R2 0.75874, RMSE 0.92829                     | Test: Loss 1.14094, R2 0.67728, RMSE 1.03123\n",
      "Epoch [33/500]       | Train: Loss 0.85978, R2 0.75993, RMSE 0.92194                     | Test: Loss 1.05658, R2 0.72627, RMSE 0.96202\n",
      "Epoch [34/500]       | Train: Loss 0.84583, R2 0.76383, RMSE 0.91661                     | Test: Loss 1.08908, R2 0.69838, RMSE 1.00113\n",
      "Epoch [35/500]       | Train: Loss 0.82632, R2 0.76704, RMSE 0.90560                     | Test: Loss 1.12325, R2 0.69088, RMSE 1.01416\n",
      "Epoch [36/500]       | Train: Loss 0.84736, R2 0.76466, RMSE 0.91521                     | Test: Loss 1.03418, R2 0.69829, RMSE 0.97232\n",
      "Epoch [37/500]       | Train: Loss 0.84683, R2 0.76240, RMSE 0.91648                     | Test: Loss 1.04260, R2 0.70185, RMSE 0.97155\n",
      "Epoch [38/500]       | Train: Loss 0.84117, R2 0.76522, RMSE 0.91303                     | Test: Loss 1.16544, R2 0.68249, RMSE 1.03971\n",
      "Epoch [39/500]       | Train: Loss 0.83660, R2 0.76433, RMSE 0.91229                     | Test: Loss 1.09194, R2 0.69791, RMSE 1.00121\n",
      "Epoch [40/500]       | Train: Loss 0.83711, R2 0.76807, RMSE 0.91079                     | Test: Loss 1.07942, R2 0.69617, RMSE 0.99940\n",
      "Epoch [41/500]       | Train: Loss 0.83655, R2 0.76782, RMSE 0.90927                     | Test: Loss 1.03282, R2 0.71183, RMSE 0.97189\n",
      "Epoch [42/500]       | Train: Loss 0.82639, R2 0.76917, RMSE 0.90464                     | Test: Loss 1.05232, R2 0.71673, RMSE 0.97354\n",
      "Epoch [43/500]       | Train: Loss 0.80644, R2 0.77267, RMSE 0.89477                     | Test: Loss 1.06732, R2 0.70422, RMSE 0.98580\n",
      "Epoch [44/500]       | Train: Loss 0.84146, R2 0.76673, RMSE 0.91256                     | Test: Loss 1.11053, R2 0.66617, RMSE 1.01494\n",
      "Epoch [45/500]       | Train: Loss 0.80484, R2 0.77433, RMSE 0.89282                     | Test: Loss 1.08541, R2 0.70560, RMSE 0.99367\n",
      "Epoch [46/500]       | Train: Loss 0.83447, R2 0.76715, RMSE 0.90946                     | Test: Loss 1.09214, R2 0.68772, RMSE 1.00297\n",
      "Epoch [47/500]       | Train: Loss 0.80884, R2 0.77398, RMSE 0.89602                     | Test: Loss 1.09729, R2 0.68502, RMSE 1.00462\n",
      "Epoch [48/500]       | Train: Loss 0.81194, R2 0.77247, RMSE 0.89703                     | Test: Loss 1.05904, R2 0.71265, RMSE 0.98189\n",
      "Epoch [49/500]       | Train: Loss 0.81059, R2 0.77256, RMSE 0.89549                     | Test: Loss 1.04978, R2 0.73289, RMSE 0.96288\n",
      "Epoch [50/500]       | Train: Loss 0.79097, R2 0.77727, RMSE 0.88674                     | Test: Loss 1.09688, R2 0.69344, RMSE 0.99156\n",
      "Epoch [51/500]       | Train: Loss 0.80572, R2 0.77467, RMSE 0.89373                     | Test: Loss 1.06834, R2 0.71412, RMSE 0.95627\n",
      "Epoch [52/500]       | Train: Loss 0.79022, R2 0.78010, RMSE 0.88569                     | Test: Loss 1.09004, R2 0.69537, RMSE 0.99440\n",
      "Epoch [53/500]       | Train: Loss 0.80000, R2 0.77661, RMSE 0.89131                     | Test: Loss 1.10826, R2 0.69091, RMSE 1.00547\n",
      "Epoch [54/500]       | Train: Loss 0.78841, R2 0.77891, RMSE 0.88382                     | Test: Loss 1.07891, R2 0.68895, RMSE 0.98348\n",
      "Epoch [55/500]       | Train: Loss 0.78003, R2 0.78218, RMSE 0.87809                     | Test: Loss 1.06098, R2 0.70927, RMSE 0.96724\n",
      "Epoch [56/500]       | Train: Loss 0.79437, R2 0.77699, RMSE 0.88761                     | Test: Loss 1.04397, R2 0.70738, RMSE 0.93614\n",
      "Epoch [57/500]       | Train: Loss 0.78634, R2 0.78172, RMSE 0.88293                     | Test: Loss 1.13056, R2 0.67226, RMSE 1.01436\n",
      "Epoch [58/500]       | Train: Loss 0.80169, R2 0.77559, RMSE 0.89190                     | Test: Loss 1.08983, R2 0.70126, RMSE 1.00228\n",
      "Epoch [59/500]       | Train: Loss 0.78573, R2 0.78054, RMSE 0.88150                     | Test: Loss 1.13103, R2 0.68796, RMSE 0.99555\n",
      "Epoch [60/500]       | Train: Loss 0.78502, R2 0.77945, RMSE 0.88252                     | Test: Loss 1.08220, R2 0.71962, RMSE 0.98875\n",
      "Epoch [61/500]       | Train: Loss 0.79013, R2 0.77930, RMSE 0.88584                     | Test: Loss 1.05391, R2 0.70142, RMSE 0.97941\n",
      "Epoch [62/500]       | Train: Loss 0.78139, R2 0.78149, RMSE 0.87987                     | Test: Loss 1.05614, R2 0.69602, RMSE 0.96973\n",
      "Epoch [63/500]       | Train: Loss 0.76867, R2 0.78559, RMSE 0.87319                     | Test: Loss 1.08150, R2 0.71465, RMSE 0.97134\n",
      "Epoch [64/500]       | Train: Loss 0.78483, R2 0.78026, RMSE 0.88242                     | Test: Loss 1.04094, R2 0.68581, RMSE 0.96521\n",
      "Epoch [65/500]       | Train: Loss 0.76668, R2 0.78655, RMSE 0.87147                     | Test: Loss 1.04347, R2 0.72182, RMSE 0.96935\n",
      "Epoch [66/500]       | Train: Loss 0.77134, R2 0.78409, RMSE 0.87404                     | Test: Loss 1.03057, R2 0.68776, RMSE 0.96854\n",
      "Epoch [67/500]       | Train: Loss 0.77535, R2 0.78328, RMSE 0.87628                     | Test: Loss 1.03659, R2 0.71703, RMSE 0.96011\n",
      "Epoch [68/500]       | Train: Loss 0.77335, R2 0.78282, RMSE 0.87683                     | Test: Loss 1.07288, R2 0.69234, RMSE 0.99306\n",
      "Epoch [69/500]       | Train: Loss 0.77123, R2 0.78364, RMSE 0.87436                     | Test: Loss 1.01035, R2 0.73713, RMSE 0.94173\n",
      "Epoch [70/500]       | Train: Loss 0.75696, R2 0.78818, RMSE 0.86715                     | Test: Loss 1.01927, R2 0.72192, RMSE 0.94828\n",
      "Epoch [71/500]       | Train: Loss 0.75285, R2 0.78795, RMSE 0.86431                     | Test: Loss 1.01543, R2 0.72177, RMSE 0.95463\n",
      "Epoch [72/500]       | Train: Loss 0.76010, R2 0.78844, RMSE 0.86862                     | Test: Loss 1.03605, R2 0.72544, RMSE 0.97763\n",
      "Epoch [73/500]       | Train: Loss 0.75777, R2 0.78969, RMSE 0.86612                     | Test: Loss 1.03992, R2 0.72851, RMSE 0.97007\n",
      "Epoch [74/500]       | Train: Loss 0.76489, R2 0.78548, RMSE 0.87097                     | Test: Loss 1.03018, R2 0.72280, RMSE 0.98035\n",
      "Epoch [75/500]       | Train: Loss 0.76783, R2 0.78343, RMSE 0.87158                     | Test: Loss 0.99937, R2 0.74318, RMSE 0.96099\n",
      "Epoch [76/500]       | Train: Loss 0.76406, R2 0.78794, RMSE 0.87002                     | Test: Loss 0.96824, R2 0.73627, RMSE 0.95266\n",
      "Epoch [77/500]       | Train: Loss 0.74069, R2 0.79307, RMSE 0.85748                     | Test: Loss 0.94975, R2 0.70692, RMSE 0.93605\n",
      "Epoch [78/500]       | Train: Loss 0.76079, R2 0.78661, RMSE 0.86841                     | Test: Loss 0.95426, R2 0.75516, RMSE 0.94616\n",
      "Epoch [79/500]       | Train: Loss 0.74467, R2 0.79080, RMSE 0.86035                     | Test: Loss 0.88926, R2 0.75449, RMSE 0.91330\n",
      "Epoch [80/500]       | Train: Loss 0.75185, R2 0.78976, RMSE 0.86483                     | Test: Loss 0.89820, R2 0.74754, RMSE 0.91211\n",
      "Epoch [81/500]       | Train: Loss 0.72636, R2 0.79667, RMSE 0.84770                     | Test: Loss 0.95474, R2 0.74319, RMSE 0.95524\n",
      "Epoch [82/500]       | Train: Loss 0.73053, R2 0.79502, RMSE 0.85102                     | Test: Loss 0.90022, R2 0.74971, RMSE 0.92432\n",
      "Epoch [83/500]       | Train: Loss 0.74094, R2 0.79236, RMSE 0.85764                     | Test: Loss 0.86423, R2 0.76623, RMSE 0.89290\n",
      "Epoch [84/500]       | Train: Loss 0.73751, R2 0.79372, RMSE 0.85505                     | Test: Loss 0.87105, R2 0.75720, RMSE 0.91743\n",
      "Epoch [85/500]       | Train: Loss 0.72341, R2 0.79807, RMSE 0.84677                     | Test: Loss 0.89736, R2 0.74436, RMSE 0.91689\n",
      "Epoch [86/500]       | Train: Loss 0.73986, R2 0.79277, RMSE 0.85635                     | Test: Loss 0.87933, R2 0.75881, RMSE 0.92014\n",
      "Epoch [87/500]       | Train: Loss 0.71952, R2 0.79952, RMSE 0.84390                     | Test: Loss 1.71835, R2 0.42303, RMSE 1.07781\n",
      "Epoch [88/500]       | Train: Loss 0.71598, R2 0.79964, RMSE 0.84196                     | Test: Loss 0.83944, R2 0.75121, RMSE 0.89862\n",
      "Epoch [89/500]       | Train: Loss 0.71658, R2 0.79747, RMSE 0.84297                     | Test: Loss 0.80784, R2 0.77478, RMSE 0.88680\n",
      "Epoch [90/500]       | Train: Loss 0.70731, R2 0.80065, RMSE 0.83720                     | Test: Loss 0.82666, R2 0.77869, RMSE 0.87923\n",
      "Epoch [91/500]       | Train: Loss 0.72103, R2 0.79849, RMSE 0.84511                     | Test: Loss 0.81986, R2 0.77145, RMSE 0.88871\n",
      "Epoch [92/500]       | Train: Loss 0.70484, R2 0.80170, RMSE 0.83630                     | Test: Loss 0.79715, R2 0.78117, RMSE 0.87848\n",
      "Epoch [93/500]       | Train: Loss 0.70294, R2 0.80310, RMSE 0.83614                     | Test: Loss 0.80602, R2 0.78432, RMSE 0.87989\n",
      "Epoch [94/500]       | Train: Loss 0.70672, R2 0.80203, RMSE 0.83778                     | Test: Loss 0.90328, R2 0.77016, RMSE 0.93128\n",
      "Epoch [95/500]       | Train: Loss 0.72218, R2 0.79843, RMSE 0.84633                     | Test: Loss 0.84849, R2 0.77480, RMSE 0.90635\n",
      "Epoch [96/500]       | Train: Loss 0.72839, R2 0.79744, RMSE 0.84941                     | Test: Loss 0.91668, R2 0.76048, RMSE 0.93140\n",
      "Epoch [97/500]       | Train: Loss 0.68864, R2 0.80808, RMSE 0.82637                     | Test: Loss 0.75629, R2 0.78477, RMSE 0.85149\n",
      "Epoch [98/500]       | Train: Loss 0.70516, R2 0.80247, RMSE 0.83722                     | Test: Loss 0.73132, R2 0.79336, RMSE 0.84274\n",
      "Epoch [99/500]       | Train: Loss 0.72471, R2 0.79724, RMSE 0.84739                     | Test: Loss 0.80641, R2 0.77485, RMSE 0.88155\n",
      "Epoch [100/500]      | Train: Loss 0.70319, R2 0.80298, RMSE 0.83545                     | Test: Loss 0.80025, R2 0.79063, RMSE 0.87952\n",
      "Epoch [101/500]      | Train: Loss 0.70075, R2 0.80384, RMSE 0.83389                     | Test: Loss 1.11484, R2 0.54128, RMSE 0.96317\n",
      "Epoch [102/500]      | Train: Loss 0.67468, R2 0.81168, RMSE 0.81893                     | Test: Loss 0.74982, R2 0.79486, RMSE 0.85482\n",
      "Epoch [103/500]      | Train: Loss 0.68659, R2 0.80842, RMSE 0.82521                     | Test: Loss 0.76082, R2 0.78779, RMSE 0.85974\n",
      "Epoch [104/500]      | Train: Loss 0.68340, R2 0.80633, RMSE 0.82251                     | Test: Loss 0.81357, R2 0.79187, RMSE 0.89364\n",
      "Epoch [105/500]      | Train: Loss 0.67948, R2 0.80821, RMSE 0.82131                     | Test: Loss 0.72101, R2 0.79964, RMSE 0.84096\n",
      "Epoch [106/500]      | Train: Loss 0.68237, R2 0.80908, RMSE 0.82291                     | Test: Loss 0.76614, R2 0.77313, RMSE 0.86581\n",
      "Epoch [107/500]      | Train: Loss 0.67864, R2 0.81006, RMSE 0.82074                     | Test: Loss 0.73104, R2 0.80517, RMSE 0.84048\n",
      "Epoch [108/500]      | Train: Loss 0.66132, R2 0.81420, RMSE 0.81061                     | Test: Loss 0.75456, R2 0.79474, RMSE 0.85978\n",
      "Epoch [109/500]      | Train: Loss 0.69317, R2 0.80517, RMSE 0.82904                     | Test: Loss 0.75973, R2 0.78322, RMSE 0.86441\n",
      "Epoch [110/500]      | Train: Loss 0.67994, R2 0.80902, RMSE 0.82079                     | Test: Loss 0.69120, R2 0.81130, RMSE 0.81946\n",
      "Epoch [111/500]      | Train: Loss 0.69969, R2 0.80388, RMSE 0.83276                     | Test: Loss 0.99737, R2 0.70831, RMSE 0.97307\n",
      "Epoch [112/500]      | Train: Loss 0.67023, R2 0.81098, RMSE 0.81608                     | Test: Loss 0.69684, R2 0.78660, RMSE 0.82165\n",
      "Epoch [113/500]      | Train: Loss 0.68655, R2 0.80799, RMSE 0.82547                     | Test: Loss 0.70367, R2 0.81071, RMSE 0.83344\n",
      "Epoch [114/500]      | Train: Loss 0.66239, R2 0.81357, RMSE 0.81037                     | Test: Loss 0.77598, R2 0.78851, RMSE 0.86832\n",
      "Epoch [115/500]      | Train: Loss 0.65579, R2 0.81686, RMSE 0.80688                     | Test: Loss 0.70883, R2 0.81047, RMSE 0.83188\n",
      "Epoch [116/500]      | Train: Loss 0.65802, R2 0.81507, RMSE 0.80794                     | Test: Loss 0.69333, R2 0.80553, RMSE 0.82633\n",
      "Epoch [117/500]      | Train: Loss 0.64985, R2 0.81863, RMSE 0.80325                     | Test: Loss 0.67563, R2 0.81569, RMSE 0.80569\n",
      "Epoch [118/500]      | Train: Loss 0.65166, R2 0.81690, RMSE 0.80471                     | Test: Loss 0.78575, R2 0.78120, RMSE 0.86561\n",
      "Epoch [119/500]      | Train: Loss 0.64227, R2 0.81882, RMSE 0.79826                     | Test: Loss 0.72614, R2 0.80584, RMSE 0.84450\n",
      "Epoch [120/500]      | Train: Loss 0.63046, R2 0.82262, RMSE 0.79148                     | Test: Loss 0.73946, R2 0.80110, RMSE 0.85278\n",
      "Epoch [121/500]      | Train: Loss 0.65014, R2 0.81803, RMSE 0.80251                     | Test: Loss 0.70554, R2 0.81214, RMSE 0.82988\n",
      "Epoch [122/500]      | Train: Loss 0.64871, R2 0.81920, RMSE 0.80298                     | Test: Loss 0.76953, R2 0.79038, RMSE 0.86850\n",
      "Epoch [123/500]      | Train: Loss 0.63969, R2 0.81997, RMSE 0.79652                     | Test: Loss 0.69505, R2 0.81782, RMSE 0.82769\n",
      "Epoch [124/500]      | Train: Loss 0.64311, R2 0.81925, RMSE 0.79936                     | Test: Loss 0.68667, R2 0.79853, RMSE 0.82122\n",
      "Epoch [125/500]      | Train: Loss 0.63167, R2 0.82205, RMSE 0.79241                     | Test: Loss 0.69896, R2 0.80533, RMSE 0.82469\n",
      "Epoch [126/500]      | Train: Loss 0.63974, R2 0.82068, RMSE 0.79690                     | Test: Loss 0.69544, R2 0.80648, RMSE 0.82532\n",
      "Epoch [127/500]      | Train: Loss 0.63527, R2 0.82160, RMSE 0.79452                     | Test: Loss 0.68903, R2 0.80772, RMSE 0.82475\n",
      "Epoch [128/500]      | Train: Loss 0.61518, R2 0.82636, RMSE 0.78217                     | Test: Loss 0.72948, R2 0.81109, RMSE 0.84630\n",
      "Epoch [129/500]      | Train: Loss 0.61769, R2 0.82636, RMSE 0.78266                     | Test: Loss 0.69874, R2 0.81085, RMSE 0.82675\n",
      "Epoch [130/500]      | Train: Loss 0.61464, R2 0.82775, RMSE 0.78144                     | Test: Loss 0.70695, R2 0.80245, RMSE 0.83596\n",
      "Epoch [131/500]      | Train: Loss 0.62036, R2 0.82564, RMSE 0.78488                     | Test: Loss 0.70121, R2 0.79738, RMSE 0.83292\n",
      "Epoch [132/500]      | Train: Loss 0.62779, R2 0.82477, RMSE 0.78916                     | Test: Loss 0.69220, R2 0.81233, RMSE 0.82488\n",
      "Epoch [133/500]      | Train: Loss 0.62458, R2 0.82420, RMSE 0.78599                     | Test: Loss 0.68727, R2 0.81396, RMSE 0.82369\n",
      "Epoch [134/500]      | Train: Loss 0.61253, R2 0.82747, RMSE 0.77993                     | Test: Loss 0.67618, R2 0.80696, RMSE 0.81140\n",
      "Epoch [135/500]      | Train: Loss 0.63021, R2 0.82329, RMSE 0.79013                     | Test: Loss 0.70418, R2 0.81119, RMSE 0.82811\n",
      "Epoch [136/500]      | Train: Loss 0.61965, R2 0.82694, RMSE 0.78471                     | Test: Loss 0.69241, R2 0.81290, RMSE 0.82723\n",
      "Epoch [137/500]      | Train: Loss 0.60532, R2 0.83014, RMSE 0.77508                     | Test: Loss 0.70484, R2 0.81261, RMSE 0.83141\n",
      "Epoch [138/500]      | Train: Loss 0.62453, R2 0.82534, RMSE 0.78787                     | Test: Loss 0.68947, R2 0.80851, RMSE 0.82226\n",
      "Epoch [139/500]      | Train: Loss 0.59881, R2 0.83166, RMSE 0.76977                     | Test: Loss 0.71583, R2 0.80956, RMSE 0.84113\n",
      "Epoch [140/500]      | Train: Loss 0.58476, R2 0.83630, RMSE 0.76303                     | Test: Loss 0.67139, R2 0.81780, RMSE 0.80863\n",
      "Epoch [141/500]      | Train: Loss 0.60448, R2 0.83118, RMSE 0.77418                     | Test: Loss 0.70823, R2 0.80776, RMSE 0.83700\n",
      "Epoch [142/500]      | Train: Loss 0.59494, R2 0.83404, RMSE 0.76872                     | Test: Loss 0.73451, R2 0.78539, RMSE 0.84974\n",
      "Epoch [143/500]      | Train: Loss 0.59525, R2 0.83344, RMSE 0.76944                     | Test: Loss 0.78238, R2 0.79719, RMSE 0.87133\n",
      "Epoch [144/500]      | Train: Loss 0.59853, R2 0.83224, RMSE 0.77004                     | Test: Loss 0.70354, R2 0.81155, RMSE 0.82901\n",
      "Epoch [145/500]      | Train: Loss 0.58489, R2 0.83731, RMSE 0.76147                     | Test: Loss 0.67880, R2 0.81293, RMSE 0.81300\n",
      "Epoch [146/500]      | Train: Loss 0.59032, R2 0.83453, RMSE 0.76540                     | Test: Loss 0.73669, R2 0.80570, RMSE 0.85060\n",
      "Epoch [147/500]      | Train: Loss 0.58668, R2 0.83341, RMSE 0.76329                     | Test: Loss 0.69208, R2 0.80733, RMSE 0.82707\n",
      "Epoch [148/500]      | Train: Loss 0.58133, R2 0.83669, RMSE 0.75968                     | Test: Loss 0.70400, R2 0.79845, RMSE 0.83273\n",
      "Epoch [149/500]      | Train: Loss 0.57926, R2 0.83761, RMSE 0.75745                     | Test: Loss 0.68896, R2 0.80784, RMSE 0.81915\n",
      "Epoch [150/500]      | Train: Loss 0.58082, R2 0.83785, RMSE 0.75982                     | Test: Loss 0.71068, R2 0.81058, RMSE 0.83387\n",
      "Epoch [151/500]      | Train: Loss 0.57430, R2 0.83910, RMSE 0.75409                     | Test: Loss 0.70458, R2 0.81017, RMSE 0.83237\n",
      "Epoch [152/500]      | Train: Loss 0.57191, R2 0.83949, RMSE 0.75428                     | Test: Loss 0.67088, R2 0.81974, RMSE 0.81300\n",
      "Epoch [153/500]      | Train: Loss 0.56861, R2 0.83861, RMSE 0.75225                     | Test: Loss 0.67477, R2 0.81332, RMSE 0.81031\n",
      "Epoch [154/500]      | Train: Loss 0.56339, R2 0.84315, RMSE 0.74773                     | Test: Loss 0.70474, R2 0.79609, RMSE 0.83417\n",
      "Epoch [155/500]      | Train: Loss 0.56339, R2 0.84081, RMSE 0.74848                     | Test: Loss 0.67054, R2 0.81401, RMSE 0.81159\n",
      "Epoch [156/500]      | Train: Loss 0.57410, R2 0.83865, RMSE 0.75559                     | Test: Loss 0.73666, R2 0.80099, RMSE 0.84791\n",
      "Epoch [157/500]      | Train: Loss 0.57207, R2 0.83779, RMSE 0.75342                     | Test: Loss 0.71122, R2 0.80016, RMSE 0.83760\n",
      "Epoch [158/500]      | Train: Loss 0.55708, R2 0.84391, RMSE 0.74387                     | Test: Loss 0.70689, R2 0.80639, RMSE 0.83365\n",
      "Epoch [159/500]      | Train: Loss 0.55130, R2 0.84354, RMSE 0.73982                     | Test: Loss 0.64610, R2 0.82231, RMSE 0.79751\n",
      "Epoch [160/500]      | Train: Loss 0.55012, R2 0.84629, RMSE 0.73916                     | Test: Loss 0.68367, R2 0.80809, RMSE 0.82068\n",
      "Epoch [161/500]      | Train: Loss 0.54416, R2 0.84736, RMSE 0.73576                     | Test: Loss 0.69748, R2 0.80940, RMSE 0.83090\n",
      "Epoch [162/500]      | Train: Loss 0.55344, R2 0.84201, RMSE 0.74215                     | Test: Loss 0.76050, R2 0.79747, RMSE 0.86683\n",
      "Epoch [163/500]      | Train: Loss 0.54359, R2 0.84855, RMSE 0.73511                     | Test: Loss 0.70941, R2 0.80821, RMSE 0.83584\n",
      "Epoch [164/500]      | Train: Loss 0.56237, R2 0.84123, RMSE 0.74671                     | Test: Loss 1.16620, R2 0.75220, RMSE 0.96283\n",
      "Epoch [165/500]      | Train: Loss 0.55064, R2 0.84397, RMSE 0.73958                     | Test: Loss 0.69343, R2 0.80736, RMSE 0.82333\n",
      "Epoch [166/500]      | Train: Loss 0.53834, R2 0.84989, RMSE 0.73142                     | Test: Loss 0.77532, R2 0.79525, RMSE 0.86706\n",
      "Epoch [167/500]      | Train: Loss 0.53977, R2 0.84786, RMSE 0.73187                     | Test: Loss 0.67576, R2 0.81070, RMSE 0.81485\n",
      "Epoch [168/500]      | Train: Loss 0.53876, R2 0.84834, RMSE 0.73165                     | Test: Loss 0.68944, R2 0.79411, RMSE 0.82461\n",
      "Epoch [169/500]      | Train: Loss 0.52827, R2 0.85171, RMSE 0.72406                     | Test: Loss 0.64270, R2 0.82484, RMSE 0.79333\n",
      "Epoch [170/500]      | Train: Loss 0.53020, R2 0.85031, RMSE 0.72606                     | Test: Loss 0.66739, R2 0.81960, RMSE 0.81168\n",
      "Epoch [171/500]      | Train: Loss 0.55319, R2 0.84391, RMSE 0.74130                     | Test: Loss 0.64132, R2 0.81781, RMSE 0.79062\n",
      "Epoch [172/500]      | Train: Loss 0.53989, R2 0.84814, RMSE 0.73163                     | Test: Loss 0.68666, R2 0.79715, RMSE 0.82048\n",
      "Epoch [173/500]      | Train: Loss 0.52779, R2 0.85216, RMSE 0.72462                     | Test: Loss 0.67678, R2 0.81315, RMSE 0.81440\n",
      "Epoch [174/500]      | Train: Loss 0.52766, R2 0.85133, RMSE 0.72468                     | Test: Loss 0.65145, R2 0.81765, RMSE 0.79499\n",
      "Epoch [175/500]      | Train: Loss 0.52932, R2 0.84930, RMSE 0.72565                     | Test: Loss 0.69317, R2 0.80148, RMSE 0.82198\n",
      "Epoch [176/500]      | Train: Loss 0.51643, R2 0.85548, RMSE 0.71669                     | Test: Loss 0.68492, R2 0.81215, RMSE 0.81765\n",
      "Epoch [177/500]      | Train: Loss 0.52443, R2 0.85314, RMSE 0.72155                     | Test: Loss 0.69440, R2 0.81418, RMSE 0.82911\n",
      "Epoch [178/500]      | Train: Loss 0.51400, R2 0.85458, RMSE 0.71484                     | Test: Loss 0.67997, R2 0.80515, RMSE 0.81865\n",
      "Epoch [179/500]      | Train: Loss 0.50348, R2 0.85909, RMSE 0.70697                     | Test: Loss 0.69743, R2 0.79529, RMSE 0.83246\n",
      "Epoch [180/500]      | Train: Loss 0.50838, R2 0.85626, RMSE 0.71149                     | Test: Loss 0.67522, R2 0.80947, RMSE 0.81121\n",
      "Epoch [181/500]      | Train: Loss 0.51176, R2 0.85570, RMSE 0.71329                     | Test: Loss 0.65459, R2 0.80702, RMSE 0.79857\n",
      "Epoch [182/500]      | Train: Loss 0.51426, R2 0.85338, RMSE 0.71478                     | Test: Loss 0.74895, R2 0.79927, RMSE 0.85832\n",
      "Epoch [183/500]      | Train: Loss 0.52438, R2 0.85396, RMSE 0.72136                     | Test: Loss 0.68450, R2 0.81844, RMSE 0.81769\n",
      "Epoch [184/500]      | Train: Loss 0.50204, R2 0.85821, RMSE 0.70685                     | Test: Loss 0.67898, R2 0.81075, RMSE 0.81728\n",
      "Epoch [185/500]      | Train: Loss 0.50575, R2 0.85782, RMSE 0.70898                     | Test: Loss 0.64274, R2 0.82329, RMSE 0.78899\n",
      "Epoch [186/500]      | Train: Loss 0.49816, R2 0.85904, RMSE 0.70359                     | Test: Loss 0.71986, R2 0.81825, RMSE 0.84053\n",
      "Epoch [187/500]      | Train: Loss 0.50062, R2 0.85974, RMSE 0.70572                     | Test: Loss 0.66211, R2 0.81672, RMSE 0.80176\n",
      "Epoch [188/500]      | Train: Loss 0.50083, R2 0.85951, RMSE 0.70531                     | Test: Loss 0.67695, R2 0.81292, RMSE 0.81247\n",
      "Epoch [189/500]      | Train: Loss 0.48588, R2 0.86467, RMSE 0.69472                     | Test: Loss 0.71564, R2 0.81302, RMSE 0.83784\n",
      "Epoch [190/500]      | Train: Loss 0.50800, R2 0.85743, RMSE 0.71089                     | Test: Loss 0.65672, R2 0.81891, RMSE 0.80208\n",
      "Epoch [191/500]      | Train: Loss 0.50092, R2 0.85846, RMSE 0.70568                     | Test: Loss 0.71320, R2 0.80809, RMSE 0.83959\n",
      "Epoch [192/500]      | Train: Loss 0.49947, R2 0.86026, RMSE 0.70474                     | Test: Loss 0.68372, R2 0.81091, RMSE 0.82044\n",
      "Epoch [193/500]      | Train: Loss 0.48807, R2 0.86206, RMSE 0.69600                     | Test: Loss 0.69071, R2 0.81388, RMSE 0.82543\n",
      "Epoch [194/500]      | Train: Loss 0.48750, R2 0.86280, RMSE 0.69652                     | Test: Loss 0.68641, R2 0.81458, RMSE 0.82402\n",
      "Epoch [195/500]      | Train: Loss 0.49179, R2 0.86187, RMSE 0.69893                     | Test: Loss 0.70113, R2 0.81702, RMSE 0.83060\n",
      "Epoch [196/500]      | Train: Loss 0.48716, R2 0.86274, RMSE 0.69585                     | Test: Loss 0.69071, R2 0.80482, RMSE 0.82508\n",
      "Epoch [197/500]      | Train: Loss 0.48435, R2 0.86295, RMSE 0.69381                     | Test: Loss 0.65320, R2 0.82354, RMSE 0.79525\n",
      "Epoch [198/500]      | Train: Loss 0.47106, R2 0.86591, RMSE 0.68456                     | Test: Loss 0.69322, R2 0.80521, RMSE 0.82821\n",
      "Epoch [199/500]      | Train: Loss 0.48514, R2 0.86351, RMSE 0.69424                     | Test: Loss 0.68774, R2 0.81453, RMSE 0.81325\n",
      "Epoch [200/500]      | Train: Loss 0.48588, R2 0.86296, RMSE 0.69484                     | Test: Loss 0.66637, R2 0.82131, RMSE 0.80399\n",
      "Epoch [201/500]      | Train: Loss 0.47100, R2 0.86754, RMSE 0.68340                     | Test: Loss 0.70136, R2 0.79733, RMSE 0.83174\n",
      "Epoch [202/500]      | Train: Loss 0.46979, R2 0.86783, RMSE 0.68344                     | Test: Loss 0.70435, R2 0.80733, RMSE 0.83219\n",
      "Epoch [203/500]      | Train: Loss 0.46776, R2 0.86824, RMSE 0.68183                     | Test: Loss 0.66430, R2 0.82268, RMSE 0.80307\n",
      "Epoch [204/500]      | Train: Loss 0.46370, R2 0.86981, RMSE 0.67888                     | Test: Loss 0.68351, R2 0.79184, RMSE 0.82253\n",
      "Epoch [205/500]      | Train: Loss 0.46851, R2 0.86773, RMSE 0.68246                     | Test: Loss 0.67333, R2 0.80534, RMSE 0.81354\n",
      "Epoch [206/500]      | Train: Loss 0.45932, R2 0.87007, RMSE 0.67602                     | Test: Loss 0.66877, R2 0.80043, RMSE 0.81166\n",
      "Epoch [207/500]      | Train: Loss 0.46316, R2 0.86877, RMSE 0.67939                     | Test: Loss 0.69698, R2 0.80958, RMSE 0.82591\n",
      "Epoch [208/500]      | Train: Loss 0.47456, R2 0.86543, RMSE 0.68710                     | Test: Loss 0.64784, R2 0.82393, RMSE 0.79222\n",
      "Epoch [209/500]      | Train: Loss 0.45121, R2 0.87224, RMSE 0.66991                     | Test: Loss 0.67364, R2 0.80928, RMSE 0.81317\n",
      "Epoch [210/500]      | Train: Loss 0.46317, R2 0.86934, RMSE 0.67908                     | Test: Loss 0.66546, R2 0.81525, RMSE 0.80930\n",
      "Epoch [211/500]      | Train: Loss 0.47087, R2 0.86802, RMSE 0.68443                     | Test: Loss 0.70465, R2 0.81846, RMSE 0.82881\n",
      "Epoch [212/500]      | Train: Loss 0.46560, R2 0.86913, RMSE 0.68064                     | Test: Loss 0.66329, R2 0.81488, RMSE 0.80711\n",
      "Epoch [213/500]      | Train: Loss 0.45468, R2 0.87131, RMSE 0.67216                     | Test: Loss 0.65422, R2 0.82625, RMSE 0.79395\n",
      "Epoch [214/500]      | Train: Loss 0.44805, R2 0.87345, RMSE 0.66764                     | Test: Loss 0.64359, R2 0.81768, RMSE 0.78874\n",
      "Epoch [215/500]      | Train: Loss 0.44563, R2 0.87379, RMSE 0.66608                     | Test: Loss 0.67210, R2 0.81472, RMSE 0.81297\n",
      "Epoch [216/500]      | Train: Loss 0.44356, R2 0.87406, RMSE 0.66439                     | Test: Loss 0.66866, R2 0.82016, RMSE 0.81097\n",
      "Epoch [217/500]      | Train: Loss 0.45013, R2 0.87220, RMSE 0.66914                     | Test: Loss 0.79230, R2 0.78121, RMSE 0.88287\n",
      "Epoch [218/500]      | Train: Loss 0.46444, R2 0.86910, RMSE 0.67919                     | Test: Loss 0.70013, R2 0.81268, RMSE 0.82867\n",
      "Epoch [219/500]      | Train: Loss 0.46414, R2 0.86884, RMSE 0.67943                     | Test: Loss 0.73216, R2 0.80006, RMSE 0.84966\n",
      "Epoch [220/500]      | Train: Loss 0.45000, R2 0.87346, RMSE 0.66929                     | Test: Loss 0.74105, R2 0.80790, RMSE 0.84960\n",
      "Epoch [221/500]      | Train: Loss 0.43768, R2 0.87555, RMSE 0.65946                     | Test: Loss 0.69957, R2 0.81054, RMSE 0.82682\n",
      "Epoch [222/500]      | Train: Loss 0.43071, R2 0.87793, RMSE 0.65507                     | Test: Loss 0.67468, R2 0.80839, RMSE 0.81456\n",
      "Epoch [223/500]      | Train: Loss 0.44510, R2 0.87348, RMSE 0.66569                     | Test: Loss 0.78683, R2 0.78169, RMSE 0.86839\n",
      "Epoch [224/500]      | Train: Loss 0.43462, R2 0.87593, RMSE 0.65746                     | Test: Loss 0.71986, R2 0.80715, RMSE 0.84123\n",
      "Epoch [225/500]      | Train: Loss 0.42717, R2 0.87974, RMSE 0.65110                     | Test: Loss 1.08934, R2 0.75488, RMSE 0.94010\n",
      "Epoch [226/500]      | Train: Loss 0.43882, R2 0.87619, RMSE 0.66098                     | Test: Loss 0.67733, R2 0.81139, RMSE 0.81363\n",
      "Epoch [227/500]      | Train: Loss 0.43128, R2 0.87793, RMSE 0.65500                     | Test: Loss 0.67489, R2 0.81650, RMSE 0.80879\n",
      "Epoch [228/500]      | Train: Loss 0.43223, R2 0.87767, RMSE 0.65574                     | Test: Loss 0.71889, R2 0.79379, RMSE 0.84351\n",
      "Epoch [229/500]      | Train: Loss 0.42975, R2 0.87786, RMSE 0.65407                     | Test: Loss 0.69677, R2 0.81563, RMSE 0.82780\n",
      "Epoch [230/500]      | Train: Loss 0.43739, R2 0.87684, RMSE 0.65947                     | Test: Loss 0.67170, R2 0.81176, RMSE 0.81295\n",
      "Epoch [231/500]      | Train: Loss 0.43484, R2 0.87763, RMSE 0.65752                     | Test: Loss 0.67173, R2 0.81109, RMSE 0.80275\n",
      "Epoch [232/500]      | Train: Loss 0.42430, R2 0.88110, RMSE 0.64968                     | Test: Loss 0.70741, R2 0.80335, RMSE 0.83796\n",
      "Epoch [233/500]      | Train: Loss 0.41799, R2 0.88150, RMSE 0.64547                     | Test: Loss 0.68449, R2 0.81292, RMSE 0.81811\n",
      "Epoch [234/500]      | Train: Loss 0.42984, R2 0.87889, RMSE 0.65407                     | Test: Loss 0.71676, R2 0.81737, RMSE 0.83909\n",
      "Epoch [235/500]      | Train: Loss 0.40636, R2 0.88604, RMSE 0.63579                     | Test: Loss 0.70963, R2 0.81086, RMSE 0.83499\n",
      "Epoch [236/500]      | Train: Loss 0.41945, R2 0.88252, RMSE 0.64592                     | Test: Loss 0.69768, R2 0.80597, RMSE 0.83187\n",
      "Epoch [237/500]      | Train: Loss 0.41439, R2 0.88256, RMSE 0.64219                     | Test: Loss 0.69580, R2 0.80402, RMSE 0.82949\n",
      "Epoch [238/500]      | Train: Loss 0.41688, R2 0.88311, RMSE 0.64435                     | Test: Loss 0.69678, R2 0.80424, RMSE 0.82771\n",
      "Epoch [239/500]      | Train: Loss 0.41379, R2 0.88399, RMSE 0.64141                     | Test: Loss 0.68442, R2 0.81220, RMSE 0.81378\n",
      "Epoch [240/500]      | Train: Loss 0.41427, R2 0.88280, RMSE 0.64209                     | Test: Loss 0.74835, R2 0.81023, RMSE 0.85444\n",
      "Epoch [241/500]      | Train: Loss 0.41006, R2 0.88378, RMSE 0.63861                     | Test: Loss 0.73401, R2 0.81086, RMSE 0.84646\n",
      "Epoch [242/500]      | Train: Loss 0.42338, R2 0.88006, RMSE 0.64897                     | Test: Loss 0.67350, R2 0.80755, RMSE 0.81637\n",
      "Epoch [243/500]      | Train: Loss 0.40797, R2 0.88518, RMSE 0.63762                     | Test: Loss 0.65865, R2 0.82018, RMSE 0.79713\n",
      "Epoch [244/500]      | Train: Loss 0.40769, R2 0.88568, RMSE 0.63722                     | Test: Loss 0.66946, R2 0.81333, RMSE 0.80740\n",
      "Epoch [245/500]      | Train: Loss 0.40903, R2 0.88400, RMSE 0.63844                     | Test: Loss 0.72844, R2 0.80138, RMSE 0.84569\n",
      "Epoch [246/500]      | Train: Loss 0.39276, R2 0.88984, RMSE 0.62516                     | Test: Loss 0.68151, R2 0.80486, RMSE 0.81641\n",
      "Epoch [247/500]      | Train: Loss 0.41143, R2 0.88494, RMSE 0.63959                     | Test: Loss 0.67916, R2 0.81375, RMSE 0.81528\n",
      "Epoch [248/500]      | Train: Loss 0.41353, R2 0.88274, RMSE 0.64146                     | Test: Loss 0.68252, R2 0.81293, RMSE 0.81658\n",
      "Epoch [249/500]      | Train: Loss 0.40455, R2 0.88635, RMSE 0.63463                     | Test: Loss 0.67650, R2 0.81349, RMSE 0.81649\n",
      "Epoch [250/500]      | Train: Loss 0.39910, R2 0.88685, RMSE 0.63000                     | Test: Loss 0.69248, R2 0.80694, RMSE 0.82624\n",
      "Epoch [251/500]      | Train: Loss 0.41428, R2 0.88273, RMSE 0.64219                     | Test: Loss 0.75297, R2 0.80425, RMSE 0.85543\n",
      "Epoch [252/500]      | Train: Loss 0.39458, R2 0.88909, RMSE 0.62676                     | Test: Loss 0.69608, R2 0.81069, RMSE 0.82375\n",
      "Epoch [253/500]      | Train: Loss 0.39864, R2 0.88725, RMSE 0.62973                     | Test: Loss 0.65596, R2 0.82056, RMSE 0.79575\n",
      "Epoch [254/500]      | Train: Loss 0.39447, R2 0.88891, RMSE 0.62635                     | Test: Loss 1.13624, R2 0.76120, RMSE 0.95001\n",
      "Epoch [255/500]      | Train: Loss 0.39734, R2 0.88808, RMSE 0.62851                     | Test: Loss 0.78647, R2 0.77240, RMSE 0.86886\n",
      "Epoch [256/500]      | Train: Loss 0.39559, R2 0.88827, RMSE 0.62759                     | Test: Loss 0.67227, R2 0.80988, RMSE 0.81132\n",
      "Epoch [257/500]      | Train: Loss 0.39418, R2 0.88746, RMSE 0.62657                     | Test: Loss 0.71399, R2 0.80071, RMSE 0.83810\n",
      "Epoch [258/500]      | Train: Loss 0.40419, R2 0.88429, RMSE 0.63375                     | Test: Loss 0.73085, R2 0.79469, RMSE 0.85024\n",
      "Epoch [259/500]      | Train: Loss 0.38747, R2 0.89034, RMSE 0.62083                     | Test: Loss 0.68228, R2 0.81328, RMSE 0.81578\n",
      "Epoch [260/500]      | Train: Loss 0.38398, R2 0.89179, RMSE 0.61784                     | Test: Loss 0.73212, R2 0.79726, RMSE 0.84731\n",
      "Epoch [261/500]      | Train: Loss 0.38983, R2 0.89000, RMSE 0.62271                     | Test: Loss 0.72811, R2 0.77992, RMSE 0.84389\n",
      "Epoch [262/500]      | Train: Loss 0.37770, R2 0.89278, RMSE 0.61324                     | Test: Loss 0.70969, R2 0.79693, RMSE 0.83749\n",
      "Epoch [263/500]      | Train: Loss 0.38187, R2 0.89245, RMSE 0.61624                     | Test: Loss 0.74329, R2 0.79252, RMSE 0.85277\n",
      "Epoch [264/500]      | Train: Loss 0.38360, R2 0.89184, RMSE 0.61802                     | Test: Loss 0.67583, R2 0.80932, RMSE 0.81050\n",
      "Epoch [265/500]      | Train: Loss 0.37807, R2 0.89344, RMSE 0.61365                     | Test: Loss 0.70455, R2 0.81338, RMSE 0.83459\n",
      "Epoch [266/500]      | Train: Loss 0.37825, R2 0.89427, RMSE 0.61355                     | Test: Loss 0.67842, R2 0.81000, RMSE 0.81453\n",
      "Epoch [267/500]      | Train: Loss 0.38368, R2 0.89197, RMSE 0.61772                     | Test: Loss 0.68968, R2 0.79884, RMSE 0.82288\n",
      "Epoch [268/500]      | Train: Loss 0.37680, R2 0.89272, RMSE 0.61220                     | Test: Loss 0.68968, R2 0.79532, RMSE 0.82175\n",
      "Epoch [269/500]      | Train: Loss 0.38092, R2 0.89272, RMSE 0.61504                     | Test: Loss 0.71803, R2 0.80615, RMSE 0.84192\n",
      "Epoch [270/500]      | Train: Loss 0.38138, R2 0.89174, RMSE 0.61570                     | Test: Loss 0.69177, R2 0.80092, RMSE 0.82008\n",
      "Epoch [271/500]      | Train: Loss 0.37304, R2 0.89370, RMSE 0.60946                     | Test: Loss 0.70334, R2 0.80281, RMSE 0.83212\n",
      "Epoch [272/500]      | Train: Loss 0.36552, R2 0.89517, RMSE 0.60336                     | Test: Loss 0.70251, R2 0.80305, RMSE 0.83108\n",
      "Epoch [273/500]      | Train: Loss 0.37715, R2 0.89290, RMSE 0.61286                     | Test: Loss 0.70390, R2 0.80391, RMSE 0.83010\n",
      "Epoch [274/500]      | Train: Loss 0.38100, R2 0.89256, RMSE 0.61592                     | Test: Loss 0.73741, R2 0.80062, RMSE 0.84773\n",
      "Epoch [275/500]      | Train: Loss 0.38487, R2 0.89189, RMSE 0.61826                     | Test: Loss 0.73655, R2 0.77861, RMSE 0.85002\n",
      "Epoch [276/500]      | Train: Loss 0.36750, R2 0.89647, RMSE 0.60492                     | Test: Loss 0.74073, R2 0.79513, RMSE 0.85421\n",
      "Epoch [277/500]      | Train: Loss 0.35632, R2 0.89930, RMSE 0.59577                     | Test: Loss 0.68418, R2 0.80707, RMSE 0.81892\n",
      "Epoch [278/500]      | Train: Loss 0.36472, R2 0.89638, RMSE 0.60238                     | Test: Loss 0.69985, R2 0.80985, RMSE 0.82336\n",
      "Epoch [279/500]      | Train: Loss 0.36305, R2 0.89613, RMSE 0.60106                     | Test: Loss 0.70065, R2 0.79792, RMSE 0.83069\n",
      "Epoch [280/500]      | Train: Loss 0.36336, R2 0.89800, RMSE 0.60146                     | Test: Loss 0.70539, R2 0.80395, RMSE 0.83460\n",
      "Epoch [281/500]      | Train: Loss 0.35624, R2 0.89959, RMSE 0.59562                     | Test: Loss 0.70629, R2 0.80676, RMSE 0.83470\n",
      "Epoch [282/500]      | Train: Loss 0.35925, R2 0.89833, RMSE 0.59829                     | Test: Loss 0.69238, R2 0.81262, RMSE 0.82120\n",
      "Epoch [283/500]      | Train: Loss 0.35989, R2 0.89899, RMSE 0.59850                     | Test: Loss 0.67994, R2 0.81406, RMSE 0.81540\n",
      "Epoch [284/500]      | Train: Loss 0.35233, R2 0.90045, RMSE 0.59249                     | Test: Loss 0.73077, R2 0.79463, RMSE 0.84419\n",
      "Epoch [285/500]      | Train: Loss 0.35610, R2 0.90018, RMSE 0.59509                     | Test: Loss 0.74884, R2 0.78479, RMSE 0.85617\n",
      "Epoch [286/500]      | Train: Loss 0.35917, R2 0.89841, RMSE 0.59814                     | Test: Loss 0.75515, R2 0.79575, RMSE 0.86442\n",
      "Epoch [287/500]      | Train: Loss 0.35338, R2 0.89973, RMSE 0.59324                     | Test: Loss 0.85326, R2 0.78064, RMSE 0.89356\n",
      "Epoch [288/500]      | Train: Loss 0.35635, R2 0.89925, RMSE 0.59574                     | Test: Loss 0.75331, R2 0.80611, RMSE 0.85635\n",
      "Epoch [289/500]      | Train: Loss 0.36075, R2 0.89791, RMSE 0.59907                     | Test: Loss 0.73449, R2 0.79539, RMSE 0.84950\n",
      "Epoch [290/500]      | Train: Loss 0.34483, R2 0.90274, RMSE 0.58592                     | Test: Loss 0.74555, R2 0.80548, RMSE 0.85471\n",
      "Epoch [291/500]      | Train: Loss 0.34576, R2 0.90252, RMSE 0.58675                     | Test: Loss 0.79405, R2 0.79047, RMSE 0.87521\n",
      "Epoch [292/500]      | Train: Loss 0.35663, R2 0.89942, RMSE 0.59597                     | Test: Loss 0.73762, R2 0.80162, RMSE 0.84837\n",
      "Epoch [293/500]      | Train: Loss 0.35292, R2 0.90121, RMSE 0.59280                     | Test: Loss 0.71916, R2 0.80179, RMSE 0.84010\n",
      "Epoch [294/500]      | Train: Loss 0.34703, R2 0.90187, RMSE 0.58809                     | Test: Loss 0.68570, R2 0.80213, RMSE 0.81873\n",
      "Epoch [295/500]      | Train: Loss 0.34223, R2 0.90400, RMSE 0.58359                     | Test: Loss 0.79109, R2 0.79064, RMSE 0.87661\n",
      "Epoch [296/500]      | Train: Loss 0.34553, R2 0.90281, RMSE 0.58673                     | Test: Loss 0.69818, R2 0.81604, RMSE 0.82779\n",
      "Epoch [297/500]      | Train: Loss 0.34018, R2 0.90390, RMSE 0.58243                     | Test: Loss 0.73026, R2 0.79690, RMSE 0.84220\n",
      "Epoch [298/500]      | Train: Loss 0.34915, R2 0.90201, RMSE 0.58912                     | Test: Loss 0.73424, R2 0.80280, RMSE 0.84837\n",
      "Epoch [299/500]      | Train: Loss 0.33602, R2 0.90513, RMSE 0.57863                     | Test: Loss 0.72194, R2 0.80015, RMSE 0.84153\n",
      "Epoch [300/500]      | Train: Loss 0.33546, R2 0.90484, RMSE 0.57810                     | Test: Loss 0.87355, R2 0.79199, RMSE 0.90008\n",
      "Epoch [301/500]      | Train: Loss 0.34215, R2 0.90374, RMSE 0.58350                     | Test: Loss 0.71099, R2 0.80765, RMSE 0.83525\n",
      "Epoch [302/500]      | Train: Loss 0.34558, R2 0.90118, RMSE 0.58665                     | Test: Loss 0.75258, R2 0.79929, RMSE 0.85901\n",
      "Epoch [303/500]      | Train: Loss 0.32689, R2 0.90742, RMSE 0.57081                     | Test: Loss 0.70946, R2 0.79630, RMSE 0.83678\n",
      "Epoch [304/500]      | Train: Loss 0.34645, R2 0.90206, RMSE 0.58711                     | Test: Loss 0.71811, R2 0.80746, RMSE 0.83704\n",
      "Epoch [305/500]      | Train: Loss 0.33700, R2 0.90457, RMSE 0.57936                     | Test: Loss 0.70890, R2 0.79473, RMSE 0.83447\n",
      "Epoch [306/500]      | Train: Loss 0.33491, R2 0.90524, RMSE 0.57775                     | Test: Loss 0.71136, R2 0.80639, RMSE 0.83951\n",
      "Epoch [307/500]      | Train: Loss 0.32831, R2 0.90639, RMSE 0.57206                     | Test: Loss 0.70241, R2 0.79888, RMSE 0.82663\n",
      "Epoch [308/500]      | Train: Loss 0.33322, R2 0.90631, RMSE 0.57626                     | Test: Loss 0.73091, R2 0.80936, RMSE 0.84607\n",
      "Epoch [309/500]      | Train: Loss 0.33322, R2 0.90601, RMSE 0.57598                     | Test: Loss 0.71888, R2 0.80785, RMSE 0.83941\n",
      "Epoch [310/500]      | Train: Loss 0.33088, R2 0.90659, RMSE 0.57413                     | Test: Loss 0.72549, R2 0.78375, RMSE 0.84535\n",
      "Epoch [311/500]      | Train: Loss 0.33388, R2 0.90581, RMSE 0.57664                     | Test: Loss 0.74272, R2 0.77970, RMSE 0.85666\n",
      "Epoch [312/500]      | Train: Loss 0.33327, R2 0.90496, RMSE 0.57604                     | Test: Loss 0.71836, R2 0.80074, RMSE 0.83742\n",
      "Epoch [313/500]      | Train: Loss 0.32706, R2 0.90767, RMSE 0.57078                     | Test: Loss 0.69736, R2 0.80123, RMSE 0.82160\n",
      "Epoch [314/500]      | Train: Loss 0.32660, R2 0.90743, RMSE 0.57047                     | Test: Loss 0.72494, R2 0.79223, RMSE 0.84378\n",
      "Epoch [315/500]      | Train: Loss 0.33025, R2 0.90714, RMSE 0.57358                     | Test: Loss 0.72487, R2 0.79184, RMSE 0.84388\n",
      "Epoch [316/500]      | Train: Loss 0.32104, R2 0.90846, RMSE 0.56524                     | Test: Loss 0.75167, R2 0.79361, RMSE 0.86081\n",
      "Epoch [317/500]      | Train: Loss 0.32828, R2 0.90683, RMSE 0.57185                     | Test: Loss 0.70926, R2 0.80599, RMSE 0.83231\n",
      "Epoch [318/500]      | Train: Loss 0.32740, R2 0.90713, RMSE 0.57042                     | Test: Loss 0.69386, R2 0.81006, RMSE 0.82373\n",
      "Epoch [319/500]      | Train: Loss 0.32635, R2 0.90808, RMSE 0.57017                     | Test: Loss 0.71653, R2 0.79891, RMSE 0.84022\n",
      "Epoch [320/500]      | Train: Loss 0.32151, R2 0.90853, RMSE 0.56636                     | Test: Loss 0.77472, R2 0.78960, RMSE 0.87263\n",
      "Epoch [321/500]      | Train: Loss 0.31909, R2 0.90962, RMSE 0.56357                     | Test: Loss 0.73586, R2 0.80147, RMSE 0.84826\n",
      "Epoch [322/500]      | Train: Loss 0.32117, R2 0.90997, RMSE 0.56522                     | Test: Loss 0.69949, R2 0.80967, RMSE 0.83096\n",
      "Epoch [323/500]      | Train: Loss 0.32611, R2 0.90816, RMSE 0.56966                     | Test: Loss 0.74272, R2 0.78831, RMSE 0.85492\n",
      "Epoch [324/500]      | Train: Loss 0.31404, R2 0.91105, RMSE 0.55918                     | Test: Loss 0.72680, R2 0.79869, RMSE 0.84659\n",
      "Epoch [325/500]      | Train: Loss 0.31221, R2 0.91183, RMSE 0.55764                     | Test: Loss 0.75616, R2 0.79574, RMSE 0.86410\n",
      "Epoch [326/500]      | Train: Loss 0.31251, R2 0.91227, RMSE 0.55809                     | Test: Loss 0.71003, R2 0.80811, RMSE 0.82750\n",
      "Epoch [327/500]      | Train: Loss 0.31707, R2 0.91054, RMSE 0.56198                     | Test: Loss 0.71161, R2 0.80629, RMSE 0.82587\n",
      "Epoch [328/500]      | Train: Loss 0.30866, R2 0.91258, RMSE 0.55426                     | Test: Loss 0.76057, R2 0.78248, RMSE 0.86191\n",
      "Epoch [329/500]      | Train: Loss 0.30377, R2 0.91455, RMSE 0.54973                     | Test: Loss 0.81007, R2 0.72493, RMSE 0.88057\n",
      "Epoch [330/500]      | Train: Loss 0.31058, R2 0.91262, RMSE 0.55605                     | Test: Loss 0.76914, R2 0.80691, RMSE 0.86391\n",
      "Epoch [331/500]      | Train: Loss 0.31329, R2 0.91214, RMSE 0.55837                     | Test: Loss 0.74814, R2 0.78892, RMSE 0.85940\n",
      "Epoch [332/500]      | Train: Loss 0.30775, R2 0.91317, RMSE 0.55393                     | Test: Loss 0.73506, R2 0.77815, RMSE 0.85128\n",
      "Epoch [333/500]      | Train: Loss 0.31059, R2 0.91134, RMSE 0.55596                     | Test: Loss 0.74155, R2 0.78768, RMSE 0.85770\n",
      "Epoch [334/500]      | Train: Loss 0.30622, R2 0.91335, RMSE 0.55244                     | Test: Loss 0.74473, R2 0.79544, RMSE 0.85649\n",
      "Epoch [335/500]      | Train: Loss 0.30668, R2 0.91385, RMSE 0.55231                     | Test: Loss 0.74533, R2 0.79274, RMSE 0.85513\n",
      "Epoch [336/500]      | Train: Loss 0.32117, R2 0.90972, RMSE 0.56540                     | Test: Loss 0.70601, R2 0.81189, RMSE 0.83199\n",
      "Epoch [337/500]      | Train: Loss 0.30479, R2 0.91408, RMSE 0.55112                     | Test: Loss 0.69991, R2 0.79928, RMSE 0.82632\n",
      "Epoch [338/500]      | Train: Loss 0.30038, R2 0.91626, RMSE 0.54666                     | Test: Loss 0.72468, R2 0.79932, RMSE 0.83898\n",
      "Epoch [339/500]      | Train: Loss 0.30175, R2 0.91395, RMSE 0.54827                     | Test: Loss 0.73935, R2 0.79189, RMSE 0.85323\n",
      "Epoch [340/500]      | Train: Loss 0.29902, R2 0.91533, RMSE 0.54556                     | Test: Loss 0.73247, R2 0.80168, RMSE 0.84746\n",
      "Epoch [341/500]      | Train: Loss 0.29925, R2 0.91541, RMSE 0.54628                     | Test: Loss 0.75415, R2 0.79576, RMSE 0.85841\n",
      "Epoch [342/500]      | Train: Loss 0.30007, R2 0.91522, RMSE 0.54648                     | Test: Loss 0.75633, R2 0.78947, RMSE 0.86373\n",
      "Epoch [343/500]      | Train: Loss 0.29109, R2 0.91766, RMSE 0.53838                     | Test: Loss 0.72947, R2 0.80254, RMSE 0.84952\n",
      "Epoch [344/500]      | Train: Loss 0.30467, R2 0.91381, RMSE 0.55104                     | Test: Loss 0.77116, R2 0.80034, RMSE 0.87072\n",
      "Epoch [345/500]      | Train: Loss 0.29923, R2 0.91509, RMSE 0.54618                     | Test: Loss 0.72133, R2 0.80224, RMSE 0.84399\n",
      "Epoch [346/500]      | Train: Loss 0.29941, R2 0.91569, RMSE 0.54581                     | Test: Loss 0.79180, R2 0.79706, RMSE 0.87711\n",
      "Epoch [347/500]      | Train: Loss 0.29774, R2 0.91509, RMSE 0.54448                     | Test: Loss 0.70180, R2 0.80540, RMSE 0.82502\n",
      "Epoch [348/500]      | Train: Loss 0.28926, R2 0.91800, RMSE 0.53688                     | Test: Loss 0.73559, R2 0.80128, RMSE 0.85264\n",
      "Epoch [349/500]      | Train: Loss 0.29787, R2 0.91582, RMSE 0.54465                     | Test: Loss 0.75842, R2 0.77418, RMSE 0.86195\n",
      "Epoch [350/500]      | Train: Loss 0.29746, R2 0.91542, RMSE 0.54451                     | Test: Loss 0.71679, R2 0.80405, RMSE 0.83743\n",
      "Epoch [351/500]      | Train: Loss 0.29126, R2 0.91696, RMSE 0.53848                     | Test: Loss 0.71450, R2 0.80784, RMSE 0.83022\n",
      "Epoch [352/500]      | Train: Loss 0.29458, R2 0.91735, RMSE 0.54169                     | Test: Loss 0.74673, R2 0.79422, RMSE 0.85636\n",
      "Epoch [353/500]      | Train: Loss 0.29881, R2 0.91598, RMSE 0.54528                     | Test: Loss 0.73080, R2 0.78380, RMSE 0.84889\n",
      "Epoch [354/500]      | Train: Loss 0.28221, R2 0.92030, RMSE 0.53048                     | Test: Loss 0.78610, R2 0.79637, RMSE 0.87885\n",
      "Epoch [355/500]      | Train: Loss 0.29255, R2 0.91748, RMSE 0.53989                     | Test: Loss 0.70440, R2 0.80465, RMSE 0.83117\n",
      "Epoch [356/500]      | Train: Loss 0.29231, R2 0.91807, RMSE 0.53973                     | Test: Loss 0.76020, R2 0.79747, RMSE 0.86624\n",
      "Epoch [357/500]      | Train: Loss 0.29591, R2 0.91643, RMSE 0.54277                     | Test: Loss 0.72899, R2 0.79670, RMSE 0.84520\n",
      "Epoch [358/500]      | Train: Loss 0.28644, R2 0.91888, RMSE 0.53439                     | Test: Loss 0.78492, R2 0.78040, RMSE 0.87773\n",
      "Epoch [359/500]      | Train: Loss 0.28077, R2 0.91981, RMSE 0.52878                     | Test: Loss 0.72145, R2 0.80461, RMSE 0.84188\n",
      "Epoch [360/500]      | Train: Loss 0.28112, R2 0.92084, RMSE 0.52937                     | Test: Loss 0.71391, R2 0.80172, RMSE 0.83543\n",
      "Epoch [361/500]      | Train: Loss 0.27706, R2 0.92200, RMSE 0.52538                     | Test: Loss 0.73723, R2 0.78338, RMSE 0.85221\n",
      "Epoch [362/500]      | Train: Loss 0.27944, R2 0.92082, RMSE 0.52720                     | Test: Loss 0.73131, R2 0.79578, RMSE 0.84640\n",
      "Epoch [363/500]      | Train: Loss 0.27840, R2 0.92190, RMSE 0.52684                     | Test: Loss 0.74803, R2 0.78568, RMSE 0.85679\n",
      "Epoch [364/500]      | Train: Loss 0.28162, R2 0.92025, RMSE 0.52952                     | Test: Loss 0.76944, R2 0.80304, RMSE 0.86718\n",
      "Epoch [365/500]      | Train: Loss 0.27855, R2 0.92133, RMSE 0.52691                     | Test: Loss 0.71712, R2 0.80431, RMSE 0.83796\n",
      "Epoch [366/500]      | Train: Loss 0.27579, R2 0.92152, RMSE 0.52426                     | Test: Loss 0.73310, R2 0.80257, RMSE 0.84869\n",
      "Epoch [367/500]      | Train: Loss 0.27871, R2 0.92125, RMSE 0.52691                     | Test: Loss 0.72807, R2 0.79631, RMSE 0.84014\n",
      "Epoch [368/500]      | Train: Loss 0.28319, R2 0.91970, RMSE 0.53108                     | Test: Loss 0.81470, R2 0.75719, RMSE 0.89273\n",
      "Epoch [369/500]      | Train: Loss 0.27213, R2 0.92331, RMSE 0.52047                     | Test: Loss 0.72320, R2 0.79828, RMSE 0.84230\n",
      "Epoch [370/500]      | Train: Loss 0.28753, R2 0.91955, RMSE 0.53501                     | Test: Loss 0.73696, R2 0.79340, RMSE 0.85019\n",
      "Epoch [371/500]      | Train: Loss 0.28127, R2 0.92012, RMSE 0.52941                     | Test: Loss 0.76191, R2 0.78344, RMSE 0.86401\n",
      "Epoch [372/500]      | Train: Loss 0.28004, R2 0.92120, RMSE 0.52809                     | Test: Loss 0.73869, R2 0.79849, RMSE 0.85261\n",
      "Epoch [373/500]      | Train: Loss 0.26997, R2 0.92366, RMSE 0.51889                     | Test: Loss 0.70974, R2 0.80620, RMSE 0.83361\n",
      "Epoch [374/500]      | Train: Loss 0.28002, R2 0.92145, RMSE 0.52821                     | Test: Loss 0.73781, R2 0.78198, RMSE 0.85302\n",
      "Epoch [375/500]      | Train: Loss 0.27940, R2 0.92059, RMSE 0.52769                     | Test: Loss 0.73844, R2 0.78916, RMSE 0.85524\n",
      "Epoch [376/500]      | Train: Loss 0.27558, R2 0.92218, RMSE 0.52392                     | Test: Loss 0.72695, R2 0.79699, RMSE 0.84494\n",
      "Epoch [377/500]      | Train: Loss 0.26702, R2 0.92480, RMSE 0.51572                     | Test: Loss 0.72742, R2 0.79252, RMSE 0.84089\n",
      "Epoch [378/500]      | Train: Loss 0.26697, R2 0.92481, RMSE 0.51591                     | Test: Loss 0.76714, R2 0.78781, RMSE 0.86869\n",
      "Epoch [379/500]      | Train: Loss 0.27513, R2 0.92246, RMSE 0.52354                     | Test: Loss 0.76321, R2 0.78361, RMSE 0.86629\n",
      "Epoch [380/500]      | Train: Loss 0.27146, R2 0.92260, RMSE 0.52002                     | Test: Loss 0.74790, R2 0.79024, RMSE 0.85889\n",
      "Epoch [381/500]      | Train: Loss 0.25696, R2 0.92730, RMSE 0.50570                     | Test: Loss 0.71415, R2 0.80930, RMSE 0.83668\n",
      "Epoch [382/500]      | Train: Loss 0.25879, R2 0.92710, RMSE 0.50768                     | Test: Loss 0.72767, R2 0.80220, RMSE 0.84708\n",
      "Epoch [383/500]      | Train: Loss 0.26851, R2 0.92362, RMSE 0.51697                     | Test: Loss 0.75135, R2 0.79722, RMSE 0.86223\n",
      "Epoch [384/500]      | Train: Loss 0.26496, R2 0.92551, RMSE 0.51383                     | Test: Loss 0.82597, R2 0.78213, RMSE 0.90334\n",
      "Epoch [385/500]      | Train: Loss 0.26722, R2 0.92365, RMSE 0.51594                     | Test: Loss 0.75073, R2 0.79488, RMSE 0.85824\n",
      "Epoch [386/500]      | Train: Loss 0.26564, R2 0.92455, RMSE 0.51389                     | Test: Loss 0.75290, R2 0.79036, RMSE 0.86212\n",
      "Epoch [387/500]      | Train: Loss 0.26280, R2 0.92507, RMSE 0.51177                     | Test: Loss 1.24177, R2 0.75025, RMSE 0.99805\n",
      "Epoch [388/500]      | Train: Loss 0.26930, R2 0.92407, RMSE 0.51764                     | Test: Loss 0.72592, R2 0.80255, RMSE 0.84537\n",
      "Epoch [389/500]      | Train: Loss 0.26597, R2 0.92461, RMSE 0.51452                     | Test: Loss 0.72536, R2 0.79198, RMSE 0.84405\n",
      "Epoch [390/500]      | Train: Loss 0.25962, R2 0.92656, RMSE 0.50858                     | Test: Loss 0.72964, R2 0.78942, RMSE 0.84850\n",
      "Epoch [391/500]      | Train: Loss 0.25468, R2 0.92813, RMSE 0.50345                     | Test: Loss 0.74421, R2 0.79748, RMSE 0.84819\n",
      "Epoch [392/500]      | Train: Loss 0.26526, R2 0.92442, RMSE 0.51421                     | Test: Loss 0.76111, R2 0.78370, RMSE 0.86834\n",
      "Epoch [393/500]      | Train: Loss 0.26325, R2 0.92488, RMSE 0.51221                     | Test: Loss 0.74980, R2 0.79416, RMSE 0.85929\n",
      "Epoch [394/500]      | Train: Loss 0.26431, R2 0.92612, RMSE 0.51289                     | Test: Loss 0.75022, R2 0.80419, RMSE 0.86046\n",
      "Epoch [395/500]      | Train: Loss 0.26148, R2 0.92597, RMSE 0.51072                     | Test: Loss 0.78022, R2 0.79001, RMSE 0.87332\n",
      "Epoch [396/500]      | Train: Loss 0.25584, R2 0.92772, RMSE 0.50498                     | Test: Loss 0.72449, R2 0.80424, RMSE 0.83934\n",
      "Epoch [397/500]      | Train: Loss 0.25714, R2 0.92743, RMSE 0.50598                     | Test: Loss 0.79842, R2 0.79253, RMSE 0.88410\n",
      "Epoch [398/500]      | Train: Loss 0.25691, R2 0.92700, RMSE 0.50601                     | Test: Loss 0.79690, R2 0.79611, RMSE 0.88367\n",
      "Epoch [399/500]      | Train: Loss 0.25365, R2 0.92768, RMSE 0.50261                     | Test: Loss 0.74775, R2 0.79212, RMSE 0.85785\n",
      "Epoch [400/500]      | Train: Loss 0.25295, R2 0.92860, RMSE 0.50210                     | Test: Loss 0.75035, R2 0.78046, RMSE 0.85970\n",
      "Epoch [401/500]      | Train: Loss 0.25071, R2 0.92942, RMSE 0.49958                     | Test: Loss 0.72036, R2 0.79697, RMSE 0.83917\n",
      "Epoch [402/500]      | Train: Loss 0.25204, R2 0.92912, RMSE 0.50104                     | Test: Loss 0.73627, R2 0.79668, RMSE 0.84929\n",
      "Epoch [403/500]      | Train: Loss 0.25114, R2 0.92944, RMSE 0.50029                     | Test: Loss 0.76490, R2 0.79276, RMSE 0.86969\n",
      "Epoch [404/500]      | Train: Loss 0.25270, R2 0.92895, RMSE 0.50182                     | Test: Loss 0.75394, R2 0.79849, RMSE 0.85479\n",
      "Epoch [405/500]      | Train: Loss 0.25602, R2 0.92695, RMSE 0.50508                     | Test: Loss 0.74478, R2 0.78877, RMSE 0.85715\n",
      "Epoch [406/500]      | Train: Loss 0.25105, R2 0.92833, RMSE 0.50016                     | Test: Loss 0.72244, R2 0.80310, RMSE 0.84212\n",
      "Epoch [407/500]      | Train: Loss 0.25212, R2 0.92843, RMSE 0.50115                     | Test: Loss 0.73493, R2 0.79879, RMSE 0.84563\n",
      "Epoch [408/500]      | Train: Loss 0.24936, R2 0.92952, RMSE 0.49865                     | Test: Loss 0.74830, R2 0.79247, RMSE 0.85840\n",
      "Epoch [409/500]      | Train: Loss 0.25235, R2 0.92863, RMSE 0.50131                     | Test: Loss 0.76994, R2 0.78372, RMSE 0.87082\n",
      "Epoch [410/500]      | Train: Loss 0.24347, R2 0.93160, RMSE 0.49266                     | Test: Loss 0.72854, R2 0.80087, RMSE 0.83985\n",
      "Epoch [411/500]      | Train: Loss 0.24284, R2 0.93172, RMSE 0.49168                     | Test: Loss 0.72712, R2 0.79951, RMSE 0.84326\n",
      "Epoch [412/500]      | Train: Loss 0.24818, R2 0.93056, RMSE 0.49699                     | Test: Loss 0.73336, R2 0.79740, RMSE 0.84327\n",
      "Epoch [413/500]      | Train: Loss 0.24584, R2 0.93027, RMSE 0.49499                     | Test: Loss 0.75059, R2 0.79289, RMSE 0.86049\n",
      "Epoch [414/500]      | Train: Loss 0.23976, R2 0.93179, RMSE 0.48878                     | Test: Loss 0.75034, R2 0.78319, RMSE 0.85687\n",
      "Epoch [415/500]      | Train: Loss 0.24978, R2 0.92979, RMSE 0.49893                     | Test: Loss 0.77956, R2 0.78142, RMSE 0.87869\n",
      "Epoch [416/500]      | Train: Loss 0.24167, R2 0.93204, RMSE 0.49065                     | Test: Loss 0.74459, R2 0.79826, RMSE 0.85742\n",
      "Epoch [417/500]      | Train: Loss 0.24614, R2 0.93043, RMSE 0.49492                     | Test: Loss 0.80234, R2 0.74582, RMSE 0.88837\n",
      "Epoch [418/500]      | Train: Loss 0.25104, R2 0.92897, RMSE 0.50004                     | Test: Loss 0.79291, R2 0.79078, RMSE 0.88293\n",
      "Epoch [419/500]      | Train: Loss 0.25440, R2 0.92862, RMSE 0.50261                     | Test: Loss 0.73637, R2 0.79754, RMSE 0.84133\n",
      "Epoch [420/500]      | Train: Loss 0.24261, R2 0.93157, RMSE 0.49147                     | Test: Loss 0.72648, R2 0.78791, RMSE 0.84091\n",
      "Epoch [421/500]      | Train: Loss 0.24967, R2 0.93012, RMSE 0.49834                     | Test: Loss 0.73731, R2 0.79722, RMSE 0.85031\n",
      "Epoch [422/500]      | Train: Loss 0.24232, R2 0.93150, RMSE 0.49120                     | Test: Loss 0.74962, R2 0.78462, RMSE 0.85441\n",
      "Epoch [423/500]      | Train: Loss 0.23754, R2 0.93189, RMSE 0.48650                     | Test: Loss 0.76998, R2 0.79535, RMSE 0.86987\n",
      "Epoch [424/500]      | Train: Loss 0.23580, R2 0.93249, RMSE 0.48496                     | Test: Loss 0.73303, R2 0.78695, RMSE 0.84996\n",
      "Epoch [425/500]      | Train: Loss 0.23763, R2 0.93276, RMSE 0.48663                     | Test: Loss 0.74536, R2 0.78880, RMSE 0.85264\n",
      "Epoch [426/500]      | Train: Loss 0.23974, R2 0.93193, RMSE 0.48870                     | Test: Loss 0.76989, R2 0.79623, RMSE 0.87168\n",
      "Epoch [427/500]      | Train: Loss 0.23375, R2 0.93374, RMSE 0.48271                     | Test: Loss 0.75902, R2 0.77613, RMSE 0.86671\n",
      "Epoch [428/500]      | Train: Loss 0.23783, R2 0.93242, RMSE 0.48679                     | Test: Loss 0.74798, R2 0.79084, RMSE 0.85754\n",
      "Epoch [429/500]      | Train: Loss 0.23552, R2 0.93371, RMSE 0.48447                     | Test: Loss 0.79746, R2 0.78145, RMSE 0.88433\n",
      "Epoch [430/500]      | Train: Loss 0.23992, R2 0.93193, RMSE 0.48873                     | Test: Loss 0.74178, R2 0.80095, RMSE 0.85019\n",
      "Epoch [431/500]      | Train: Loss 0.23450, R2 0.93327, RMSE 0.48334                     | Test: Loss 0.80231, R2 0.78251, RMSE 0.88915\n",
      "Epoch [432/500]      | Train: Loss 0.23704, R2 0.93323, RMSE 0.48616                     | Test: Loss 0.73737, R2 0.79351, RMSE 0.85225\n",
      "Epoch [433/500]      | Train: Loss 0.23244, R2 0.93402, RMSE 0.48125                     | Test: Loss 0.78267, R2 0.79629, RMSE 0.87267\n",
      "Epoch [434/500]      | Train: Loss 0.23063, R2 0.93485, RMSE 0.47911                     | Test: Loss 0.72592, R2 0.79053, RMSE 0.84284\n",
      "Epoch [435/500]      | Train: Loss 0.23460, R2 0.93386, RMSE 0.48368                     | Test: Loss 0.74146, R2 0.80041, RMSE 0.84768\n",
      "Epoch [436/500]      | Train: Loss 0.24019, R2 0.93224, RMSE 0.48892                     | Test: Loss 0.74924, R2 0.78998, RMSE 0.85824\n",
      "Epoch [437/500]      | Train: Loss 0.22982, R2 0.93522, RMSE 0.47861                     | Test: Loss 0.73015, R2 0.79708, RMSE 0.84478\n",
      "Epoch [438/500]      | Train: Loss 0.22944, R2 0.93527, RMSE 0.47777                     | Test: Loss 0.78296, R2 0.76235, RMSE 0.87739\n",
      "Epoch [439/500]      | Train: Loss 0.23261, R2 0.93457, RMSE 0.48104                     | Test: Loss 0.83023, R2 0.78043, RMSE 0.90219\n",
      "Epoch [440/500]      | Train: Loss 0.22438, R2 0.93662, RMSE 0.47271                     | Test: Loss 0.72972, R2 0.79095, RMSE 0.84253\n",
      "Epoch [441/500]      | Train: Loss 0.22437, R2 0.93643, RMSE 0.47279                     | Test: Loss 0.78192, R2 0.79688, RMSE 0.87633\n",
      "Epoch [442/500]      | Train: Loss 0.23683, R2 0.93308, RMSE 0.48560                     | Test: Loss 0.78002, R2 0.79283, RMSE 0.87638\n",
      "Epoch [443/500]      | Train: Loss 0.23326, R2 0.93466, RMSE 0.48184                     | Test: Loss 0.73506, R2 0.79605, RMSE 0.84783\n",
      "Epoch [444/500]      | Train: Loss 0.22979, R2 0.93474, RMSE 0.47817                     | Test: Loss 0.74789, R2 0.79723, RMSE 0.85949\n",
      "Epoch [445/500]      | Train: Loss 0.22474, R2 0.93684, RMSE 0.47316                     | Test: Loss 0.73355, R2 0.78877, RMSE 0.84924\n",
      "Epoch [446/500]      | Train: Loss 0.21656, R2 0.93858, RMSE 0.46425                     | Test: Loss 0.76486, R2 0.78728, RMSE 0.86614\n",
      "Epoch [447/500]      | Train: Loss 0.21815, R2 0.93833, RMSE 0.46613                     | Test: Loss 0.74891, R2 0.78241, RMSE 0.85711\n",
      "Epoch [448/500]      | Train: Loss 0.22940, R2 0.93491, RMSE 0.47814                     | Test: Loss 0.76066, R2 0.78107, RMSE 0.86209\n",
      "Epoch [449/500]      | Train: Loss 0.22570, R2 0.93564, RMSE 0.47445                     | Test: Loss 0.77206, R2 0.78041, RMSE 0.87321\n",
      "Epoch [450/500]      | Train: Loss 0.23191, R2 0.93359, RMSE 0.48087                     | Test: Loss 0.74954, R2 0.79133, RMSE 0.85943\n",
      "Epoch [451/500]      | Train: Loss 0.22297, R2 0.93703, RMSE 0.47133                     | Test: Loss 0.77103, R2 0.77201, RMSE 0.87346\n",
      "Epoch [452/500]      | Train: Loss 0.22120, R2 0.93726, RMSE 0.46958                     | Test: Loss 0.74792, R2 0.78114, RMSE 0.85759\n",
      "Epoch [453/500]      | Train: Loss 0.21550, R2 0.93937, RMSE 0.46326                     | Test: Loss 0.85306, R2 0.78869, RMSE 0.90182\n",
      "Epoch [454/500]      | Train: Loss 0.22111, R2 0.93705, RMSE 0.46942                     | Test: Loss 0.76381, R2 0.78569, RMSE 0.86741\n",
      "Epoch [455/500]      | Train: Loss 0.22254, R2 0.93704, RMSE 0.47103                     | Test: Loss 0.76377, R2 0.79485, RMSE 0.86791\n",
      "Epoch [456/500]      | Train: Loss 0.21993, R2 0.93785, RMSE 0.46808                     | Test: Loss 0.74083, R2 0.78723, RMSE 0.85377\n",
      "Epoch [457/500]      | Train: Loss 0.21622, R2 0.93864, RMSE 0.46429                     | Test: Loss 0.75647, R2 0.78569, RMSE 0.85976\n",
      "Epoch [458/500]      | Train: Loss 0.21670, R2 0.93892, RMSE 0.46443                     | Test: Loss 0.73833, R2 0.79147, RMSE 0.85150\n",
      "Epoch [459/500]      | Train: Loss 0.21541, R2 0.93957, RMSE 0.46300                     | Test: Loss 0.72568, R2 0.80527, RMSE 0.83901\n",
      "Epoch [460/500]      | Train: Loss 0.22220, R2 0.93722, RMSE 0.47044                     | Test: Loss 0.72467, R2 0.79071, RMSE 0.83906\n",
      "Epoch [461/500]      | Train: Loss 0.21627, R2 0.93885, RMSE 0.46369                     | Test: Loss 0.75391, R2 0.79576, RMSE 0.86007\n",
      "Epoch [462/500]      | Train: Loss 0.21256, R2 0.93991, RMSE 0.45992                     | Test: Loss 0.85616, R2 0.73743, RMSE 0.90401\n",
      "Epoch [463/500]      | Train: Loss 0.21568, R2 0.93840, RMSE 0.46319                     | Test: Loss 0.75317, R2 0.78855, RMSE 0.85823\n",
      "Epoch [464/500]      | Train: Loss 0.21635, R2 0.93789, RMSE 0.46416                     | Test: Loss 0.76171, R2 0.77730, RMSE 0.86246\n",
      "Epoch [465/500]      | Train: Loss 0.21074, R2 0.94000, RMSE 0.45835                     | Test: Loss 0.75895, R2 0.78527, RMSE 0.86361\n",
      "Epoch [466/500]      | Train: Loss 0.21250, R2 0.93965, RMSE 0.46029                     | Test: Loss 0.74096, R2 0.79077, RMSE 0.85636\n",
      "Epoch [467/500]      | Train: Loss 0.21165, R2 0.94017, RMSE 0.45902                     | Test: Loss 0.73873, R2 0.79510, RMSE 0.84860\n",
      "Epoch [468/500]      | Train: Loss 0.20526, R2 0.94221, RMSE 0.45208                     | Test: Loss 0.86692, R2 0.78117, RMSE 0.91236\n",
      "Epoch [469/500]      | Train: Loss 0.20548, R2 0.94217, RMSE 0.45252                     | Test: Loss 0.74806, R2 0.78699, RMSE 0.85693\n",
      "Epoch [470/500]      | Train: Loss 0.20511, R2 0.94242, RMSE 0.45196                     | Test: Loss 0.72047, R2 0.80551, RMSE 0.82931\n",
      "Epoch [471/500]      | Train: Loss 0.20602, R2 0.94172, RMSE 0.45301                     | Test: Loss 0.75885, R2 0.79389, RMSE 0.86349\n",
      "Epoch [472/500]      | Train: Loss 0.20667, R2 0.94123, RMSE 0.45394                     | Test: Loss 0.77711, R2 0.78425, RMSE 0.87476\n",
      "Epoch [473/500]      | Train: Loss 0.21385, R2 0.93982, RMSE 0.46151                     | Test: Loss 0.73984, R2 0.79577, RMSE 0.84850\n",
      "Epoch [474/500]      | Train: Loss 0.20352, R2 0.94231, RMSE 0.45032                     | Test: Loss 0.78591, R2 0.77766, RMSE 0.87772\n",
      "Epoch [475/500]      | Train: Loss 0.21397, R2 0.93977, RMSE 0.46185                     | Test: Loss 0.75200, R2 0.79988, RMSE 0.85825\n",
      "Epoch [476/500]      | Train: Loss 0.20900, R2 0.94083, RMSE 0.45634                     | Test: Loss 0.76717, R2 0.79048, RMSE 0.86740\n",
      "Epoch [477/500]      | Train: Loss 0.20690, R2 0.94224, RMSE 0.45411                     | Test: Loss 0.82638, R2 0.74488, RMSE 0.89743\n",
      "Epoch [478/500]      | Train: Loss 0.20813, R2 0.94123, RMSE 0.45538                     | Test: Loss 0.79308, R2 0.77746, RMSE 0.88647\n",
      "Epoch [479/500]      | Train: Loss 0.20719, R2 0.94196, RMSE 0.45436                     | Test: Loss 0.74602, R2 0.78189, RMSE 0.85743\n",
      "Epoch [480/500]      | Train: Loss 0.20573, R2 0.94187, RMSE 0.45278                     | Test: Loss 0.75686, R2 0.79306, RMSE 0.85946\n",
      "Epoch [481/500]      | Train: Loss 0.20699, R2 0.94186, RMSE 0.45415                     | Test: Loss 0.77189, R2 0.75065, RMSE 0.87158\n",
      "Epoch [482/500]      | Train: Loss 0.20335, R2 0.94213, RMSE 0.45037                     | Test: Loss 0.75105, R2 0.79214, RMSE 0.85927\n",
      "Epoch [483/500]      | Train: Loss 0.20733, R2 0.94103, RMSE 0.45451                     | Test: Loss 0.74890, R2 0.78453, RMSE 0.85510\n",
      "Epoch [484/500]      | Train: Loss 0.21269, R2 0.94053, RMSE 0.46002                     | Test: Loss 0.77494, R2 0.78360, RMSE 0.87452\n",
      "Epoch [485/500]      | Train: Loss 0.20173, R2 0.94325, RMSE 0.44844                     | Test: Loss 0.75938, R2 0.79229, RMSE 0.86643\n",
      "Epoch [486/500]      | Train: Loss 0.20849, R2 0.94095, RMSE 0.45579                     | Test: Loss 0.83396, R2 0.76556, RMSE 0.90724\n",
      "Epoch [487/500]      | Train: Loss 0.21879, R2 0.93758, RMSE 0.46645                     | Test: Loss 0.77753, R2 0.79679, RMSE 0.87590\n",
      "Epoch [488/500]      | Train: Loss 0.20205, R2 0.94328, RMSE 0.44857                     | Test: Loss 0.76979, R2 0.79140, RMSE 0.87252\n",
      "Epoch [489/500]      | Train: Loss 0.19357, R2 0.94562, RMSE 0.43914                     | Test: Loss 0.79271, R2 0.78074, RMSE 0.87983\n",
      "Epoch [490/500]      | Train: Loss 0.19656, R2 0.94443, RMSE 0.44243                     | Test: Loss 0.76818, R2 0.79015, RMSE 0.86785\n",
      "Epoch [491/500]      | Train: Loss 0.19690, R2 0.94413, RMSE 0.44304                     | Test: Loss 0.77961, R2 0.78447, RMSE 0.87674\n",
      "Epoch [492/500]      | Train: Loss 0.19978, R2 0.94339, RMSE 0.44630                     | Test: Loss 0.74647, R2 0.78388, RMSE 0.85186\n",
      "Epoch [493/500]      | Train: Loss 0.20922, R2 0.94070, RMSE 0.45661                     | Test: Loss 0.77462, R2 0.77816, RMSE 0.87070\n",
      "Epoch [494/500]      | Train: Loss 0.20238, R2 0.94298, RMSE 0.44904                     | Test: Loss 0.75512, R2 0.78886, RMSE 0.85919\n",
      "Epoch [495/500]      | Train: Loss 0.20070, R2 0.94304, RMSE 0.44692                     | Test: Loss 0.79739, R2 0.78971, RMSE 0.88277\n",
      "Epoch [496/500]      | Train: Loss 0.19772, R2 0.94429, RMSE 0.44401                     | Test: Loss 0.77508, R2 0.78478, RMSE 0.87411\n",
      "Epoch [497/500]      | Train: Loss 0.19619, R2 0.94445, RMSE 0.44227                     | Test: Loss 0.77335, R2 0.77973, RMSE 0.87439\n",
      "Epoch [498/500]      | Train: Loss 0.20321, R2 0.94273, RMSE 0.45006                     | Test: Loss 0.76088, R2 0.78910, RMSE 0.86622\n",
      "Epoch [499/500]      | Train: Loss 0.20171, R2 0.94278, RMSE 0.44811                     | Test: Loss 0.74385, R2 0.77953, RMSE 0.85344\n",
      "Epoch [500/500]      | Train: Loss 0.19697, R2 0.94445, RMSE 0.44285                     | Test: Loss 0.74049, R2 0.79477, RMSE 0.85127\n",
      "Best rmse 0.7887400342867925\n",
      "100 epochs of training and evaluation took, 3941.032437449001\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class LossFunction(nn.Module):\n",
    "    def __init__(self, num_tasks):\n",
    "        super(LossFunction, self).__init__()\n",
    "        self.num_tasks = num_tasks\n",
    "\n",
    "        self.loss_fns = [nn.MSELoss() for x in range(num_tasks)] \n",
    "\n",
    "    def forward(self, predictions, labels_task1):\n",
    "\n",
    "        #task 1\n",
    "        target = labels_task1\n",
    "        prediction = predictions\n",
    "        loss_fn = self.loss_fns[0]\n",
    "\n",
    "        task_loss = loss_fn(prediction, target)\n",
    "        \n",
    "        return task_loss\n",
    "    \n",
    "model = CATTransformer(rff_on=True,\n",
    "                       sigma=0.5,\n",
    "                       embed_size=160,\n",
    "                       heads=5,\n",
    "                       forward_expansion=8,\n",
    "                       pre_norm_on=False,\n",
    "                       mlp_scale_classification=8,\n",
    "                   \n",
    "                       decoder_dropout=0,\n",
    "                       classification_dropout=0.1,\n",
    "                       targets_classes=target_classes,\n",
    "                       n_cont=len(cont_columns),\n",
    "              \n",
    "                       embedding_scheme='log-linear_periodic',\n",
    "                       trainable=False,\n",
    "                       linear_on=True).to(device_in_use)\n",
    "\n",
    "def format_metric(value): # Used to format the metrics output\n",
    "    return f\"{value:.5f}\"\n",
    "\n",
    "loss_functions = LossFunction(1)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr = 0.0001) # Maybe try messing around with optimizers. try other torch optimizers with different configurations.\n",
    "epochs = 500 #Set the number of epochs\n",
    "\n",
    "train_losses = []\n",
    "train_rmse = [] \n",
    "test_losses = []\n",
    "test_rmse = []  \n",
    "\n",
    "\n",
    "#Time will be recorded for all 100 epochs - This means the results will not be comparable to Xgboost but that is ok, we will only compare between transformer models who will also train for 100 epochs\n",
    "start_time = time.process_time()\n",
    "\n",
    "for t in range(epochs):\n",
    "    train_loss, r2_train, rmse_train = train(train_dataloader, model, loss_functions, optimizer, device_in_use=device_in_use)\n",
    "    test_loss, r2_test, rmse_test = test(test_dataloader, model, loss_functions, device_in_use=device_in_use)\n",
    "    train_losses.append(train_loss)\n",
    "    train_rmse.append(rmse_train)\n",
    "    # train_accuracies_2.append(train_accuracy_2)\n",
    "    # train_recalls.append(train_recall) \n",
    "    # train_f1_scores.append(train_f1)\n",
    "    test_losses.append(test_loss)\n",
    "    test_rmse.append(rmse_test)\n",
    "    # test_accuracies_2.append(test_accuracy_2)\n",
    "    # test_recalls.append(test_recall)\n",
    "    # test_f1_scores.append(test_f1)\n",
    "    # Formatting for easier reading\n",
    "    epoch_str = f\"Epoch [{t+1:2}/{epochs}]\"\n",
    "    train_metrics = f\"Train: Loss {format_metric(train_loss)}, R2 {format_metric(r2_train)}, RMSE {format_metric(rmse_train)}\"\n",
    "    test_metrics = f\"Test: Loss {format_metric(test_loss)}, R2 {format_metric(r2_test)}, RMSE {format_metric(rmse_test)}\"\n",
    "    print(f\"{epoch_str:20} | {train_metrics:65} | {test_metrics}\")\n",
    "\n",
    "total_time = time.process_time() - start_time\n",
    "\n",
    "# Plotting the loss curves\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, epochs+1), train_losses, label='Train Loss')\n",
    "plt.plot(range(1, epochs+1), [l for l in test_losses], label='Test Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Test Loss Curve')\n",
    "plt.legend()\n",
    "\n",
    "best_index = test_rmse.index(min(test_rmse))\n",
    "print(f\"Best rmse {test_rmse[best_index]}\")\n",
    "print(f\"100 epochs of training and evaluation took, {total_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\sigma = 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [ 1/500]       | Train: Loss 4.32444, R2 -0.23182, RMSE 2.04175                    | Test: Loss 3.34203, R2 0.05480, RMSE 1.81563\n",
      "Epoch [ 2/500]       | Train: Loss 2.33227, R2 0.34701, RMSE 1.50658                     | Test: Loss 1.33807, R2 0.64581, RMSE 1.14950\n",
      "Epoch [ 3/500]       | Train: Loss 1.33251, R2 0.62631, RMSE 1.14928                     | Test: Loss 1.12294, R2 0.68547, RMSE 1.05280\n",
      "Epoch [ 4/500]       | Train: Loss 1.22394, R2 0.65923, RMSE 1.10138                     | Test: Loss 1.07679, R2 0.70798, RMSE 1.02962\n",
      "Epoch [ 5/500]       | Train: Loss 1.14544, R2 0.67851, RMSE 1.06601                     | Test: Loss 1.03192, R2 0.71527, RMSE 1.01213\n",
      "Epoch [ 6/500]       | Train: Loss 1.07980, R2 0.69739, RMSE 1.03470                     | Test: Loss 1.12300, R2 0.68964, RMSE 1.05343\n",
      "Epoch [ 7/500]       | Train: Loss 1.07684, R2 0.69900, RMSE 1.03078                     | Test: Loss 1.05952, R2 0.71290, RMSE 1.01548\n",
      "Epoch [ 8/500]       | Train: Loss 1.03933, R2 0.70857, RMSE 1.01423                     | Test: Loss 0.93531, R2 0.73197, RMSE 0.95944\n",
      "Epoch [ 9/500]       | Train: Loss 1.04793, R2 0.70729, RMSE 1.01878                     | Test: Loss 0.96731, R2 0.74208, RMSE 0.97392\n",
      "Epoch [10/500]       | Train: Loss 1.01224, R2 0.71821, RMSE 0.99968                     | Test: Loss 1.03757, R2 0.71772, RMSE 1.00911\n",
      "Epoch [11/500]       | Train: Loss 1.01235, R2 0.71821, RMSE 1.00202                     | Test: Loss 0.90895, R2 0.73960, RMSE 0.93823\n",
      "Epoch [12/500]       | Train: Loss 0.98732, R2 0.72374, RMSE 0.99065                     | Test: Loss 0.95117, R2 0.74854, RMSE 0.96860\n",
      "Epoch [13/500]       | Train: Loss 0.97823, R2 0.72681, RMSE 0.98484                     | Test: Loss 0.91853, R2 0.75365, RMSE 0.94740\n",
      "Epoch [14/500]       | Train: Loss 0.97932, R2 0.72652, RMSE 0.98474                     | Test: Loss 0.95057, R2 0.74054, RMSE 0.96478\n",
      "Epoch [15/500]       | Train: Loss 0.98998, R2 0.72574, RMSE 0.98823                     | Test: Loss 0.96110, R2 0.68435, RMSE 0.97275\n",
      "Epoch [16/500]       | Train: Loss 0.96682, R2 0.73028, RMSE 0.97825                     | Test: Loss 0.93700, R2 0.75551, RMSE 0.95679\n",
      "Epoch [17/500]       | Train: Loss 0.93846, R2 0.73968, RMSE 0.96331                     | Test: Loss 0.91807, R2 0.75554, RMSE 0.94884\n",
      "Epoch [18/500]       | Train: Loss 0.93505, R2 0.73749, RMSE 0.96339                     | Test: Loss 0.87582, R2 0.75895, RMSE 0.92399\n",
      "Epoch [19/500]       | Train: Loss 0.94095, R2 0.73738, RMSE 0.96563                     | Test: Loss 0.88333, R2 0.76099, RMSE 0.92458\n",
      "Epoch [20/500]       | Train: Loss 0.93710, R2 0.73821, RMSE 0.96395                     | Test: Loss 0.88423, R2 0.76150, RMSE 0.92484\n",
      "Epoch [21/500]       | Train: Loss 0.90846, R2 0.74470, RMSE 0.94955                     | Test: Loss 0.93578, R2 0.75125, RMSE 0.94831\n",
      "Epoch [22/500]       | Train: Loss 0.92105, R2 0.74181, RMSE 0.95435                     | Test: Loss 0.87074, R2 0.74909, RMSE 0.92406\n",
      "Epoch [23/500]       | Train: Loss 0.91089, R2 0.74457, RMSE 0.95079                     | Test: Loss 1.02353, R2 0.73174, RMSE 1.00099\n",
      "Epoch [24/500]       | Train: Loss 0.91705, R2 0.74230, RMSE 0.95330                     | Test: Loss 0.96441, R2 0.73518, RMSE 0.96873\n",
      "Epoch [25/500]       | Train: Loss 0.90850, R2 0.74825, RMSE 0.94758                     | Test: Loss 0.84609, R2 0.76436, RMSE 0.90334\n",
      "Epoch [26/500]       | Train: Loss 0.88737, R2 0.75065, RMSE 0.93757                     | Test: Loss 0.87319, R2 0.75025, RMSE 0.91928\n",
      "Epoch [27/500]       | Train: Loss 0.89259, R2 0.75234, RMSE 0.94020                     | Test: Loss 0.85215, R2 0.77339, RMSE 0.91013\n",
      "Epoch [28/500]       | Train: Loss 0.87502, R2 0.75729, RMSE 0.92998                     | Test: Loss 0.91923, R2 0.74973, RMSE 0.94605\n",
      "Epoch [29/500]       | Train: Loss 0.88334, R2 0.75438, RMSE 0.93432                     | Test: Loss 0.87930, R2 0.77002, RMSE 0.92863\n",
      "Epoch [30/500]       | Train: Loss 0.87508, R2 0.75570, RMSE 0.92960                     | Test: Loss 0.86854, R2 0.76911, RMSE 0.91788\n",
      "Epoch [31/500]       | Train: Loss 0.87161, R2 0.75574, RMSE 0.92860                     | Test: Loss 0.84278, R2 0.77125, RMSE 0.90777\n",
      "Epoch [32/500]       | Train: Loss 0.88000, R2 0.75396, RMSE 0.93452                     | Test: Loss 0.90233, R2 0.76810, RMSE 0.93801\n",
      "Epoch [33/500]       | Train: Loss 0.85968, R2 0.76150, RMSE 0.92032                     | Test: Loss 0.81645, R2 0.76872, RMSE 0.89709\n",
      "Epoch [34/500]       | Train: Loss 0.84108, R2 0.76571, RMSE 0.91264                     | Test: Loss 0.85892, R2 0.77210, RMSE 0.91478\n",
      "Epoch [35/500]       | Train: Loss 0.83739, R2 0.76798, RMSE 0.90995                     | Test: Loss 0.78314, R2 0.77920, RMSE 0.87619\n",
      "Epoch [36/500]       | Train: Loss 0.82786, R2 0.76978, RMSE 0.90463                     | Test: Loss 0.86273, R2 0.77356, RMSE 0.91551\n",
      "Epoch [37/500]       | Train: Loss 0.82133, R2 0.76885, RMSE 0.90308                     | Test: Loss 0.78707, R2 0.77973, RMSE 0.87923\n",
      "Epoch [38/500]       | Train: Loss 0.83662, R2 0.76594, RMSE 0.90936                     | Test: Loss 0.75212, R2 0.79566, RMSE 0.85461\n",
      "Epoch [39/500]       | Train: Loss 0.81502, R2 0.77290, RMSE 0.89796                     | Test: Loss 0.77913, R2 0.78568, RMSE 0.87620\n",
      "Epoch [40/500]       | Train: Loss 0.83612, R2 0.76755, RMSE 0.90901                     | Test: Loss 0.76881, R2 0.78269, RMSE 0.86955\n",
      "Epoch [41/500]       | Train: Loss 0.80501, R2 0.77442, RMSE 0.89367                     | Test: Loss 0.88267, R2 0.76695, RMSE 0.93340\n",
      "Epoch [42/500]       | Train: Loss 0.80121, R2 0.77372, RMSE 0.89125                     | Test: Loss 0.74480, R2 0.79564, RMSE 0.85626\n",
      "Epoch [43/500]       | Train: Loss 0.82997, R2 0.76862, RMSE 0.90689                     | Test: Loss 0.75575, R2 0.79420, RMSE 0.86121\n",
      "Epoch [44/500]       | Train: Loss 0.81661, R2 0.77332, RMSE 0.89943                     | Test: Loss 0.76600, R2 0.78262, RMSE 0.86470\n",
      "Epoch [45/500]       | Train: Loss 0.78547, R2 0.77950, RMSE 0.88190                     | Test: Loss 0.84675, R2 0.78376, RMSE 0.90301\n",
      "Epoch [46/500]       | Train: Loss 0.77970, R2 0.78125, RMSE 0.87883                     | Test: Loss 0.87238, R2 0.76166, RMSE 0.92231\n",
      "Epoch [47/500]       | Train: Loss 0.79577, R2 0.77817, RMSE 0.88679                     | Test: Loss 0.75139, R2 0.79021, RMSE 0.85777\n",
      "Epoch [48/500]       | Train: Loss 0.82029, R2 0.77090, RMSE 0.90166                     | Test: Loss 0.76173, R2 0.78910, RMSE 0.86298\n",
      "Epoch [49/500]       | Train: Loss 0.77544, R2 0.78236, RMSE 0.87718                     | Test: Loss 0.75921, R2 0.78403, RMSE 0.86534\n",
      "Epoch [50/500]       | Train: Loss 0.77150, R2 0.78458, RMSE 0.87483                     | Test: Loss 0.78174, R2 0.78896, RMSE 0.87822\n",
      "Epoch [51/500]       | Train: Loss 0.75789, R2 0.78624, RMSE 0.86676                     | Test: Loss 1.14063, R2 0.76075, RMSE 0.96552\n",
      "Epoch [52/500]       | Train: Loss 0.78267, R2 0.78142, RMSE 0.88190                     | Test: Loss 0.75495, R2 0.79242, RMSE 0.86308\n",
      "Epoch [53/500]       | Train: Loss 0.76810, R2 0.78627, RMSE 0.87243                     | Test: Loss 0.73184, R2 0.80294, RMSE 0.84429\n",
      "Epoch [54/500]       | Train: Loss 0.76826, R2 0.78365, RMSE 0.87317                     | Test: Loss 0.76301, R2 0.78429, RMSE 0.86418\n",
      "Epoch [55/500]       | Train: Loss 0.77176, R2 0.78335, RMSE 0.87504                     | Test: Loss 0.71686, R2 0.79367, RMSE 0.83978\n",
      "Epoch [56/500]       | Train: Loss 0.75823, R2 0.78766, RMSE 0.86695                     | Test: Loss 0.74002, R2 0.80480, RMSE 0.85514\n",
      "Epoch [57/500]       | Train: Loss 0.74963, R2 0.79106, RMSE 0.86275                     | Test: Loss 0.72967, R2 0.79069, RMSE 0.84412\n",
      "Epoch [58/500]       | Train: Loss 0.76415, R2 0.78585, RMSE 0.87122                     | Test: Loss 0.73129, R2 0.79170, RMSE 0.84422\n",
      "Epoch [59/500]       | Train: Loss 0.76155, R2 0.78928, RMSE 0.86695                     | Test: Loss 0.84344, R2 0.74353, RMSE 0.91230\n",
      "Epoch [60/500]       | Train: Loss 0.77203, R2 0.78418, RMSE 0.87563                     | Test: Loss 0.71597, R2 0.80204, RMSE 0.84079\n",
      "Epoch [61/500]       | Train: Loss 0.76674, R2 0.78380, RMSE 0.87268                     | Test: Loss 0.79570, R2 0.78517, RMSE 0.88778\n",
      "Epoch [62/500]       | Train: Loss 0.76861, R2 0.78575, RMSE 0.87258                     | Test: Loss 0.71436, R2 0.80305, RMSE 0.83744\n",
      "Epoch [63/500]       | Train: Loss 0.73943, R2 0.79245, RMSE 0.85604                     | Test: Loss 0.70725, R2 0.80231, RMSE 0.82801\n",
      "Epoch [64/500]       | Train: Loss 0.74966, R2 0.78819, RMSE 0.86225                     | Test: Loss 0.78770, R2 0.80119, RMSE 0.87810\n",
      "Epoch [65/500]       | Train: Loss 0.73778, R2 0.79133, RMSE 0.85490                     | Test: Loss 0.76095, R2 0.77900, RMSE 0.86372\n",
      "Epoch [66/500]       | Train: Loss 0.72189, R2 0.79919, RMSE 0.84401                     | Test: Loss 0.68871, R2 0.81182, RMSE 0.81950\n",
      "Epoch [67/500]       | Train: Loss 0.72673, R2 0.79604, RMSE 0.84940                     | Test: Loss 0.74969, R2 0.79418, RMSE 0.85817\n",
      "Epoch [68/500]       | Train: Loss 0.71971, R2 0.79869, RMSE 0.84470                     | Test: Loss 0.71351, R2 0.79249, RMSE 0.83608\n",
      "Epoch [69/500]       | Train: Loss 0.72566, R2 0.79539, RMSE 0.84802                     | Test: Loss 0.81367, R2 0.78110, RMSE 0.89592\n",
      "Epoch [70/500]       | Train: Loss 0.72261, R2 0.79786, RMSE 0.84733                     | Test: Loss 0.77792, R2 0.79323, RMSE 0.86884\n",
      "Epoch [71/500]       | Train: Loss 0.73774, R2 0.79384, RMSE 0.85559                     | Test: Loss 0.74080, R2 0.78379, RMSE 0.85654\n",
      "Epoch [72/500]       | Train: Loss 0.70317, R2 0.80323, RMSE 0.83551                     | Test: Loss 0.74091, R2 0.80263, RMSE 0.85583\n",
      "Epoch [73/500]       | Train: Loss 0.72326, R2 0.79610, RMSE 0.84763                     | Test: Loss 0.70678, R2 0.80426, RMSE 0.83326\n",
      "Epoch [74/500]       | Train: Loss 0.70874, R2 0.80024, RMSE 0.83812                     | Test: Loss 0.75389, R2 0.79433, RMSE 0.85884\n",
      "Epoch [75/500]       | Train: Loss 0.72150, R2 0.79702, RMSE 0.84636                     | Test: Loss 0.69722, R2 0.79653, RMSE 0.83021\n",
      "Epoch [76/500]       | Train: Loss 0.72739, R2 0.79399, RMSE 0.84932                     | Test: Loss 0.70305, R2 0.79425, RMSE 0.83261\n",
      "Epoch [77/500]       | Train: Loss 0.70502, R2 0.80268, RMSE 0.83659                     | Test: Loss 0.68477, R2 0.81754, RMSE 0.81198\n",
      "Epoch [78/500]       | Train: Loss 0.70192, R2 0.80193, RMSE 0.83435                     | Test: Loss 0.70633, R2 0.80500, RMSE 0.83670\n",
      "Epoch [79/500]       | Train: Loss 0.69137, R2 0.80798, RMSE 0.82804                     | Test: Loss 0.72488, R2 0.79936, RMSE 0.84545\n",
      "Epoch [80/500]       | Train: Loss 0.70051, R2 0.80393, RMSE 0.83349                     | Test: Loss 0.72432, R2 0.79860, RMSE 0.84259\n",
      "Epoch [81/500]       | Train: Loss 0.67592, R2 0.81028, RMSE 0.81939                     | Test: Loss 0.70058, R2 0.81347, RMSE 0.82620\n",
      "Epoch [82/500]       | Train: Loss 0.70679, R2 0.80369, RMSE 0.83716                     | Test: Loss 0.73351, R2 0.80110, RMSE 0.84952\n",
      "Epoch [83/500]       | Train: Loss 0.69246, R2 0.80571, RMSE 0.82874                     | Test: Loss 0.79738, R2 0.79158, RMSE 0.87901\n",
      "Epoch [84/500]       | Train: Loss 0.67570, R2 0.81078, RMSE 0.81906                     | Test: Loss 0.85948, R2 0.78429, RMSE 0.90001\n",
      "Epoch [85/500]       | Train: Loss 0.68421, R2 0.80771, RMSE 0.82430                     | Test: Loss 0.72296, R2 0.78483, RMSE 0.84546\n",
      "Epoch [86/500]       | Train: Loss 0.67740, R2 0.80818, RMSE 0.81981                     | Test: Loss 0.72911, R2 0.80194, RMSE 0.84783\n",
      "Epoch [87/500]       | Train: Loss 0.68376, R2 0.80821, RMSE 0.82433                     | Test: Loss 0.68045, R2 0.81012, RMSE 0.81610\n",
      "Epoch [88/500]       | Train: Loss 0.67704, R2 0.81288, RMSE 0.81928                     | Test: Loss 0.70742, R2 0.78583, RMSE 0.83532\n",
      "Epoch [89/500]       | Train: Loss 0.68968, R2 0.80793, RMSE 0.82694                     | Test: Loss 0.68056, R2 0.81001, RMSE 0.81341\n",
      "Epoch [90/500]       | Train: Loss 0.67710, R2 0.81088, RMSE 0.81905                     | Test: Loss 0.70678, R2 0.79643, RMSE 0.83137\n",
      "Epoch [91/500]       | Train: Loss 0.67020, R2 0.80919, RMSE 0.81605                     | Test: Loss 0.68935, R2 0.81115, RMSE 0.82112\n",
      "Epoch [92/500]       | Train: Loss 0.66749, R2 0.81124, RMSE 0.81464                     | Test: Loss 0.76443, R2 0.78230, RMSE 0.86902\n",
      "Epoch [93/500]       | Train: Loss 0.65420, R2 0.81795, RMSE 0.80556                     | Test: Loss 0.71353, R2 0.80988, RMSE 0.84166\n",
      "Epoch [94/500]       | Train: Loss 0.65850, R2 0.81507, RMSE 0.80797                     | Test: Loss 0.75335, R2 0.80157, RMSE 0.86232\n",
      "Epoch [95/500]       | Train: Loss 0.66412, R2 0.81405, RMSE 0.81067                     | Test: Loss 0.72556, R2 0.80794, RMSE 0.84504\n",
      "Epoch [96/500]       | Train: Loss 0.64501, R2 0.81817, RMSE 0.80074                     | Test: Loss 0.70287, R2 0.80835, RMSE 0.82826\n",
      "Epoch [97/500]       | Train: Loss 0.63931, R2 0.82093, RMSE 0.79724                     | Test: Loss 0.73056, R2 0.79783, RMSE 0.84802\n",
      "Epoch [98/500]       | Train: Loss 0.64822, R2 0.81849, RMSE 0.80200                     | Test: Loss 0.69935, R2 0.81298, RMSE 0.82943\n",
      "Epoch [99/500]       | Train: Loss 0.64302, R2 0.81940, RMSE 0.79919                     | Test: Loss 0.77091, R2 0.77400, RMSE 0.87281\n",
      "Epoch [100/500]      | Train: Loss 0.63897, R2 0.82092, RMSE 0.79711                     | Test: Loss 0.77000, R2 0.79739, RMSE 0.86710\n",
      "Epoch [101/500]      | Train: Loss 0.64759, R2 0.81697, RMSE 0.80184                     | Test: Loss 0.71981, R2 0.75842, RMSE 0.84125\n",
      "Epoch [102/500]      | Train: Loss 0.62179, R2 0.82326, RMSE 0.78536                     | Test: Loss 0.74041, R2 0.80163, RMSE 0.85517\n",
      "Epoch [103/500]      | Train: Loss 0.63964, R2 0.82049, RMSE 0.79741                     | Test: Loss 0.75681, R2 0.79545, RMSE 0.85987\n",
      "Epoch [104/500]      | Train: Loss 0.62361, R2 0.82347, RMSE 0.78743                     | Test: Loss 0.69924, R2 0.81345, RMSE 0.82745\n",
      "Epoch [105/500]      | Train: Loss 0.62700, R2 0.82408, RMSE 0.78844                     | Test: Loss 0.78651, R2 0.79591, RMSE 0.87707\n",
      "Epoch [106/500]      | Train: Loss 0.61831, R2 0.82531, RMSE 0.78325                     | Test: Loss 0.70840, R2 0.80237, RMSE 0.83468\n",
      "Epoch [107/500]      | Train: Loss 0.63319, R2 0.82177, RMSE 0.79287                     | Test: Loss 0.77648, R2 0.78526, RMSE 0.87745\n",
      "Epoch [108/500]      | Train: Loss 0.62220, R2 0.82384, RMSE 0.78691                     | Test: Loss 0.73995, R2 0.79975, RMSE 0.85400\n",
      "Epoch [109/500]      | Train: Loss 0.62431, R2 0.82389, RMSE 0.78826                     | Test: Loss 0.69152, R2 0.79884, RMSE 0.82629\n",
      "Epoch [110/500]      | Train: Loss 0.61510, R2 0.82676, RMSE 0.78212                     | Test: Loss 0.72888, R2 0.81104, RMSE 0.84613\n",
      "Epoch [111/500]      | Train: Loss 0.60374, R2 0.83141, RMSE 0.77386                     | Test: Loss 0.68797, R2 0.80885, RMSE 0.82294\n",
      "Epoch [112/500]      | Train: Loss 0.60738, R2 0.82945, RMSE 0.77704                     | Test: Loss 0.76344, R2 0.79577, RMSE 0.86855\n",
      "Epoch [113/500]      | Train: Loss 0.60623, R2 0.82985, RMSE 0.77618                     | Test: Loss 0.67257, R2 0.80755, RMSE 0.80646\n",
      "Epoch [114/500]      | Train: Loss 0.62492, R2 0.82413, RMSE 0.78781                     | Test: Loss 0.71985, R2 0.80929, RMSE 0.83944\n",
      "Epoch [115/500]      | Train: Loss 0.60477, R2 0.83062, RMSE 0.77465                     | Test: Loss 0.77764, R2 0.77894, RMSE 0.87218\n",
      "Epoch [116/500]      | Train: Loss 0.61246, R2 0.82657, RMSE 0.78067                     | Test: Loss 0.72951, R2 0.80487, RMSE 0.84332\n",
      "Epoch [117/500]      | Train: Loss 0.60905, R2 0.82637, RMSE 0.77873                     | Test: Loss 0.70472, R2 0.80433, RMSE 0.82921\n",
      "Epoch [118/500]      | Train: Loss 0.60709, R2 0.82921, RMSE 0.77713                     | Test: Loss 0.68306, R2 0.81275, RMSE 0.80860\n",
      "Epoch [119/500]      | Train: Loss 0.61723, R2 0.82717, RMSE 0.78160                     | Test: Loss 0.72766, R2 0.79837, RMSE 0.84656\n",
      "Epoch [120/500]      | Train: Loss 0.58793, R2 0.83286, RMSE 0.76388                     | Test: Loss 0.72490, R2 0.80597, RMSE 0.84296\n",
      "Epoch [121/500]      | Train: Loss 0.59523, R2 0.83126, RMSE 0.76911                     | Test: Loss 0.76728, R2 0.79599, RMSE 0.86913\n",
      "Epoch [122/500]      | Train: Loss 0.59170, R2 0.83266, RMSE 0.76692                     | Test: Loss 0.68957, R2 0.80911, RMSE 0.81989\n",
      "Epoch [123/500]      | Train: Loss 0.58408, R2 0.83774, RMSE 0.76050                     | Test: Loss 0.82013, R2 0.76417, RMSE 0.89584\n",
      "Epoch [124/500]      | Train: Loss 0.58042, R2 0.83635, RMSE 0.75887                     | Test: Loss 0.73084, R2 0.79902, RMSE 0.84339\n",
      "Epoch [125/500]      | Train: Loss 0.58544, R2 0.83615, RMSE 0.76189                     | Test: Loss 0.78426, R2 0.79572, RMSE 0.87546\n",
      "Epoch [126/500]      | Train: Loss 0.57866, R2 0.83851, RMSE 0.75786                     | Test: Loss 0.72834, R2 0.79220, RMSE 0.84603\n",
      "Epoch [127/500]      | Train: Loss 0.59000, R2 0.83488, RMSE 0.76484                     | Test: Loss 0.76351, R2 0.79980, RMSE 0.85771\n",
      "Epoch [128/500]      | Train: Loss 0.57748, R2 0.83814, RMSE 0.75692                     | Test: Loss 0.72324, R2 0.79851, RMSE 0.84348\n",
      "Epoch [129/500]      | Train: Loss 0.55863, R2 0.84301, RMSE 0.74497                     | Test: Loss 0.71043, R2 0.80237, RMSE 0.83492\n",
      "Epoch [130/500]      | Train: Loss 0.55778, R2 0.84264, RMSE 0.74431                     | Test: Loss 0.68104, R2 0.81546, RMSE 0.81038\n",
      "Epoch [131/500]      | Train: Loss 0.56784, R2 0.83978, RMSE 0.75058                     | Test: Loss 0.74580, R2 0.79034, RMSE 0.85283\n",
      "Epoch [132/500]      | Train: Loss 0.55975, R2 0.84454, RMSE 0.74561                     | Test: Loss 0.72636, R2 0.80517, RMSE 0.84508\n",
      "Epoch [133/500]      | Train: Loss 0.57087, R2 0.83916, RMSE 0.75302                     | Test: Loss 0.70352, R2 0.80345, RMSE 0.82943\n",
      "Epoch [134/500]      | Train: Loss 0.55704, R2 0.84391, RMSE 0.74354                     | Test: Loss 0.68917, R2 0.80367, RMSE 0.82571\n",
      "Epoch [135/500]      | Train: Loss 0.55114, R2 0.84466, RMSE 0.73948                     | Test: Loss 0.69467, R2 0.80672, RMSE 0.82595\n",
      "Epoch [136/500]      | Train: Loss 0.55252, R2 0.84413, RMSE 0.74067                     | Test: Loss 0.70142, R2 0.79689, RMSE 0.82826\n",
      "Epoch [137/500]      | Train: Loss 0.54487, R2 0.84705, RMSE 0.73570                     | Test: Loss 0.72874, R2 0.78566, RMSE 0.84712\n",
      "Epoch [138/500]      | Train: Loss 0.54882, R2 0.84636, RMSE 0.73868                     | Test: Loss 0.70614, R2 0.81015, RMSE 0.83390\n",
      "Epoch [139/500]      | Train: Loss 0.53579, R2 0.84838, RMSE 0.72957                     | Test: Loss 0.69282, R2 0.80990, RMSE 0.82529\n",
      "Epoch [140/500]      | Train: Loss 0.54244, R2 0.84695, RMSE 0.73487                     | Test: Loss 0.69927, R2 0.81483, RMSE 0.82751\n",
      "Epoch [141/500]      | Train: Loss 0.53211, R2 0.84898, RMSE 0.72729                     | Test: Loss 0.70849, R2 0.79422, RMSE 0.83432\n",
      "Epoch [142/500]      | Train: Loss 0.53192, R2 0.85167, RMSE 0.72698                     | Test: Loss 0.78717, R2 0.77644, RMSE 0.87919\n",
      "Epoch [143/500]      | Train: Loss 0.54893, R2 0.84547, RMSE 0.73904                     | Test: Loss 0.71511, R2 0.79305, RMSE 0.83621\n",
      "Epoch [144/500]      | Train: Loss 0.54590, R2 0.84553, RMSE 0.73639                     | Test: Loss 0.79730, R2 0.79462, RMSE 0.87602\n",
      "Epoch [145/500]      | Train: Loss 0.52818, R2 0.85011, RMSE 0.72481                     | Test: Loss 0.70811, R2 0.80710, RMSE 0.83804\n",
      "Epoch [146/500]      | Train: Loss 0.52624, R2 0.85259, RMSE 0.72299                     | Test: Loss 0.86751, R2 0.79790, RMSE 0.89264\n",
      "Epoch [147/500]      | Train: Loss 0.52227, R2 0.85217, RMSE 0.72086                     | Test: Loss 0.73626, R2 0.80417, RMSE 0.84493\n",
      "Epoch [148/500]      | Train: Loss 0.52327, R2 0.85279, RMSE 0.72054                     | Test: Loss 0.72146, R2 0.80131, RMSE 0.83943\n",
      "Epoch [149/500]      | Train: Loss 0.54137, R2 0.84687, RMSE 0.73293                     | Test: Loss 0.71666, R2 0.80840, RMSE 0.83599\n",
      "Epoch [150/500]      | Train: Loss 0.52421, R2 0.85162, RMSE 0.72203                     | Test: Loss 0.74515, R2 0.80361, RMSE 0.84828\n",
      "Epoch [151/500]      | Train: Loss 0.51695, R2 0.85406, RMSE 0.71599                     | Test: Loss 0.69321, R2 0.79866, RMSE 0.82683\n",
      "Epoch [152/500]      | Train: Loss 0.51408, R2 0.85529, RMSE 0.71508                     | Test: Loss 0.74177, R2 0.80078, RMSE 0.85327\n",
      "Epoch [153/500]      | Train: Loss 0.53515, R2 0.84886, RMSE 0.72963                     | Test: Loss 0.68151, R2 0.80414, RMSE 0.81425\n",
      "Epoch [154/500]      | Train: Loss 0.51574, R2 0.85450, RMSE 0.71581                     | Test: Loss 0.73248, R2 0.80380, RMSE 0.84753\n",
      "Epoch [155/500]      | Train: Loss 0.51028, R2 0.85688, RMSE 0.71213                     | Test: Loss 0.69150, R2 0.81012, RMSE 0.82619\n",
      "Epoch [156/500]      | Train: Loss 0.50406, R2 0.85868, RMSE 0.70809                     | Test: Loss 0.69891, R2 0.80719, RMSE 0.82695\n",
      "Epoch [157/500]      | Train: Loss 0.51468, R2 0.85386, RMSE 0.71547                     | Test: Loss 0.69998, R2 0.80352, RMSE 0.82499\n",
      "Epoch [158/500]      | Train: Loss 0.50014, R2 0.85893, RMSE 0.70452                     | Test: Loss 0.71805, R2 0.80694, RMSE 0.84071\n",
      "Epoch [159/500]      | Train: Loss 0.50417, R2 0.85775, RMSE 0.70744                     | Test: Loss 0.73407, R2 0.79324, RMSE 0.84732\n",
      "Epoch [160/500]      | Train: Loss 0.51141, R2 0.85689, RMSE 0.71306                     | Test: Loss 0.72192, R2 0.80813, RMSE 0.83857\n",
      "Epoch [161/500]      | Train: Loss 0.49969, R2 0.85891, RMSE 0.70531                     | Test: Loss 0.69695, R2 0.80859, RMSE 0.82828\n",
      "Epoch [162/500]      | Train: Loss 0.51919, R2 0.85294, RMSE 0.71832                     | Test: Loss 0.72140, R2 0.80203, RMSE 0.84040\n",
      "Epoch [163/500]      | Train: Loss 0.50401, R2 0.85900, RMSE 0.70746                     | Test: Loss 0.69594, R2 0.81074, RMSE 0.81892\n",
      "Epoch [164/500]      | Train: Loss 0.50305, R2 0.85793, RMSE 0.70771                     | Test: Loss 0.68539, R2 0.81083, RMSE 0.81821\n",
      "Epoch [165/500]      | Train: Loss 0.49530, R2 0.86115, RMSE 0.70204                     | Test: Loss 0.72966, R2 0.73830, RMSE 0.84966\n",
      "Epoch [166/500]      | Train: Loss 0.49698, R2 0.86002, RMSE 0.70343                     | Test: Loss 0.75028, R2 0.79212, RMSE 0.85525\n",
      "Epoch [167/500]      | Train: Loss 0.49723, R2 0.86121, RMSE 0.70201                     | Test: Loss 0.69918, R2 0.80126, RMSE 0.82998\n",
      "Epoch [168/500]      | Train: Loss 0.49484, R2 0.86240, RMSE 0.70019                     | Test: Loss 0.75230, R2 0.78510, RMSE 0.86224\n",
      "Epoch [169/500]      | Train: Loss 0.48641, R2 0.86403, RMSE 0.69523                     | Test: Loss 0.76161, R2 0.80007, RMSE 0.85792\n",
      "Epoch [170/500]      | Train: Loss 0.48647, R2 0.86326, RMSE 0.69550                     | Test: Loss 0.70739, R2 0.79664, RMSE 0.83462\n",
      "Epoch [171/500]      | Train: Loss 0.48138, R2 0.86468, RMSE 0.69153                     | Test: Loss 0.76059, R2 0.79287, RMSE 0.86645\n",
      "Epoch [172/500]      | Train: Loss 0.48360, R2 0.86402, RMSE 0.69346                     | Test: Loss 0.74229, R2 0.79756, RMSE 0.85518\n",
      "Epoch [173/500]      | Train: Loss 0.48480, R2 0.86214, RMSE 0.69434                     | Test: Loss 0.74398, R2 0.79333, RMSE 0.85466\n",
      "Epoch [174/500]      | Train: Loss 0.47564, R2 0.86587, RMSE 0.68816                     | Test: Loss 0.69195, R2 0.80986, RMSE 0.82144\n",
      "Epoch [175/500]      | Train: Loss 0.48100, R2 0.86502, RMSE 0.69026                     | Test: Loss 0.72210, R2 0.80205, RMSE 0.84217\n",
      "Epoch [176/500]      | Train: Loss 0.46334, R2 0.86727, RMSE 0.67895                     | Test: Loss 0.71081, R2 0.80123, RMSE 0.83343\n",
      "Epoch [177/500]      | Train: Loss 0.48542, R2 0.86277, RMSE 0.69414                     | Test: Loss 0.72781, R2 0.79939, RMSE 0.84696\n",
      "Epoch [178/500]      | Train: Loss 0.47500, R2 0.86537, RMSE 0.68746                     | Test: Loss 0.77197, R2 0.79860, RMSE 0.86918\n",
      "Epoch [179/500]      | Train: Loss 0.46687, R2 0.86929, RMSE 0.68158                     | Test: Loss 0.68187, R2 0.81358, RMSE 0.81655\n",
      "Epoch [180/500]      | Train: Loss 0.47112, R2 0.86754, RMSE 0.68443                     | Test: Loss 0.73170, R2 0.79842, RMSE 0.84741\n",
      "Epoch [181/500]      | Train: Loss 0.46637, R2 0.86907, RMSE 0.68065                     | Test: Loss 0.69010, R2 0.81154, RMSE 0.82360\n",
      "Epoch [182/500]      | Train: Loss 0.46023, R2 0.87130, RMSE 0.67557                     | Test: Loss 0.68860, R2 0.80972, RMSE 0.81857\n",
      "Epoch [183/500]      | Train: Loss 0.45109, R2 0.87356, RMSE 0.66958                     | Test: Loss 0.69991, R2 0.81000, RMSE 0.82768\n",
      "Epoch [184/500]      | Train: Loss 0.45610, R2 0.87138, RMSE 0.67372                     | Test: Loss 0.72136, R2 0.80098, RMSE 0.84339\n",
      "Epoch [185/500]      | Train: Loss 0.45487, R2 0.87059, RMSE 0.67257                     | Test: Loss 0.71451, R2 0.79823, RMSE 0.83778\n",
      "Epoch [186/500]      | Train: Loss 0.45694, R2 0.87198, RMSE 0.67466                     | Test: Loss 0.76816, R2 0.78019, RMSE 0.86682\n",
      "Epoch [187/500]      | Train: Loss 0.45669, R2 0.87090, RMSE 0.67436                     | Test: Loss 0.70844, R2 0.80122, RMSE 0.83005\n",
      "Epoch [188/500]      | Train: Loss 0.45309, R2 0.87213, RMSE 0.67094                     | Test: Loss 0.71252, R2 0.80388, RMSE 0.83618\n",
      "Epoch [189/500]      | Train: Loss 0.46109, R2 0.87065, RMSE 0.67717                     | Test: Loss 0.71191, R2 0.80645, RMSE 0.83449\n",
      "Epoch [190/500]      | Train: Loss 0.45598, R2 0.87216, RMSE 0.67290                     | Test: Loss 0.71518, R2 0.79330, RMSE 0.82923\n",
      "Epoch [191/500]      | Train: Loss 0.44592, R2 0.87494, RMSE 0.66587                     | Test: Loss 0.74803, R2 0.79934, RMSE 0.85790\n",
      "Epoch [192/500]      | Train: Loss 0.44552, R2 0.87509, RMSE 0.66570                     | Test: Loss 0.74361, R2 0.78643, RMSE 0.85476\n",
      "Epoch [193/500]      | Train: Loss 0.43279, R2 0.87730, RMSE 0.65635                     | Test: Loss 0.76902, R2 0.79383, RMSE 0.86844\n",
      "Epoch [194/500]      | Train: Loss 0.44329, R2 0.87552, RMSE 0.66366                     | Test: Loss 0.73287, R2 0.79868, RMSE 0.84926\n",
      "Epoch [195/500]      | Train: Loss 0.44750, R2 0.87420, RMSE 0.66743                     | Test: Loss 0.70617, R2 0.80616, RMSE 0.83083\n",
      "Epoch [196/500]      | Train: Loss 0.43613, R2 0.87692, RMSE 0.65862                     | Test: Loss 0.72652, R2 0.79595, RMSE 0.84334\n",
      "Epoch [197/500]      | Train: Loss 0.43914, R2 0.87519, RMSE 0.66091                     | Test: Loss 0.71011, R2 0.81234, RMSE 0.82933\n",
      "Epoch [198/500]      | Train: Loss 0.43229, R2 0.87750, RMSE 0.65554                     | Test: Loss 0.76675, R2 0.79307, RMSE 0.86304\n",
      "Epoch [199/500]      | Train: Loss 0.42843, R2 0.87972, RMSE 0.65216                     | Test: Loss 0.71452, R2 0.79809, RMSE 0.83602\n",
      "Epoch [200/500]      | Train: Loss 0.42689, R2 0.87900, RMSE 0.65212                     | Test: Loss 0.77443, R2 0.79098, RMSE 0.86967\n",
      "Epoch [201/500]      | Train: Loss 0.44221, R2 0.87695, RMSE 0.66256                     | Test: Loss 0.76823, R2 0.80158, RMSE 0.86584\n",
      "Epoch [202/500]      | Train: Loss 0.43118, R2 0.87907, RMSE 0.65470                     | Test: Loss 0.79027, R2 0.77803, RMSE 0.87453\n",
      "Epoch [203/500]      | Train: Loss 0.42797, R2 0.87897, RMSE 0.65274                     | Test: Loss 0.74688, R2 0.79612, RMSE 0.85567\n",
      "Epoch [204/500]      | Train: Loss 0.43797, R2 0.87580, RMSE 0.66012                     | Test: Loss 0.82811, R2 0.75807, RMSE 0.90060\n",
      "Epoch [205/500]      | Train: Loss 0.43977, R2 0.87475, RMSE 0.66170                     | Test: Loss 0.74227, R2 0.79038, RMSE 0.85375\n",
      "Epoch [206/500]      | Train: Loss 0.42334, R2 0.88161, RMSE 0.64875                     | Test: Loss 0.73595, R2 0.79354, RMSE 0.84905\n",
      "Epoch [207/500]      | Train: Loss 0.41764, R2 0.88146, RMSE 0.64507                     | Test: Loss 0.71645, R2 0.80140, RMSE 0.84102\n",
      "Epoch [208/500]      | Train: Loss 0.41504, R2 0.88288, RMSE 0.64276                     | Test: Loss 0.71936, R2 0.80208, RMSE 0.83919\n",
      "Epoch [209/500]      | Train: Loss 0.41514, R2 0.88146, RMSE 0.64304                     | Test: Loss 0.71949, R2 0.79914, RMSE 0.84244\n",
      "Epoch [210/500]      | Train: Loss 0.41718, R2 0.88302, RMSE 0.64425                     | Test: Loss 0.69694, R2 0.80698, RMSE 0.82047\n",
      "Epoch [211/500]      | Train: Loss 0.41292, R2 0.88355, RMSE 0.64098                     | Test: Loss 0.80817, R2 0.79470, RMSE 0.88121\n",
      "Epoch [212/500]      | Train: Loss 0.40863, R2 0.88466, RMSE 0.63769                     | Test: Loss 0.76504, R2 0.78004, RMSE 0.86820\n",
      "Epoch [213/500]      | Train: Loss 0.40893, R2 0.88475, RMSE 0.63818                     | Test: Loss 0.71158, R2 0.79975, RMSE 0.83397\n",
      "Epoch [214/500]      | Train: Loss 0.41983, R2 0.88195, RMSE 0.64610                     | Test: Loss 0.72041, R2 0.80245, RMSE 0.83814\n",
      "Epoch [215/500]      | Train: Loss 0.41821, R2 0.88328, RMSE 0.64450                     | Test: Loss 0.72861, R2 0.80623, RMSE 0.84706\n",
      "Epoch [216/500]      | Train: Loss 0.40112, R2 0.88688, RMSE 0.63177                     | Test: Loss 0.84148, R2 0.79439, RMSE 0.89797\n",
      "Epoch [217/500]      | Train: Loss 0.40877, R2 0.88408, RMSE 0.63758                     | Test: Loss 0.73376, R2 0.79511, RMSE 0.84831\n",
      "Epoch [218/500]      | Train: Loss 0.39966, R2 0.88671, RMSE 0.63042                     | Test: Loss 0.72452, R2 0.80312, RMSE 0.84262\n",
      "Epoch [219/500]      | Train: Loss 0.39975, R2 0.88582, RMSE 0.63067                     | Test: Loss 0.79117, R2 0.79250, RMSE 0.88165\n",
      "Epoch [220/500]      | Train: Loss 0.39636, R2 0.88819, RMSE 0.62830                     | Test: Loss 0.73632, R2 0.80215, RMSE 0.85078\n",
      "Epoch [221/500]      | Train: Loss 0.39911, R2 0.88664, RMSE 0.62983                     | Test: Loss 0.72793, R2 0.80490, RMSE 0.84611\n",
      "Epoch [222/500]      | Train: Loss 0.39040, R2 0.89050, RMSE 0.62350                     | Test: Loss 0.78438, R2 0.79904, RMSE 0.87543\n",
      "Epoch [223/500]      | Train: Loss 0.39371, R2 0.88807, RMSE 0.62608                     | Test: Loss 0.72207, R2 0.80601, RMSE 0.84178\n",
      "Epoch [224/500]      | Train: Loss 0.38719, R2 0.89041, RMSE 0.62090                     | Test: Loss 0.70709, R2 0.80879, RMSE 0.82700\n",
      "Epoch [225/500]      | Train: Loss 0.39049, R2 0.88913, RMSE 0.62360                     | Test: Loss 0.75848, R2 0.79534, RMSE 0.86175\n",
      "Epoch [226/500]      | Train: Loss 0.39904, R2 0.88890, RMSE 0.63003                     | Test: Loss 0.79975, R2 0.76538, RMSE 0.88968\n",
      "Epoch [227/500]      | Train: Loss 0.40149, R2 0.88651, RMSE 0.63218                     | Test: Loss 0.74037, R2 0.79073, RMSE 0.84873\n",
      "Epoch [228/500]      | Train: Loss 0.39172, R2 0.88863, RMSE 0.62454                     | Test: Loss 0.74156, R2 0.78932, RMSE 0.85349\n",
      "Epoch [229/500]      | Train: Loss 0.38371, R2 0.89122, RMSE 0.61782                     | Test: Loss 0.71267, R2 0.80543, RMSE 0.83179\n",
      "Epoch [230/500]      | Train: Loss 0.38618, R2 0.89069, RMSE 0.61972                     | Test: Loss 0.72929, R2 0.79746, RMSE 0.84599\n",
      "Epoch [231/500]      | Train: Loss 0.38990, R2 0.89026, RMSE 0.62294                     | Test: Loss 0.72662, R2 0.80505, RMSE 0.84223\n",
      "Epoch [232/500]      | Train: Loss 0.39764, R2 0.88714, RMSE 0.62868                     | Test: Loss 0.77047, R2 0.79061, RMSE 0.87068\n",
      "Epoch [233/500]      | Train: Loss 0.38505, R2 0.89158, RMSE 0.61957                     | Test: Loss 0.72783, R2 0.80492, RMSE 0.84738\n",
      "Epoch [234/500]      | Train: Loss 0.38182, R2 0.89183, RMSE 0.61661                     | Test: Loss 0.71358, R2 0.80167, RMSE 0.83217\n",
      "Epoch [235/500]      | Train: Loss 0.37981, R2 0.89191, RMSE 0.61518                     | Test: Loss 0.73978, R2 0.79520, RMSE 0.85120\n",
      "Epoch [236/500]      | Train: Loss 0.37604, R2 0.89349, RMSE 0.61209                     | Test: Loss 0.71913, R2 0.80205, RMSE 0.83227\n",
      "Epoch [237/500]      | Train: Loss 0.37271, R2 0.89463, RMSE 0.60931                     | Test: Loss 0.73972, R2 0.79752, RMSE 0.84988\n",
      "Epoch [238/500]      | Train: Loss 0.39415, R2 0.88897, RMSE 0.62608                     | Test: Loss 0.78372, R2 0.76948, RMSE 0.87891\n",
      "Epoch [239/500]      | Train: Loss 0.36799, R2 0.89605, RMSE 0.60569                     | Test: Loss 0.72170, R2 0.80487, RMSE 0.84187\n",
      "Epoch [240/500]      | Train: Loss 0.37219, R2 0.89502, RMSE 0.60877                     | Test: Loss 0.75829, R2 0.78221, RMSE 0.86294\n",
      "Epoch [241/500]      | Train: Loss 0.36569, R2 0.89745, RMSE 0.60342                     | Test: Loss 0.76545, R2 0.78447, RMSE 0.86594\n",
      "Epoch [242/500]      | Train: Loss 0.37144, R2 0.89444, RMSE 0.60801                     | Test: Loss 0.78933, R2 0.77421, RMSE 0.87626\n",
      "Epoch [243/500]      | Train: Loss 0.36463, R2 0.89731, RMSE 0.60222                     | Test: Loss 0.74349, R2 0.80120, RMSE 0.84895\n",
      "Epoch [244/500]      | Train: Loss 0.37413, R2 0.89374, RMSE 0.61015                     | Test: Loss 0.75595, R2 0.78471, RMSE 0.86082\n",
      "Epoch [245/500]      | Train: Loss 0.36422, R2 0.89607, RMSE 0.60249                     | Test: Loss 0.76565, R2 0.79006, RMSE 0.86984\n",
      "Epoch [246/500]      | Train: Loss 0.37620, R2 0.89448, RMSE 0.61191                     | Test: Loss 0.76564, R2 0.78391, RMSE 0.86708\n",
      "Epoch [247/500]      | Train: Loss 0.36168, R2 0.89796, RMSE 0.59998                     | Test: Loss 0.74091, R2 0.79585, RMSE 0.85537\n",
      "Epoch [248/500]      | Train: Loss 0.36665, R2 0.89614, RMSE 0.60405                     | Test: Loss 0.75091, R2 0.79371, RMSE 0.85677\n",
      "Epoch [249/500]      | Train: Loss 0.36002, R2 0.89863, RMSE 0.59848                     | Test: Loss 0.73971, R2 0.79872, RMSE 0.85013\n",
      "Epoch [250/500]      | Train: Loss 0.35228, R2 0.90003, RMSE 0.59227                     | Test: Loss 0.74805, R2 0.80158, RMSE 0.85673\n",
      "Epoch [251/500]      | Train: Loss 0.35161, R2 0.90050, RMSE 0.59194                     | Test: Loss 0.76951, R2 0.79285, RMSE 0.86980\n",
      "Epoch [252/500]      | Train: Loss 0.35488, R2 0.90001, RMSE 0.59449                     | Test: Loss 0.72622, R2 0.80078, RMSE 0.84149\n",
      "Epoch [253/500]      | Train: Loss 0.35946, R2 0.89918, RMSE 0.59828                     | Test: Loss 0.80920, R2 0.78546, RMSE 0.88795\n",
      "Epoch [254/500]      | Train: Loss 0.35068, R2 0.90067, RMSE 0.59072                     | Test: Loss 0.75401, R2 0.77989, RMSE 0.85851\n",
      "Epoch [255/500]      | Train: Loss 0.34544, R2 0.90295, RMSE 0.58621                     | Test: Loss 0.73644, R2 0.79581, RMSE 0.84626\n",
      "Epoch [256/500]      | Train: Loss 0.35732, R2 0.89922, RMSE 0.59648                     | Test: Loss 0.85565, R2 0.77781, RMSE 0.90491\n",
      "Epoch [257/500]      | Train: Loss 0.35084, R2 0.89997, RMSE 0.59133                     | Test: Loss 0.77858, R2 0.77338, RMSE 0.87344\n",
      "Epoch [258/500]      | Train: Loss 0.35280, R2 0.90098, RMSE 0.59288                     | Test: Loss 0.80848, R2 0.78683, RMSE 0.89280\n",
      "Epoch [259/500]      | Train: Loss 0.34145, R2 0.90405, RMSE 0.58310                     | Test: Loss 0.75195, R2 0.79262, RMSE 0.85856\n",
      "Epoch [260/500]      | Train: Loss 0.34515, R2 0.90208, RMSE 0.58640                     | Test: Loss 0.75993, R2 0.76842, RMSE 0.86224\n",
      "Epoch [261/500]      | Train: Loss 0.35038, R2 0.90167, RMSE 0.59069                     | Test: Loss 0.74486, R2 0.79667, RMSE 0.84833\n",
      "Epoch [262/500]      | Train: Loss 0.34485, R2 0.90254, RMSE 0.58620                     | Test: Loss 0.73828, R2 0.79627, RMSE 0.85059\n",
      "Epoch [263/500]      | Train: Loss 0.34715, R2 0.90159, RMSE 0.58816                     | Test: Loss 0.74072, R2 0.79546, RMSE 0.84781\n",
      "Epoch [264/500]      | Train: Loss 0.33869, R2 0.90359, RMSE 0.58099                     | Test: Loss 0.76372, R2 0.79299, RMSE 0.86202\n",
      "Epoch [265/500]      | Train: Loss 0.34208, R2 0.90299, RMSE 0.58340                     | Test: Loss 0.75932, R2 0.78544, RMSE 0.85807\n",
      "Epoch [266/500]      | Train: Loss 0.33719, R2 0.90513, RMSE 0.57940                     | Test: Loss 0.76383, R2 0.78936, RMSE 0.86617\n",
      "Epoch [267/500]      | Train: Loss 0.33329, R2 0.90624, RMSE 0.57626                     | Test: Loss 0.78345, R2 0.76844, RMSE 0.87869\n",
      "Epoch [268/500]      | Train: Loss 0.33174, R2 0.90641, RMSE 0.57455                     | Test: Loss 0.74976, R2 0.78876, RMSE 0.85702\n",
      "Epoch [269/500]      | Train: Loss 0.34018, R2 0.90441, RMSE 0.58197                     | Test: Loss 0.86541, R2 0.75652, RMSE 0.91115\n",
      "Epoch [270/500]      | Train: Loss 0.33997, R2 0.90301, RMSE 0.58156                     | Test: Loss 0.84414, R2 0.77271, RMSE 0.90865\n",
      "Epoch [271/500]      | Train: Loss 0.33475, R2 0.90421, RMSE 0.57745                     | Test: Loss 0.76138, R2 0.78655, RMSE 0.86533\n",
      "Epoch [272/500]      | Train: Loss 0.33131, R2 0.90560, RMSE 0.57460                     | Test: Loss 0.74576, R2 0.79138, RMSE 0.85315\n",
      "Epoch [273/500]      | Train: Loss 0.32396, R2 0.90777, RMSE 0.56822                     | Test: Loss 0.76360, R2 0.77812, RMSE 0.86664\n",
      "Epoch [274/500]      | Train: Loss 0.32931, R2 0.90579, RMSE 0.57273                     | Test: Loss 0.77165, R2 0.77497, RMSE 0.86948\n",
      "Epoch [275/500]      | Train: Loss 0.32951, R2 0.90724, RMSE 0.57268                     | Test: Loss 0.80651, R2 0.78130, RMSE 0.89058\n",
      "Epoch [276/500]      | Train: Loss 0.32904, R2 0.90748, RMSE 0.57212                     | Test: Loss 0.74136, R2 0.79983, RMSE 0.84688\n",
      "Epoch [277/500]      | Train: Loss 0.33142, R2 0.90627, RMSE 0.57385                     | Test: Loss 0.75084, R2 0.78971, RMSE 0.85698\n",
      "Epoch [278/500]      | Train: Loss 0.32719, R2 0.90822, RMSE 0.57071                     | Test: Loss 0.79121, R2 0.78832, RMSE 0.87850\n",
      "Epoch [279/500]      | Train: Loss 0.32269, R2 0.90846, RMSE 0.56672                     | Test: Loss 0.82429, R2 0.76513, RMSE 0.90205\n",
      "Epoch [280/500]      | Train: Loss 0.31978, R2 0.90937, RMSE 0.56437                     | Test: Loss 0.76935, R2 0.78987, RMSE 0.86286\n",
      "Epoch [281/500]      | Train: Loss 0.32434, R2 0.90868, RMSE 0.56836                     | Test: Loss 0.78501, R2 0.77815, RMSE 0.88052\n",
      "Epoch [282/500]      | Train: Loss 0.33078, R2 0.90602, RMSE 0.57405                     | Test: Loss 0.79490, R2 0.78812, RMSE 0.88664\n",
      "Epoch [283/500]      | Train: Loss 0.32050, R2 0.90942, RMSE 0.56520                     | Test: Loss 0.74291, R2 0.78855, RMSE 0.85337\n",
      "Epoch [284/500]      | Train: Loss 0.32177, R2 0.90874, RMSE 0.56623                     | Test: Loss 0.77375, R2 0.77691, RMSE 0.87264\n",
      "Epoch [285/500]      | Train: Loss 0.31190, R2 0.91181, RMSE 0.55692                     | Test: Loss 0.77478, R2 0.78685, RMSE 0.87001\n",
      "Epoch [286/500]      | Train: Loss 0.32198, R2 0.90960, RMSE 0.56671                     | Test: Loss 0.77587, R2 0.78397, RMSE 0.87223\n",
      "Epoch [287/500]      | Train: Loss 0.32768, R2 0.90818, RMSE 0.57096                     | Test: Loss 0.79266, R2 0.78681, RMSE 0.87948\n",
      "Epoch [288/500]      | Train: Loss 0.32142, R2 0.90869, RMSE 0.56509                     | Test: Loss 0.76308, R2 0.79371, RMSE 0.86694\n",
      "Epoch [289/500]      | Train: Loss 0.30791, R2 0.91326, RMSE 0.55407                     | Test: Loss 0.87025, R2 0.77960, RMSE 0.91641\n",
      "Epoch [290/500]      | Train: Loss 0.31243, R2 0.91213, RMSE 0.55757                     | Test: Loss 0.75907, R2 0.78854, RMSE 0.86044\n",
      "Epoch [291/500]      | Train: Loss 0.30597, R2 0.91315, RMSE 0.55210                     | Test: Loss 0.78636, R2 0.79023, RMSE 0.87978\n",
      "Epoch [292/500]      | Train: Loss 0.30376, R2 0.91510, RMSE 0.55023                     | Test: Loss 0.80532, R2 0.78296, RMSE 0.88675\n",
      "Epoch [293/500]      | Train: Loss 0.30922, R2 0.91302, RMSE 0.55483                     | Test: Loss 0.77223, R2 0.78715, RMSE 0.86622\n",
      "Epoch [294/500]      | Train: Loss 0.30772, R2 0.91257, RMSE 0.55380                     | Test: Loss 0.78644, R2 0.77658, RMSE 0.87905\n",
      "Epoch [295/500]      | Train: Loss 0.30474, R2 0.91483, RMSE 0.55104                     | Test: Loss 0.73762, R2 0.79723, RMSE 0.84836\n",
      "Epoch [296/500]      | Train: Loss 0.30576, R2 0.91374, RMSE 0.55166                     | Test: Loss 0.78333, R2 0.77753, RMSE 0.87833\n",
      "Epoch [297/500]      | Train: Loss 0.30176, R2 0.91456, RMSE 0.54814                     | Test: Loss 0.76176, R2 0.78786, RMSE 0.86261\n",
      "Epoch [298/500]      | Train: Loss 0.31446, R2 0.91170, RMSE 0.55936                     | Test: Loss 0.83633, R2 0.78279, RMSE 0.90486\n",
      "Epoch [299/500]      | Train: Loss 0.31332, R2 0.91093, RMSE 0.55861                     | Test: Loss 0.78201, R2 0.77655, RMSE 0.87292\n",
      "Epoch [300/500]      | Train: Loss 0.30053, R2 0.91549, RMSE 0.54725                     | Test: Loss 0.77247, R2 0.78099, RMSE 0.87278\n",
      "Epoch [301/500]      | Train: Loss 0.30889, R2 0.91331, RMSE 0.55489                     | Test: Loss 1.22982, R2 0.73397, RMSE 0.99827\n",
      "Epoch [302/500]      | Train: Loss 0.29697, R2 0.91658, RMSE 0.54387                     | Test: Loss 0.76105, R2 0.78998, RMSE 0.86681\n",
      "Epoch [303/500]      | Train: Loss 0.29221, R2 0.91751, RMSE 0.53933                     | Test: Loss 0.76140, R2 0.78864, RMSE 0.85581\n",
      "Epoch [304/500]      | Train: Loss 0.29412, R2 0.91705, RMSE 0.54131                     | Test: Loss 0.82080, R2 0.78844, RMSE 0.90133\n",
      "Epoch [305/500]      | Train: Loss 0.29151, R2 0.91792, RMSE 0.53899                     | Test: Loss 0.74554, R2 0.78996, RMSE 0.84828\n",
      "Epoch [306/500]      | Train: Loss 0.29050, R2 0.91781, RMSE 0.53808                     | Test: Loss 0.77104, R2 0.78331, RMSE 0.87043\n",
      "Epoch [307/500]      | Train: Loss 0.30571, R2 0.91320, RMSE 0.55160                     | Test: Loss 0.80096, R2 0.78018, RMSE 0.88987\n",
      "Epoch [308/500]      | Train: Loss 0.29851, R2 0.91525, RMSE 0.54516                     | Test: Loss 0.80768, R2 0.78328, RMSE 0.89387\n",
      "Epoch [309/500]      | Train: Loss 0.29669, R2 0.91604, RMSE 0.54367                     | Test: Loss 0.82515, R2 0.77934, RMSE 0.89825\n",
      "Epoch [310/500]      | Train: Loss 0.28455, R2 0.91936, RMSE 0.53262                     | Test: Loss 0.76445, R2 0.78480, RMSE 0.86480\n",
      "Epoch [311/500]      | Train: Loss 0.29037, R2 0.91684, RMSE 0.53783                     | Test: Loss 0.75904, R2 0.79618, RMSE 0.86034\n",
      "Epoch [312/500]      | Train: Loss 0.28816, R2 0.91879, RMSE 0.53583                     | Test: Loss 0.78397, R2 0.78537, RMSE 0.87425\n",
      "Epoch [313/500]      | Train: Loss 0.28853, R2 0.91788, RMSE 0.53619                     | Test: Loss 0.76491, R2 0.79069, RMSE 0.86516\n",
      "Epoch [314/500]      | Train: Loss 0.30094, R2 0.91423, RMSE 0.54705                     | Test: Loss 0.86346, R2 0.77456, RMSE 0.91514\n",
      "Epoch [315/500]      | Train: Loss 0.28642, R2 0.91776, RMSE 0.53434                     | Test: Loss 0.77786, R2 0.78594, RMSE 0.87788\n",
      "Epoch [316/500]      | Train: Loss 0.27991, R2 0.92054, RMSE 0.52760                     | Test: Loss 0.78807, R2 0.78668, RMSE 0.88104\n",
      "Epoch [317/500]      | Train: Loss 0.28517, R2 0.91933, RMSE 0.53287                     | Test: Loss 0.79054, R2 0.77833, RMSE 0.88114\n",
      "Epoch [318/500]      | Train: Loss 0.28249, R2 0.91995, RMSE 0.53053                     | Test: Loss 0.79557, R2 0.77934, RMSE 0.87961\n",
      "Epoch [319/500]      | Train: Loss 0.29001, R2 0.91839, RMSE 0.53755                     | Test: Loss 0.86690, R2 0.77506, RMSE 0.91468\n",
      "Epoch [320/500]      | Train: Loss 0.28637, R2 0.91940, RMSE 0.53420                     | Test: Loss 0.78514, R2 0.78444, RMSE 0.87603\n",
      "Epoch [321/500]      | Train: Loss 0.28382, R2 0.92037, RMSE 0.53166                     | Test: Loss 0.76080, R2 0.79552, RMSE 0.85905\n",
      "Epoch [322/500]      | Train: Loss 0.28214, R2 0.92054, RMSE 0.53009                     | Test: Loss 0.78055, R2 0.76488, RMSE 0.87756\n",
      "Epoch [323/500]      | Train: Loss 0.27677, R2 0.92178, RMSE 0.52503                     | Test: Loss 0.87398, R2 0.77763, RMSE 0.92202\n",
      "Epoch [324/500]      | Train: Loss 0.27997, R2 0.92003, RMSE 0.52820                     | Test: Loss 0.78707, R2 0.78678, RMSE 0.87847\n",
      "Epoch [325/500]      | Train: Loss 0.28242, R2 0.91972, RMSE 0.53048                     | Test: Loss 0.79068, R2 0.77646, RMSE 0.88434\n",
      "Epoch [326/500]      | Train: Loss 0.27520, R2 0.92152, RMSE 0.52363                     | Test: Loss 0.78978, R2 0.78635, RMSE 0.87878\n",
      "Epoch [327/500]      | Train: Loss 0.27631, R2 0.92157, RMSE 0.52487                     | Test: Loss 0.82952, R2 0.76908, RMSE 0.89690\n",
      "Epoch [328/500]      | Train: Loss 0.28986, R2 0.91744, RMSE 0.53737                     | Test: Loss 0.77495, R2 0.78334, RMSE 0.86816\n",
      "Epoch [329/500]      | Train: Loss 0.26563, R2 0.92494, RMSE 0.51467                     | Test: Loss 0.81400, R2 0.77289, RMSE 0.89581\n",
      "Epoch [330/500]      | Train: Loss 0.27080, R2 0.92347, RMSE 0.51904                     | Test: Loss 0.82596, R2 0.78020, RMSE 0.90163\n",
      "Epoch [331/500]      | Train: Loss 0.27110, R2 0.92362, RMSE 0.51961                     | Test: Loss 0.81449, R2 0.77609, RMSE 0.89177\n",
      "Epoch [332/500]      | Train: Loss 0.26285, R2 0.92538, RMSE 0.51177                     | Test: Loss 0.78394, R2 0.78540, RMSE 0.87206\n",
      "Epoch [333/500]      | Train: Loss 0.27774, R2 0.92107, RMSE 0.52606                     | Test: Loss 0.78617, R2 0.78214, RMSE 0.87502\n",
      "Epoch [334/500]      | Train: Loss 0.27694, R2 0.92097, RMSE 0.52533                     | Test: Loss 0.81507, R2 0.78078, RMSE 0.89077\n",
      "Epoch [335/500]      | Train: Loss 0.27542, R2 0.92253, RMSE 0.52401                     | Test: Loss 0.80372, R2 0.77763, RMSE 0.89011\n",
      "Epoch [336/500]      | Train: Loss 0.27430, R2 0.92277, RMSE 0.52247                     | Test: Loss 0.79299, R2 0.78438, RMSE 0.87675\n",
      "Epoch [337/500]      | Train: Loss 0.26338, R2 0.92551, RMSE 0.51194                     | Test: Loss 0.81820, R2 0.76781, RMSE 0.89874\n",
      "Epoch [338/500]      | Train: Loss 0.26867, R2 0.92454, RMSE 0.51747                     | Test: Loss 0.79149, R2 0.77900, RMSE 0.88180\n",
      "Epoch [339/500]      | Train: Loss 0.26324, R2 0.92610, RMSE 0.51179                     | Test: Loss 0.81283, R2 0.78359, RMSE 0.89583\n",
      "Epoch [340/500]      | Train: Loss 0.26366, R2 0.92580, RMSE 0.51245                     | Test: Loss 0.78689, R2 0.78378, RMSE 0.87699\n",
      "Epoch [341/500]      | Train: Loss 0.26057, R2 0.92669, RMSE 0.50957                     | Test: Loss 0.78954, R2 0.78061, RMSE 0.88066\n",
      "Epoch [342/500]      | Train: Loss 0.26132, R2 0.92601, RMSE 0.51030                     | Test: Loss 0.83518, R2 0.77161, RMSE 0.90404\n",
      "Epoch [343/500]      | Train: Loss 0.25621, R2 0.92740, RMSE 0.50507                     | Test: Loss 0.78511, R2 0.78503, RMSE 0.87348\n",
      "Epoch [344/500]      | Train: Loss 0.25803, R2 0.92750, RMSE 0.50717                     | Test: Loss 0.83542, R2 0.77890, RMSE 0.90448\n",
      "Epoch [345/500]      | Train: Loss 0.26006, R2 0.92672, RMSE 0.50920                     | Test: Loss 0.78674, R2 0.78374, RMSE 0.86912\n",
      "Epoch [346/500]      | Train: Loss 0.25782, R2 0.92766, RMSE 0.50644                     | Test: Loss 0.87595, R2 0.76664, RMSE 0.92871\n",
      "Epoch [347/500]      | Train: Loss 0.26176, R2 0.92676, RMSE 0.51061                     | Test: Loss 0.81344, R2 0.77434, RMSE 0.89283\n",
      "Epoch [348/500]      | Train: Loss 0.26103, R2 0.92553, RMSE 0.51022                     | Test: Loss 0.79774, R2 0.77941, RMSE 0.88317\n",
      "Epoch [349/500]      | Train: Loss 0.25648, R2 0.92744, RMSE 0.50550                     | Test: Loss 0.80019, R2 0.78353, RMSE 0.88323\n",
      "Epoch [350/500]      | Train: Loss 0.26094, R2 0.92580, RMSE 0.50974                     | Test: Loss 0.83301, R2 0.73513, RMSE 0.90117\n",
      "Epoch [351/500]      | Train: Loss 0.26098, R2 0.92635, RMSE 0.51004                     | Test: Loss 0.80059, R2 0.77992, RMSE 0.88323\n",
      "Epoch [352/500]      | Train: Loss 0.25192, R2 0.92889, RMSE 0.50095                     | Test: Loss 0.83070, R2 0.77607, RMSE 0.90368\n",
      "Epoch [353/500]      | Train: Loss 0.25005, R2 0.92952, RMSE 0.49907                     | Test: Loss 0.80459, R2 0.77811, RMSE 0.88860\n",
      "Epoch [354/500]      | Train: Loss 0.24715, R2 0.92983, RMSE 0.49632                     | Test: Loss 0.79850, R2 0.76665, RMSE 0.88535\n",
      "Epoch [355/500]      | Train: Loss 0.24354, R2 0.93076, RMSE 0.49277                     | Test: Loss 0.78412, R2 0.77956, RMSE 0.87395\n",
      "Epoch [356/500]      | Train: Loss 0.24312, R2 0.93137, RMSE 0.49224                     | Test: Loss 0.80850, R2 0.77868, RMSE 0.89196\n",
      "Epoch [357/500]      | Train: Loss 0.25397, R2 0.92781, RMSE 0.50291                     | Test: Loss 0.95715, R2 0.75939, RMSE 0.95128\n",
      "Epoch [358/500]      | Train: Loss 0.24950, R2 0.92955, RMSE 0.49866                     | Test: Loss 0.79316, R2 0.78259, RMSE 0.88140\n",
      "Epoch [359/500]      | Train: Loss 0.25050, R2 0.92906, RMSE 0.49959                     | Test: Loss 0.80695, R2 0.78027, RMSE 0.89163\n",
      "Epoch [360/500]      | Train: Loss 0.25235, R2 0.92820, RMSE 0.50137                     | Test: Loss 0.78421, R2 0.78520, RMSE 0.87226\n",
      "Epoch [361/500]      | Train: Loss 0.25426, R2 0.92871, RMSE 0.50347                     | Test: Loss 0.77123, R2 0.78050, RMSE 0.86845\n",
      "Epoch [362/500]      | Train: Loss 0.24814, R2 0.93010, RMSE 0.49720                     | Test: Loss 0.81048, R2 0.77520, RMSE 0.89334\n",
      "Epoch [363/500]      | Train: Loss 0.25048, R2 0.92856, RMSE 0.49968                     | Test: Loss 0.82332, R2 0.77168, RMSE 0.90009\n",
      "Epoch [364/500]      | Train: Loss 0.24562, R2 0.93060, RMSE 0.49472                     | Test: Loss 0.78642, R2 0.78132, RMSE 0.87750\n",
      "Epoch [365/500]      | Train: Loss 0.24419, R2 0.93034, RMSE 0.49338                     | Test: Loss 0.81287, R2 0.78344, RMSE 0.89393\n",
      "Epoch [366/500]      | Train: Loss 0.24191, R2 0.93184, RMSE 0.49101                     | Test: Loss 0.76061, R2 0.78974, RMSE 0.85844\n",
      "Epoch [367/500]      | Train: Loss 0.24098, R2 0.93209, RMSE 0.49027                     | Test: Loss 0.76445, R2 0.79284, RMSE 0.86259\n",
      "Epoch [368/500]      | Train: Loss 0.23841, R2 0.93230, RMSE 0.48756                     | Test: Loss 0.78972, R2 0.77584, RMSE 0.88184\n",
      "Epoch [369/500]      | Train: Loss 0.24010, R2 0.93223, RMSE 0.48908                     | Test: Loss 0.84058, R2 0.77801, RMSE 0.90962\n",
      "Epoch [370/500]      | Train: Loss 0.24375, R2 0.93147, RMSE 0.49258                     | Test: Loss 0.83012, R2 0.78082, RMSE 0.90071\n",
      "Epoch [371/500]      | Train: Loss 0.24575, R2 0.93078, RMSE 0.49474                     | Test: Loss 0.78764, R2 0.76449, RMSE 0.88149\n",
      "Epoch [372/500]      | Train: Loss 0.24308, R2 0.93143, RMSE 0.49216                     | Test: Loss 0.81232, R2 0.78664, RMSE 0.89419\n",
      "Epoch [373/500]      | Train: Loss 0.23961, R2 0.93210, RMSE 0.48878                     | Test: Loss 0.79557, R2 0.78596, RMSE 0.88342\n",
      "Epoch [374/500]      | Train: Loss 0.23415, R2 0.93435, RMSE 0.48311                     | Test: Loss 0.77642, R2 0.78707, RMSE 0.87188\n",
      "Epoch [375/500]      | Train: Loss 0.23523, R2 0.93410, RMSE 0.48438                     | Test: Loss 0.82257, R2 0.78123, RMSE 0.89980\n",
      "Epoch [376/500]      | Train: Loss 0.23632, R2 0.93304, RMSE 0.48524                     | Test: Loss 0.82437, R2 0.76814, RMSE 0.90198\n",
      "Epoch [377/500]      | Train: Loss 0.23741, R2 0.93260, RMSE 0.48627                     | Test: Loss 0.81048, R2 0.77685, RMSE 0.89377\n",
      "Epoch [378/500]      | Train: Loss 0.23603, R2 0.93311, RMSE 0.48505                     | Test: Loss 0.83898, R2 0.75466, RMSE 0.90729\n",
      "Epoch [379/500]      | Train: Loss 0.23202, R2 0.93493, RMSE 0.48093                     | Test: Loss 0.77972, R2 0.78350, RMSE 0.86996\n",
      "Epoch [380/500]      | Train: Loss 0.23458, R2 0.93370, RMSE 0.48357                     | Test: Loss 0.83767, R2 0.76717, RMSE 0.90704\n",
      "Epoch [381/500]      | Train: Loss 0.24065, R2 0.93066, RMSE 0.48976                     | Test: Loss 0.79790, R2 0.78293, RMSE 0.88334\n",
      "Epoch [382/500]      | Train: Loss 0.23793, R2 0.93232, RMSE 0.48683                     | Test: Loss 0.78219, R2 0.78722, RMSE 0.87504\n",
      "Epoch [383/500]      | Train: Loss 0.23285, R2 0.93385, RMSE 0.48191                     | Test: Loss 0.81259, R2 0.77291, RMSE 0.89303\n",
      "Epoch [384/500]      | Train: Loss 0.22807, R2 0.93588, RMSE 0.47674                     | Test: Loss 0.82024, R2 0.77458, RMSE 0.89399\n",
      "Epoch [385/500]      | Train: Loss 0.22639, R2 0.93588, RMSE 0.47495                     | Test: Loss 0.78311, R2 0.78346, RMSE 0.87553\n",
      "Epoch [386/500]      | Train: Loss 0.22334, R2 0.93655, RMSE 0.47187                     | Test: Loss 0.84749, R2 0.77066, RMSE 0.90990\n",
      "Epoch [387/500]      | Train: Loss 0.23545, R2 0.93348, RMSE 0.48433                     | Test: Loss 0.79172, R2 0.77948, RMSE 0.87750\n",
      "Epoch [388/500]      | Train: Loss 0.24504, R2 0.93042, RMSE 0.49417                     | Test: Loss 0.77988, R2 0.78431, RMSE 0.86536\n",
      "Epoch [389/500]      | Train: Loss 0.22964, R2 0.93502, RMSE 0.47813                     | Test: Loss 0.83928, R2 0.77856, RMSE 0.90803\n",
      "Epoch [390/500]      | Train: Loss 0.22454, R2 0.93625, RMSE 0.47311                     | Test: Loss 0.86913, R2 0.76940, RMSE 0.92739\n",
      "Epoch [391/500]      | Train: Loss 0.23292, R2 0.93354, RMSE 0.48165                     | Test: Loss 0.79706, R2 0.78070, RMSE 0.87755\n",
      "Epoch [392/500]      | Train: Loss 0.22888, R2 0.93547, RMSE 0.47736                     | Test: Loss 0.81584, R2 0.75561, RMSE 0.89559\n",
      "Epoch [393/500]      | Train: Loss 0.22854, R2 0.93556, RMSE 0.47733                     | Test: Loss 0.77586, R2 0.78391, RMSE 0.86982\n",
      "Epoch [394/500]      | Train: Loss 0.22111, R2 0.93740, RMSE 0.46962                     | Test: Loss 0.79645, R2 0.78068, RMSE 0.88072\n",
      "Epoch [395/500]      | Train: Loss 0.22146, R2 0.93724, RMSE 0.46989                     | Test: Loss 0.79076, R2 0.78188, RMSE 0.87672\n",
      "Epoch [396/500]      | Train: Loss 0.21548, R2 0.93858, RMSE 0.46315                     | Test: Loss 0.79828, R2 0.78624, RMSE 0.88220\n",
      "Epoch [397/500]      | Train: Loss 0.22241, R2 0.93660, RMSE 0.47084                     | Test: Loss 0.81274, R2 0.76956, RMSE 0.89658\n",
      "Epoch [398/500]      | Train: Loss 0.21770, R2 0.93840, RMSE 0.46584                     | Test: Loss 0.80055, R2 0.75977, RMSE 0.88445\n",
      "Epoch [399/500]      | Train: Loss 0.22058, R2 0.93756, RMSE 0.46885                     | Test: Loss 0.84357, R2 0.76748, RMSE 0.90893\n",
      "Epoch [400/500]      | Train: Loss 0.22344, R2 0.93630, RMSE 0.47166                     | Test: Loss 0.80629, R2 0.76797, RMSE 0.89188\n",
      "Epoch [401/500]      | Train: Loss 0.23280, R2 0.93383, RMSE 0.48148                     | Test: Loss 0.79903, R2 0.78776, RMSE 0.88639\n",
      "Epoch [402/500]      | Train: Loss 0.21707, R2 0.93851, RMSE 0.46506                     | Test: Loss 0.78374, R2 0.78500, RMSE 0.87456\n",
      "Epoch [403/500]      | Train: Loss 0.22286, R2 0.93705, RMSE 0.47149                     | Test: Loss 0.79748, R2 0.77112, RMSE 0.88508\n",
      "Epoch [404/500]      | Train: Loss 0.21919, R2 0.93759, RMSE 0.46742                     | Test: Loss 0.80749, R2 0.78474, RMSE 0.89087\n",
      "Epoch [405/500]      | Train: Loss 0.22349, R2 0.93727, RMSE 0.47169                     | Test: Loss 0.80416, R2 0.77135, RMSE 0.88896\n",
      "Epoch [406/500]      | Train: Loss 0.21839, R2 0.93798, RMSE 0.46661                     | Test: Loss 0.79120, R2 0.78555, RMSE 0.88061\n",
      "Epoch [407/500]      | Train: Loss 0.21691, R2 0.93900, RMSE 0.46488                     | Test: Loss 0.81143, R2 0.78433, RMSE 0.89470\n",
      "Epoch [408/500]      | Train: Loss 0.21098, R2 0.94062, RMSE 0.45874                     | Test: Loss 0.82215, R2 0.78046, RMSE 0.90212\n",
      "Epoch [409/500]      | Train: Loss 0.21258, R2 0.93952, RMSE 0.46030                     | Test: Loss 0.78744, R2 0.78584, RMSE 0.87837\n",
      "Epoch [410/500]      | Train: Loss 0.21260, R2 0.94024, RMSE 0.46022                     | Test: Loss 0.82761, R2 0.76670, RMSE 0.90311\n",
      "Epoch [411/500]      | Train: Loss 0.20514, R2 0.94228, RMSE 0.45216                     | Test: Loss 0.83333, R2 0.77191, RMSE 0.90761\n",
      "Epoch [412/500]      | Train: Loss 0.21344, R2 0.93970, RMSE 0.46124                     | Test: Loss 0.83384, R2 0.75592, RMSE 0.90832\n",
      "Epoch [413/500]      | Train: Loss 0.21446, R2 0.93947, RMSE 0.46222                     | Test: Loss 0.79930, R2 0.78668, RMSE 0.87911\n",
      "Epoch [414/500]      | Train: Loss 0.20676, R2 0.94070, RMSE 0.45380                     | Test: Loss 0.79521, R2 0.78205, RMSE 0.88196\n",
      "Epoch [415/500]      | Train: Loss 0.21040, R2 0.94019, RMSE 0.45779                     | Test: Loss 0.81940, R2 0.77872, RMSE 0.90038\n",
      "Epoch [416/500]      | Train: Loss 0.20879, R2 0.94098, RMSE 0.45621                     | Test: Loss 0.85621, R2 0.76641, RMSE 0.91950\n",
      "Epoch [417/500]      | Train: Loss 0.20286, R2 0.94252, RMSE 0.44934                     | Test: Loss 0.84535, R2 0.76819, RMSE 0.91372\n",
      "Epoch [418/500]      | Train: Loss 0.21671, R2 0.93882, RMSE 0.46417                     | Test: Loss 0.78394, R2 0.78282, RMSE 0.87156\n",
      "Epoch [419/500]      | Train: Loss 0.21118, R2 0.94007, RMSE 0.45869                     | Test: Loss 0.82431, R2 0.76526, RMSE 0.89639\n",
      "Epoch [420/500]      | Train: Loss 0.20893, R2 0.94108, RMSE 0.45646                     | Test: Loss 0.81838, R2 0.76988, RMSE 0.89289\n",
      "Epoch [421/500]      | Train: Loss 0.20487, R2 0.94220, RMSE 0.45203                     | Test: Loss 0.79183, R2 0.78606, RMSE 0.87651\n",
      "Epoch [422/500]      | Train: Loss 0.20588, R2 0.94225, RMSE 0.45231                     | Test: Loss 0.81442, R2 0.77684, RMSE 0.89834\n",
      "Epoch [423/500]      | Train: Loss 0.20430, R2 0.94256, RMSE 0.45113                     | Test: Loss 0.94241, R2 0.73667, RMSE 0.94883\n",
      "Epoch [424/500]      | Train: Loss 0.20321, R2 0.94271, RMSE 0.45007                     | Test: Loss 0.85492, R2 0.77270, RMSE 0.91498\n",
      "Epoch [425/500]      | Train: Loss 0.20431, R2 0.94203, RMSE 0.45121                     | Test: Loss 0.86444, R2 0.77494, RMSE 0.92311\n",
      "Epoch [426/500]      | Train: Loss 0.20785, R2 0.94141, RMSE 0.45512                     | Test: Loss 0.81699, R2 0.77940, RMSE 0.89146\n",
      "Epoch [427/500]      | Train: Loss 0.19853, R2 0.94414, RMSE 0.44505                     | Test: Loss 0.81479, R2 0.78037, RMSE 0.89562\n",
      "Epoch [428/500]      | Train: Loss 0.19727, R2 0.94440, RMSE 0.44355                     | Test: Loss 0.84651, R2 0.77451, RMSE 0.91434\n",
      "Epoch [429/500]      | Train: Loss 0.20042, R2 0.94318, RMSE 0.44684                     | Test: Loss 0.80550, R2 0.77860, RMSE 0.88630\n",
      "Epoch [430/500]      | Train: Loss 0.20269, R2 0.94266, RMSE 0.44945                     | Test: Loss 0.79376, R2 0.78271, RMSE 0.88407\n",
      "Epoch [431/500]      | Train: Loss 0.21059, R2 0.94025, RMSE 0.45775                     | Test: Loss 0.83158, R2 0.76126, RMSE 0.90621\n",
      "Epoch [432/500]      | Train: Loss 0.20421, R2 0.94213, RMSE 0.45101                     | Test: Loss 0.82309, R2 0.77542, RMSE 0.89937\n",
      "Epoch [433/500]      | Train: Loss 0.20752, R2 0.94111, RMSE 0.45477                     | Test: Loss 0.81514, R2 0.73560, RMSE 0.89597\n",
      "Epoch [434/500]      | Train: Loss 0.20010, R2 0.94336, RMSE 0.44661                     | Test: Loss 0.82346, R2 0.76302, RMSE 0.90032\n",
      "Epoch [435/500]      | Train: Loss 0.20137, R2 0.94262, RMSE 0.44807                     | Test: Loss 0.81069, R2 0.78177, RMSE 0.88738\n",
      "Epoch [436/500]      | Train: Loss 0.19634, R2 0.94484, RMSE 0.44225                     | Test: Loss 0.86292, R2 0.75410, RMSE 0.91787\n",
      "Epoch [437/500]      | Train: Loss 0.19772, R2 0.94424, RMSE 0.44393                     | Test: Loss 1.06694, R2 0.76999, RMSE 0.97149\n",
      "Epoch [438/500]      | Train: Loss 0.20023, R2 0.94241, RMSE 0.44682                     | Test: Loss 0.81473, R2 0.76174, RMSE 0.89336\n",
      "Epoch [439/500]      | Train: Loss 0.20362, R2 0.94259, RMSE 0.45030                     | Test: Loss 0.81495, R2 0.77993, RMSE 0.89881\n",
      "Epoch [440/500]      | Train: Loss 0.19622, R2 0.94436, RMSE 0.44208                     | Test: Loss 0.81422, R2 0.77267, RMSE 0.89537\n",
      "Epoch [441/500]      | Train: Loss 0.18697, R2 0.94686, RMSE 0.43177                     | Test: Loss 0.79225, R2 0.78408, RMSE 0.88042\n",
      "Epoch [442/500]      | Train: Loss 0.19780, R2 0.94407, RMSE 0.44412                     | Test: Loss 0.82839, R2 0.77575, RMSE 0.90370\n",
      "Epoch [443/500]      | Train: Loss 0.19120, R2 0.94596, RMSE 0.43638                     | Test: Loss 0.82454, R2 0.76930, RMSE 0.90412\n",
      "Epoch [444/500]      | Train: Loss 0.19415, R2 0.94523, RMSE 0.43966                     | Test: Loss 0.81016, R2 0.77900, RMSE 0.89234\n",
      "Epoch [445/500]      | Train: Loss 0.18973, R2 0.94643, RMSE 0.43510                     | Test: Loss 0.83075, R2 0.76853, RMSE 0.90622\n",
      "Epoch [446/500]      | Train: Loss 0.18848, R2 0.94688, RMSE 0.43349                     | Test: Loss 0.83177, R2 0.74893, RMSE 0.90575\n",
      "Epoch [447/500]      | Train: Loss 0.19006, R2 0.94595, RMSE 0.43514                     | Test: Loss 0.86451, R2 0.77238, RMSE 0.92148\n",
      "Epoch [448/500]      | Train: Loss 0.19627, R2 0.94458, RMSE 0.44209                     | Test: Loss 0.85811, R2 0.76748, RMSE 0.92016\n",
      "Epoch [449/500]      | Train: Loss 0.19105, R2 0.94612, RMSE 0.43602                     | Test: Loss 0.83227, R2 0.77186, RMSE 0.90526\n",
      "Epoch [450/500]      | Train: Loss 0.18576, R2 0.94741, RMSE 0.43015                     | Test: Loss 0.81134, R2 0.77771, RMSE 0.89453\n",
      "Epoch [451/500]      | Train: Loss 0.18727, R2 0.94707, RMSE 0.43206                     | Test: Loss 0.81962, R2 0.77934, RMSE 0.90017\n",
      "Epoch [452/500]      | Train: Loss 0.18664, R2 0.94746, RMSE 0.43112                     | Test: Loss 0.82456, R2 0.76264, RMSE 0.90239\n",
      "Epoch [453/500]      | Train: Loss 0.19287, R2 0.94544, RMSE 0.43817                     | Test: Loss 0.82235, R2 0.76815, RMSE 0.90251\n",
      "Epoch [454/500]      | Train: Loss 0.19169, R2 0.94572, RMSE 0.43710                     | Test: Loss 0.92585, R2 0.69208, RMSE 0.94216\n",
      "Epoch [455/500]      | Train: Loss 0.18737, R2 0.94725, RMSE 0.43218                     | Test: Loss 0.84352, R2 0.75899, RMSE 0.91464\n",
      "Epoch [456/500]      | Train: Loss 0.19108, R2 0.94616, RMSE 0.43642                     | Test: Loss 0.81092, R2 0.77039, RMSE 0.89091\n",
      "Epoch [457/500]      | Train: Loss 0.18757, R2 0.94710, RMSE 0.43227                     | Test: Loss 0.82022, R2 0.77915, RMSE 0.89971\n",
      "Epoch [458/500]      | Train: Loss 0.18806, R2 0.94624, RMSE 0.43335                     | Test: Loss 0.82313, R2 0.77618, RMSE 0.90093\n",
      "Epoch [459/500]      | Train: Loss 0.19758, R2 0.94398, RMSE 0.44374                     | Test: Loss 0.81629, R2 0.78252, RMSE 0.89042\n",
      "Epoch [460/500]      | Train: Loss 0.18700, R2 0.94740, RMSE 0.43140                     | Test: Loss 0.79946, R2 0.77013, RMSE 0.88856\n",
      "Epoch [461/500]      | Train: Loss 0.18539, R2 0.94784, RMSE 0.42967                     | Test: Loss 0.80071, R2 0.77846, RMSE 0.88552\n",
      "Epoch [462/500]      | Train: Loss 0.18683, R2 0.94766, RMSE 0.43144                     | Test: Loss 0.81018, R2 0.76909, RMSE 0.88637\n",
      "Epoch [463/500]      | Train: Loss 0.18283, R2 0.94767, RMSE 0.42693                     | Test: Loss 0.82192, R2 0.77349, RMSE 0.89652\n",
      "Epoch [464/500]      | Train: Loss 0.18536, R2 0.94782, RMSE 0.42975                     | Test: Loss 0.85536, R2 0.77308, RMSE 0.91306\n",
      "Epoch [465/500]      | Train: Loss 0.18280, R2 0.94810, RMSE 0.42685                     | Test: Loss 0.82703, R2 0.77702, RMSE 0.90070\n",
      "Epoch [466/500]      | Train: Loss 0.18502, R2 0.94749, RMSE 0.42931                     | Test: Loss 0.83614, R2 0.77647, RMSE 0.90764\n",
      "Epoch [467/500]      | Train: Loss 0.17492, R2 0.94994, RMSE 0.41732                     | Test: Loss 0.83797, R2 0.77042, RMSE 0.90625\n",
      "Epoch [468/500]      | Train: Loss 0.18454, R2 0.94818, RMSE 0.42864                     | Test: Loss 0.80048, R2 0.77750, RMSE 0.88609\n",
      "Epoch [469/500]      | Train: Loss 0.18854, R2 0.94654, RMSE 0.43342                     | Test: Loss 0.81392, R2 0.77404, RMSE 0.89440\n",
      "Epoch [470/500]      | Train: Loss 0.18108, R2 0.94872, RMSE 0.42488                     | Test: Loss 0.97018, R2 0.75383, RMSE 0.95552\n",
      "Epoch [471/500]      | Train: Loss 0.17724, R2 0.95010, RMSE 0.42029                     | Test: Loss 0.83015, R2 0.77490, RMSE 0.89852\n",
      "Epoch [472/500]      | Train: Loss 0.17521, R2 0.95082, RMSE 0.41787                     | Test: Loss 0.96783, R2 0.69259, RMSE 0.95684\n",
      "Epoch [473/500]      | Train: Loss 0.17660, R2 0.95032, RMSE 0.41966                     | Test: Loss 0.80416, R2 0.78297, RMSE 0.88020\n",
      "Epoch [474/500]      | Train: Loss 0.17895, R2 0.94966, RMSE 0.42217                     | Test: Loss 0.79146, R2 0.78525, RMSE 0.87944\n",
      "Epoch [475/500]      | Train: Loss 0.17547, R2 0.95056, RMSE 0.41793                     | Test: Loss 0.88116, R2 0.77063, RMSE 0.92525\n",
      "Epoch [476/500]      | Train: Loss 0.18061, R2 0.94862, RMSE 0.42434                     | Test: Loss 0.79437, R2 0.77976, RMSE 0.88292\n",
      "Epoch [477/500]      | Train: Loss 0.17402, R2 0.95092, RMSE 0.41649                     | Test: Loss 0.82778, R2 0.77068, RMSE 0.89810\n",
      "Epoch [478/500]      | Train: Loss 0.18414, R2 0.94792, RMSE 0.42807                     | Test: Loss 0.80739, R2 0.77632, RMSE 0.89224\n",
      "Epoch [479/500]      | Train: Loss 0.17496, R2 0.95042, RMSE 0.41768                     | Test: Loss 0.81285, R2 0.78635, RMSE 0.89233\n",
      "Epoch [480/500]      | Train: Loss 0.17218, R2 0.95161, RMSE 0.41427                     | Test: Loss 0.79807, R2 0.78562, RMSE 0.88224\n",
      "Epoch [481/500]      | Train: Loss 0.16678, R2 0.95264, RMSE 0.40767                     | Test: Loss 0.80879, R2 0.77054, RMSE 0.89389\n",
      "Epoch [482/500]      | Train: Loss 0.17533, R2 0.95032, RMSE 0.41781                     | Test: Loss 0.79759, R2 0.77563, RMSE 0.87870\n",
      "Epoch [483/500]      | Train: Loss 0.17160, R2 0.95180, RMSE 0.41377                     | Test: Loss 0.81676, R2 0.75843, RMSE 0.89317\n",
      "Epoch [484/500]      | Train: Loss 0.18448, R2 0.94789, RMSE 0.42855                     | Test: Loss 0.93547, R2 0.71171, RMSE 0.94711\n",
      "Epoch [485/500]      | Train: Loss 0.17826, R2 0.94931, RMSE 0.42138                     | Test: Loss 0.83013, R2 0.77092, RMSE 0.90216\n",
      "Epoch [486/500]      | Train: Loss 0.17109, R2 0.95121, RMSE 0.41294                     | Test: Loss 0.86606, R2 0.76808, RMSE 0.92089\n",
      "Epoch [487/500]      | Train: Loss 0.16901, R2 0.95264, RMSE 0.41068                     | Test: Loss 0.83085, R2 0.76625, RMSE 0.90765\n",
      "Epoch [488/500]      | Train: Loss 0.17199, R2 0.95152, RMSE 0.41414                     | Test: Loss 0.82072, R2 0.76806, RMSE 0.89654\n",
      "Epoch [489/500]      | Train: Loss 0.17546, R2 0.94998, RMSE 0.41830                     | Test: Loss 0.80432, R2 0.76424, RMSE 0.89092\n",
      "Epoch [490/500]      | Train: Loss 0.17730, R2 0.94998, RMSE 0.42045                     | Test: Loss 0.80435, R2 0.78096, RMSE 0.88863\n",
      "Epoch [491/500]      | Train: Loss 0.16563, R2 0.95323, RMSE 0.40604                     | Test: Loss 0.84522, R2 0.75546, RMSE 0.91250\n",
      "Epoch [492/500]      | Train: Loss 0.17464, R2 0.95047, RMSE 0.41688                     | Test: Loss 0.82622, R2 0.78091, RMSE 0.90308\n",
      "Epoch [493/500]      | Train: Loss 0.17823, R2 0.94935, RMSE 0.42149                     | Test: Loss 0.81992, R2 0.76334, RMSE 0.89843\n",
      "Epoch [494/500]      | Train: Loss 0.17182, R2 0.95132, RMSE 0.41361                     | Test: Loss 0.81983, R2 0.76589, RMSE 0.90054\n",
      "Epoch [495/500]      | Train: Loss 0.18411, R2 0.94776, RMSE 0.42745                     | Test: Loss 0.80582, R2 0.77654, RMSE 0.88836\n",
      "Epoch [496/500]      | Train: Loss 0.16813, R2 0.95259, RMSE 0.40939                     | Test: Loss 0.81753, R2 0.77601, RMSE 0.89799\n",
      "Epoch [497/500]      | Train: Loss 0.16787, R2 0.95298, RMSE 0.40898                     | Test: Loss 0.83441, R2 0.76770, RMSE 0.90655\n",
      "Epoch [498/500]      | Train: Loss 0.16646, R2 0.95302, RMSE 0.40675                     | Test: Loss 0.82541, R2 0.77309, RMSE 0.90154\n",
      "Epoch [499/500]      | Train: Loss 0.16624, R2 0.95295, RMSE 0.40683                     | Test: Loss 0.84642, R2 0.77484, RMSE 0.91276\n",
      "Epoch [500/500]      | Train: Loss 0.16378, R2 0.95363, RMSE 0.40387                     | Test: Loss 0.82866, R2 0.77018, RMSE 0.90048\n",
      "Best rmse 0.8064636519322028\n",
      "100 epochs of training and evaluation took, 4024.7418354520014\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class LossFunction(nn.Module):\n",
    "    def __init__(self, num_tasks):\n",
    "        super(LossFunction, self).__init__()\n",
    "        self.num_tasks = num_tasks\n",
    "\n",
    "        self.loss_fns = [nn.MSELoss() for x in range(num_tasks)] \n",
    "\n",
    "    def forward(self, predictions, labels_task1):\n",
    "\n",
    "        #task 1\n",
    "        target = labels_task1\n",
    "        prediction = predictions\n",
    "        loss_fn = self.loss_fns[0]\n",
    "\n",
    "        task_loss = loss_fn(prediction, target)\n",
    "        \n",
    "        return task_loss\n",
    "    \n",
    "model = CATTransformer(rff_on=True,\n",
    "                       sigma=1,\n",
    "                       embed_size=160,\n",
    "                       heads=5,\n",
    "                       forward_expansion=8,\n",
    "                       pre_norm_on=False,\n",
    "                       mlp_scale_classification=8,\n",
    "                   \n",
    "                       decoder_dropout=0,\n",
    "                       classification_dropout=0.1,\n",
    "                       targets_classes=target_classes,\n",
    "                       n_cont=len(cont_columns),\n",
    "              \n",
    "                       embedding_scheme='log-linear_periodic',\n",
    "                       trainable=False,\n",
    "                       linear_on=True).to(device_in_use)\n",
    "\n",
    "def format_metric(value): # Used to format the metrics output\n",
    "    return f\"{value:.5f}\"\n",
    "\n",
    "loss_functions = LossFunction(1)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr = 0.0001) # Maybe try messing around with optimizers. try other torch optimizers with different configurations.\n",
    "epochs = 500 #Set the number of epochs\n",
    "\n",
    "train_losses = []\n",
    "train_rmse = [] \n",
    "test_losses = []\n",
    "test_rmse = []  \n",
    "\n",
    "\n",
    "#Time will be recorded for all 100 epochs - This means the results will not be comparable to Xgboost but that is ok, we will only compare between transformer models who will also train for 100 epochs\n",
    "start_time = time.process_time()\n",
    "\n",
    "for t in range(epochs):\n",
    "    train_loss, r2_train, rmse_train = train(train_dataloader, model, loss_functions, optimizer, device_in_use=device_in_use)\n",
    "    test_loss, r2_test, rmse_test = test(test_dataloader, model, loss_functions, device_in_use=device_in_use)\n",
    "    train_losses.append(train_loss)\n",
    "    train_rmse.append(rmse_train)\n",
    "    # train_accuracies_2.append(train_accuracy_2)\n",
    "    # train_recalls.append(train_recall) \n",
    "    # train_f1_scores.append(train_f1)\n",
    "    test_losses.append(test_loss)\n",
    "    test_rmse.append(rmse_test)\n",
    "    # test_accuracies_2.append(test_accuracy_2)\n",
    "    # test_recalls.append(test_recall)\n",
    "    # test_f1_scores.append(test_f1)\n",
    "    # Formatting for easier reading\n",
    "    epoch_str = f\"Epoch [{t+1:2}/{epochs}]\"\n",
    "    train_metrics = f\"Train: Loss {format_metric(train_loss)}, R2 {format_metric(r2_train)}, RMSE {format_metric(rmse_train)}\"\n",
    "    test_metrics = f\"Test: Loss {format_metric(test_loss)}, R2 {format_metric(r2_test)}, RMSE {format_metric(rmse_test)}\"\n",
    "    print(f\"{epoch_str:20} | {train_metrics:65} | {test_metrics}\")\n",
    "\n",
    "total_time = time.process_time() - start_time\n",
    "\n",
    "# Plotting the loss curves\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, epochs+1), train_losses, label='Train Loss')\n",
    "plt.plot(range(1, epochs+1), [l for l in test_losses], label='Test Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Test Loss Curve')\n",
    "plt.legend()\n",
    "\n",
    "best_index = test_rmse.index(min(test_rmse))\n",
    "print(f\"Best rmse {test_rmse[best_index]}\")\n",
    "print(f\"100 epochs of training and evaluation took, {total_time}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
