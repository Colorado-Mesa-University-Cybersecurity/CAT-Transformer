{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from rff.layers import GaussianEncoding #pip install random-fourier-features-pytorch\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import os\n",
    "from tab_transformer_pytorch import FTTransformer, TabTransformer\n",
    "import sys\n",
    "import time\n",
    "from torch import Tensor\n",
    "from typing import Literal\n",
    "\n",
    "device_in_use='cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train = pd.read_csv('/home/cscadmin/CyberResearch/CAT-Transformer/datasets/higgs/train.csv')\n",
    "# df_test = pd.read_csv('/home/cscadmin/CyberResearch/CAT-Transformer/datasets/higgs/test.csv')\n",
    "# df_val = pd.read_csv('/home/cscadmin/CyberResearch/CAT-Transformer/datasets/higgs/validation.csv') #READ FROM RIGHT SPOT\n",
    "\n",
    "# df_train = pd.read_csv(r'C:\\Users\\smbm2\\projects\\CAT-Transformer\\datasets\\california\\train.csv')\n",
    "# df_test = pd.read_csv(r'C:\\Users\\smbm2\\projects\\CAT-Transformer\\datasets\\california\\test.csv')\n",
    "# df_val = pd.read_csv(r'C:\\Users\\smbm2\\projects\\CAT-Transformer\\datasets\\california\\validation.csv') #READ FROM RIGHT SPOT\n",
    "\n",
    "df_train = pd.read_csv('/home/warin/projects/CAT-Transformer/datasets/california_sklearn/train.csv')\n",
    "df_test = pd.read_csv('/home/warin/projects/CAT-Transformer/datasets/california_sklearn/test.csv')\n",
    "df_val = pd.read_csv('/home/warin/projects/CAT-Transformer/datasets/california_sklearn/validation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9851]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cont_columns = [ 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup',\n",
    "       'Latitude', 'Longitude']\n",
    "target = ['MedInc']\n",
    "\n",
    "#CHECKING TO MAKE SURE YOUR LIST IS CORRECT (NO NEED TO TOUCH)\n",
    "yourlist = cont_columns + target\n",
    "yourlist.sort()\n",
    "oglist = list(df_train.columns)\n",
    "oglist.sort()\n",
    "\n",
    "assert(yourlist == oglist), \"You may of spelled feature name wrong or you forgot to put on of them in the list\"\n",
    "\n",
    "target_classes = [max(len(df_train[target].value_counts()), len(df_val[target].value_counts()),len(df_test[target].value_counts()))]\n",
    "print(target_classes)\n",
    "# Create a StandardScaler and fit it to the cont features\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(df_train[cont_columns])\n",
    "\n",
    "# Transform the training, test, and validation datasets\n",
    "df_train[cont_columns] = scaler.transform(df_train[cont_columns])\n",
    "df_test[cont_columns] = scaler.transform(df_test[cont_columns])\n",
    "df_val[cont_columns] = scaler.transform(df_val[cont_columns])\n",
    "\n",
    "class SingleTaskDataset(Dataset):\n",
    "    def __init__(self, df : pd.DataFrame, num_columns,task1_column):\n",
    "        self.n = df.shape[0]\n",
    "        \n",
    "        self.task1_labels = df[task1_column].astype(np.float32).values\n",
    "\n",
    "        self.num = df[num_columns].astype(np.float32).values\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Retrieve features and labels from the dataframe using column names\n",
    "        num_features = self.num[idx]\n",
    "        labels_task1 = self.task1_labels[idx]\n",
    "\n",
    "        return num_features, labels_task1\n",
    "\n",
    "#Wrapping in Dataset\n",
    "train_dataset = SingleTaskDataset(df_train, cont_columns, 'MedInc')\n",
    "val_dataset = SingleTaskDataset(df_val, cont_columns, 'MedInc')\n",
    "test_dataset = SingleTaskDataset(df_test, cont_columns, 'MedInc')\n",
    "\n",
    "#This is a hyperparameter that is not tuned. Maybe mess with what makes sense here\n",
    "#Also try looking to see what other papers have done\n",
    "batch_size = 256\n",
    "\n",
    "# Wrapping with DataLoader for easy batch extraction\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_size, heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        self.head_dim = embed_size // heads\n",
    "        assert(self.head_dim * heads == embed_size), \"Embed size needs to be div by heads\"\n",
    "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.keys =nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.fc_out = nn.Linear(heads*self.head_dim, embed_size)\n",
    "\n",
    "\n",
    "    def forward(self, values, keys, query):\n",
    "        N = query.shape[0]\n",
    "        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
    "\n",
    "        values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
    "        keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n",
    "        queries = query.reshape(N, query_len, self.heads, self.head_dim)\n",
    "\n",
    "        values = self.values(values)\n",
    "        keys = self.keys(keys)\n",
    "        queries = self.queries(queries)\n",
    "\n",
    "        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
    "\n",
    "        attention = torch.softmax(energy / (self.embed_size ** (1/2)), dim=3) #(batch_size, head_dim, #query_embeddings, #key_embeddings)\n",
    "\n",
    "        # Calculate simplified attention scores\n",
    "        avg_attention = attention.mean(dim=0)  # Average across batches\n",
    "        # print(\"batch average\", avg_attention.shape)\n",
    "        avg_attention = avg_attention.mean(dim=0).squeeze(dim=0)\n",
    "        # print(\"head average\", avg_attention.shape)\n",
    "\n",
    "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(N, query_len, self.heads*self.head_dim) #(batch_size, n_features, embed_size)\n",
    "        out = self.fc_out(out)\n",
    "\n",
    "        return out, avg_attention\n",
    "    \n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_size, heads, dropout, forward_expansion, pre_norm_on):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "\n",
    "        self.pre_norm_on = pre_norm_on\n",
    "        if self.pre_norm_on:\n",
    "            self.pre_norm = nn.LayerNorm(embed_size)\n",
    "        self.attention = MultiHeadAttention(embed_size, heads)\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "\n",
    "        self.feed_forward = nn.Sequential(nn.Linear(embed_size, forward_expansion*embed_size),\n",
    "                                          nn.ReLU(),\n",
    "                                          nn.Linear(forward_expansion*embed_size, embed_size)\n",
    "                                          )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self,value,key,query):\n",
    "        if self.pre_norm_on:\n",
    "            query = self.pre_norm(query)\n",
    "            key = self.pre_norm(key)\n",
    "            value = self.pre_norm(value)\n",
    "            \n",
    "        attention, avg_attention = self.attention(value, key, query)\n",
    "        x = self.dropout(self.norm1(attention + query))\n",
    "        forward = self.feed_forward(x)\n",
    "        out = self.dropout(self.norm2(forward + x))\n",
    "        return out, avg_attention\n",
    "    \n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, embed_size, heads, forward_expansion, dropout, pre_norm_on):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "\n",
    "        self.attention = MultiHeadAttention(embed_size, heads)\n",
    "        self.norm = nn.LayerNorm(embed_size)\n",
    "        self.transformer_block = TransformerBlock(embed_size, heads, dropout, forward_expansion, pre_norm_on)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, value, key):\n",
    "        out, avg_attention = self.transformer_block(value, key, x)\n",
    "\n",
    "        return out, avg_attention\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 embed_size,\n",
    "                 num_layers,\n",
    "                 heads,\n",
    "                 forward_expansion,\n",
    "                 decoder_dropout,\n",
    "                 pre_norm_on\n",
    "    ):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "                [\n",
    "                    DecoderBlock(\n",
    "                        embed_size,\n",
    "                        heads,\n",
    "                        dropout=decoder_dropout,\n",
    "                        forward_expansion=forward_expansion,\n",
    "                        pre_norm_on=pre_norm_on\n",
    "                    )\n",
    "                    for _ in range(num_layers)\n",
    "                ]\n",
    "            )\n",
    "        self.avg_attention = None\n",
    "\n",
    "    def forward(self, class_embed, context):\n",
    "        for layer in self.layers:\n",
    "            # x is the classification embedding (CLS Token)\n",
    "            # context are the feature embeddings that will be used as key and value\n",
    "            x, self.avg_attention = layer(class_embed, context, context)\n",
    "  \n",
    "        return x \n",
    "\n",
    "class EmbeddingsRFFforIndividualFeatures(nn.Module):\n",
    "    def __init__(self, sigma, embed_size, input_size, n_cont,  num_target_labels, rff_on, linear_on:bool):\n",
    "        super(EmbeddingsRFFforIndividualFeatures, self).__init__()\n",
    "\n",
    "        self.rff_on = rff_on\n",
    "\n",
    "        self.linear_on = linear_on\n",
    "\n",
    "        if self.rff_on:\n",
    "            self.rffs = nn.ModuleList([GaussianEncoding(sigma=sigma, input_size=input_size, encoded_size=embed_size//2) for _ in range(n_cont)])\n",
    "            self.mlp_in = embed_size\n",
    "        else:\n",
    "            self.mlp_in = input_size\n",
    "\n",
    "        if self.linear_on:\n",
    "            self.cont_embeddings = nn.ModuleList([nn.Linear(in_features=self.mlp_in, out_features=embed_size) for _ in range(n_cont)])\n",
    "\n",
    "        # Classifcation Embeddings for each target label\n",
    "        self.target_label_embeddings = nn.ModuleList([nn.Embedding(1, embed_size) for _ in range(num_target_labels)])\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(2) #(batch_size, n_features) -> (batch_size, n_features, 1)\n",
    "        rff_vectors = []\n",
    "        if self.rff_on:\n",
    "            for i, r in enumerate(self.rffs):\n",
    "                input = x[:,i,:]\n",
    "                out = r(input)\n",
    "                rff_vectors.append(out)\n",
    "\n",
    "        embeddings = []\n",
    "        if self.linear_on:\n",
    "            x = torch.stack(rff_vectors, dim=1)\n",
    "            for i, e in enumerate(self.cont_embeddings):\n",
    "                goin_in = x[:,i,:]\n",
    "                goin_out = e(goin_in)\n",
    "                embeddings.append(goin_out)\n",
    "        else:\n",
    "            embeddings = rff_vectors\n",
    "\n",
    "        target_label_embeddings_ = []\n",
    "        for e in self.target_label_embeddings:\n",
    "            input = torch.tensor([0], device=x.device)\n",
    "            temp = e(input)\n",
    "            temp = temp.repeat(x.size(0), 1)\n",
    "            tmep = temp.unsqueeze(1)\n",
    "            target_label_embeddings_.append(temp)\n",
    "\n",
    "        class_embeddings = torch.stack(target_label_embeddings_, dim=1)\n",
    "\n",
    "        context = torch.stack(embeddings, dim=1)\n",
    "\n",
    "\n",
    "        return class_embeddings, context\n",
    "\n",
    "class EmbeddingsRFFforAllFeatures(nn.Module):\n",
    "    def __init__(self, sigma, embed_size, input_size, n_cont, num_target_labels, rff_on, linear_on:bool):\n",
    "        super(EmbeddingsRFFforAllFeatures, self).__init__()\n",
    "\n",
    "        self.rff_on = rff_on\n",
    "        self.n_cont = n_cont\n",
    "        self.linear_on=linear_on\n",
    "\n",
    "        if self.rff_on:\n",
    "            self.rff = GaussianEncoding(sigma=sigma, input_size=input_size, encoded_size=embed_size//2) \n",
    "            self.mlp_in = embed_size\n",
    "        else:\n",
    "            self.mlp_in = input_size\n",
    "\n",
    "        if self.linear_on:\n",
    "            self.cont_embeddings = nn.ModuleList([nn.Linear(in_features=self.mlp_in, out_features=embed_size) for _ in range(n_cont)])\n",
    "\n",
    "        \n",
    "        # Classifcation Embeddings for each target label\n",
    "        self.target_label_embeddings = nn.ModuleList([nn.Embedding(1, embed_size) for _ in range(num_target_labels)])\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(2) #(batch_size, n_features) -> (batch_size, n_features, 1)\n",
    "        rff_vectors = []\n",
    "        if self.rff_on:\n",
    "            for i in range(self.n_cont):\n",
    "                input = x[:,i,:]\n",
    "                out = self.rff(input)\n",
    "                rff_vectors.append(out)\n",
    "        \n",
    "        embeddings = []\n",
    "        if self.linear_on:\n",
    "            x = torch.stack(rff_vectors, dim=1)\n",
    "            for i, e in enumerate(self.cont_embeddings):\n",
    "                goin_in = x[:,i,:]\n",
    "                goin_out = e(goin_in)\n",
    "                embeddings.append(goin_out)\n",
    "        else:\n",
    "            embeddings = rff_vectors\n",
    "\n",
    "        target_label_embeddings_ = []\n",
    "        for e in self.target_label_embeddings:\n",
    "            input = torch.tensor([0], device=x.device)\n",
    "            temp = e(input)\n",
    "            temp = temp.repeat(x.size(0), 1)\n",
    "            tmep = temp.unsqueeze(1)\n",
    "            target_label_embeddings_.append(temp)\n",
    "\n",
    "        class_embeddings = torch.stack(target_label_embeddings_, dim=1)\n",
    "\n",
    "        context = torch.stack(embeddings, dim=1)\n",
    "\n",
    "        return class_embeddings, context\n",
    "    \n",
    "class PeriodicActivation(nn.Module):\n",
    "    def __init__(self, sigma, embed_size, n_cont, num_target_labels,trainable: bool, initialization: str, linear_on:bool):\n",
    "        super(PeriodicActivation, self).__init__()\n",
    "\n",
    "        self.n = embed_size\n",
    "        self.sigma = sigma\n",
    "        self.trainable = trainable\n",
    "        self.initialization = initialization\n",
    "        self.linear_on = linear_on\n",
    "        self.n_cont = n_cont\n",
    "\n",
    "        if self.initialization == 'log-linear':\n",
    "            coefficients = self.sigma ** (torch.arange(self.n//2) / self.n) # sigma matters here but more in the way of >1 or <1\n",
    "            coefficients = coefficients[None] #same as unsqueeze(0)\n",
    "        else:\n",
    "            assert self.initialization == 'normal'\n",
    "            coefficients = torch.normal(0.0, self.sigma, (1, self.n//2))\n",
    "\n",
    "        if self.trainable:\n",
    "            self.coefficients = nn.Parameter(coefficients)\n",
    "        else:\n",
    "            self.register_buffer('coefficients', coefficients)\n",
    "\n",
    "        if self.linear_on:\n",
    "            self.cont_embeddings = nn.ModuleList([nn.Linear(in_features=embed_size, out_features=embed_size) for _ in range(n_cont)])\n",
    "\n",
    "        \n",
    "        # Classifcation Embeddings for each target label\n",
    "        self.target_label_embeddings = nn.ModuleList([nn.Embedding(1, embed_size) for _ in range(num_target_labels)])\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(2) #(batch_size, n_features) -> (batch_size, n_features, 1)\n",
    "\n",
    "        temp = []\n",
    "        for i in range(self.n_cont):\n",
    "            input = x[:,i,:]\n",
    "            out = torch.cat([torch.cos(self.coefficients * input), torch.sin(self.coefficients * input)], dim=-1)\n",
    "            temp.append(out)\n",
    "        \n",
    "        embeddings = []\n",
    "        if self.linear_on:\n",
    "            x = torch.stack(temp, dim=1)\n",
    "            for i, e in enumerate(self.cont_embeddings):\n",
    "                goin_in = x[:,i,:]\n",
    "                goin_out = e(goin_in)\n",
    "                embeddings.append(goin_out)\n",
    "        else:\n",
    "            embeddings = temp\n",
    "\n",
    "        target_label_embeddings_ = []\n",
    "        for e in self.target_label_embeddings:\n",
    "            input = torch.tensor([0], device=x.device)\n",
    "            temp = e(input)\n",
    "            temp = temp.repeat(x.size(0), 1)\n",
    "            tmep = temp.unsqueeze(1)\n",
    "            target_label_embeddings_.append(temp)\n",
    "\n",
    "        class_embeddings = torch.stack(target_label_embeddings_, dim=1)\n",
    "\n",
    "        context = torch.stack(embeddings, dim=1)\n",
    "\n",
    "        return class_embeddings, context\n",
    "\n",
    "\n",
    "class classificationHead(nn.Module):\n",
    "    def __init__(self, embed_size, dropout, mlp_scale_classification, num_target_classes):\n",
    "        super(classificationHead, self).__init__()\n",
    "        \n",
    "        #flattening the embeddings out so each sample in batch is represented with a 460 dimensional vector\n",
    "        self.input = embed_size\n",
    "        self.lin1 = nn.Linear(self.input, mlp_scale_classification*self.input)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.lin2 = nn.Linear(mlp_scale_classification*self.input, mlp_scale_classification*self.input)\n",
    "        self.lin3 = nn.Linear(mlp_scale_classification*self.input, self.input)\n",
    "        self.lin4 = nn.Linear(self.input, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.initialize_weights()\n",
    "\n",
    "    def initialize_weights(self): #he_initialization.\n",
    "        torch.nn.init.kaiming_normal_(self.lin1.weight, nonlinearity='relu')\n",
    "        torch.nn.init.zeros_(self.lin1.bias)\n",
    "\n",
    "        torch.nn.init.kaiming_normal_(self.lin3.weight, nonlinearity='relu')\n",
    "        torch.nn.init.zeros_(self.lin3.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x= torch.reshape(x, (-1, self.input))\n",
    "\n",
    "        x = self.lin1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.lin2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.lin3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.lin4(x)\n",
    "  \n",
    "        return x\n",
    "\n",
    "class CATTransformer(nn.Module):\n",
    "    def __init__(self, \n",
    "                 rff_on = True,\n",
    "                 sigma=4,\n",
    "                 embed_size=20,\n",
    "                 input_size=1,\n",
    "                 n_cont = 0,\n",
    "                 num_layers=1,\n",
    "                 heads=1,\n",
    "                 forward_expansion=4, # Determines how wide the MLP is in the encoder. Its a scaling factor. \n",
    "                 decoder_dropout=0,\n",
    "                 classification_dropout = 0,\n",
    "                 pre_norm_on = False,\n",
    "                 mlp_scale_classification = 4,\n",
    "                 targets_classes : list=  [3,8],\n",
    "                embedding_scheme =\"rff_unique\",\n",
    "                trainable=True,\n",
    "                linear_on=True\n",
    "                 ):\n",
    "        super(CATTransformer, self).__init__()\n",
    "\n",
    "        assert(embedding_scheme in ['rff_unique', 'rff', 'log-linear_periodic', 'normal_periodic']), \"wrong embedding_scheme\"\n",
    "\n",
    "        if embedding_scheme == 'rff_unique':\n",
    "            self.embeddings = EmbeddingsRFFforIndividualFeatures(rff_on=rff_on, sigma=sigma, embed_size=embed_size, input_size=input_size, \n",
    "                                     n_cont=n_cont, num_target_labels=len(targets_classes), linear_on=linear_on)\n",
    "        elif embedding_scheme == 'log-linear_periodic':\n",
    "            self.embeddings = PeriodicActivation(sigma=sigma, embed_size=embed_size, n_cont=n_cont, \n",
    "                                                 num_target_labels=len(target_classes),trainable=trainable,initialization='log-linear', linear_on=linear_on)\n",
    "        elif embedding_scheme == 'normal_periodic':\n",
    "            self.embeddings = PeriodicActivation(sigma=sigma, embed_size=embed_size, n_cont=n_cont, \n",
    "                                                 num_target_labels=len(target_classes),trainable=trainable,initialization='normal',linear_on=linear_on)\n",
    "        else:\n",
    "            self.embeddings = EmbeddingsRFFforAllFeatures(rff_on=rff_on, sigma=sigma, embed_size=embed_size, input_size=input_size, \n",
    "                                     n_cont=n_cont, num_target_labels=len(targets_classes),linear_on=linear_on)\n",
    "            \n",
    "        self.decoder = Decoder(embed_size=embed_size, num_layers=num_layers, heads=heads, forward_expansion=forward_expansion, \n",
    "                               decoder_dropout=decoder_dropout, pre_norm_on=pre_norm_on)\n",
    "        self.classifying_heads = nn.ModuleList([classificationHead(embed_size=embed_size, dropout=classification_dropout, \n",
    "                                                                   mlp_scale_classification=mlp_scale_classification, \n",
    "                                                                   num_target_classes=x) for x in targets_classes])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        class_embed, context = self.embeddings(x)\n",
    "\n",
    "        x = self.decoder(class_embed, context)\n",
    "        \n",
    "        probability_dist_raw = []\n",
    "        for i, e in enumerate(self.classifying_heads):\n",
    "            input = x[:, i,:]\n",
    "            output = e(input)\n",
    "            probability_dist_raw.append(output)\n",
    "        \n",
    "        return probability_dist_raw\n",
    "    \n",
    "    # Training and Testing Loops\n",
    "def train(dataloader, model, loss_function, optimizer, device_in_use):\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "  \n",
    "    total_loss = 0\n",
    "    total_r2_score = 0\n",
    "    root_mean_squared_error_total = 0\n",
    "\n",
    "    for (features,labels_task1) in dataloader:\n",
    "        features,labels_task1 = features.to(device_in_use),labels_task1.to(device_in_use)\n",
    "\n",
    "        task_predictions = model(features) #contains a list of the tensor outputs for each task\n",
    "\n",
    "        loss = loss_function(task_predictions[0].squeeze(1), labels_task1)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Calculate R^2 score for the regression task\n",
    "        r2 = r2_score_manual(labels_task1, task_predictions[0].squeeze(1))\n",
    "        total_r2_score += r2\n",
    "\n",
    "        # Calculate RMSE score for the regression task\n",
    "        rmse_value = rmse(labels_task1, task_predictions[0].squeeze(1))\n",
    "        root_mean_squared_error_total+=rmse_value\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    avg_r2_score = total_r2_score / len(dataloader)\n",
    "    avg_rmse_score = root_mean_squared_error_total / len(dataloader)\n",
    "\n",
    "    return avg_loss, avg_r2_score, avg_rmse_score\n",
    "\n",
    "def test(dataloader, model, loss_function, device_in_use):\n",
    "  model.eval()\n",
    "  total_loss = 0\n",
    "  \n",
    "  total_loss = 0\n",
    "  total_r2_score = 0\n",
    "  root_mean_squared_error_total = 0\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for (features,labels_task1) in dataloader:\n",
    "        features,labels_task1 = features.to(device_in_use),labels_task1.to(device_in_use)\n",
    "\n",
    "        #compute prediction error\n",
    "        task_predictions = model(features) #contains a list of the tensor outputs for each task\n",
    "\n",
    "        loss = loss_function(task_predictions[0].squeeze(1), labels_task1)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Calculate R^2 score for the regression task\n",
    "        r2 = r2_score_manual(labels_task1, task_predictions[0].squeeze(1))\n",
    "        total_r2_score += r2\n",
    "        \n",
    "        # Calculate RMSE score for the regression task\n",
    "        rmse_value = rmse(labels_task1, task_predictions[0].squeeze(1))\n",
    "        root_mean_squared_error_total+=rmse_value\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    avg_r2_score = total_r2_score / len(dataloader)\n",
    "    avg_rmse_score = root_mean_squared_error_total / len(dataloader)\n",
    "\n",
    "    return avg_loss, avg_r2_score, avg_rmse_score\n",
    "\n",
    "def format_metric(value): # Used to format the metrics output\n",
    "    return f\"{value:.4f}\"\n",
    "\n",
    "def r2_score_manual(y_true, y_pred):\n",
    "    # Calculate the mean of true labels\n",
    "    y_mean = torch.mean(y_true)\n",
    "\n",
    "    # Calculate the total sum of squares\n",
    "    total_ss = torch.sum((y_true - y_mean)**2)\n",
    "\n",
    "    # Calculate the residual sum of squares\n",
    "    residual_ss = torch.sum((y_true - y_pred)**2)\n",
    "\n",
    "    # Calculate R-squared\n",
    "    r2 = 1 - (residual_ss / total_ss)\n",
    "\n",
    "    return r2.item()  # Convert to a Python float\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    # Calculate the squared differences\n",
    "    squared_diff = (y_true - y_pred)**2\n",
    "\n",
    "    # Calculate the mean of the squared differences\n",
    "    mean_squared_diff = torch.mean(squared_diff)\n",
    "\n",
    "    # Calculate the square root to obtain RMSE\n",
    "    rmse = torch.sqrt(mean_squared_diff)\n",
    "\n",
    "    return rmse.item()  # Convert to a Python float"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normal RFF to each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [ 1/500]       | Train: Loss 4.47777, R2 -0.22844, RMSE 2.05485                    | Test: Loss 3.52993, R2 0.03679, RMSE 1.87258\n",
      "Epoch [ 2/500]       | Train: Loss 2.77145, R2 0.23472, RMSE 1.65277                     | Test: Loss 1.95990, R2 0.46385, RMSE 1.38976\n",
      "Epoch [ 3/500]       | Train: Loss 1.82077, R2 0.49892, RMSE 1.34005                     | Test: Loss 1.43184, R2 0.61386, RMSE 1.17471\n",
      "Epoch [ 4/500]       | Train: Loss 1.42149, R2 0.60675, RMSE 1.18544                     | Test: Loss 1.22608, R2 0.67632, RMSE 1.10340\n",
      "Epoch [ 5/500]       | Train: Loss 1.20249, R2 0.66617, RMSE 1.09001                     | Test: Loss 1.05030, R2 0.70260, RMSE 1.02028\n",
      "Epoch [ 6/500]       | Train: Loss 1.04894, R2 0.70732, RMSE 1.01889                     | Test: Loss 0.97709, R2 0.72902, RMSE 0.98338\n",
      "Epoch [ 7/500]       | Train: Loss 0.99439, R2 0.72333, RMSE 0.99227                     | Test: Loss 0.92223, R2 0.73570, RMSE 0.95275\n",
      "Epoch [ 8/500]       | Train: Loss 0.95170, R2 0.73409, RMSE 0.97136                     | Test: Loss 0.98106, R2 0.72312, RMSE 0.98679\n",
      "Epoch [ 9/500]       | Train: Loss 0.91496, R2 0.74629, RMSE 0.95237                     | Test: Loss 0.89262, R2 0.75331, RMSE 0.93918\n",
      "Epoch [10/500]       | Train: Loss 0.89753, R2 0.74856, RMSE 0.94268                     | Test: Loss 0.83967, R2 0.76275, RMSE 0.90510\n",
      "Epoch [11/500]       | Train: Loss 0.86901, R2 0.75734, RMSE 0.92901                     | Test: Loss 0.92707, R2 0.74213, RMSE 0.95881\n",
      "Epoch [12/500]       | Train: Loss 0.85833, R2 0.76127, RMSE 0.92161                     | Test: Loss 0.85706, R2 0.75955, RMSE 0.91753\n",
      "Epoch [13/500]       | Train: Loss 0.84040, R2 0.76439, RMSE 0.91277                     | Test: Loss 0.92172, R2 0.73956, RMSE 0.95020\n",
      "Epoch [14/500]       | Train: Loss 0.85682, R2 0.75745, RMSE 0.92198                     | Test: Loss 0.85280, R2 0.74068, RMSE 0.91786\n",
      "Epoch [15/500]       | Train: Loss 0.81143, R2 0.76983, RMSE 0.89777                     | Test: Loss 0.82984, R2 0.77785, RMSE 0.90375\n",
      "Epoch [16/500]       | Train: Loss 0.80832, R2 0.77396, RMSE 0.89439                     | Test: Loss 0.83549, R2 0.75992, RMSE 0.90939\n",
      "Epoch [17/500]       | Train: Loss 0.82276, R2 0.76826, RMSE 0.90422                     | Test: Loss 0.96035, R2 0.73087, RMSE 0.96428\n",
      "Epoch [18/500]       | Train: Loss 0.79260, R2 0.77716, RMSE 0.88713                     | Test: Loss 0.83060, R2 0.77104, RMSE 0.90369\n",
      "Epoch [19/500]       | Train: Loss 0.79835, R2 0.77591, RMSE 0.89063                     | Test: Loss 0.85844, R2 0.75827, RMSE 0.91732\n",
      "Epoch [20/500]       | Train: Loss 0.80416, R2 0.77270, RMSE 0.89378                     | Test: Loss 0.87794, R2 0.76378, RMSE 0.92914\n",
      "Epoch [21/500]       | Train: Loss 0.77989, R2 0.78333, RMSE 0.88065                     | Test: Loss 0.81472, R2 0.77598, RMSE 0.89288\n",
      "Epoch [22/500]       | Train: Loss 0.75954, R2 0.78773, RMSE 0.86865                     | Test: Loss 1.12144, R2 0.74283, RMSE 0.98748\n",
      "Epoch [23/500]       | Train: Loss 0.75929, R2 0.78770, RMSE 0.86830                     | Test: Loss 0.81910, R2 0.77558, RMSE 0.89519\n",
      "Epoch [24/500]       | Train: Loss 0.75397, R2 0.78787, RMSE 0.86484                     | Test: Loss 0.79383, R2 0.78040, RMSE 0.88352\n",
      "Epoch [25/500]       | Train: Loss 0.75425, R2 0.78862, RMSE 0.86470                     | Test: Loss 0.82857, R2 0.77302, RMSE 0.90668\n",
      "Epoch [26/500]       | Train: Loss 0.73703, R2 0.79322, RMSE 0.85580                     | Test: Loss 0.79239, R2 0.78040, RMSE 0.88411\n",
      "Epoch [27/500]       | Train: Loss 0.72805, R2 0.79572, RMSE 0.84989                     | Test: Loss 0.80303, R2 0.78187, RMSE 0.89209\n",
      "Epoch [28/500]       | Train: Loss 0.73333, R2 0.79351, RMSE 0.85273                     | Test: Loss 0.83289, R2 0.76763, RMSE 0.90288\n",
      "Epoch [29/500]       | Train: Loss 0.71203, R2 0.80078, RMSE 0.84059                     | Test: Loss 0.80193, R2 0.77493, RMSE 0.89071\n",
      "Epoch [30/500]       | Train: Loss 0.70975, R2 0.80061, RMSE 0.83862                     | Test: Loss 0.81864, R2 0.74298, RMSE 0.89841\n",
      "Epoch [31/500]       | Train: Loss 0.70650, R2 0.80071, RMSE 0.83728                     | Test: Loss 0.85076, R2 0.76710, RMSE 0.91620\n",
      "Epoch [32/500]       | Train: Loss 0.71170, R2 0.79978, RMSE 0.84083                     | Test: Loss 0.79516, R2 0.77896, RMSE 0.88532\n",
      "Epoch [33/500]       | Train: Loss 0.71585, R2 0.80009, RMSE 0.84308                     | Test: Loss 0.76373, R2 0.79332, RMSE 0.86392\n",
      "Epoch [34/500]       | Train: Loss 0.68079, R2 0.80874, RMSE 0.82271                     | Test: Loss 0.91854, R2 0.75598, RMSE 0.94273\n",
      "Epoch [35/500]       | Train: Loss 0.70893, R2 0.80166, RMSE 0.83898                     | Test: Loss 0.81085, R2 0.78019, RMSE 0.89700\n",
      "Epoch [36/500]       | Train: Loss 0.69425, R2 0.80645, RMSE 0.82944                     | Test: Loss 0.84788, R2 0.77039, RMSE 0.91581\n",
      "Epoch [37/500]       | Train: Loss 0.67736, R2 0.80874, RMSE 0.82030                     | Test: Loss 0.76602, R2 0.79047, RMSE 0.86231\n",
      "Epoch [38/500]       | Train: Loss 0.67923, R2 0.81046, RMSE 0.81966                     | Test: Loss 0.80363, R2 0.78240, RMSE 0.89166\n",
      "Epoch [39/500]       | Train: Loss 0.67430, R2 0.80953, RMSE 0.81824                     | Test: Loss 0.77878, R2 0.78377, RMSE 0.86659\n",
      "Epoch [40/500]       | Train: Loss 0.68422, R2 0.80745, RMSE 0.82366                     | Test: Loss 0.77640, R2 0.79087, RMSE 0.87553\n",
      "Epoch [41/500]       | Train: Loss 0.65159, R2 0.81666, RMSE 0.80444                     | Test: Loss 0.77738, R2 0.78643, RMSE 0.86859\n",
      "Epoch [42/500]       | Train: Loss 0.66140, R2 0.81359, RMSE 0.81051                     | Test: Loss 0.77784, R2 0.79118, RMSE 0.87293\n",
      "Epoch [43/500]       | Train: Loss 0.65518, R2 0.81598, RMSE 0.80632                     | Test: Loss 0.79635, R2 0.78896, RMSE 0.88910\n",
      "Epoch [44/500]       | Train: Loss 0.65561, R2 0.81378, RMSE 0.80665                     | Test: Loss 0.74840, R2 0.79518, RMSE 0.85737\n",
      "Epoch [45/500]       | Train: Loss 0.64762, R2 0.81714, RMSE 0.80176                     | Test: Loss 0.78765, R2 0.78175, RMSE 0.87125\n",
      "Epoch [46/500]       | Train: Loss 0.65713, R2 0.81473, RMSE 0.80779                     | Test: Loss 0.77628, R2 0.78307, RMSE 0.87261\n",
      "Epoch [47/500]       | Train: Loss 0.62769, R2 0.82300, RMSE 0.78995                     | Test: Loss 0.77842, R2 0.78725, RMSE 0.87647\n",
      "Epoch [48/500]       | Train: Loss 0.64015, R2 0.81949, RMSE 0.79728                     | Test: Loss 0.86682, R2 0.75900, RMSE 0.92395\n",
      "Epoch [49/500]       | Train: Loss 0.63171, R2 0.82296, RMSE 0.79318                     | Test: Loss 0.79716, R2 0.79115, RMSE 0.88621\n",
      "Epoch [50/500]       | Train: Loss 0.62073, R2 0.82621, RMSE 0.78583                     | Test: Loss 0.74925, R2 0.78565, RMSE 0.85628\n",
      "Epoch [51/500]       | Train: Loss 0.62809, R2 0.82367, RMSE 0.78970                     | Test: Loss 0.81677, R2 0.77883, RMSE 0.89692\n",
      "Epoch [52/500]       | Train: Loss 0.60945, R2 0.82724, RMSE 0.77839                     | Test: Loss 0.81315, R2 0.73642, RMSE 0.89599\n",
      "Epoch [53/500]       | Train: Loss 0.60470, R2 0.83041, RMSE 0.77476                     | Test: Loss 0.89561, R2 0.73290, RMSE 0.91667\n",
      "Epoch [54/500]       | Train: Loss 0.60680, R2 0.83014, RMSE 0.77539                     | Test: Loss 0.73785, R2 0.79435, RMSE 0.84553\n",
      "Epoch [55/500]       | Train: Loss 0.59551, R2 0.83195, RMSE 0.76885                     | Test: Loss 0.77853, R2 0.78384, RMSE 0.87603\n",
      "Epoch [56/500]       | Train: Loss 0.59393, R2 0.83125, RMSE 0.76832                     | Test: Loss 0.77566, R2 0.79364, RMSE 0.87612\n",
      "Epoch [57/500]       | Train: Loss 0.59357, R2 0.83311, RMSE 0.76728                     | Test: Loss 0.78260, R2 0.78548, RMSE 0.87195\n",
      "Epoch [58/500]       | Train: Loss 0.61815, R2 0.82615, RMSE 0.78336                     | Test: Loss 0.83830, R2 0.78351, RMSE 0.90746\n",
      "Epoch [59/500]       | Train: Loss 0.59740, R2 0.83199, RMSE 0.77072                     | Test: Loss 0.83903, R2 0.75917, RMSE 0.90951\n",
      "Epoch [60/500]       | Train: Loss 0.60654, R2 0.82873, RMSE 0.77689                     | Test: Loss 0.74783, R2 0.79821, RMSE 0.85575\n",
      "Epoch [61/500]       | Train: Loss 0.58678, R2 0.83498, RMSE 0.76393                     | Test: Loss 0.80150, R2 0.79082, RMSE 0.88992\n",
      "Epoch [62/500]       | Train: Loss 0.58003, R2 0.83603, RMSE 0.75891                     | Test: Loss 0.77879, R2 0.79184, RMSE 0.87179\n",
      "Epoch [63/500]       | Train: Loss 0.57121, R2 0.83936, RMSE 0.75373                     | Test: Loss 0.80173, R2 0.78385, RMSE 0.88847\n",
      "Epoch [64/500]       | Train: Loss 0.56890, R2 0.84004, RMSE 0.75259                     | Test: Loss 0.80020, R2 0.78404, RMSE 0.88631\n",
      "Epoch [65/500]       | Train: Loss 0.55528, R2 0.84385, RMSE 0.74332                     | Test: Loss 0.77548, R2 0.78877, RMSE 0.87315\n",
      "Epoch [66/500]       | Train: Loss 0.56603, R2 0.83943, RMSE 0.74993                     | Test: Loss 0.77405, R2 0.79476, RMSE 0.87310\n",
      "Epoch [67/500]       | Train: Loss 0.56291, R2 0.84165, RMSE 0.74820                     | Test: Loss 0.88311, R2 0.78175, RMSE 0.91648\n",
      "Epoch [68/500]       | Train: Loss 0.55235, R2 0.84398, RMSE 0.74054                     | Test: Loss 0.76691, R2 0.78600, RMSE 0.86338\n",
      "Epoch [69/500]       | Train: Loss 0.54495, R2 0.84513, RMSE 0.73640                     | Test: Loss 0.73887, R2 0.79784, RMSE 0.84474\n",
      "Epoch [70/500]       | Train: Loss 0.53221, R2 0.84945, RMSE 0.72772                     | Test: Loss 0.84479, R2 0.76991, RMSE 0.90382\n",
      "Epoch [71/500]       | Train: Loss 0.53582, R2 0.85013, RMSE 0.72927                     | Test: Loss 0.78010, R2 0.77278, RMSE 0.87266\n",
      "Epoch [72/500]       | Train: Loss 0.53479, R2 0.84911, RMSE 0.72922                     | Test: Loss 0.76868, R2 0.78356, RMSE 0.86485\n",
      "Epoch [73/500]       | Train: Loss 0.53839, R2 0.84950, RMSE 0.73199                     | Test: Loss 0.79488, R2 0.78097, RMSE 0.87590\n",
      "Epoch [74/500]       | Train: Loss 0.52029, R2 0.85326, RMSE 0.71930                     | Test: Loss 1.03518, R2 0.71329, RMSE 0.96569\n",
      "Epoch [75/500]       | Train: Loss 0.52800, R2 0.85190, RMSE 0.72372                     | Test: Loss 0.78930, R2 0.77783, RMSE 0.88393\n",
      "Epoch [76/500]       | Train: Loss 0.52612, R2 0.85221, RMSE 0.72285                     | Test: Loss 0.77511, R2 0.78103, RMSE 0.86955\n",
      "Epoch [77/500]       | Train: Loss 0.51910, R2 0.85452, RMSE 0.71832                     | Test: Loss 0.88252, R2 0.77104, RMSE 0.92808\n",
      "Epoch [78/500]       | Train: Loss 0.53310, R2 0.85072, RMSE 0.72777                     | Test: Loss 0.77117, R2 0.78745, RMSE 0.86580\n",
      "Epoch [79/500]       | Train: Loss 0.53281, R2 0.84883, RMSE 0.72823                     | Test: Loss 0.83944, R2 0.75703, RMSE 0.91291\n",
      "Epoch [80/500]       | Train: Loss 0.52231, R2 0.85244, RMSE 0.72014                     | Test: Loss 0.78928, R2 0.78425, RMSE 0.88225\n",
      "Epoch [81/500]       | Train: Loss 0.50448, R2 0.85654, RMSE 0.70817                     | Test: Loss 0.84525, R2 0.76625, RMSE 0.91163\n",
      "Epoch [82/500]       | Train: Loss 0.53305, R2 0.84917, RMSE 0.72676                     | Test: Loss 0.76755, R2 0.78904, RMSE 0.85868\n",
      "Epoch [83/500]       | Train: Loss 0.50290, R2 0.85737, RMSE 0.70659                     | Test: Loss 0.78595, R2 0.77596, RMSE 0.87522\n",
      "Epoch [84/500]       | Train: Loss 0.49898, R2 0.85981, RMSE 0.70432                     | Test: Loss 0.75903, R2 0.78432, RMSE 0.86210\n",
      "Epoch [85/500]       | Train: Loss 0.49190, R2 0.86179, RMSE 0.69957                     | Test: Loss 0.79501, R2 0.78062, RMSE 0.88544\n",
      "Epoch [86/500]       | Train: Loss 0.48392, R2 0.86452, RMSE 0.69340                     | Test: Loss 0.84244, R2 0.77740, RMSE 0.90629\n",
      "Epoch [87/500]       | Train: Loss 0.49183, R2 0.86151, RMSE 0.69957                     | Test: Loss 0.76243, R2 0.77935, RMSE 0.86639\n",
      "Epoch [88/500]       | Train: Loss 0.49982, R2 0.85936, RMSE 0.70507                     | Test: Loss 0.88645, R2 0.75102, RMSE 0.93066\n",
      "Epoch [89/500]       | Train: Loss 0.48203, R2 0.86458, RMSE 0.69299                     | Test: Loss 0.80402, R2 0.76113, RMSE 0.89134\n",
      "Epoch [90/500]       | Train: Loss 0.48058, R2 0.86408, RMSE 0.69160                     | Test: Loss 0.88055, R2 0.75947, RMSE 0.92763\n",
      "Epoch [91/500]       | Train: Loss 0.47231, R2 0.86760, RMSE 0.68550                     | Test: Loss 0.80753, R2 0.78521, RMSE 0.89062\n",
      "Epoch [92/500]       | Train: Loss 0.46861, R2 0.86786, RMSE 0.68280                     | Test: Loss 0.81016, R2 0.77161, RMSE 0.88953\n",
      "Epoch [93/500]       | Train: Loss 0.46845, R2 0.86738, RMSE 0.68261                     | Test: Loss 0.79892, R2 0.78232, RMSE 0.88955\n",
      "Epoch [94/500]       | Train: Loss 0.46865, R2 0.86636, RMSE 0.68296                     | Test: Loss 0.92747, R2 0.75593, RMSE 0.94346\n",
      "Epoch [95/500]       | Train: Loss 0.45859, R2 0.86886, RMSE 0.67521                     | Test: Loss 0.77907, R2 0.78266, RMSE 0.86735\n",
      "Epoch [96/500]       | Train: Loss 0.47084, R2 0.86692, RMSE 0.68403                     | Test: Loss 0.79421, R2 0.78483, RMSE 0.88467\n",
      "Epoch [97/500]       | Train: Loss 0.44910, R2 0.87216, RMSE 0.66858                     | Test: Loss 0.79984, R2 0.78225, RMSE 0.87575\n",
      "Epoch [98/500]       | Train: Loss 0.44109, R2 0.87522, RMSE 0.66263                     | Test: Loss 0.80540, R2 0.78207, RMSE 0.89053\n",
      "Epoch [99/500]       | Train: Loss 0.44073, R2 0.87447, RMSE 0.66251                     | Test: Loss 0.80251, R2 0.78214, RMSE 0.88771\n",
      "Epoch [100/500]      | Train: Loss 0.44153, R2 0.87524, RMSE 0.66320                     | Test: Loss 0.78323, R2 0.78943, RMSE 0.87113\n",
      "Epoch [101/500]      | Train: Loss 0.43262, R2 0.87780, RMSE 0.65585                     | Test: Loss 0.85491, R2 0.76073, RMSE 0.91340\n",
      "Epoch [102/500]      | Train: Loss 0.43548, R2 0.87651, RMSE 0.65856                     | Test: Loss 0.81395, R2 0.77533, RMSE 0.89597\n",
      "Epoch [103/500]      | Train: Loss 0.42808, R2 0.87948, RMSE 0.65308                     | Test: Loss 0.81500, R2 0.77682, RMSE 0.89501\n",
      "Epoch [104/500]      | Train: Loss 0.43910, R2 0.87686, RMSE 0.66075                     | Test: Loss 0.83455, R2 0.76158, RMSE 0.90295\n",
      "Epoch [105/500]      | Train: Loss 0.43529, R2 0.87739, RMSE 0.65878                     | Test: Loss 0.88906, R2 0.76519, RMSE 0.92232\n",
      "Epoch [106/500]      | Train: Loss 0.41681, R2 0.88153, RMSE 0.64423                     | Test: Loss 0.82899, R2 0.76739, RMSE 0.90483\n",
      "Epoch [107/500]      | Train: Loss 0.41136, R2 0.88487, RMSE 0.64017                     | Test: Loss 0.79789, R2 0.77733, RMSE 0.88381\n",
      "Epoch [108/500]      | Train: Loss 0.41521, R2 0.88311, RMSE 0.64344                     | Test: Loss 1.05661, R2 0.74035, RMSE 0.96836\n",
      "Epoch [109/500]      | Train: Loss 0.41470, R2 0.88300, RMSE 0.64272                     | Test: Loss 0.79755, R2 0.77940, RMSE 0.88714\n",
      "Epoch [110/500]      | Train: Loss 0.42671, R2 0.87926, RMSE 0.65198                     | Test: Loss 0.82977, R2 0.77863, RMSE 0.90487\n",
      "Epoch [111/500]      | Train: Loss 0.40196, R2 0.88520, RMSE 0.63283                     | Test: Loss 0.80812, R2 0.76985, RMSE 0.89115\n",
      "Epoch [112/500]      | Train: Loss 0.39330, R2 0.88859, RMSE 0.62617                     | Test: Loss 0.80864, R2 0.77528, RMSE 0.88679\n",
      "Epoch [113/500]      | Train: Loss 0.40174, R2 0.88660, RMSE 0.63296                     | Test: Loss 0.77298, R2 0.77863, RMSE 0.86418\n",
      "Epoch [114/500]      | Train: Loss 0.39849, R2 0.88742, RMSE 0.63003                     | Test: Loss 1.02602, R2 0.75414, RMSE 0.96995\n",
      "Epoch [115/500]      | Train: Loss 0.39670, R2 0.88800, RMSE 0.62878                     | Test: Loss 0.79859, R2 0.77845, RMSE 0.87399\n",
      "Epoch [116/500]      | Train: Loss 0.40598, R2 0.88501, RMSE 0.63572                     | Test: Loss 0.78896, R2 0.78520, RMSE 0.86865\n",
      "Epoch [117/500]      | Train: Loss 0.38923, R2 0.88967, RMSE 0.62284                     | Test: Loss 0.90676, R2 0.75851, RMSE 0.94178\n",
      "Epoch [118/500]      | Train: Loss 0.38888, R2 0.88988, RMSE 0.62259                     | Test: Loss 0.81261, R2 0.77048, RMSE 0.89294\n",
      "Epoch [119/500]      | Train: Loss 0.38016, R2 0.89314, RMSE 0.61564                     | Test: Loss 0.79681, R2 0.76643, RMSE 0.88127\n",
      "Epoch [120/500]      | Train: Loss 0.38321, R2 0.89204, RMSE 0.61764                     | Test: Loss 0.79943, R2 0.77509, RMSE 0.88495\n",
      "Epoch [121/500]      | Train: Loss 0.38075, R2 0.89216, RMSE 0.61573                     | Test: Loss 0.82056, R2 0.77308, RMSE 0.89823\n",
      "Epoch [122/500]      | Train: Loss 0.38433, R2 0.88992, RMSE 0.61918                     | Test: Loss 0.90508, R2 0.76973, RMSE 0.94018\n",
      "Epoch [123/500]      | Train: Loss 0.36871, R2 0.89440, RMSE 0.60636                     | Test: Loss 0.84267, R2 0.77380, RMSE 0.91070\n",
      "Epoch [124/500]      | Train: Loss 0.36685, R2 0.89657, RMSE 0.60467                     | Test: Loss 0.84245, R2 0.76324, RMSE 0.91097\n",
      "Epoch [125/500]      | Train: Loss 0.35760, R2 0.89913, RMSE 0.59730                     | Test: Loss 0.80121, R2 0.78131, RMSE 0.88485\n",
      "Epoch [126/500]      | Train: Loss 0.35652, R2 0.89766, RMSE 0.59624                     | Test: Loss 0.88341, R2 0.74641, RMSE 0.93343\n",
      "Epoch [127/500]      | Train: Loss 0.35910, R2 0.89913, RMSE 0.59812                     | Test: Loss 0.84310, R2 0.75220, RMSE 0.91043\n",
      "Epoch [128/500]      | Train: Loss 0.35547, R2 0.89886, RMSE 0.59492                     | Test: Loss 0.88447, R2 0.73734, RMSE 0.93652\n",
      "Epoch [129/500]      | Train: Loss 0.33888, R2 0.90417, RMSE 0.58119                     | Test: Loss 0.81514, R2 0.77134, RMSE 0.89540\n",
      "Epoch [130/500]      | Train: Loss 0.35852, R2 0.89885, RMSE 0.59776                     | Test: Loss 0.84003, R2 0.77707, RMSE 0.91092\n",
      "Epoch [131/500]      | Train: Loss 0.34958, R2 0.90106, RMSE 0.58989                     | Test: Loss 0.84858, R2 0.76768, RMSE 0.91339\n",
      "Epoch [132/500]      | Train: Loss 0.34305, R2 0.90277, RMSE 0.58481                     | Test: Loss 0.90239, R2 0.76363, RMSE 0.94032\n",
      "Epoch [133/500]      | Train: Loss 0.34223, R2 0.90295, RMSE 0.58394                     | Test: Loss 0.82548, R2 0.76568, RMSE 0.90257\n",
      "Epoch [134/500]      | Train: Loss 0.33965, R2 0.90439, RMSE 0.58159                     | Test: Loss 0.88734, R2 0.76401, RMSE 0.93078\n",
      "Epoch [135/500]      | Train: Loss 0.34147, R2 0.90387, RMSE 0.58355                     | Test: Loss 0.83392, R2 0.75983, RMSE 0.90549\n",
      "Epoch [136/500]      | Train: Loss 0.32640, R2 0.90846, RMSE 0.57028                     | Test: Loss 0.81617, R2 0.78070, RMSE 0.88846\n",
      "Epoch [137/500]      | Train: Loss 0.32812, R2 0.90719, RMSE 0.57202                     | Test: Loss 0.89955, R2 0.75850, RMSE 0.93506\n",
      "Epoch [138/500]      | Train: Loss 0.32800, R2 0.90670, RMSE 0.57169                     | Test: Loss 0.83999, R2 0.77284, RMSE 0.90335\n",
      "Epoch [139/500]      | Train: Loss 0.31567, R2 0.91029, RMSE 0.56080                     | Test: Loss 0.85182, R2 0.75691, RMSE 0.91004\n",
      "Epoch [140/500]      | Train: Loss 0.31744, R2 0.91000, RMSE 0.56218                     | Test: Loss 0.86492, R2 0.75124, RMSE 0.92125\n",
      "Epoch [141/500]      | Train: Loss 0.34175, R2 0.90284, RMSE 0.58287                     | Test: Loss 0.80744, R2 0.78230, RMSE 0.88607\n",
      "Epoch [142/500]      | Train: Loss 0.31491, R2 0.91058, RMSE 0.56010                     | Test: Loss 0.81111, R2 0.77451, RMSE 0.89088\n",
      "Epoch [143/500]      | Train: Loss 0.31532, R2 0.91047, RMSE 0.56060                     | Test: Loss 0.86552, R2 0.76651, RMSE 0.92191\n",
      "Epoch [144/500]      | Train: Loss 0.30986, R2 0.91189, RMSE 0.55596                     | Test: Loss 0.83975, R2 0.77489, RMSE 0.91259\n",
      "Epoch [145/500]      | Train: Loss 0.29920, R2 0.91487, RMSE 0.54640                     | Test: Loss 0.86608, R2 0.76385, RMSE 0.92413\n",
      "Epoch [146/500]      | Train: Loss 0.30046, R2 0.91467, RMSE 0.54715                     | Test: Loss 0.93719, R2 0.74560, RMSE 0.95645\n",
      "Epoch [147/500]      | Train: Loss 0.30237, R2 0.91453, RMSE 0.54896                     | Test: Loss 0.83585, R2 0.74501, RMSE 0.90831\n",
      "Epoch [148/500]      | Train: Loss 0.29489, R2 0.91640, RMSE 0.54227                     | Test: Loss 0.83151, R2 0.76969, RMSE 0.90611\n",
      "Epoch [149/500]      | Train: Loss 0.29869, R2 0.91609, RMSE 0.54577                     | Test: Loss 0.83323, R2 0.77420, RMSE 0.90774\n",
      "Epoch [150/500]      | Train: Loss 0.29139, R2 0.91658, RMSE 0.53893                     | Test: Loss 0.83803, R2 0.76478, RMSE 0.90649\n",
      "Epoch [151/500]      | Train: Loss 0.29583, R2 0.91577, RMSE 0.54333                     | Test: Loss 0.82572, R2 0.77684, RMSE 0.89706\n",
      "Epoch [152/500]      | Train: Loss 0.29552, R2 0.91588, RMSE 0.54296                     | Test: Loss 0.82509, R2 0.78163, RMSE 0.90337\n",
      "Epoch [153/500]      | Train: Loss 0.28942, R2 0.91751, RMSE 0.53732                     | Test: Loss 0.84022, R2 0.75664, RMSE 0.91035\n",
      "Epoch [154/500]      | Train: Loss 0.28695, R2 0.91919, RMSE 0.53484                     | Test: Loss 0.88295, R2 0.75737, RMSE 0.93216\n",
      "Epoch [155/500]      | Train: Loss 0.28975, R2 0.91873, RMSE 0.53709                     | Test: Loss 0.84543, R2 0.76662, RMSE 0.91233\n",
      "Epoch [156/500]      | Train: Loss 0.27743, R2 0.92119, RMSE 0.52596                     | Test: Loss 0.86170, R2 0.76957, RMSE 0.92034\n",
      "Epoch [157/500]      | Train: Loss 0.26494, R2 0.92485, RMSE 0.51398                     | Test: Loss 0.93634, R2 0.70321, RMSE 0.95539\n",
      "Epoch [158/500]      | Train: Loss 0.27299, R2 0.92230, RMSE 0.52179                     | Test: Loss 0.87181, R2 0.76677, RMSE 0.92011\n",
      "Epoch [159/500]      | Train: Loss 0.26569, R2 0.92516, RMSE 0.51474                     | Test: Loss 0.83714, R2 0.76880, RMSE 0.91028\n",
      "Epoch [160/500]      | Train: Loss 0.26669, R2 0.92472, RMSE 0.51565                     | Test: Loss 0.87085, R2 0.76048, RMSE 0.92670\n",
      "Epoch [161/500]      | Train: Loss 0.26356, R2 0.92569, RMSE 0.51294                     | Test: Loss 0.85932, R2 0.73137, RMSE 0.92357\n",
      "Epoch [162/500]      | Train: Loss 0.25780, R2 0.92625, RMSE 0.50689                     | Test: Loss 0.86094, R2 0.77477, RMSE 0.92268\n",
      "Epoch [163/500]      | Train: Loss 0.26142, R2 0.92609, RMSE 0.51068                     | Test: Loss 0.82279, R2 0.77021, RMSE 0.89758\n",
      "Epoch [164/500]      | Train: Loss 0.25877, R2 0.92663, RMSE 0.50798                     | Test: Loss 0.85618, R2 0.75404, RMSE 0.91912\n",
      "Epoch [165/500]      | Train: Loss 0.26750, R2 0.92473, RMSE 0.51591                     | Test: Loss 0.83702, R2 0.75797, RMSE 0.91020\n",
      "Epoch [166/500]      | Train: Loss 0.25276, R2 0.92840, RMSE 0.50198                     | Test: Loss 0.85554, R2 0.76570, RMSE 0.91917\n",
      "Epoch [167/500]      | Train: Loss 0.24737, R2 0.92895, RMSE 0.49675                     | Test: Loss 0.86048, R2 0.76468, RMSE 0.91678\n",
      "Epoch [168/500]      | Train: Loss 0.24523, R2 0.93053, RMSE 0.49435                     | Test: Loss 0.86637, R2 0.76692, RMSE 0.92174\n",
      "Epoch [169/500]      | Train: Loss 0.23545, R2 0.93298, RMSE 0.48450                     | Test: Loss 0.87162, R2 0.76178, RMSE 0.92650\n",
      "Epoch [170/500]      | Train: Loss 0.24666, R2 0.92987, RMSE 0.49557                     | Test: Loss 0.87535, R2 0.76787, RMSE 0.93323\n",
      "Epoch [171/500]      | Train: Loss 0.24676, R2 0.93026, RMSE 0.49599                     | Test: Loss 0.82925, R2 0.76433, RMSE 0.90037\n",
      "Epoch [172/500]      | Train: Loss 0.23453, R2 0.93365, RMSE 0.48365                     | Test: Loss 0.86573, R2 0.75734, RMSE 0.92186\n",
      "Epoch [173/500]      | Train: Loss 0.23207, R2 0.93423, RMSE 0.48085                     | Test: Loss 0.82602, R2 0.76701, RMSE 0.90041\n",
      "Epoch [174/500]      | Train: Loss 0.22904, R2 0.93528, RMSE 0.47787                     | Test: Loss 0.89024, R2 0.76653, RMSE 0.93060\n",
      "Epoch [175/500]      | Train: Loss 0.23237, R2 0.93430, RMSE 0.48144                     | Test: Loss 0.86972, R2 0.76791, RMSE 0.92812\n",
      "Epoch [176/500]      | Train: Loss 0.23308, R2 0.93398, RMSE 0.48192                     | Test: Loss 0.87001, R2 0.76432, RMSE 0.92722\n",
      "Epoch [177/500]      | Train: Loss 0.22554, R2 0.93604, RMSE 0.47434                     | Test: Loss 0.87862, R2 0.74507, RMSE 0.92914\n",
      "Epoch [178/500]      | Train: Loss 0.22226, R2 0.93753, RMSE 0.47043                     | Test: Loss 0.84871, R2 0.75544, RMSE 0.91356\n",
      "Epoch [179/500]      | Train: Loss 0.21476, R2 0.93889, RMSE 0.46285                     | Test: Loss 0.84624, R2 0.75398, RMSE 0.91366\n",
      "Epoch [180/500]      | Train: Loss 0.22236, R2 0.93810, RMSE 0.47084                     | Test: Loss 0.86649, R2 0.75094, RMSE 0.92109\n",
      "Epoch [181/500]      | Train: Loss 0.21538, R2 0.93883, RMSE 0.46327                     | Test: Loss 0.87904, R2 0.75985, RMSE 0.92991\n",
      "Epoch [182/500]      | Train: Loss 0.22784, R2 0.93587, RMSE 0.47653                     | Test: Loss 0.88181, R2 0.75618, RMSE 0.92816\n",
      "Epoch [183/500]      | Train: Loss 0.21553, R2 0.93840, RMSE 0.46329                     | Test: Loss 0.88128, R2 0.75296, RMSE 0.93192\n",
      "Epoch [184/500]      | Train: Loss 0.21162, R2 0.93995, RMSE 0.45910                     | Test: Loss 0.85864, R2 0.76410, RMSE 0.91069\n",
      "Epoch [185/500]      | Train: Loss 0.20081, R2 0.94353, RMSE 0.44741                     | Test: Loss 0.90497, R2 0.73338, RMSE 0.94467\n",
      "Epoch [186/500]      | Train: Loss 0.20553, R2 0.94157, RMSE 0.45248                     | Test: Loss 0.89141, R2 0.75946, RMSE 0.93922\n",
      "Epoch [187/500]      | Train: Loss 0.19984, R2 0.94393, RMSE 0.44650                     | Test: Loss 0.90983, R2 0.76885, RMSE 0.94383\n",
      "Epoch [188/500]      | Train: Loss 0.20221, R2 0.94269, RMSE 0.44895                     | Test: Loss 0.87240, R2 0.75214, RMSE 0.92808\n",
      "Epoch [189/500]      | Train: Loss 0.20438, R2 0.94191, RMSE 0.45138                     | Test: Loss 0.82086, R2 0.77448, RMSE 0.89260\n",
      "Epoch [190/500]      | Train: Loss 0.19377, R2 0.94519, RMSE 0.43946                     | Test: Loss 0.88261, R2 0.74519, RMSE 0.93334\n",
      "Epoch [191/500]      | Train: Loss 0.19019, R2 0.94656, RMSE 0.43528                     | Test: Loss 0.87437, R2 0.75413, RMSE 0.92779\n",
      "Epoch [192/500]      | Train: Loss 0.18975, R2 0.94581, RMSE 0.43489                     | Test: Loss 0.87004, R2 0.76645, RMSE 0.92736\n",
      "Epoch [193/500]      | Train: Loss 0.18980, R2 0.94626, RMSE 0.43507                     | Test: Loss 0.87983, R2 0.75129, RMSE 0.93073\n",
      "Epoch [194/500]      | Train: Loss 0.18799, R2 0.94707, RMSE 0.43291                     | Test: Loss 0.84460, R2 0.76072, RMSE 0.90427\n",
      "Epoch [195/500]      | Train: Loss 0.17957, R2 0.94928, RMSE 0.42303                     | Test: Loss 0.91041, R2 0.75741, RMSE 0.94716\n",
      "Epoch [196/500]      | Train: Loss 0.18932, R2 0.94623, RMSE 0.43421                     | Test: Loss 0.85031, R2 0.76418, RMSE 0.90862\n",
      "Epoch [197/500]      | Train: Loss 0.19627, R2 0.94475, RMSE 0.44201                     | Test: Loss 0.89666, R2 0.71927, RMSE 0.94393\n",
      "Epoch [198/500]      | Train: Loss 0.18066, R2 0.94869, RMSE 0.42404                     | Test: Loss 0.98971, R2 0.72205, RMSE 0.97730\n",
      "Epoch [199/500]      | Train: Loss 0.17982, R2 0.94897, RMSE 0.42356                     | Test: Loss 1.04015, R2 0.69794, RMSE 0.98940\n",
      "Epoch [200/500]      | Train: Loss 0.18036, R2 0.94870, RMSE 0.42383                     | Test: Loss 0.85543, R2 0.76424, RMSE 0.91582\n",
      "Epoch [201/500]      | Train: Loss 0.18426, R2 0.94804, RMSE 0.42854                     | Test: Loss 0.88666, R2 0.76119, RMSE 0.92979\n",
      "Epoch [202/500]      | Train: Loss 0.17261, R2 0.95124, RMSE 0.41472                     | Test: Loss 0.92208, R2 0.73001, RMSE 0.95489\n",
      "Epoch [203/500]      | Train: Loss 0.17088, R2 0.95133, RMSE 0.41271                     | Test: Loss 0.91813, R2 0.73506, RMSE 0.95382\n",
      "Epoch [204/500]      | Train: Loss 0.16804, R2 0.95275, RMSE 0.40926                     | Test: Loss 0.86624, R2 0.76257, RMSE 0.92425\n",
      "Epoch [205/500]      | Train: Loss 0.16421, R2 0.95301, RMSE 0.40477                     | Test: Loss 0.93972, R2 0.73044, RMSE 0.96332\n",
      "Epoch [206/500]      | Train: Loss 0.16249, R2 0.95437, RMSE 0.40241                     | Test: Loss 0.84718, R2 0.76620, RMSE 0.91074\n",
      "Epoch [207/500]      | Train: Loss 0.16429, R2 0.95330, RMSE 0.40463                     | Test: Loss 0.87872, R2 0.75328, RMSE 0.93307\n",
      "Epoch [208/500]      | Train: Loss 0.16912, R2 0.95195, RMSE 0.41047                     | Test: Loss 0.91731, R2 0.74531, RMSE 0.95580\n",
      "Epoch [209/500]      | Train: Loss 0.16265, R2 0.95399, RMSE 0.40246                     | Test: Loss 0.87039, R2 0.76068, RMSE 0.91956\n",
      "Epoch [210/500]      | Train: Loss 0.16579, R2 0.95279, RMSE 0.40643                     | Test: Loss 0.93596, R2 0.71772, RMSE 0.95814\n",
      "Epoch [211/500]      | Train: Loss 0.16085, R2 0.95504, RMSE 0.40025                     | Test: Loss 0.92689, R2 0.75696, RMSE 0.95431\n",
      "Epoch [212/500]      | Train: Loss 0.15828, R2 0.95531, RMSE 0.39735                     | Test: Loss 0.90504, R2 0.74173, RMSE 0.94282\n",
      "Epoch [213/500]      | Train: Loss 0.15177, R2 0.95663, RMSE 0.38883                     | Test: Loss 0.86792, R2 0.76068, RMSE 0.92552\n",
      "Epoch [214/500]      | Train: Loss 0.15787, R2 0.95554, RMSE 0.39668                     | Test: Loss 0.91149, R2 0.75029, RMSE 0.95045\n",
      "Epoch [215/500]      | Train: Loss 0.15155, R2 0.95725, RMSE 0.38870                     | Test: Loss 0.88102, R2 0.76032, RMSE 0.92941\n",
      "Epoch [216/500]      | Train: Loss 0.15274, R2 0.95646, RMSE 0.39031                     | Test: Loss 0.92693, R2 0.75264, RMSE 0.95622\n",
      "Epoch [217/500]      | Train: Loss 0.15655, R2 0.95537, RMSE 0.39508                     | Test: Loss 0.89737, R2 0.75561, RMSE 0.94023\n",
      "Epoch [218/500]      | Train: Loss 0.14994, R2 0.95751, RMSE 0.38651                     | Test: Loss 0.88874, R2 0.75392, RMSE 0.93513\n",
      "Epoch [219/500]      | Train: Loss 0.14581, R2 0.95869, RMSE 0.38124                     | Test: Loss 0.89863, R2 0.76282, RMSE 0.94151\n",
      "Epoch [220/500]      | Train: Loss 0.14133, R2 0.95995, RMSE 0.37477                     | Test: Loss 0.95141, R2 0.64666, RMSE 0.96862\n",
      "Epoch [221/500]      | Train: Loss 0.14284, R2 0.95988, RMSE 0.37712                     | Test: Loss 0.90512, R2 0.75334, RMSE 0.94860\n",
      "Epoch [222/500]      | Train: Loss 0.14364, R2 0.95963, RMSE 0.37825                     | Test: Loss 0.88164, R2 0.75784, RMSE 0.93278\n",
      "Epoch [223/500]      | Train: Loss 0.14609, R2 0.95876, RMSE 0.38134                     | Test: Loss 0.88717, R2 0.74782, RMSE 0.93165\n",
      "Epoch [224/500]      | Train: Loss 0.14211, R2 0.95974, RMSE 0.37632                     | Test: Loss 0.87979, R2 0.76195, RMSE 0.93345\n",
      "Epoch [225/500]      | Train: Loss 0.14076, R2 0.96024, RMSE 0.37434                     | Test: Loss 0.91654, R2 0.75025, RMSE 0.95159\n",
      "Epoch [226/500]      | Train: Loss 0.14066, R2 0.96019, RMSE 0.37427                     | Test: Loss 1.01670, R2 0.73896, RMSE 0.98969\n",
      "Epoch [227/500]      | Train: Loss 0.13918, R2 0.96061, RMSE 0.37246                     | Test: Loss 0.86291, R2 0.75860, RMSE 0.91353\n",
      "Epoch [228/500]      | Train: Loss 0.13460, R2 0.96207, RMSE 0.36631                     | Test: Loss 0.90919, R2 0.76002, RMSE 0.94382\n",
      "Epoch [229/500]      | Train: Loss 0.13384, R2 0.96232, RMSE 0.36513                     | Test: Loss 0.96208, R2 0.75148, RMSE 0.96705\n",
      "Epoch [230/500]      | Train: Loss 0.13353, R2 0.96183, RMSE 0.36469                     | Test: Loss 0.90374, R2 0.73241, RMSE 0.94368\n",
      "Epoch [231/500]      | Train: Loss 0.13563, R2 0.96187, RMSE 0.36751                     | Test: Loss 0.90613, R2 0.74451, RMSE 0.94868\n",
      "Epoch [232/500]      | Train: Loss 0.13007, R2 0.96317, RMSE 0.36008                     | Test: Loss 0.90438, R2 0.75584, RMSE 0.94427\n",
      "Epoch [233/500]      | Train: Loss 0.13903, R2 0.96060, RMSE 0.37200                     | Test: Loss 0.98291, R2 0.73833, RMSE 0.97474\n",
      "Epoch [234/500]      | Train: Loss 0.12802, R2 0.96370, RMSE 0.35704                     | Test: Loss 0.89441, R2 0.75599, RMSE 0.93945\n",
      "Epoch [235/500]      | Train: Loss 0.12889, R2 0.96356, RMSE 0.35851                     | Test: Loss 0.91171, R2 0.74181, RMSE 0.94809\n",
      "Epoch [236/500]      | Train: Loss 0.12421, R2 0.96505, RMSE 0.35196                     | Test: Loss 0.97646, R2 0.72680, RMSE 0.97921\n",
      "Epoch [237/500]      | Train: Loss 0.12411, R2 0.96490, RMSE 0.35158                     | Test: Loss 0.93933, R2 0.75514, RMSE 0.96137\n",
      "Epoch [238/500]      | Train: Loss 0.11912, R2 0.96638, RMSE 0.34440                     | Test: Loss 0.90330, R2 0.74206, RMSE 0.93949\n",
      "Epoch [239/500]      | Train: Loss 0.12204, R2 0.96512, RMSE 0.34868                     | Test: Loss 0.86949, R2 0.75848, RMSE 0.92372\n",
      "Epoch [240/500]      | Train: Loss 0.12241, R2 0.96559, RMSE 0.34917                     | Test: Loss 0.89610, R2 0.73859, RMSE 0.93820\n",
      "Epoch [241/500]      | Train: Loss 0.12489, R2 0.96458, RMSE 0.35254                     | Test: Loss 0.92347, R2 0.75559, RMSE 0.95288\n",
      "Epoch [242/500]      | Train: Loss 0.12358, R2 0.96498, RMSE 0.35098                     | Test: Loss 0.92456, R2 0.73925, RMSE 0.95451\n",
      "Epoch [243/500]      | Train: Loss 0.11900, R2 0.96641, RMSE 0.34423                     | Test: Loss 0.88947, R2 0.75279, RMSE 0.93795\n",
      "Epoch [244/500]      | Train: Loss 0.11538, R2 0.96748, RMSE 0.33894                     | Test: Loss 0.95579, R2 0.74052, RMSE 0.97277\n",
      "Epoch [245/500]      | Train: Loss 0.12197, R2 0.96562, RMSE 0.34844                     | Test: Loss 0.88408, R2 0.75774, RMSE 0.92817\n",
      "Epoch [246/500]      | Train: Loss 0.11994, R2 0.96618, RMSE 0.34564                     | Test: Loss 0.87649, R2 0.74980, RMSE 0.92583\n",
      "Epoch [247/500]      | Train: Loss 0.11144, R2 0.96866, RMSE 0.33327                     | Test: Loss 0.94985, R2 0.75034, RMSE 0.96570\n",
      "Epoch [248/500]      | Train: Loss 0.11242, R2 0.96826, RMSE 0.33478                     | Test: Loss 0.90390, R2 0.74840, RMSE 0.94332\n",
      "Epoch [249/500]      | Train: Loss 0.11436, R2 0.96746, RMSE 0.33736                     | Test: Loss 0.91496, R2 0.75740, RMSE 0.95298\n",
      "Epoch [250/500]      | Train: Loss 0.11521, R2 0.96701, RMSE 0.33898                     | Test: Loss 0.90978, R2 0.74552, RMSE 0.94366\n",
      "Epoch [251/500]      | Train: Loss 0.11369, R2 0.96785, RMSE 0.33642                     | Test: Loss 0.87197, R2 0.76028, RMSE 0.92796\n",
      "Epoch [252/500]      | Train: Loss 0.11201, R2 0.96868, RMSE 0.33401                     | Test: Loss 0.99144, R2 0.70398, RMSE 0.98052\n",
      "Epoch [253/500]      | Train: Loss 0.10759, R2 0.96943, RMSE 0.32747                     | Test: Loss 0.95742, R2 0.75639, RMSE 0.96473\n",
      "Epoch [254/500]      | Train: Loss 0.11420, R2 0.96789, RMSE 0.33730                     | Test: Loss 0.88478, R2 0.75874, RMSE 0.93337\n",
      "Epoch [255/500]      | Train: Loss 0.10943, R2 0.96920, RMSE 0.33002                     | Test: Loss 0.88244, R2 0.75496, RMSE 0.93087\n",
      "Epoch [256/500]      | Train: Loss 0.10897, R2 0.96915, RMSE 0.32936                     | Test: Loss 0.93621, R2 0.74465, RMSE 0.95789\n",
      "Epoch [257/500]      | Train: Loss 0.10767, R2 0.96957, RMSE 0.32753                     | Test: Loss 0.87483, R2 0.76892, RMSE 0.92517\n",
      "Epoch [258/500]      | Train: Loss 0.10707, R2 0.96958, RMSE 0.32672                     | Test: Loss 1.23620, R2 0.71754, RMSE 1.04358\n",
      "Epoch [259/500]      | Train: Loss 0.10737, R2 0.96971, RMSE 0.32708                     | Test: Loss 1.21369, R2 0.73059, RMSE 1.03969\n",
      "Epoch [260/500]      | Train: Loss 0.11370, R2 0.96785, RMSE 0.33625                     | Test: Loss 0.96370, R2 0.73563, RMSE 0.97634\n",
      "Epoch [261/500]      | Train: Loss 0.11683, R2 0.96718, RMSE 0.34093                     | Test: Loss 0.88412, R2 0.75212, RMSE 0.92671\n",
      "Epoch [262/500]      | Train: Loss 0.10534, R2 0.97015, RMSE 0.32381                     | Test: Loss 0.88951, R2 0.75369, RMSE 0.93474\n",
      "Epoch [263/500]      | Train: Loss 0.11305, R2 0.96841, RMSE 0.33525                     | Test: Loss 0.91804, R2 0.72207, RMSE 0.95075\n",
      "Epoch [264/500]      | Train: Loss 0.10497, R2 0.97031, RMSE 0.32333                     | Test: Loss 0.90725, R2 0.74389, RMSE 0.94529\n",
      "Epoch [265/500]      | Train: Loss 0.10206, R2 0.97129, RMSE 0.31890                     | Test: Loss 0.88480, R2 0.76380, RMSE 0.92764\n",
      "Epoch [266/500]      | Train: Loss 0.10320, R2 0.97109, RMSE 0.32050                     | Test: Loss 0.95197, R2 0.74474, RMSE 0.96987\n",
      "Epoch [267/500]      | Train: Loss 0.10027, R2 0.97173, RMSE 0.31605                     | Test: Loss 0.87296, R2 0.75968, RMSE 0.92883\n",
      "Epoch [268/500]      | Train: Loss 0.10189, R2 0.97136, RMSE 0.31847                     | Test: Loss 0.88184, R2 0.76088, RMSE 0.93009\n",
      "Epoch [269/500]      | Train: Loss 0.10352, R2 0.97068, RMSE 0.32088                     | Test: Loss 0.91129, R2 0.74973, RMSE 0.94720\n",
      "Epoch [270/500]      | Train: Loss 0.09935, R2 0.97172, RMSE 0.31468                     | Test: Loss 0.95988, R2 0.75072, RMSE 0.96829\n",
      "Epoch [271/500]      | Train: Loss 0.10106, R2 0.97132, RMSE 0.31725                     | Test: Loss 0.91267, R2 0.74515, RMSE 0.95100\n",
      "Epoch [272/500]      | Train: Loss 0.10073, R2 0.97145, RMSE 0.31681                     | Test: Loss 0.91767, R2 0.74685, RMSE 0.95314\n",
      "Epoch [273/500]      | Train: Loss 0.09739, R2 0.97270, RMSE 0.31153                     | Test: Loss 0.90929, R2 0.75449, RMSE 0.94799\n",
      "Epoch [274/500]      | Train: Loss 0.09834, R2 0.97233, RMSE 0.31271                     | Test: Loss 0.86905, R2 0.75014, RMSE 0.92037\n",
      "Epoch [275/500]      | Train: Loss 0.09592, R2 0.97302, RMSE 0.30899                     | Test: Loss 0.88783, R2 0.73754, RMSE 0.93428\n",
      "Epoch [276/500]      | Train: Loss 0.09788, R2 0.97259, RMSE 0.31208                     | Test: Loss 0.87944, R2 0.74948, RMSE 0.93125\n",
      "Epoch [277/500]      | Train: Loss 0.09675, R2 0.97277, RMSE 0.31052                     | Test: Loss 0.92753, R2 0.75811, RMSE 0.95584\n",
      "Epoch [278/500]      | Train: Loss 0.09765, R2 0.97251, RMSE 0.31182                     | Test: Loss 0.93671, R2 0.74913, RMSE 0.96132\n",
      "Epoch [279/500]      | Train: Loss 0.09537, R2 0.97309, RMSE 0.30828                     | Test: Loss 0.93019, R2 0.73929, RMSE 0.95708\n",
      "Epoch [280/500]      | Train: Loss 0.09169, R2 0.97417, RMSE 0.30245                     | Test: Loss 0.87927, R2 0.75437, RMSE 0.92831\n",
      "Epoch [281/500]      | Train: Loss 0.09460, R2 0.97343, RMSE 0.30693                     | Test: Loss 0.96942, R2 0.73645, RMSE 0.96924\n",
      "Epoch [282/500]      | Train: Loss 0.09688, R2 0.97302, RMSE 0.31075                     | Test: Loss 0.88161, R2 0.75199, RMSE 0.92881\n",
      "Epoch [283/500]      | Train: Loss 0.09268, R2 0.97390, RMSE 0.30380                     | Test: Loss 0.87913, R2 0.75440, RMSE 0.92565\n",
      "Epoch [284/500]      | Train: Loss 0.09633, R2 0.97251, RMSE 0.30956                     | Test: Loss 0.87711, R2 0.75998, RMSE 0.92935\n",
      "Epoch [285/500]      | Train: Loss 0.09555, R2 0.97311, RMSE 0.30842                     | Test: Loss 1.01302, R2 0.75133, RMSE 0.98778\n",
      "Epoch [286/500]      | Train: Loss 0.09127, R2 0.97430, RMSE 0.30169                     | Test: Loss 0.91169, R2 0.75189, RMSE 0.94223\n",
      "Epoch [287/500]      | Train: Loss 0.09480, R2 0.97319, RMSE 0.30720                     | Test: Loss 1.00308, R2 0.72881, RMSE 0.98168\n",
      "Epoch [288/500]      | Train: Loss 0.09051, R2 0.97433, RMSE 0.30030                     | Test: Loss 0.94101, R2 0.72719, RMSE 0.96072\n",
      "Epoch [289/500]      | Train: Loss 0.09145, R2 0.97430, RMSE 0.30175                     | Test: Loss 0.91212, R2 0.73287, RMSE 0.95171\n",
      "Epoch [290/500]      | Train: Loss 0.09496, R2 0.97341, RMSE 0.30760                     | Test: Loss 0.91326, R2 0.73729, RMSE 0.95013\n",
      "Epoch [291/500]      | Train: Loss 0.08914, R2 0.97474, RMSE 0.29780                     | Test: Loss 0.92349, R2 0.75781, RMSE 0.95752\n",
      "Epoch [292/500]      | Train: Loss 0.08881, R2 0.97504, RMSE 0.29759                     | Test: Loss 0.92431, R2 0.74641, RMSE 0.95458\n",
      "Epoch [293/500]      | Train: Loss 0.09013, R2 0.97447, RMSE 0.29974                     | Test: Loss 0.90300, R2 0.75035, RMSE 0.94483\n",
      "Epoch [294/500]      | Train: Loss 0.09494, R2 0.97314, RMSE 0.30735                     | Test: Loss 0.89904, R2 0.75231, RMSE 0.93988\n",
      "Epoch [295/500]      | Train: Loss 0.09677, R2 0.97290, RMSE 0.31030                     | Test: Loss 0.90144, R2 0.74948, RMSE 0.94146\n",
      "Epoch [296/500]      | Train: Loss 0.09881, R2 0.97235, RMSE 0.31320                     | Test: Loss 0.91397, R2 0.75934, RMSE 0.94927\n",
      "Epoch [297/500]      | Train: Loss 0.09047, R2 0.97433, RMSE 0.30019                     | Test: Loss 0.93809, R2 0.74952, RMSE 0.96171\n",
      "Epoch [298/500]      | Train: Loss 0.08869, R2 0.97519, RMSE 0.29726                     | Test: Loss 0.89181, R2 0.75144, RMSE 0.93629\n",
      "Epoch [299/500]      | Train: Loss 0.08750, R2 0.97538, RMSE 0.29498                     | Test: Loss 0.88216, R2 0.75646, RMSE 0.93156\n",
      "Epoch [300/500]      | Train: Loss 0.08696, R2 0.97528, RMSE 0.29424                     | Test: Loss 0.87261, R2 0.74906, RMSE 0.92407\n",
      "Epoch [301/500]      | Train: Loss 0.08952, R2 0.97456, RMSE 0.29844                     | Test: Loss 0.89192, R2 0.75618, RMSE 0.93457\n",
      "Epoch [302/500]      | Train: Loss 0.08456, R2 0.97609, RMSE 0.29030                     | Test: Loss 0.89851, R2 0.74934, RMSE 0.93910\n",
      "Epoch [303/500]      | Train: Loss 0.08636, R2 0.97572, RMSE 0.29309                     | Test: Loss 0.85558, R2 0.76776, RMSE 0.90919\n",
      "Epoch [304/500]      | Train: Loss 0.08776, R2 0.97544, RMSE 0.29565                     | Test: Loss 0.98170, R2 0.73230, RMSE 0.97408\n",
      "Epoch [305/500]      | Train: Loss 0.08846, R2 0.97519, RMSE 0.29653                     | Test: Loss 0.89808, R2 0.73342, RMSE 0.93730\n",
      "Epoch [306/500]      | Train: Loss 0.08959, R2 0.97471, RMSE 0.29864                     | Test: Loss 1.19723, R2 0.72303, RMSE 1.03486\n",
      "Epoch [307/500]      | Train: Loss 0.08950, R2 0.97476, RMSE 0.29859                     | Test: Loss 0.86639, R2 0.75399, RMSE 0.92049\n",
      "Epoch [308/500]      | Train: Loss 0.08742, R2 0.97521, RMSE 0.29495                     | Test: Loss 0.87871, R2 0.75692, RMSE 0.92767\n",
      "Epoch [309/500]      | Train: Loss 0.08471, R2 0.97617, RMSE 0.29027                     | Test: Loss 0.92328, R2 0.73617, RMSE 0.95484\n",
      "Epoch [310/500]      | Train: Loss 0.08675, R2 0.97548, RMSE 0.29398                     | Test: Loss 0.89129, R2 0.74736, RMSE 0.93609\n",
      "Epoch [311/500]      | Train: Loss 0.08564, R2 0.97588, RMSE 0.29153                     | Test: Loss 0.90393, R2 0.76037, RMSE 0.94142\n",
      "Epoch [312/500]      | Train: Loss 0.08654, R2 0.97575, RMSE 0.29332                     | Test: Loss 1.02095, R2 0.70415, RMSE 0.99199\n",
      "Epoch [313/500]      | Train: Loss 0.08675, R2 0.97564, RMSE 0.29399                     | Test: Loss 0.93344, R2 0.73839, RMSE 0.96008\n",
      "Epoch [314/500]      | Train: Loss 0.08647, R2 0.97563, RMSE 0.29336                     | Test: Loss 0.91951, R2 0.72541, RMSE 0.95456\n",
      "Epoch [315/500]      | Train: Loss 0.08625, R2 0.97559, RMSE 0.29294                     | Test: Loss 0.86676, R2 0.76653, RMSE 0.91957\n",
      "Epoch [316/500]      | Train: Loss 0.08623, R2 0.97526, RMSE 0.29295                     | Test: Loss 0.91956, R2 0.75789, RMSE 0.95619\n",
      "Epoch [317/500]      | Train: Loss 0.08364, R2 0.97647, RMSE 0.28868                     | Test: Loss 0.88895, R2 0.75829, RMSE 0.92787\n",
      "Epoch [318/500]      | Train: Loss 0.08054, R2 0.97740, RMSE 0.28313                     | Test: Loss 0.91443, R2 0.75122, RMSE 0.95115\n",
      "Epoch [319/500]      | Train: Loss 0.08214, R2 0.97703, RMSE 0.28600                     | Test: Loss 0.91046, R2 0.71743, RMSE 0.94925\n",
      "Epoch [320/500]      | Train: Loss 0.08118, R2 0.97700, RMSE 0.28404                     | Test: Loss 0.90215, R2 0.75757, RMSE 0.93756\n",
      "Epoch [321/500]      | Train: Loss 0.08177, R2 0.97687, RMSE 0.28536                     | Test: Loss 0.89133, R2 0.75976, RMSE 0.93433\n",
      "Epoch [322/500]      | Train: Loss 0.08400, R2 0.97616, RMSE 0.28894                     | Test: Loss 0.91303, R2 0.74872, RMSE 0.95032\n",
      "Epoch [323/500]      | Train: Loss 0.08618, R2 0.97588, RMSE 0.29277                     | Test: Loss 0.89613, R2 0.75552, RMSE 0.93532\n",
      "Epoch [324/500]      | Train: Loss 0.08283, R2 0.97651, RMSE 0.28716                     | Test: Loss 0.90148, R2 0.73955, RMSE 0.94044\n",
      "Epoch [325/500]      | Train: Loss 0.07867, R2 0.97765, RMSE 0.27995                     | Test: Loss 0.89389, R2 0.75258, RMSE 0.94203\n",
      "Epoch [326/500]      | Train: Loss 0.08065, R2 0.97715, RMSE 0.28343                     | Test: Loss 0.88126, R2 0.75951, RMSE 0.93179\n",
      "Epoch [327/500]      | Train: Loss 0.08489, R2 0.97593, RMSE 0.29053                     | Test: Loss 0.85844, R2 0.75234, RMSE 0.91706\n",
      "Epoch [328/500]      | Train: Loss 0.08340, R2 0.97652, RMSE 0.28819                     | Test: Loss 0.90972, R2 0.75375, RMSE 0.94591\n",
      "Epoch [329/500]      | Train: Loss 0.08178, R2 0.97701, RMSE 0.28524                     | Test: Loss 0.87501, R2 0.74791, RMSE 0.92617\n",
      "Epoch [330/500]      | Train: Loss 0.08155, R2 0.97687, RMSE 0.28498                     | Test: Loss 0.93547, R2 0.73955, RMSE 0.95828\n",
      "Epoch [331/500]      | Train: Loss 0.08889, R2 0.97505, RMSE 0.29700                     | Test: Loss 0.85561, R2 0.76805, RMSE 0.91237\n",
      "Epoch [332/500]      | Train: Loss 0.07898, R2 0.97750, RMSE 0.28032                     | Test: Loss 0.87666, R2 0.76363, RMSE 0.93020\n",
      "Epoch [333/500]      | Train: Loss 0.07345, R2 0.97914, RMSE 0.27045                     | Test: Loss 0.91184, R2 0.75766, RMSE 0.95023\n",
      "Epoch [334/500]      | Train: Loss 0.08006, R2 0.97717, RMSE 0.28222                     | Test: Loss 0.94993, R2 0.74927, RMSE 0.96154\n",
      "Epoch [335/500]      | Train: Loss 0.07880, R2 0.97784, RMSE 0.28021                     | Test: Loss 0.88032, R2 0.75277, RMSE 0.93188\n",
      "Epoch [336/500]      | Train: Loss 0.07922, R2 0.97762, RMSE 0.28094                     | Test: Loss 0.87005, R2 0.75811, RMSE 0.92523\n",
      "Epoch [337/500]      | Train: Loss 0.07987, R2 0.97757, RMSE 0.28169                     | Test: Loss 0.90472, R2 0.75247, RMSE 0.94211\n",
      "Epoch [338/500]      | Train: Loss 0.07858, R2 0.97786, RMSE 0.27969                     | Test: Loss 0.87068, R2 0.76673, RMSE 0.92476\n",
      "Epoch [339/500]      | Train: Loss 0.07784, R2 0.97809, RMSE 0.27855                     | Test: Loss 0.87744, R2 0.74728, RMSE 0.93008\n",
      "Epoch [340/500]      | Train: Loss 0.07985, R2 0.97746, RMSE 0.28210                     | Test: Loss 0.88908, R2 0.75481, RMSE 0.92877\n",
      "Epoch [341/500]      | Train: Loss 0.07837, R2 0.97778, RMSE 0.27910                     | Test: Loss 0.87951, R2 0.75019, RMSE 0.92727\n",
      "Epoch [342/500]      | Train: Loss 0.07946, R2 0.97768, RMSE 0.28102                     | Test: Loss 0.88948, R2 0.73944, RMSE 0.93455\n",
      "Epoch [343/500]      | Train: Loss 0.08085, R2 0.97718, RMSE 0.28362                     | Test: Loss 0.87894, R2 0.75308, RMSE 0.93126\n",
      "Epoch [344/500]      | Train: Loss 0.07681, R2 0.97831, RMSE 0.27669                     | Test: Loss 0.89163, R2 0.71781, RMSE 0.93926\n",
      "Epoch [345/500]      | Train: Loss 0.07562, R2 0.97868, RMSE 0.27447                     | Test: Loss 0.88230, R2 0.75780, RMSE 0.92891\n",
      "Epoch [346/500]      | Train: Loss 0.07416, R2 0.97897, RMSE 0.27162                     | Test: Loss 0.93522, R2 0.74764, RMSE 0.96301\n",
      "Epoch [347/500]      | Train: Loss 0.07622, R2 0.97872, RMSE 0.27541                     | Test: Loss 0.91491, R2 0.75375, RMSE 0.94943\n",
      "Epoch [348/500]      | Train: Loss 0.07893, R2 0.97772, RMSE 0.28029                     | Test: Loss 0.87867, R2 0.75803, RMSE 0.92820\n",
      "Epoch [349/500]      | Train: Loss 0.07612, R2 0.97850, RMSE 0.27531                     | Test: Loss 0.88633, R2 0.74697, RMSE 0.93378\n",
      "Epoch [350/500]      | Train: Loss 0.07780, R2 0.97792, RMSE 0.27819                     | Test: Loss 0.88458, R2 0.74303, RMSE 0.93211\n",
      "Epoch [351/500]      | Train: Loss 0.07343, R2 0.97922, RMSE 0.27015                     | Test: Loss 0.91079, R2 0.75755, RMSE 0.94554\n",
      "Epoch [352/500]      | Train: Loss 0.07579, R2 0.97854, RMSE 0.27464                     | Test: Loss 0.86561, R2 0.76701, RMSE 0.91783\n",
      "Epoch [353/500]      | Train: Loss 0.07733, R2 0.97834, RMSE 0.27735                     | Test: Loss 0.88983, R2 0.75867, RMSE 0.93832\n",
      "Epoch [354/500]      | Train: Loss 0.07560, R2 0.97877, RMSE 0.27429                     | Test: Loss 0.89507, R2 0.74912, RMSE 0.93691\n",
      "Epoch [355/500]      | Train: Loss 0.08363, R2 0.97655, RMSE 0.28825                     | Test: Loss 0.90268, R2 0.73265, RMSE 0.94064\n",
      "Epoch [356/500]      | Train: Loss 0.07643, R2 0.97855, RMSE 0.27555                     | Test: Loss 0.87981, R2 0.75103, RMSE 0.93271\n",
      "Epoch [357/500]      | Train: Loss 0.07443, R2 0.97913, RMSE 0.27221                     | Test: Loss 0.88506, R2 0.75031, RMSE 0.93527\n",
      "Epoch [358/500]      | Train: Loss 0.07229, R2 0.97945, RMSE 0.26812                     | Test: Loss 0.92140, R2 0.73655, RMSE 0.94979\n",
      "Epoch [359/500]      | Train: Loss 0.07580, R2 0.97881, RMSE 0.27463                     | Test: Loss 0.89516, R2 0.73260, RMSE 0.93799\n",
      "Epoch [360/500]      | Train: Loss 0.07392, R2 0.97923, RMSE 0.27117                     | Test: Loss 0.91674, R2 0.75287, RMSE 0.94853\n",
      "Epoch [361/500]      | Train: Loss 0.07754, R2 0.97828, RMSE 0.27742                     | Test: Loss 0.90628, R2 0.75358, RMSE 0.94467\n",
      "Epoch [362/500]      | Train: Loss 0.07786, R2 0.97811, RMSE 0.27822                     | Test: Loss 0.85468, R2 0.76184, RMSE 0.90827\n",
      "Epoch [363/500]      | Train: Loss 0.07514, R2 0.97871, RMSE 0.27326                     | Test: Loss 0.89385, R2 0.75319, RMSE 0.93819\n",
      "Epoch [364/500]      | Train: Loss 0.07498, R2 0.97873, RMSE 0.27321                     | Test: Loss 0.88667, R2 0.75379, RMSE 0.93541\n",
      "Epoch [365/500]      | Train: Loss 0.06894, R2 0.98040, RMSE 0.26198                     | Test: Loss 0.91647, R2 0.72717, RMSE 0.95261\n",
      "Epoch [366/500]      | Train: Loss 0.07357, R2 0.97915, RMSE 0.27069                     | Test: Loss 0.89615, R2 0.75757, RMSE 0.93898\n",
      "Epoch [367/500]      | Train: Loss 0.07255, R2 0.97954, RMSE 0.26874                     | Test: Loss 0.87128, R2 0.74018, RMSE 0.92244\n",
      "Epoch [368/500]      | Train: Loss 0.07136, R2 0.98006, RMSE 0.26636                     | Test: Loss 0.87195, R2 0.74339, RMSE 0.92354\n",
      "Epoch [369/500]      | Train: Loss 0.07128, R2 0.97979, RMSE 0.26624                     | Test: Loss 0.91245, R2 0.74899, RMSE 0.95029\n",
      "Epoch [370/500]      | Train: Loss 0.07302, R2 0.97944, RMSE 0.26971                     | Test: Loss 0.87698, R2 0.75410, RMSE 0.93169\n",
      "Epoch [371/500]      | Train: Loss 0.07070, R2 0.98019, RMSE 0.26540                     | Test: Loss 0.87250, R2 0.75958, RMSE 0.92383\n",
      "Epoch [372/500]      | Train: Loss 0.07323, R2 0.97939, RMSE 0.26990                     | Test: Loss 0.87506, R2 0.74944, RMSE 0.92641\n",
      "Epoch [373/500]      | Train: Loss 0.07488, R2 0.97905, RMSE 0.27252                     | Test: Loss 0.86193, R2 0.76634, RMSE 0.91615\n",
      "Epoch [374/500]      | Train: Loss 0.07581, R2 0.97858, RMSE 0.27476                     | Test: Loss 0.86294, R2 0.75901, RMSE 0.92035\n",
      "Epoch [375/500]      | Train: Loss 0.07210, R2 0.97964, RMSE 0.26792                     | Test: Loss 0.88041, R2 0.74666, RMSE 0.93054\n",
      "Epoch [376/500]      | Train: Loss 0.07066, R2 0.97998, RMSE 0.26518                     | Test: Loss 0.86064, R2 0.75655, RMSE 0.91379\n",
      "Epoch [377/500]      | Train: Loss 0.07069, R2 0.98015, RMSE 0.26509                     | Test: Loss 0.87092, R2 0.75917, RMSE 0.92739\n",
      "Epoch [378/500]      | Train: Loss 0.06985, R2 0.98018, RMSE 0.26368                     | Test: Loss 0.91358, R2 0.73389, RMSE 0.95088\n",
      "Epoch [379/500]      | Train: Loss 0.07447, R2 0.97906, RMSE 0.27213                     | Test: Loss 0.97797, R2 0.74361, RMSE 0.97103\n",
      "Epoch [380/500]      | Train: Loss 0.07387, R2 0.97927, RMSE 0.27113                     | Test: Loss 0.87861, R2 0.75391, RMSE 0.93209\n",
      "Epoch [381/500]      | Train: Loss 0.07006, R2 0.98010, RMSE 0.26382                     | Test: Loss 0.88928, R2 0.75254, RMSE 0.93870\n",
      "Epoch [382/500]      | Train: Loss 0.07560, R2 0.97860, RMSE 0.27381                     | Test: Loss 0.91556, R2 0.75533, RMSE 0.94999\n",
      "Epoch [383/500]      | Train: Loss 0.07170, R2 0.97982, RMSE 0.26692                     | Test: Loss 0.88569, R2 0.76214, RMSE 0.92957\n",
      "Epoch [384/500]      | Train: Loss 0.07107, R2 0.97994, RMSE 0.26585                     | Test: Loss 0.91288, R2 0.74570, RMSE 0.94590\n",
      "Epoch [385/500]      | Train: Loss 0.06879, R2 0.98079, RMSE 0.26160                     | Test: Loss 0.88584, R2 0.75105, RMSE 0.93410\n",
      "Epoch [386/500]      | Train: Loss 0.07126, R2 0.97996, RMSE 0.26602                     | Test: Loss 0.86794, R2 0.76023, RMSE 0.92624\n",
      "Epoch [387/500]      | Train: Loss 0.06848, R2 0.98074, RMSE 0.26106                     | Test: Loss 0.96068, R2 0.74692, RMSE 0.96537\n",
      "Epoch [388/500]      | Train: Loss 0.06883, R2 0.98062, RMSE 0.26153                     | Test: Loss 0.91209, R2 0.75457, RMSE 0.95036\n",
      "Epoch [389/500]      | Train: Loss 0.07412, R2 0.97913, RMSE 0.27145                     | Test: Loss 0.87600, R2 0.75602, RMSE 0.92779\n",
      "Epoch [390/500]      | Train: Loss 0.07019, R2 0.98025, RMSE 0.26443                     | Test: Loss 0.86084, R2 0.75870, RMSE 0.91818\n",
      "Epoch [391/500]      | Train: Loss 0.07070, R2 0.98005, RMSE 0.26504                     | Test: Loss 0.90195, R2 0.76397, RMSE 0.94380\n",
      "Epoch [392/500]      | Train: Loss 0.07062, R2 0.98010, RMSE 0.26507                     | Test: Loss 0.85436, R2 0.76041, RMSE 0.91126\n",
      "Epoch [393/500]      | Train: Loss 0.07002, R2 0.98023, RMSE 0.26386                     | Test: Loss 0.92232, R2 0.74723, RMSE 0.95608\n",
      "Epoch [394/500]      | Train: Loss 0.07170, R2 0.97985, RMSE 0.26713                     | Test: Loss 0.86697, R2 0.76061, RMSE 0.92167\n",
      "Epoch [395/500]      | Train: Loss 0.07259, R2 0.97953, RMSE 0.26851                     | Test: Loss 0.84923, R2 0.76978, RMSE 0.91298\n",
      "Epoch [396/500]      | Train: Loss 0.06875, R2 0.98051, RMSE 0.26138                     | Test: Loss 0.85838, R2 0.76766, RMSE 0.90951\n",
      "Epoch [397/500]      | Train: Loss 0.07450, R2 0.97910, RMSE 0.27181                     | Test: Loss 0.86376, R2 0.76049, RMSE 0.91967\n",
      "Epoch [398/500]      | Train: Loss 0.07072, R2 0.98006, RMSE 0.26509                     | Test: Loss 0.92733, R2 0.75084, RMSE 0.95380\n",
      "Epoch [399/500]      | Train: Loss 0.06574, R2 0.98139, RMSE 0.25582                     | Test: Loss 0.94900, R2 0.72585, RMSE 0.96297\n",
      "Epoch [400/500]      | Train: Loss 0.06769, R2 0.98082, RMSE 0.25965                     | Test: Loss 0.88077, R2 0.76390, RMSE 0.93010\n",
      "Epoch [401/500]      | Train: Loss 0.06831, R2 0.98071, RMSE 0.26100                     | Test: Loss 0.93597, R2 0.72549, RMSE 0.95871\n",
      "Epoch [402/500]      | Train: Loss 0.06768, R2 0.98085, RMSE 0.25944                     | Test: Loss 0.84810, R2 0.76684, RMSE 0.90688\n",
      "Epoch [403/500]      | Train: Loss 0.06818, R2 0.98101, RMSE 0.26015                     | Test: Loss 0.86553, R2 0.76545, RMSE 0.91768\n",
      "Epoch [404/500]      | Train: Loss 0.06694, R2 0.98108, RMSE 0.25830                     | Test: Loss 0.89834, R2 0.76278, RMSE 0.93776\n",
      "Epoch [405/500]      | Train: Loss 0.06854, R2 0.98060, RMSE 0.26118                     | Test: Loss 0.90866, R2 0.75394, RMSE 0.94101\n",
      "Epoch [406/500]      | Train: Loss 0.06758, R2 0.98107, RMSE 0.25932                     | Test: Loss 0.89669, R2 0.76557, RMSE 0.93708\n",
      "Epoch [407/500]      | Train: Loss 0.06773, R2 0.98089, RMSE 0.25957                     | Test: Loss 0.84654, R2 0.76285, RMSE 0.90863\n",
      "Epoch [408/500]      | Train: Loss 0.06671, R2 0.98119, RMSE 0.25767                     | Test: Loss 0.85518, R2 0.76311, RMSE 0.91744\n",
      "Epoch [409/500]      | Train: Loss 0.06607, R2 0.98147, RMSE 0.25650                     | Test: Loss 0.95283, R2 0.75022, RMSE 0.96526\n",
      "Epoch [410/500]      | Train: Loss 0.06729, R2 0.98095, RMSE 0.25905                     | Test: Loss 0.88645, R2 0.75173, RMSE 0.93923\n",
      "Epoch [411/500]      | Train: Loss 0.06727, R2 0.98101, RMSE 0.25872                     | Test: Loss 0.86841, R2 0.76056, RMSE 0.92179\n",
      "Epoch [412/500]      | Train: Loss 0.06922, R2 0.98044, RMSE 0.26248                     | Test: Loss 0.87322, R2 0.74773, RMSE 0.92784\n",
      "Epoch [413/500]      | Train: Loss 0.06553, R2 0.98163, RMSE 0.25533                     | Test: Loss 0.88265, R2 0.76276, RMSE 0.93273\n",
      "Epoch [414/500]      | Train: Loss 0.06636, R2 0.98102, RMSE 0.25688                     | Test: Loss 0.85780, R2 0.77039, RMSE 0.91639\n",
      "Epoch [415/500]      | Train: Loss 0.06835, R2 0.98087, RMSE 0.26037                     | Test: Loss 0.84260, R2 0.76643, RMSE 0.90287\n",
      "Epoch [416/500]      | Train: Loss 0.06276, R2 0.98234, RMSE 0.24973                     | Test: Loss 0.86324, R2 0.76861, RMSE 0.91934\n",
      "Epoch [417/500]      | Train: Loss 0.06437, R2 0.98197, RMSE 0.25318                     | Test: Loss 0.85184, R2 0.76619, RMSE 0.91574\n",
      "Epoch [418/500]      | Train: Loss 0.06445, R2 0.98185, RMSE 0.25327                     | Test: Loss 0.85296, R2 0.76633, RMSE 0.91330\n",
      "Epoch [419/500]      | Train: Loss 0.06601, R2 0.98162, RMSE 0.25616                     | Test: Loss 0.85934, R2 0.75510, RMSE 0.91751\n",
      "Epoch [420/500]      | Train: Loss 0.06809, R2 0.98072, RMSE 0.26021                     | Test: Loss 0.93185, R2 0.74334, RMSE 0.95426\n",
      "Epoch [421/500]      | Train: Loss 0.06522, R2 0.98141, RMSE 0.25485                     | Test: Loss 0.88240, R2 0.72678, RMSE 0.93272\n",
      "Epoch [422/500]      | Train: Loss 0.06583, R2 0.98149, RMSE 0.25537                     | Test: Loss 0.97278, R2 0.75297, RMSE 0.97324\n",
      "Epoch [423/500]      | Train: Loss 0.06769, R2 0.98099, RMSE 0.25956                     | Test: Loss 0.86061, R2 0.75925, RMSE 0.91770\n",
      "Epoch [424/500]      | Train: Loss 0.06419, R2 0.98192, RMSE 0.25243                     | Test: Loss 0.85919, R2 0.75491, RMSE 0.92018\n",
      "Epoch [425/500]      | Train: Loss 0.06548, R2 0.98151, RMSE 0.25529                     | Test: Loss 0.88982, R2 0.75743, RMSE 0.93644\n",
      "Epoch [426/500]      | Train: Loss 0.06442, R2 0.98166, RMSE 0.25320                     | Test: Loss 0.84723, R2 0.77092, RMSE 0.90699\n",
      "Epoch [427/500]      | Train: Loss 0.06737, R2 0.98115, RMSE 0.25854                     | Test: Loss 0.84690, R2 0.76933, RMSE 0.90576\n",
      "Epoch [428/500]      | Train: Loss 0.06673, R2 0.98130, RMSE 0.25750                     | Test: Loss 0.84818, R2 0.76719, RMSE 0.91151\n",
      "Epoch [429/500]      | Train: Loss 0.06604, R2 0.98154, RMSE 0.25622                     | Test: Loss 0.85094, R2 0.76579, RMSE 0.91442\n",
      "Epoch [430/500]      | Train: Loss 0.06459, R2 0.98172, RMSE 0.25349                     | Test: Loss 0.90569, R2 0.75529, RMSE 0.94243\n",
      "Epoch [431/500]      | Train: Loss 0.06579, R2 0.98147, RMSE 0.25570                     | Test: Loss 0.87307, R2 0.76061, RMSE 0.92900\n",
      "Epoch [432/500]      | Train: Loss 0.06590, R2 0.98162, RMSE 0.25585                     | Test: Loss 0.99937, R2 0.74922, RMSE 0.97471\n",
      "Epoch [433/500]      | Train: Loss 0.06530, R2 0.98174, RMSE 0.25476                     | Test: Loss 0.96164, R2 0.72164, RMSE 0.96842\n",
      "Epoch [434/500]      | Train: Loss 0.06492, R2 0.98155, RMSE 0.25356                     | Test: Loss 0.85635, R2 0.75636, RMSE 0.92004\n",
      "Epoch [435/500]      | Train: Loss 0.06372, R2 0.98217, RMSE 0.25176                     | Test: Loss 0.88283, R2 0.76614, RMSE 0.93319\n",
      "Epoch [436/500]      | Train: Loss 0.06567, R2 0.98158, RMSE 0.25556                     | Test: Loss 0.95218, R2 0.75617, RMSE 0.96379\n",
      "Epoch [437/500]      | Train: Loss 0.06477, R2 0.98185, RMSE 0.25375                     | Test: Loss 0.84556, R2 0.77142, RMSE 0.90378\n",
      "Epoch [438/500]      | Train: Loss 0.06331, R2 0.98220, RMSE 0.25123                     | Test: Loss 0.88219, R2 0.74651, RMSE 0.93377\n",
      "Epoch [439/500]      | Train: Loss 0.06384, R2 0.98185, RMSE 0.25199                     | Test: Loss 0.89607, R2 0.75527, RMSE 0.94002\n",
      "Epoch [440/500]      | Train: Loss 0.06447, R2 0.98202, RMSE 0.25317                     | Test: Loss 0.88376, R2 0.76592, RMSE 0.93240\n",
      "Epoch [441/500]      | Train: Loss 0.06804, R2 0.98087, RMSE 0.26007                     | Test: Loss 0.85192, R2 0.75468, RMSE 0.91535\n",
      "Epoch [442/500]      | Train: Loss 0.06821, R2 0.98088, RMSE 0.26004                     | Test: Loss 0.97781, R2 0.73856, RMSE 0.97154\n",
      "Epoch [443/500]      | Train: Loss 0.06493, R2 0.98174, RMSE 0.25431                     | Test: Loss 0.85111, R2 0.76980, RMSE 0.91365\n",
      "Epoch [444/500]      | Train: Loss 0.06181, R2 0.98255, RMSE 0.24799                     | Test: Loss 0.89334, R2 0.75872, RMSE 0.94125\n",
      "Epoch [445/500]      | Train: Loss 0.06669, R2 0.98118, RMSE 0.25756                     | Test: Loss 0.88400, R2 0.72836, RMSE 0.93400\n",
      "Epoch [446/500]      | Train: Loss 0.05953, R2 0.98323, RMSE 0.24342                     | Test: Loss 0.84207, R2 0.73784, RMSE 0.90212\n",
      "Epoch [447/500]      | Train: Loss 0.06370, R2 0.98216, RMSE 0.25188                     | Test: Loss 0.93046, R2 0.76017, RMSE 0.95855\n",
      "Epoch [448/500]      | Train: Loss 0.06286, R2 0.98226, RMSE 0.25010                     | Test: Loss 0.85907, R2 0.76277, RMSE 0.91337\n",
      "Epoch [449/500]      | Train: Loss 0.06265, R2 0.98230, RMSE 0.24957                     | Test: Loss 0.84193, R2 0.76939, RMSE 0.90643\n",
      "Epoch [450/500]      | Train: Loss 0.06153, R2 0.98269, RMSE 0.24746                     | Test: Loss 0.88690, R2 0.71296, RMSE 0.93289\n",
      "Epoch [451/500]      | Train: Loss 0.05983, R2 0.98318, RMSE 0.24377                     | Test: Loss 0.87904, R2 0.75260, RMSE 0.93046\n",
      "Epoch [452/500]      | Train: Loss 0.06193, R2 0.98251, RMSE 0.24835                     | Test: Loss 0.86223, R2 0.76819, RMSE 0.91635\n",
      "Epoch [453/500]      | Train: Loss 0.06278, R2 0.98236, RMSE 0.24981                     | Test: Loss 0.87352, R2 0.76587, RMSE 0.92691\n",
      "Epoch [454/500]      | Train: Loss 0.06152, R2 0.98260, RMSE 0.24752                     | Test: Loss 0.86244, R2 0.74407, RMSE 0.91972\n",
      "Epoch [455/500]      | Train: Loss 0.06583, R2 0.98137, RMSE 0.25578                     | Test: Loss 0.84150, R2 0.76360, RMSE 0.90827\n",
      "Epoch [456/500]      | Train: Loss 0.05976, R2 0.98321, RMSE 0.24355                     | Test: Loss 0.86551, R2 0.75943, RMSE 0.92475\n",
      "Epoch [457/500]      | Train: Loss 0.05843, R2 0.98357, RMSE 0.24111                     | Test: Loss 0.87366, R2 0.76756, RMSE 0.93089\n",
      "Epoch [458/500]      | Train: Loss 0.05977, R2 0.98306, RMSE 0.24360                     | Test: Loss 0.86393, R2 0.75923, RMSE 0.92313\n",
      "Epoch [459/500]      | Train: Loss 0.06306, R2 0.98228, RMSE 0.25031                     | Test: Loss 0.90145, R2 0.76071, RMSE 0.93992\n",
      "Epoch [460/500]      | Train: Loss 0.06142, R2 0.98262, RMSE 0.24726                     | Test: Loss 0.92640, R2 0.75754, RMSE 0.95351\n",
      "Epoch [461/500]      | Train: Loss 0.06080, R2 0.98296, RMSE 0.24570                     | Test: Loss 0.88656, R2 0.75703, RMSE 0.93704\n",
      "Epoch [462/500]      | Train: Loss 0.06177, R2 0.98267, RMSE 0.24773                     | Test: Loss 0.89986, R2 0.75967, RMSE 0.93801\n",
      "Epoch [463/500]      | Train: Loss 0.06040, R2 0.98299, RMSE 0.24522                     | Test: Loss 0.84735, R2 0.75990, RMSE 0.91327\n",
      "Epoch [464/500]      | Train: Loss 0.05800, R2 0.98369, RMSE 0.24038                     | Test: Loss 0.85025, R2 0.76206, RMSE 0.91826\n",
      "Epoch [465/500]      | Train: Loss 0.06178, R2 0.98240, RMSE 0.24797                     | Test: Loss 0.85460, R2 0.76018, RMSE 0.91722\n",
      "Epoch [466/500]      | Train: Loss 0.06046, R2 0.98272, RMSE 0.24532                     | Test: Loss 0.84680, R2 0.77167, RMSE 0.91052\n",
      "Epoch [467/500]      | Train: Loss 0.05939, R2 0.98338, RMSE 0.24314                     | Test: Loss 0.84708, R2 0.75256, RMSE 0.91030\n",
      "Epoch [468/500]      | Train: Loss 0.06181, R2 0.98254, RMSE 0.24798                     | Test: Loss 0.83621, R2 0.77346, RMSE 0.90225\n",
      "Epoch [469/500]      | Train: Loss 0.06307, R2 0.98228, RMSE 0.25055                     | Test: Loss 0.84347, R2 0.76246, RMSE 0.90656\n",
      "Epoch [470/500]      | Train: Loss 0.06370, R2 0.98200, RMSE 0.25150                     | Test: Loss 0.86979, R2 0.76496, RMSE 0.92727\n",
      "Epoch [471/500]      | Train: Loss 0.06351, R2 0.98227, RMSE 0.25097                     | Test: Loss 0.85504, R2 0.76598, RMSE 0.91158\n",
      "Epoch [472/500]      | Train: Loss 0.05935, R2 0.98330, RMSE 0.24302                     | Test: Loss 0.85832, R2 0.75634, RMSE 0.92065\n",
      "Epoch [473/500]      | Train: Loss 0.06119, R2 0.98299, RMSE 0.24670                     | Test: Loss 0.87230, R2 0.76551, RMSE 0.92734\n",
      "Epoch [474/500]      | Train: Loss 0.06199, R2 0.98260, RMSE 0.24861                     | Test: Loss 0.85560, R2 0.77013, RMSE 0.91494\n",
      "Epoch [475/500]      | Train: Loss 0.06264, R2 0.98241, RMSE 0.24929                     | Test: Loss 0.87773, R2 0.76068, RMSE 0.92625\n",
      "Epoch [476/500]      | Train: Loss 0.06127, R2 0.98285, RMSE 0.24670                     | Test: Loss 0.84655, R2 0.77334, RMSE 0.91212\n",
      "Epoch [477/500]      | Train: Loss 0.06207, R2 0.98250, RMSE 0.24848                     | Test: Loss 0.85433, R2 0.77267, RMSE 0.90822\n",
      "Epoch [478/500]      | Train: Loss 0.05999, R2 0.98309, RMSE 0.24429                     | Test: Loss 0.83944, R2 0.76241, RMSE 0.90721\n",
      "Epoch [479/500]      | Train: Loss 0.05687, R2 0.98402, RMSE 0.23780                     | Test: Loss 0.85361, R2 0.75924, RMSE 0.90968\n",
      "Epoch [480/500]      | Train: Loss 0.05713, R2 0.98392, RMSE 0.23837                     | Test: Loss 0.84145, R2 0.77134, RMSE 0.90790\n",
      "Epoch [481/500]      | Train: Loss 0.06108, R2 0.98266, RMSE 0.24657                     | Test: Loss 0.86640, R2 0.76893, RMSE 0.92492\n",
      "Epoch [482/500]      | Train: Loss 0.06066, R2 0.98304, RMSE 0.24540                     | Test: Loss 1.04775, R2 0.74109, RMSE 0.98982\n",
      "Epoch [483/500]      | Train: Loss 0.06075, R2 0.98290, RMSE 0.24563                     | Test: Loss 0.83618, R2 0.76491, RMSE 0.90420\n",
      "Epoch [484/500]      | Train: Loss 0.05871, R2 0.98355, RMSE 0.24165                     | Test: Loss 0.88425, R2 0.76089, RMSE 0.92946\n",
      "Epoch [485/500]      | Train: Loss 0.06078, R2 0.98273, RMSE 0.24563                     | Test: Loss 0.85203, R2 0.76992, RMSE 0.90809\n",
      "Epoch [486/500]      | Train: Loss 0.05989, R2 0.98309, RMSE 0.24404                     | Test: Loss 0.84387, R2 0.77451, RMSE 0.90530\n",
      "Epoch [487/500]      | Train: Loss 0.05880, R2 0.98356, RMSE 0.24195                     | Test: Loss 0.90167, R2 0.76561, RMSE 0.94304\n",
      "Epoch [488/500]      | Train: Loss 0.05874, R2 0.98325, RMSE 0.24174                     | Test: Loss 0.87679, R2 0.76094, RMSE 0.93069\n",
      "Epoch [489/500]      | Train: Loss 0.06129, R2 0.98276, RMSE 0.24687                     | Test: Loss 0.91904, R2 0.74051, RMSE 0.95109\n",
      "Epoch [490/500]      | Train: Loss 0.05730, R2 0.98390, RMSE 0.23869                     | Test: Loss 0.87193, R2 0.74782, RMSE 0.92857\n",
      "Epoch [491/500]      | Train: Loss 0.05778, R2 0.98373, RMSE 0.23981                     | Test: Loss 0.87517, R2 0.75605, RMSE 0.92871\n",
      "Epoch [492/500]      | Train: Loss 0.05679, R2 0.98411, RMSE 0.23769                     | Test: Loss 0.83807, R2 0.76184, RMSE 0.90708\n",
      "Epoch [493/500]      | Train: Loss 0.05901, R2 0.98322, RMSE 0.24243                     | Test: Loss 0.93875, R2 0.72914, RMSE 0.95587\n",
      "Epoch [494/500]      | Train: Loss 0.06004, R2 0.98322, RMSE 0.24418                     | Test: Loss 0.85637, R2 0.77322, RMSE 0.91894\n",
      "Epoch [495/500]      | Train: Loss 0.05833, R2 0.98360, RMSE 0.24091                     | Test: Loss 0.87694, R2 0.75757, RMSE 0.93332\n",
      "Epoch [496/500]      | Train: Loss 0.05966, R2 0.98297, RMSE 0.24325                     | Test: Loss 0.90931, R2 0.76581, RMSE 0.94538\n",
      "Epoch [497/500]      | Train: Loss 0.05652, R2 0.98401, RMSE 0.23730                     | Test: Loss 0.86831, R2 0.76568, RMSE 0.92169\n",
      "Epoch [498/500]      | Train: Loss 0.05897, R2 0.98329, RMSE 0.24209                     | Test: Loss 0.82811, R2 0.76860, RMSE 0.89626\n",
      "Epoch [499/500]      | Train: Loss 0.05732, R2 0.98395, RMSE 0.23891                     | Test: Loss 0.85484, R2 0.75509, RMSE 0.91713\n",
      "Epoch [500/500]      | Train: Loss 0.05916, R2 0.98335, RMSE 0.24258                     | Test: Loss 0.90477, R2 0.76599, RMSE 0.94222\n",
      "Best rmse 0.8447356820106506\n",
      "100 epochs of training and evaluation took, 246.59375\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class LossFunction(nn.Module):\n",
    "    def __init__(self, num_tasks):\n",
    "        super(LossFunction, self).__init__()\n",
    "        self.num_tasks = num_tasks\n",
    "\n",
    "        self.loss_fns = [nn.MSELoss() for x in range(num_tasks)] \n",
    "\n",
    "    def forward(self, predictions, labels_task1):\n",
    "\n",
    "        #task 1\n",
    "        target = labels_task1\n",
    "        prediction = predictions\n",
    "        loss_fn = self.loss_fns[0]\n",
    "\n",
    "        task_loss = loss_fn(prediction, target)\n",
    "        \n",
    "        return task_loss\n",
    "    \n",
    "model = CATTransformer(rff_on=True,\n",
    "                       sigma=2,\n",
    "                       embed_size=160,\n",
    "                       heads=5,\n",
    "                       forward_expansion=8,\n",
    "                       pre_norm_on=False,\n",
    "                       mlp_scale_classification=8,\n",
    "                       decoder_dropout=0,\n",
    "                       classification_dropout=0.1,\n",
    "                       targets_classes=target_classes,\n",
    "                       n_cont=len(cont_columns),\n",
    "                       embedding_scheme='rff_unique',\n",
    "                       linear_on=True).to(device_in_use)\n",
    "\n",
    "def format_metric(value): # Used to format the metrics output\n",
    "    return f\"{value:.5f}\"\n",
    "\n",
    "loss_functions = LossFunction(1)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr = 0.0001) # Maybe try messing around with optimizers. try other torch optimizers with different configurations.\n",
    "epochs = 500 #Set the number of epochs\n",
    "\n",
    "train_losses = []\n",
    "train_rmse = [] \n",
    "test_losses = []\n",
    "test_rmse = []  \n",
    "\n",
    "\n",
    "#Time will be recorded for all 100 epochs - This means the results will not be comparable to Xgboost but that is ok, we will only compare between transformer models who will also train for 100 epochs\n",
    "start_time = time.process_time()\n",
    "\n",
    "for t in range(epochs):\n",
    "    train_loss, r2_train, rmse_train = train(train_dataloader, model, loss_functions, optimizer, device_in_use=device_in_use)\n",
    "    test_loss, r2_test, rmse_test = test(test_dataloader, model, loss_functions, device_in_use=device_in_use)\n",
    "    train_losses.append(train_loss)\n",
    "    train_rmse.append(rmse_train)\n",
    "    # train_accuracies_2.append(train_accuracy_2)\n",
    "    # train_recalls.append(train_recall) \n",
    "    # train_f1_scores.append(train_f1)\n",
    "    test_losses.append(test_loss)\n",
    "    test_rmse.append(rmse_test)\n",
    "    # test_accuracies_2.append(test_accuracy_2)\n",
    "    # test_recalls.append(test_recall)\n",
    "    # test_f1_scores.append(test_f1)\n",
    "    # Formatting for easier reading\n",
    "    epoch_str = f\"Epoch [{t+1:2}/{epochs}]\"\n",
    "    train_metrics = f\"Train: Loss {format_metric(train_loss)}, R2 {format_metric(r2_train)}, RMSE {format_metric(rmse_train)}\"\n",
    "    test_metrics = f\"Test: Loss {format_metric(test_loss)}, R2 {format_metric(r2_test)}, RMSE {format_metric(rmse_test)}\"\n",
    "    print(f\"{epoch_str:20} | {train_metrics:65} | {test_metrics}\")\n",
    "\n",
    "total_time = time.process_time() - start_time\n",
    "\n",
    "# Plotting the loss curves\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, epochs+1), train_losses, label='Train Loss')\n",
    "plt.plot(range(1, epochs+1), [l for l in test_losses], label='Test Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Test Loss Curve')\n",
    "plt.legend()\n",
    "\n",
    "best_index = test_rmse.index(min(test_rmse))\n",
    "print(f\"Best rmse {test_rmse[best_index]}\")\n",
    "print(f\"100 epochs of training and evaluation took, {total_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mixed Normal RFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [ 1/500]       | Train: Loss 4.78050, R2 -0.30188, RMSE 2.11136                    | Test: Loss 3.67402, R2 0.03658, RMSE 1.90959\n",
      "Epoch [ 2/500]       | Train: Loss 2.85817, R2 0.20446, RMSE 1.68001                     | Test: Loss 1.91050, R2 0.42558, RMSE 1.37375\n",
      "Epoch [ 3/500]       | Train: Loss 1.74261, R2 0.51958, RMSE 1.31019                     | Test: Loss 1.52672, R2 0.61659, RMSE 1.21875\n",
      "Epoch [ 4/500]       | Train: Loss 1.31987, R2 0.63402, RMSE 1.14231                     | Test: Loss 1.16769, R2 0.67968, RMSE 1.06888\n",
      "Epoch [ 5/500]       | Train: Loss 1.16106, R2 0.67616, RMSE 1.07271                     | Test: Loss 1.09802, R2 0.70940, RMSE 1.04265\n",
      "Epoch [ 6/500]       | Train: Loss 1.10121, R2 0.69512, RMSE 1.04286                     | Test: Loss 1.01037, R2 0.70867, RMSE 0.99764\n",
      "Epoch [ 7/500]       | Train: Loss 1.07069, R2 0.69757, RMSE 1.02984                     | Test: Loss 0.93119, R2 0.73140, RMSE 0.95422\n",
      "Epoch [ 8/500]       | Train: Loss 0.99226, R2 0.72316, RMSE 0.99197                     | Test: Loss 0.93754, R2 0.73556, RMSE 0.96281\n",
      "Epoch [ 9/500]       | Train: Loss 0.96590, R2 0.72949, RMSE 0.97784                     | Test: Loss 0.96691, R2 0.75137, RMSE 0.97191\n",
      "Epoch [10/500]       | Train: Loss 0.93605, R2 0.73860, RMSE 0.96387                     | Test: Loss 0.87128, R2 0.75699, RMSE 0.91908\n",
      "Epoch [11/500]       | Train: Loss 0.92515, R2 0.74029, RMSE 0.95773                     | Test: Loss 0.97336, R2 0.73653, RMSE 0.98099\n",
      "Epoch [12/500]       | Train: Loss 0.89911, R2 0.74686, RMSE 0.94397                     | Test: Loss 1.00228, R2 0.69651, RMSE 0.98578\n",
      "Epoch [13/500]       | Train: Loss 0.87642, R2 0.75488, RMSE 0.93286                     | Test: Loss 0.87528, R2 0.76852, RMSE 0.93026\n",
      "Epoch [14/500]       | Train: Loss 0.87026, R2 0.75561, RMSE 0.92919                     | Test: Loss 0.99586, R2 0.70641, RMSE 0.98042\n",
      "Epoch [15/500]       | Train: Loss 0.84667, R2 0.76427, RMSE 0.91633                     | Test: Loss 0.84971, R2 0.76107, RMSE 0.90832\n",
      "Epoch [16/500]       | Train: Loss 0.85240, R2 0.76119, RMSE 0.91947                     | Test: Loss 0.98630, R2 0.72996, RMSE 0.97638\n",
      "Epoch [17/500]       | Train: Loss 0.84693, R2 0.76243, RMSE 0.91774                     | Test: Loss 0.96661, R2 0.74542, RMSE 0.97192\n",
      "Epoch [18/500]       | Train: Loss 0.83072, R2 0.76791, RMSE 0.90792                     | Test: Loss 0.94438, R2 0.72890, RMSE 0.94639\n",
      "Epoch [19/500]       | Train: Loss 0.80293, R2 0.77420, RMSE 0.89290                     | Test: Loss 0.84148, R2 0.77280, RMSE 0.90729\n",
      "Epoch [20/500]       | Train: Loss 0.81959, R2 0.77017, RMSE 0.90045                     | Test: Loss 0.88168, R2 0.75957, RMSE 0.93373\n",
      "Epoch [21/500]       | Train: Loss 0.82167, R2 0.77068, RMSE 0.90297                     | Test: Loss 0.82687, R2 0.77256, RMSE 0.89729\n",
      "Epoch [22/500]       | Train: Loss 0.80300, R2 0.77327, RMSE 0.89269                     | Test: Loss 0.82368, R2 0.77567, RMSE 0.89230\n",
      "Epoch [23/500]       | Train: Loss 0.78123, R2 0.77986, RMSE 0.88110                     | Test: Loss 0.85478, R2 0.77448, RMSE 0.91955\n",
      "Epoch [24/500]       | Train: Loss 0.77377, R2 0.78257, RMSE 0.87658                     | Test: Loss 0.78914, R2 0.77829, RMSE 0.87822\n",
      "Epoch [25/500]       | Train: Loss 0.77193, R2 0.78360, RMSE 0.87374                     | Test: Loss 0.92041, R2 0.75246, RMSE 0.94620\n",
      "Epoch [26/500]       | Train: Loss 0.77966, R2 0.78123, RMSE 0.87942                     | Test: Loss 1.10372, R2 0.73795, RMSE 0.98882\n",
      "Epoch [27/500]       | Train: Loss 0.77288, R2 0.78338, RMSE 0.87617                     | Test: Loss 0.84740, R2 0.76150, RMSE 0.91190\n",
      "Epoch [28/500]       | Train: Loss 0.75340, R2 0.78985, RMSE 0.86312                     | Test: Loss 0.88073, R2 0.70101, RMSE 0.92895\n",
      "Epoch [29/500]       | Train: Loss 0.76227, R2 0.78519, RMSE 0.87031                     | Test: Loss 0.89378, R2 0.75944, RMSE 0.93354\n",
      "Epoch [30/500]       | Train: Loss 0.74201, R2 0.79183, RMSE 0.85793                     | Test: Loss 0.89698, R2 0.76256, RMSE 0.93437\n",
      "Epoch [31/500]       | Train: Loss 0.75256, R2 0.78902, RMSE 0.86456                     | Test: Loss 0.91625, R2 0.74732, RMSE 0.94864\n",
      "Epoch [32/500]       | Train: Loss 0.79259, R2 0.77864, RMSE 0.88567                     | Test: Loss 0.85784, R2 0.74957, RMSE 0.91917\n",
      "Epoch [33/500]       | Train: Loss 0.74183, R2 0.79152, RMSE 0.85790                     | Test: Loss 0.82537, R2 0.76660, RMSE 0.90150\n",
      "Epoch [34/500]       | Train: Loss 0.74928, R2 0.79179, RMSE 0.86075                     | Test: Loss 0.95119, R2 0.64774, RMSE 0.94078\n",
      "Epoch [35/500]       | Train: Loss 0.72243, R2 0.79590, RMSE 0.84630                     | Test: Loss 0.83444, R2 0.76235, RMSE 0.90737\n",
      "Epoch [36/500]       | Train: Loss 0.71842, R2 0.79902, RMSE 0.84487                     | Test: Loss 0.82490, R2 0.76677, RMSE 0.90168\n",
      "Epoch [37/500]       | Train: Loss 0.72116, R2 0.79749, RMSE 0.84653                     | Test: Loss 0.79277, R2 0.77868, RMSE 0.87825\n",
      "Epoch [38/500]       | Train: Loss 0.70969, R2 0.79988, RMSE 0.84001                     | Test: Loss 1.16740, R2 0.76125, RMSE 0.99430\n",
      "Epoch [39/500]       | Train: Loss 0.72036, R2 0.79912, RMSE 0.84540                     | Test: Loss 0.82519, R2 0.76135, RMSE 0.89481\n",
      "Epoch [40/500]       | Train: Loss 0.71103, R2 0.80115, RMSE 0.84060                     | Test: Loss 0.81349, R2 0.78017, RMSE 0.89112\n",
      "Epoch [41/500]       | Train: Loss 0.70134, R2 0.80225, RMSE 0.83443                     | Test: Loss 0.86043, R2 0.76351, RMSE 0.91955\n",
      "Epoch [42/500]       | Train: Loss 0.70716, R2 0.80219, RMSE 0.83816                     | Test: Loss 0.78444, R2 0.78324, RMSE 0.86724\n",
      "Epoch [43/500]       | Train: Loss 0.69513, R2 0.80483, RMSE 0.83110                     | Test: Loss 0.79206, R2 0.77764, RMSE 0.88057\n",
      "Epoch [44/500]       | Train: Loss 0.70216, R2 0.80326, RMSE 0.83391                     | Test: Loss 0.82783, R2 0.76713, RMSE 0.90058\n",
      "Epoch [45/500]       | Train: Loss 0.70287, R2 0.80214, RMSE 0.83486                     | Test: Loss 0.79168, R2 0.78367, RMSE 0.88165\n",
      "Epoch [46/500]       | Train: Loss 0.67414, R2 0.81060, RMSE 0.81780                     | Test: Loss 0.83395, R2 0.76894, RMSE 0.90737\n",
      "Epoch [47/500]       | Train: Loss 0.68476, R2 0.80764, RMSE 0.82437                     | Test: Loss 0.82774, R2 0.77376, RMSE 0.90180\n",
      "Epoch [48/500]       | Train: Loss 0.68236, R2 0.80862, RMSE 0.82334                     | Test: Loss 0.89057, R2 0.75665, RMSE 0.93489\n",
      "Epoch [49/500]       | Train: Loss 0.67862, R2 0.80849, RMSE 0.82105                     | Test: Loss 0.86552, R2 0.76588, RMSE 0.92636\n",
      "Epoch [50/500]       | Train: Loss 0.66885, R2 0.81141, RMSE 0.81543                     | Test: Loss 0.77914, R2 0.78444, RMSE 0.87019\n",
      "Epoch [51/500]       | Train: Loss 0.65504, R2 0.81580, RMSE 0.80642                     | Test: Loss 0.78433, R2 0.78900, RMSE 0.87121\n",
      "Epoch [52/500]       | Train: Loss 0.65029, R2 0.81795, RMSE 0.80334                     | Test: Loss 0.75837, R2 0.78642, RMSE 0.85975\n",
      "Epoch [53/500]       | Train: Loss 0.66109, R2 0.81260, RMSE 0.80914                     | Test: Loss 0.85674, R2 0.77447, RMSE 0.91912\n",
      "Epoch [54/500]       | Train: Loss 0.65368, R2 0.81588, RMSE 0.80611                     | Test: Loss 0.77577, R2 0.78627, RMSE 0.87078\n",
      "Epoch [55/500]       | Train: Loss 0.68409, R2 0.80772, RMSE 0.82404                     | Test: Loss 0.80061, R2 0.76209, RMSE 0.88534\n",
      "Epoch [56/500]       | Train: Loss 0.65165, R2 0.81730, RMSE 0.80485                     | Test: Loss 0.87130, R2 0.75367, RMSE 0.92285\n",
      "Epoch [57/500]       | Train: Loss 0.64249, R2 0.82050, RMSE 0.79769                     | Test: Loss 0.94455, R2 0.74079, RMSE 0.96301\n",
      "Epoch [58/500]       | Train: Loss 0.64733, R2 0.81755, RMSE 0.80199                     | Test: Loss 0.78178, R2 0.79136, RMSE 0.87389\n",
      "Epoch [59/500]       | Train: Loss 0.65298, R2 0.81646, RMSE 0.80437                     | Test: Loss 0.79662, R2 0.78664, RMSE 0.88236\n",
      "Epoch [60/500]       | Train: Loss 0.63831, R2 0.82169, RMSE 0.79633                     | Test: Loss 0.83090, R2 0.77857, RMSE 0.90162\n",
      "Epoch [61/500]       | Train: Loss 0.62835, R2 0.82328, RMSE 0.79094                     | Test: Loss 0.77500, R2 0.78763, RMSE 0.87207\n",
      "Epoch [62/500]       | Train: Loss 0.62507, R2 0.82448, RMSE 0.78777                     | Test: Loss 0.81085, R2 0.78091, RMSE 0.89065\n",
      "Epoch [63/500]       | Train: Loss 0.61522, R2 0.82698, RMSE 0.78174                     | Test: Loss 0.80859, R2 0.78003, RMSE 0.89141\n",
      "Epoch [64/500]       | Train: Loss 0.61713, R2 0.82687, RMSE 0.78254                     | Test: Loss 0.86002, R2 0.76315, RMSE 0.91658\n",
      "Epoch [65/500]       | Train: Loss 0.61381, R2 0.82747, RMSE 0.78074                     | Test: Loss 0.81865, R2 0.76585, RMSE 0.90119\n",
      "Epoch [66/500]       | Train: Loss 0.60697, R2 0.83034, RMSE 0.77693                     | Test: Loss 0.82593, R2 0.78342, RMSE 0.89712\n",
      "Epoch [67/500]       | Train: Loss 0.61176, R2 0.82907, RMSE 0.78015                     | Test: Loss 0.77958, R2 0.78919, RMSE 0.87435\n",
      "Epoch [68/500]       | Train: Loss 0.59937, R2 0.83233, RMSE 0.77175                     | Test: Loss 0.77168, R2 0.78325, RMSE 0.86883\n",
      "Epoch [69/500]       | Train: Loss 0.60051, R2 0.83124, RMSE 0.77194                     | Test: Loss 0.79872, R2 0.78324, RMSE 0.88749\n",
      "Epoch [70/500]       | Train: Loss 0.59588, R2 0.83275, RMSE 0.76925                     | Test: Loss 0.78715, R2 0.78691, RMSE 0.87638\n",
      "Epoch [71/500]       | Train: Loss 0.60271, R2 0.83112, RMSE 0.77412                     | Test: Loss 0.76976, R2 0.79186, RMSE 0.86258\n",
      "Epoch [72/500]       | Train: Loss 0.59357, R2 0.83221, RMSE 0.76810                     | Test: Loss 0.76428, R2 0.78821, RMSE 0.86584\n",
      "Epoch [73/500]       | Train: Loss 0.59831, R2 0.83035, RMSE 0.77100                     | Test: Loss 0.79259, R2 0.78954, RMSE 0.88461\n",
      "Epoch [74/500]       | Train: Loss 0.58211, R2 0.83596, RMSE 0.76071                     | Test: Loss 0.78507, R2 0.77861, RMSE 0.87704\n",
      "Epoch [75/500]       | Train: Loss 0.59040, R2 0.83247, RMSE 0.76647                     | Test: Loss 0.79075, R2 0.78049, RMSE 0.87707\n",
      "Epoch [76/500]       | Train: Loss 0.58508, R2 0.83399, RMSE 0.76275                     | Test: Loss 0.79694, R2 0.76950, RMSE 0.88362\n",
      "Epoch [77/500]       | Train: Loss 0.56615, R2 0.84024, RMSE 0.74871                     | Test: Loss 0.86336, R2 0.76609, RMSE 0.92338\n",
      "Epoch [78/500]       | Train: Loss 0.60399, R2 0.82791, RMSE 0.77489                     | Test: Loss 0.78534, R2 0.78444, RMSE 0.87858\n",
      "Epoch [79/500]       | Train: Loss 0.57012, R2 0.83858, RMSE 0.75344                     | Test: Loss 0.79735, R2 0.78103, RMSE 0.88447\n",
      "Epoch [80/500]       | Train: Loss 0.57276, R2 0.83848, RMSE 0.75442                     | Test: Loss 0.79376, R2 0.77946, RMSE 0.87906\n",
      "Epoch [81/500]       | Train: Loss 0.56735, R2 0.83914, RMSE 0.75073                     | Test: Loss 0.78770, R2 0.78833, RMSE 0.87273\n",
      "Epoch [82/500]       | Train: Loss 0.55462, R2 0.84247, RMSE 0.74312                     | Test: Loss 0.80347, R2 0.77751, RMSE 0.88684\n",
      "Epoch [83/500]       | Train: Loss 0.55186, R2 0.84433, RMSE 0.74050                     | Test: Loss 0.80743, R2 0.77516, RMSE 0.88947\n",
      "Epoch [84/500]       | Train: Loss 0.55457, R2 0.84339, RMSE 0.74243                     | Test: Loss 0.77899, R2 0.78844, RMSE 0.87421\n",
      "Epoch [85/500]       | Train: Loss 0.55211, R2 0.84550, RMSE 0.74136                     | Test: Loss 0.90549, R2 0.72252, RMSE 0.94204\n",
      "Epoch [86/500]       | Train: Loss 0.55726, R2 0.84299, RMSE 0.74481                     | Test: Loss 0.77616, R2 0.78777, RMSE 0.87117\n",
      "Epoch [87/500]       | Train: Loss 0.55439, R2 0.84367, RMSE 0.74243                     | Test: Loss 0.84810, R2 0.77225, RMSE 0.91321\n",
      "Epoch [88/500]       | Train: Loss 0.55533, R2 0.84286, RMSE 0.74290                     | Test: Loss 0.82501, R2 0.76566, RMSE 0.90154\n",
      "Epoch [89/500]       | Train: Loss 0.55306, R2 0.84359, RMSE 0.74234                     | Test: Loss 0.77873, R2 0.78939, RMSE 0.87664\n",
      "Epoch [90/500]       | Train: Loss 0.54098, R2 0.84937, RMSE 0.73251                     | Test: Loss 0.84061, R2 0.76707, RMSE 0.91150\n",
      "Epoch [91/500]       | Train: Loss 0.55050, R2 0.84404, RMSE 0.73956                     | Test: Loss 0.80998, R2 0.78380, RMSE 0.89167\n",
      "Epoch [92/500]       | Train: Loss 0.54648, R2 0.84517, RMSE 0.73735                     | Test: Loss 0.81045, R2 0.75668, RMSE 0.88954\n",
      "Epoch [93/500]       | Train: Loss 0.53218, R2 0.84850, RMSE 0.72738                     | Test: Loss 0.78912, R2 0.77727, RMSE 0.86942\n",
      "Epoch [94/500]       | Train: Loss 0.53497, R2 0.84977, RMSE 0.72866                     | Test: Loss 0.79882, R2 0.77817, RMSE 0.88540\n",
      "Epoch [95/500]       | Train: Loss 0.52250, R2 0.85180, RMSE 0.72054                     | Test: Loss 0.77094, R2 0.79359, RMSE 0.86076\n",
      "Epoch [96/500]       | Train: Loss 0.51865, R2 0.85397, RMSE 0.71836                     | Test: Loss 0.78051, R2 0.78430, RMSE 0.87356\n",
      "Epoch [97/500]       | Train: Loss 0.51784, R2 0.85322, RMSE 0.71793                     | Test: Loss 0.81250, R2 0.77118, RMSE 0.89154\n",
      "Epoch [98/500]       | Train: Loss 0.51445, R2 0.85529, RMSE 0.71482                     | Test: Loss 0.81466, R2 0.76364, RMSE 0.88940\n",
      "Epoch [99/500]       | Train: Loss 0.51595, R2 0.85315, RMSE 0.71666                     | Test: Loss 0.79594, R2 0.77165, RMSE 0.88722\n",
      "Epoch [100/500]      | Train: Loss 0.50167, R2 0.85790, RMSE 0.70734                     | Test: Loss 0.83395, R2 0.76368, RMSE 0.90957\n",
      "Epoch [101/500]      | Train: Loss 0.50456, R2 0.85761, RMSE 0.70874                     | Test: Loss 0.78669, R2 0.78472, RMSE 0.88016\n",
      "Epoch [102/500]      | Train: Loss 0.50814, R2 0.85715, RMSE 0.71131                     | Test: Loss 0.85278, R2 0.77474, RMSE 0.91823\n",
      "Epoch [103/500]      | Train: Loss 0.51444, R2 0.85457, RMSE 0.71516                     | Test: Loss 1.12945, R2 0.75056, RMSE 0.98023\n",
      "Epoch [104/500]      | Train: Loss 0.49659, R2 0.85951, RMSE 0.70323                     | Test: Loss 0.79758, R2 0.78308, RMSE 0.88371\n",
      "Epoch [105/500]      | Train: Loss 0.49022, R2 0.86180, RMSE 0.69825                     | Test: Loss 0.82798, R2 0.77507, RMSE 0.90234\n",
      "Epoch [106/500]      | Train: Loss 0.48392, R2 0.86311, RMSE 0.69451                     | Test: Loss 0.80819, R2 0.78915, RMSE 0.88772\n",
      "Epoch [107/500]      | Train: Loss 0.49309, R2 0.86161, RMSE 0.70065                     | Test: Loss 0.87632, R2 0.73527, RMSE 0.92866\n",
      "Epoch [108/500]      | Train: Loss 0.49671, R2 0.85870, RMSE 0.70308                     | Test: Loss 0.84267, R2 0.76967, RMSE 0.90924\n",
      "Epoch [109/500]      | Train: Loss 0.47707, R2 0.86455, RMSE 0.68931                     | Test: Loss 0.81226, R2 0.77650, RMSE 0.88799\n",
      "Epoch [110/500]      | Train: Loss 0.49255, R2 0.86076, RMSE 0.70023                     | Test: Loss 0.83652, R2 0.76595, RMSE 0.90566\n",
      "Epoch [111/500]      | Train: Loss 0.47165, R2 0.86702, RMSE 0.68559                     | Test: Loss 0.79179, R2 0.77620, RMSE 0.88130\n",
      "Epoch [112/500]      | Train: Loss 0.47804, R2 0.86412, RMSE 0.68983                     | Test: Loss 0.80229, R2 0.77967, RMSE 0.87676\n",
      "Epoch [113/500]      | Train: Loss 0.47936, R2 0.86508, RMSE 0.69034                     | Test: Loss 0.80451, R2 0.78166, RMSE 0.89115\n",
      "Epoch [114/500]      | Train: Loss 0.48651, R2 0.86376, RMSE 0.69608                     | Test: Loss 1.15315, R2 0.62497, RMSE 0.99438\n",
      "Epoch [115/500]      | Train: Loss 0.47759, R2 0.86514, RMSE 0.68940                     | Test: Loss 0.79164, R2 0.78384, RMSE 0.87901\n",
      "Epoch [116/500]      | Train: Loss 0.47323, R2 0.86686, RMSE 0.68624                     | Test: Loss 0.86016, R2 0.77006, RMSE 0.91996\n",
      "Epoch [117/500]      | Train: Loss 0.45855, R2 0.86981, RMSE 0.67571                     | Test: Loss 0.77263, R2 0.78767, RMSE 0.86705\n",
      "Epoch [118/500]      | Train: Loss 0.45815, R2 0.87126, RMSE 0.67555                     | Test: Loss 0.85205, R2 0.77938, RMSE 0.91344\n",
      "Epoch [119/500]      | Train: Loss 0.45622, R2 0.87098, RMSE 0.67382                     | Test: Loss 0.81668, R2 0.77731, RMSE 0.89670\n",
      "Epoch [120/500]      | Train: Loss 0.44708, R2 0.87427, RMSE 0.66732                     | Test: Loss 0.80787, R2 0.78202, RMSE 0.88447\n",
      "Epoch [121/500]      | Train: Loss 0.45815, R2 0.87102, RMSE 0.67564                     | Test: Loss 0.79452, R2 0.77773, RMSE 0.88313\n",
      "Epoch [122/500]      | Train: Loss 0.44793, R2 0.87267, RMSE 0.66765                     | Test: Loss 0.81994, R2 0.77308, RMSE 0.89378\n",
      "Epoch [123/500]      | Train: Loss 0.44387, R2 0.87375, RMSE 0.66503                     | Test: Loss 0.77868, R2 0.77432, RMSE 0.87429\n",
      "Epoch [124/500]      | Train: Loss 0.43685, R2 0.87596, RMSE 0.65955                     | Test: Loss 0.82223, R2 0.77399, RMSE 0.89607\n",
      "Epoch [125/500]      | Train: Loss 0.44507, R2 0.87409, RMSE 0.66586                     | Test: Loss 0.80827, R2 0.76729, RMSE 0.88695\n",
      "Epoch [126/500]      | Train: Loss 0.43479, R2 0.87744, RMSE 0.65761                     | Test: Loss 0.79193, R2 0.79229, RMSE 0.86613\n",
      "Epoch [127/500]      | Train: Loss 0.42728, R2 0.87888, RMSE 0.65154                     | Test: Loss 0.83399, R2 0.76991, RMSE 0.90147\n",
      "Epoch [128/500]      | Train: Loss 0.43159, R2 0.87799, RMSE 0.65591                     | Test: Loss 0.82047, R2 0.77763, RMSE 0.89491\n",
      "Epoch [129/500]      | Train: Loss 0.43144, R2 0.87767, RMSE 0.65570                     | Test: Loss 0.80310, R2 0.77781, RMSE 0.88006\n",
      "Epoch [130/500]      | Train: Loss 0.42704, R2 0.87899, RMSE 0.65230                     | Test: Loss 0.82786, R2 0.77086, RMSE 0.89773\n",
      "Epoch [131/500]      | Train: Loss 0.41393, R2 0.88141, RMSE 0.64236                     | Test: Loss 0.79070, R2 0.77268, RMSE 0.88352\n",
      "Epoch [132/500]      | Train: Loss 0.41536, R2 0.88202, RMSE 0.64327                     | Test: Loss 0.90174, R2 0.77170, RMSE 0.93698\n",
      "Epoch [133/500]      | Train: Loss 0.41136, R2 0.88456, RMSE 0.63997                     | Test: Loss 0.79845, R2 0.77160, RMSE 0.88134\n",
      "Epoch [134/500]      | Train: Loss 0.41972, R2 0.88161, RMSE 0.64660                     | Test: Loss 0.87734, R2 0.76698, RMSE 0.92390\n",
      "Epoch [135/500]      | Train: Loss 0.41014, R2 0.88393, RMSE 0.63914                     | Test: Loss 0.84797, R2 0.76739, RMSE 0.90756\n",
      "Epoch [136/500]      | Train: Loss 0.41604, R2 0.88327, RMSE 0.64331                     | Test: Loss 0.83418, R2 0.76990, RMSE 0.90323\n",
      "Epoch [137/500]      | Train: Loss 0.41017, R2 0.88454, RMSE 0.63903                     | Test: Loss 0.80636, R2 0.77476, RMSE 0.88792\n",
      "Epoch [138/500]      | Train: Loss 0.40570, R2 0.88426, RMSE 0.63584                     | Test: Loss 0.91612, R2 0.77031, RMSE 0.93482\n",
      "Epoch [139/500]      | Train: Loss 0.40305, R2 0.88661, RMSE 0.63402                     | Test: Loss 0.84707, R2 0.76621, RMSE 0.90960\n",
      "Epoch [140/500]      | Train: Loss 0.42530, R2 0.87929, RMSE 0.65032                     | Test: Loss 0.84569, R2 0.77584, RMSE 0.90910\n",
      "Epoch [141/500]      | Train: Loss 0.40299, R2 0.88568, RMSE 0.63370                     | Test: Loss 1.01796, R2 0.71714, RMSE 0.98086\n",
      "Epoch [142/500]      | Train: Loss 0.38781, R2 0.88987, RMSE 0.62176                     | Test: Loss 0.80169, R2 0.77918, RMSE 0.88435\n",
      "Epoch [143/500]      | Train: Loss 0.39231, R2 0.88857, RMSE 0.62551                     | Test: Loss 0.81145, R2 0.77803, RMSE 0.89319\n",
      "Epoch [144/500]      | Train: Loss 0.39177, R2 0.88954, RMSE 0.62468                     | Test: Loss 0.87878, R2 0.76814, RMSE 0.92766\n",
      "Epoch [145/500]      | Train: Loss 0.39209, R2 0.88846, RMSE 0.62514                     | Test: Loss 0.79574, R2 0.78127, RMSE 0.87743\n",
      "Epoch [146/500]      | Train: Loss 0.38535, R2 0.89098, RMSE 0.61971                     | Test: Loss 0.86189, R2 0.77079, RMSE 0.92031\n",
      "Epoch [147/500]      | Train: Loss 0.38432, R2 0.89156, RMSE 0.61859                     | Test: Loss 0.87719, R2 0.74763, RMSE 0.92470\n",
      "Epoch [148/500]      | Train: Loss 0.38210, R2 0.89187, RMSE 0.61653                     | Test: Loss 0.86486, R2 0.74375, RMSE 0.91977\n",
      "Epoch [149/500]      | Train: Loss 0.37358, R2 0.89360, RMSE 0.61051                     | Test: Loss 0.87427, R2 0.75824, RMSE 0.92844\n",
      "Epoch [150/500]      | Train: Loss 0.35995, R2 0.89891, RMSE 0.59914                     | Test: Loss 0.84700, R2 0.75312, RMSE 0.91571\n",
      "Epoch [151/500]      | Train: Loss 0.37054, R2 0.89564, RMSE 0.60765                     | Test: Loss 0.84367, R2 0.74953, RMSE 0.91186\n",
      "Epoch [152/500]      | Train: Loss 0.36559, R2 0.89690, RMSE 0.60345                     | Test: Loss 0.83338, R2 0.77032, RMSE 0.89748\n",
      "Epoch [153/500]      | Train: Loss 0.37014, R2 0.89416, RMSE 0.60700                     | Test: Loss 0.89133, R2 0.76553, RMSE 0.92686\n",
      "Epoch [154/500]      | Train: Loss 0.36622, R2 0.89602, RMSE 0.60399                     | Test: Loss 0.88017, R2 0.76710, RMSE 0.93056\n",
      "Epoch [155/500]      | Train: Loss 0.35160, R2 0.89932, RMSE 0.59173                     | Test: Loss 0.86209, R2 0.76952, RMSE 0.92008\n",
      "Epoch [156/500]      | Train: Loss 0.35288, R2 0.90030, RMSE 0.59320                     | Test: Loss 0.83201, R2 0.77595, RMSE 0.89919\n",
      "Epoch [157/500]      | Train: Loss 0.36006, R2 0.89811, RMSE 0.59916                     | Test: Loss 0.83428, R2 0.76747, RMSE 0.89910\n",
      "Epoch [158/500]      | Train: Loss 0.35899, R2 0.89761, RMSE 0.59793                     | Test: Loss 0.85014, R2 0.77058, RMSE 0.91377\n",
      "Epoch [159/500]      | Train: Loss 0.35138, R2 0.90074, RMSE 0.59158                     | Test: Loss 0.84619, R2 0.73667, RMSE 0.90384\n",
      "Epoch [160/500]      | Train: Loss 0.35290, R2 0.90073, RMSE 0.59310                     | Test: Loss 0.90175, R2 0.76345, RMSE 0.93405\n",
      "Epoch [161/500]      | Train: Loss 0.34118, R2 0.90417, RMSE 0.58311                     | Test: Loss 0.85946, R2 0.76738, RMSE 0.91760\n",
      "Epoch [162/500]      | Train: Loss 0.33097, R2 0.90673, RMSE 0.57424                     | Test: Loss 0.83995, R2 0.76971, RMSE 0.91015\n",
      "Epoch [163/500]      | Train: Loss 0.34296, R2 0.90172, RMSE 0.58479                     | Test: Loss 1.12361, R2 0.75115, RMSE 0.99728\n",
      "Epoch [164/500]      | Train: Loss 0.33926, R2 0.90405, RMSE 0.58150                     | Test: Loss 0.80904, R2 0.78333, RMSE 0.88796\n",
      "Epoch [165/500]      | Train: Loss 0.33827, R2 0.90420, RMSE 0.58069                     | Test: Loss 0.82923, R2 0.76919, RMSE 0.89659\n",
      "Epoch [166/500]      | Train: Loss 0.33348, R2 0.90546, RMSE 0.57657                     | Test: Loss 0.90621, R2 0.75138, RMSE 0.93690\n",
      "Epoch [167/500]      | Train: Loss 0.33338, R2 0.90572, RMSE 0.57601                     | Test: Loss 0.92527, R2 0.76427, RMSE 0.94713\n",
      "Epoch [168/500]      | Train: Loss 0.32890, R2 0.90698, RMSE 0.57274                     | Test: Loss 0.83911, R2 0.77118, RMSE 0.90370\n",
      "Epoch [169/500]      | Train: Loss 0.31800, R2 0.91069, RMSE 0.56304                     | Test: Loss 0.85877, R2 0.76048, RMSE 0.91550\n",
      "Epoch [170/500]      | Train: Loss 0.32151, R2 0.90905, RMSE 0.56602                     | Test: Loss 0.85052, R2 0.76506, RMSE 0.91352\n",
      "Epoch [171/500]      | Train: Loss 0.31518, R2 0.91138, RMSE 0.56035                     | Test: Loss 0.84409, R2 0.77070, RMSE 0.90311\n",
      "Epoch [172/500]      | Train: Loss 0.31026, R2 0.91319, RMSE 0.55632                     | Test: Loss 0.86660, R2 0.72171, RMSE 0.92197\n",
      "Epoch [173/500]      | Train: Loss 0.31235, R2 0.91120, RMSE 0.55801                     | Test: Loss 0.95622, R2 0.74798, RMSE 0.96563\n",
      "Epoch [174/500]      | Train: Loss 0.31225, R2 0.91164, RMSE 0.55775                     | Test: Loss 0.86732, R2 0.76515, RMSE 0.92769\n",
      "Epoch [175/500]      | Train: Loss 0.30639, R2 0.91165, RMSE 0.55256                     | Test: Loss 0.88226, R2 0.73938, RMSE 0.93570\n",
      "Epoch [176/500]      | Train: Loss 0.30873, R2 0.91311, RMSE 0.55506                     | Test: Loss 0.85019, R2 0.77256, RMSE 0.91358\n",
      "Epoch [177/500]      | Train: Loss 0.31098, R2 0.91192, RMSE 0.55669                     | Test: Loss 0.85163, R2 0.76299, RMSE 0.91723\n",
      "Epoch [178/500]      | Train: Loss 0.30305, R2 0.91459, RMSE 0.54948                     | Test: Loss 0.85147, R2 0.76527, RMSE 0.91274\n",
      "Epoch [179/500]      | Train: Loss 0.29737, R2 0.91499, RMSE 0.54452                     | Test: Loss 0.82184, R2 0.77579, RMSE 0.89221\n",
      "Epoch [180/500]      | Train: Loss 0.29845, R2 0.91597, RMSE 0.54525                     | Test: Loss 0.92578, R2 0.75556, RMSE 0.95059\n",
      "Epoch [181/500]      | Train: Loss 0.29215, R2 0.91785, RMSE 0.53951                     | Test: Loss 0.84852, R2 0.75756, RMSE 0.90773\n",
      "Epoch [182/500]      | Train: Loss 0.29158, R2 0.91758, RMSE 0.53932                     | Test: Loss 0.81729, R2 0.77675, RMSE 0.89279\n",
      "Epoch [183/500]      | Train: Loss 0.29266, R2 0.91747, RMSE 0.53982                     | Test: Loss 0.85822, R2 0.75809, RMSE 0.91372\n",
      "Epoch [184/500]      | Train: Loss 0.28847, R2 0.91823, RMSE 0.53634                     | Test: Loss 0.87030, R2 0.76107, RMSE 0.92610\n",
      "Epoch [185/500]      | Train: Loss 0.28321, R2 0.91864, RMSE 0.53132                     | Test: Loss 0.84934, R2 0.77242, RMSE 0.91069\n",
      "Epoch [186/500]      | Train: Loss 0.28155, R2 0.92050, RMSE 0.52980                     | Test: Loss 0.83627, R2 0.76809, RMSE 0.90327\n",
      "Epoch [187/500]      | Train: Loss 0.28731, R2 0.91781, RMSE 0.53549                     | Test: Loss 0.89890, R2 0.76293, RMSE 0.93916\n",
      "Epoch [188/500]      | Train: Loss 0.28082, R2 0.92171, RMSE 0.52897                     | Test: Loss 0.85293, R2 0.76796, RMSE 0.92086\n",
      "Epoch [189/500]      | Train: Loss 0.27842, R2 0.92155, RMSE 0.52673                     | Test: Loss 0.83915, R2 0.77361, RMSE 0.90589\n",
      "Epoch [190/500]      | Train: Loss 0.27286, R2 0.92261, RMSE 0.52127                     | Test: Loss 0.87020, R2 0.75557, RMSE 0.92751\n",
      "Epoch [191/500]      | Train: Loss 0.26719, R2 0.92438, RMSE 0.51619                     | Test: Loss 1.20141, R2 0.70184, RMSE 1.01672\n",
      "Epoch [192/500]      | Train: Loss 0.26701, R2 0.92412, RMSE 0.51588                     | Test: Loss 0.88748, R2 0.75756, RMSE 0.93264\n",
      "Epoch [193/500]      | Train: Loss 0.26599, R2 0.92472, RMSE 0.51496                     | Test: Loss 0.86958, R2 0.75766, RMSE 0.92542\n",
      "Epoch [194/500]      | Train: Loss 0.27039, R2 0.92366, RMSE 0.51890                     | Test: Loss 0.86360, R2 0.75505, RMSE 0.92006\n",
      "Epoch [195/500]      | Train: Loss 0.26191, R2 0.92594, RMSE 0.51131                     | Test: Loss 0.86267, R2 0.76398, RMSE 0.91836\n",
      "Epoch [196/500]      | Train: Loss 0.25429, R2 0.92831, RMSE 0.50365                     | Test: Loss 0.88982, R2 0.76262, RMSE 0.93620\n",
      "Epoch [197/500]      | Train: Loss 0.25501, R2 0.92787, RMSE 0.50418                     | Test: Loss 0.87689, R2 0.75919, RMSE 0.92806\n",
      "Epoch [198/500]      | Train: Loss 0.25746, R2 0.92736, RMSE 0.50678                     | Test: Loss 0.88854, R2 0.75284, RMSE 0.93577\n",
      "Epoch [199/500]      | Train: Loss 0.25721, R2 0.92642, RMSE 0.50609                     | Test: Loss 0.86600, R2 0.76046, RMSE 0.92095\n",
      "Epoch [200/500]      | Train: Loss 0.25285, R2 0.92851, RMSE 0.50184                     | Test: Loss 0.89463, R2 0.75343, RMSE 0.93740\n",
      "Epoch [201/500]      | Train: Loss 0.24443, R2 0.93077, RMSE 0.49346                     | Test: Loss 0.85608, R2 0.75842, RMSE 0.91245\n",
      "Epoch [202/500]      | Train: Loss 0.24786, R2 0.92996, RMSE 0.49695                     | Test: Loss 0.87829, R2 0.75367, RMSE 0.92661\n",
      "Epoch [203/500]      | Train: Loss 0.24170, R2 0.93169, RMSE 0.49105                     | Test: Loss 0.84966, R2 0.76617, RMSE 0.90073\n",
      "Epoch [204/500]      | Train: Loss 0.24687, R2 0.92981, RMSE 0.49612                     | Test: Loss 0.82729, R2 0.77376, RMSE 0.89427\n",
      "Epoch [205/500]      | Train: Loss 0.23188, R2 0.93361, RMSE 0.48090                     | Test: Loss 0.92400, R2 0.72543, RMSE 0.95566\n",
      "Epoch [206/500]      | Train: Loss 0.24167, R2 0.93181, RMSE 0.49108                     | Test: Loss 0.87322, R2 0.76757, RMSE 0.92804\n",
      "Epoch [207/500]      | Train: Loss 0.23340, R2 0.93342, RMSE 0.48178                     | Test: Loss 0.85053, R2 0.76685, RMSE 0.91327\n",
      "Epoch [208/500]      | Train: Loss 0.23005, R2 0.93495, RMSE 0.47893                     | Test: Loss 0.90730, R2 0.73874, RMSE 0.94470\n",
      "Epoch [209/500]      | Train: Loss 0.23286, R2 0.93350, RMSE 0.48193                     | Test: Loss 0.87307, R2 0.76981, RMSE 0.92406\n",
      "Epoch [210/500]      | Train: Loss 0.22701, R2 0.93625, RMSE 0.47552                     | Test: Loss 0.89253, R2 0.74917, RMSE 0.93489\n",
      "Epoch [211/500]      | Train: Loss 0.22774, R2 0.93569, RMSE 0.47647                     | Test: Loss 0.85951, R2 0.76583, RMSE 0.91804\n",
      "Epoch [212/500]      | Train: Loss 0.21922, R2 0.93805, RMSE 0.46736                     | Test: Loss 0.95190, R2 0.75052, RMSE 0.95990\n",
      "Epoch [213/500]      | Train: Loss 0.22441, R2 0.93678, RMSE 0.47288                     | Test: Loss 0.84816, R2 0.76092, RMSE 0.91475\n",
      "Epoch [214/500]      | Train: Loss 0.22229, R2 0.93704, RMSE 0.47059                     | Test: Loss 0.91210, R2 0.75139, RMSE 0.94895\n",
      "Epoch [215/500]      | Train: Loss 0.21753, R2 0.93839, RMSE 0.46553                     | Test: Loss 0.94665, R2 0.71944, RMSE 0.96703\n",
      "Epoch [216/500]      | Train: Loss 0.21377, R2 0.93880, RMSE 0.46169                     | Test: Loss 0.91991, R2 0.75355, RMSE 0.95208\n",
      "Epoch [217/500]      | Train: Loss 0.22026, R2 0.93765, RMSE 0.46846                     | Test: Loss 0.89480, R2 0.74164, RMSE 0.93395\n",
      "Epoch [218/500]      | Train: Loss 0.21415, R2 0.93920, RMSE 0.46178                     | Test: Loss 0.89981, R2 0.75843, RMSE 0.94069\n",
      "Epoch [219/500]      | Train: Loss 0.21779, R2 0.93859, RMSE 0.46594                     | Test: Loss 0.91240, R2 0.75488, RMSE 0.95003\n",
      "Epoch [220/500]      | Train: Loss 0.20595, R2 0.94154, RMSE 0.45290                     | Test: Loss 0.86454, R2 0.75837, RMSE 0.92423\n",
      "Epoch [221/500]      | Train: Loss 0.21370, R2 0.93968, RMSE 0.46148                     | Test: Loss 0.99020, R2 0.72440, RMSE 0.97878\n",
      "Epoch [222/500]      | Train: Loss 0.22989, R2 0.93502, RMSE 0.47821                     | Test: Loss 0.89290, R2 0.75871, RMSE 0.93890\n",
      "Epoch [223/500]      | Train: Loss 0.20121, R2 0.94249, RMSE 0.44787                     | Test: Loss 0.87479, R2 0.75481, RMSE 0.92432\n",
      "Epoch [224/500]      | Train: Loss 0.19544, R2 0.94464, RMSE 0.44130                     | Test: Loss 0.92749, R2 0.73189, RMSE 0.95699\n",
      "Epoch [225/500]      | Train: Loss 0.20103, R2 0.94320, RMSE 0.44763                     | Test: Loss 0.96352, R2 0.74347, RMSE 0.97420\n",
      "Epoch [226/500]      | Train: Loss 0.19879, R2 0.94360, RMSE 0.44518                     | Test: Loss 0.94914, R2 0.74888, RMSE 0.96224\n",
      "Epoch [227/500]      | Train: Loss 0.19502, R2 0.94507, RMSE 0.44102                     | Test: Loss 0.92103, R2 0.75728, RMSE 0.95345\n",
      "Epoch [228/500]      | Train: Loss 0.19572, R2 0.94444, RMSE 0.44168                     | Test: Loss 0.88772, R2 0.74730, RMSE 0.93567\n",
      "Epoch [229/500]      | Train: Loss 0.18745, R2 0.94753, RMSE 0.43175                     | Test: Loss 0.92197, R2 0.75758, RMSE 0.94145\n",
      "Epoch [230/500]      | Train: Loss 0.19343, R2 0.94503, RMSE 0.43929                     | Test: Loss 1.03986, R2 0.73377, RMSE 0.99705\n",
      "Epoch [231/500]      | Train: Loss 0.19164, R2 0.94536, RMSE 0.43714                     | Test: Loss 0.96748, R2 0.73220, RMSE 0.97319\n",
      "Epoch [232/500]      | Train: Loss 0.18825, R2 0.94645, RMSE 0.43321                     | Test: Loss 0.87324, R2 0.76019, RMSE 0.92374\n",
      "Epoch [233/500]      | Train: Loss 0.18618, R2 0.94735, RMSE 0.43084                     | Test: Loss 0.91320, R2 0.73770, RMSE 0.95041\n",
      "Epoch [234/500]      | Train: Loss 0.18352, R2 0.94844, RMSE 0.42760                     | Test: Loss 0.88339, R2 0.75362, RMSE 0.93035\n",
      "Epoch [235/500]      | Train: Loss 0.18260, R2 0.94820, RMSE 0.42670                     | Test: Loss 0.92025, R2 0.73400, RMSE 0.95102\n",
      "Epoch [236/500]      | Train: Loss 0.18461, R2 0.94750, RMSE 0.42903                     | Test: Loss 0.90780, R2 0.75155, RMSE 0.94609\n",
      "Epoch [237/500]      | Train: Loss 0.18295, R2 0.94796, RMSE 0.42693                     | Test: Loss 0.88340, R2 0.75829, RMSE 0.92913\n",
      "Epoch [238/500]      | Train: Loss 0.17344, R2 0.95116, RMSE 0.41577                     | Test: Loss 0.89576, R2 0.75079, RMSE 0.94021\n",
      "Epoch [239/500]      | Train: Loss 0.18262, R2 0.94858, RMSE 0.42638                     | Test: Loss 0.96202, R2 0.73451, RMSE 0.97459\n",
      "Epoch [240/500]      | Train: Loss 0.17229, R2 0.95143, RMSE 0.41435                     | Test: Loss 0.90236, R2 0.75210, RMSE 0.94334\n",
      "Epoch [241/500]      | Train: Loss 0.17185, R2 0.95170, RMSE 0.41369                     | Test: Loss 0.93260, R2 0.74444, RMSE 0.95550\n",
      "Epoch [242/500]      | Train: Loss 0.17299, R2 0.95109, RMSE 0.41545                     | Test: Loss 1.02346, R2 0.69471, RMSE 0.99310\n",
      "Epoch [243/500]      | Train: Loss 0.16654, R2 0.95303, RMSE 0.40725                     | Test: Loss 0.90547, R2 0.73902, RMSE 0.94352\n",
      "Epoch [244/500]      | Train: Loss 0.16790, R2 0.95270, RMSE 0.40907                     | Test: Loss 0.91637, R2 0.74779, RMSE 0.94829\n",
      "Epoch [245/500]      | Train: Loss 0.17246, R2 0.95101, RMSE 0.41416                     | Test: Loss 0.89349, R2 0.73073, RMSE 0.93191\n",
      "Epoch [246/500]      | Train: Loss 0.16339, R2 0.95367, RMSE 0.40363                     | Test: Loss 0.89000, R2 0.76433, RMSE 0.93537\n",
      "Epoch [247/500]      | Train: Loss 0.16356, R2 0.95362, RMSE 0.40378                     | Test: Loss 0.94022, R2 0.75330, RMSE 0.96297\n",
      "Epoch [248/500]      | Train: Loss 0.16325, R2 0.95366, RMSE 0.40333                     | Test: Loss 0.96795, R2 0.73616, RMSE 0.97507\n",
      "Epoch [249/500]      | Train: Loss 0.16284, R2 0.95433, RMSE 0.40281                     | Test: Loss 0.93769, R2 0.74942, RMSE 0.96225\n",
      "Epoch [250/500]      | Train: Loss 0.15752, R2 0.95556, RMSE 0.39605                     | Test: Loss 0.93434, R2 0.72800, RMSE 0.95438\n",
      "Epoch [251/500]      | Train: Loss 0.15843, R2 0.95538, RMSE 0.39736                     | Test: Loss 0.95152, R2 0.71636, RMSE 0.96497\n",
      "Epoch [252/500]      | Train: Loss 0.15669, R2 0.95540, RMSE 0.39519                     | Test: Loss 0.89079, R2 0.74322, RMSE 0.93807\n",
      "Epoch [253/500]      | Train: Loss 0.16153, R2 0.95456, RMSE 0.40117                     | Test: Loss 0.87859, R2 0.75436, RMSE 0.92721\n",
      "Epoch [254/500]      | Train: Loss 0.15441, R2 0.95666, RMSE 0.39248                     | Test: Loss 0.95783, R2 0.74545, RMSE 0.96710\n",
      "Epoch [255/500]      | Train: Loss 0.15641, R2 0.95569, RMSE 0.39480                     | Test: Loss 0.87791, R2 0.75230, RMSE 0.92650\n",
      "Epoch [256/500]      | Train: Loss 0.15804, R2 0.95557, RMSE 0.39666                     | Test: Loss 0.94785, R2 0.75143, RMSE 0.96256\n",
      "Epoch [257/500]      | Train: Loss 0.14987, R2 0.95788, RMSE 0.38658                     | Test: Loss 0.90659, R2 0.75047, RMSE 0.94298\n",
      "Epoch [258/500]      | Train: Loss 0.15208, R2 0.95707, RMSE 0.38910                     | Test: Loss 0.91923, R2 0.75308, RMSE 0.95023\n",
      "Epoch [259/500]      | Train: Loss 0.14719, R2 0.95852, RMSE 0.38320                     | Test: Loss 1.09421, R2 0.73415, RMSE 1.01364\n",
      "Epoch [260/500]      | Train: Loss 0.14733, R2 0.95836, RMSE 0.38315                     | Test: Loss 0.89106, R2 0.76014, RMSE 0.93756\n",
      "Epoch [261/500]      | Train: Loss 0.14836, R2 0.95833, RMSE 0.38436                     | Test: Loss 0.94884, R2 0.73710, RMSE 0.97116\n",
      "Epoch [262/500]      | Train: Loss 0.14559, R2 0.95879, RMSE 0.38108                     | Test: Loss 0.91912, R2 0.73843, RMSE 0.95327\n",
      "Epoch [263/500]      | Train: Loss 0.14147, R2 0.95982, RMSE 0.37544                     | Test: Loss 0.91679, R2 0.75834, RMSE 0.95201\n",
      "Epoch [264/500]      | Train: Loss 0.14262, R2 0.95955, RMSE 0.37701                     | Test: Loss 0.88587, R2 0.75526, RMSE 0.92492\n",
      "Epoch [265/500]      | Train: Loss 0.13973, R2 0.96074, RMSE 0.37340                     | Test: Loss 0.94819, R2 0.74318, RMSE 0.96382\n",
      "Epoch [266/500]      | Train: Loss 0.14299, R2 0.95920, RMSE 0.37760                     | Test: Loss 0.90049, R2 0.75673, RMSE 0.94137\n",
      "Epoch [267/500]      | Train: Loss 0.13694, R2 0.96150, RMSE 0.36968                     | Test: Loss 1.09930, R2 0.69490, RMSE 1.01479\n",
      "Epoch [268/500]      | Train: Loss 0.14299, R2 0.95938, RMSE 0.37731                     | Test: Loss 0.90638, R2 0.73754, RMSE 0.94376\n",
      "Epoch [269/500]      | Train: Loss 0.14232, R2 0.95981, RMSE 0.37653                     | Test: Loss 0.90906, R2 0.75197, RMSE 0.94475\n",
      "Epoch [270/500]      | Train: Loss 0.14291, R2 0.95974, RMSE 0.37714                     | Test: Loss 0.88297, R2 0.73829, RMSE 0.93078\n",
      "Epoch [271/500]      | Train: Loss 0.13467, R2 0.96146, RMSE 0.36607                     | Test: Loss 0.88824, R2 0.74856, RMSE 0.93017\n",
      "Epoch [272/500]      | Train: Loss 0.13500, R2 0.96192, RMSE 0.36691                     | Test: Loss 0.91100, R2 0.73733, RMSE 0.94412\n",
      "Epoch [273/500]      | Train: Loss 0.12741, R2 0.96377, RMSE 0.35650                     | Test: Loss 0.90791, R2 0.75674, RMSE 0.94663\n",
      "Epoch [274/500]      | Train: Loss 0.13210, R2 0.96254, RMSE 0.36289                     | Test: Loss 0.89123, R2 0.75656, RMSE 0.93413\n",
      "Epoch [275/500]      | Train: Loss 0.13788, R2 0.96131, RMSE 0.37070                     | Test: Loss 0.90297, R2 0.74881, RMSE 0.94080\n",
      "Epoch [276/500]      | Train: Loss 0.13335, R2 0.96202, RMSE 0.36441                     | Test: Loss 0.90493, R2 0.75466, RMSE 0.94149\n",
      "Epoch [277/500]      | Train: Loss 0.13325, R2 0.96230, RMSE 0.36418                     | Test: Loss 0.95573, R2 0.74173, RMSE 0.96331\n",
      "Epoch [278/500]      | Train: Loss 0.13472, R2 0.96161, RMSE 0.36610                     | Test: Loss 0.88908, R2 0.73991, RMSE 0.93109\n",
      "Epoch [279/500]      | Train: Loss 0.12536, R2 0.96430, RMSE 0.35332                     | Test: Loss 0.89734, R2 0.75358, RMSE 0.94114\n",
      "Epoch [280/500]      | Train: Loss 0.12686, R2 0.96415, RMSE 0.35550                     | Test: Loss 0.94456, R2 0.75255, RMSE 0.96641\n",
      "Epoch [281/500]      | Train: Loss 0.12465, R2 0.96464, RMSE 0.35236                     | Test: Loss 0.97001, R2 0.73765, RMSE 0.97215\n",
      "Epoch [282/500]      | Train: Loss 0.12895, R2 0.96329, RMSE 0.35853                     | Test: Loss 0.89111, R2 0.75040, RMSE 0.93761\n",
      "Epoch [283/500]      | Train: Loss 0.12525, R2 0.96467, RMSE 0.35325                     | Test: Loss 0.96807, R2 0.74619, RMSE 0.97481\n",
      "Epoch [284/500]      | Train: Loss 0.12345, R2 0.96500, RMSE 0.35067                     | Test: Loss 0.93876, R2 0.73217, RMSE 0.95755\n",
      "Epoch [285/500]      | Train: Loss 0.12484, R2 0.96490, RMSE 0.35254                     | Test: Loss 1.06355, R2 0.71095, RMSE 1.00227\n",
      "Epoch [286/500]      | Train: Loss 0.12051, R2 0.96590, RMSE 0.34630                     | Test: Loss 0.91399, R2 0.75095, RMSE 0.94794\n",
      "Epoch [287/500]      | Train: Loss 0.12740, R2 0.96404, RMSE 0.35630                     | Test: Loss 0.91404, R2 0.74747, RMSE 0.94695\n",
      "Epoch [288/500]      | Train: Loss 0.12267, R2 0.96529, RMSE 0.34957                     | Test: Loss 0.94890, R2 0.72989, RMSE 0.96455\n",
      "Epoch [289/500]      | Train: Loss 0.12528, R2 0.96447, RMSE 0.35331                     | Test: Loss 0.93540, R2 0.75422, RMSE 0.95772\n",
      "Epoch [290/500]      | Train: Loss 0.12000, R2 0.96614, RMSE 0.34567                     | Test: Loss 0.89782, R2 0.75044, RMSE 0.93382\n",
      "Epoch [291/500]      | Train: Loss 0.11710, R2 0.96706, RMSE 0.34171                     | Test: Loss 0.88107, R2 0.75083, RMSE 0.92520\n",
      "Epoch [292/500]      | Train: Loss 0.11828, R2 0.96617, RMSE 0.34341                     | Test: Loss 0.90427, R2 0.73585, RMSE 0.94498\n",
      "Epoch [293/500]      | Train: Loss 0.12649, R2 0.96427, RMSE 0.35501                     | Test: Loss 0.92008, R2 0.73511, RMSE 0.94618\n",
      "Epoch [294/500]      | Train: Loss 0.12053, R2 0.96617, RMSE 0.34640                     | Test: Loss 0.91651, R2 0.75227, RMSE 0.94908\n",
      "Epoch [295/500]      | Train: Loss 0.11537, R2 0.96744, RMSE 0.33882                     | Test: Loss 0.89648, R2 0.74235, RMSE 0.94010\n",
      "Epoch [296/500]      | Train: Loss 0.11567, R2 0.96718, RMSE 0.33962                     | Test: Loss 0.96355, R2 0.73071, RMSE 0.97146\n",
      "Epoch [297/500]      | Train: Loss 0.11885, R2 0.96657, RMSE 0.34398                     | Test: Loss 0.91644, R2 0.74121, RMSE 0.95210\n",
      "Epoch [298/500]      | Train: Loss 0.11289, R2 0.96820, RMSE 0.33524                     | Test: Loss 0.87477, R2 0.76231, RMSE 0.92091\n",
      "Epoch [299/500]      | Train: Loss 0.11207, R2 0.96823, RMSE 0.33411                     | Test: Loss 0.93169, R2 0.75507, RMSE 0.96027\n",
      "Epoch [300/500]      | Train: Loss 0.11145, R2 0.96877, RMSE 0.33301                     | Test: Loss 0.92485, R2 0.74700, RMSE 0.95200\n",
      "Epoch [301/500]      | Train: Loss 0.11025, R2 0.96846, RMSE 0.33157                     | Test: Loss 0.87227, R2 0.76096, RMSE 0.91768\n",
      "Epoch [302/500]      | Train: Loss 0.10802, R2 0.96916, RMSE 0.32810                     | Test: Loss 0.87870, R2 0.75910, RMSE 0.92927\n",
      "Epoch [303/500]      | Train: Loss 0.11483, R2 0.96757, RMSE 0.33814                     | Test: Loss 0.91900, R2 0.75266, RMSE 0.94788\n",
      "Epoch [304/500]      | Train: Loss 0.10873, R2 0.96906, RMSE 0.32905                     | Test: Loss 0.93373, R2 0.73829, RMSE 0.96159\n",
      "Epoch [305/500]      | Train: Loss 0.11232, R2 0.96821, RMSE 0.33444                     | Test: Loss 0.90119, R2 0.74830, RMSE 0.93626\n",
      "Epoch [306/500]      | Train: Loss 0.10985, R2 0.96924, RMSE 0.33078                     | Test: Loss 0.91217, R2 0.74914, RMSE 0.95201\n",
      "Epoch [307/500]      | Train: Loss 0.10923, R2 0.96926, RMSE 0.32984                     | Test: Loss 0.90093, R2 0.75549, RMSE 0.93835\n",
      "Epoch [308/500]      | Train: Loss 0.10849, R2 0.96927, RMSE 0.32876                     | Test: Loss 0.91189, R2 0.75040, RMSE 0.94774\n",
      "Epoch [309/500]      | Train: Loss 0.10676, R2 0.96956, RMSE 0.32628                     | Test: Loss 0.89342, R2 0.74681, RMSE 0.92996\n",
      "Epoch [310/500]      | Train: Loss 0.11132, R2 0.96865, RMSE 0.33254                     | Test: Loss 0.89705, R2 0.73689, RMSE 0.93642\n",
      "Epoch [311/500]      | Train: Loss 0.10973, R2 0.96890, RMSE 0.33050                     | Test: Loss 0.93211, R2 0.74230, RMSE 0.95757\n",
      "Epoch [312/500]      | Train: Loss 0.10960, R2 0.96928, RMSE 0.33030                     | Test: Loss 0.99722, R2 0.73228, RMSE 0.98817\n",
      "Epoch [313/500]      | Train: Loss 0.11228, R2 0.96833, RMSE 0.33416                     | Test: Loss 0.93090, R2 0.72385, RMSE 0.95703\n",
      "Epoch [314/500]      | Train: Loss 0.10925, R2 0.96927, RMSE 0.32980                     | Test: Loss 0.96886, R2 0.72380, RMSE 0.96694\n",
      "Epoch [315/500]      | Train: Loss 0.10210, R2 0.97110, RMSE 0.31894                     | Test: Loss 0.96057, R2 0.72984, RMSE 0.97229\n",
      "Epoch [316/500]      | Train: Loss 0.10315, R2 0.97099, RMSE 0.32049                     | Test: Loss 0.89644, R2 0.75286, RMSE 0.93953\n",
      "Epoch [317/500]      | Train: Loss 0.10709, R2 0.96972, RMSE 0.32664                     | Test: Loss 0.90640, R2 0.75845, RMSE 0.94222\n",
      "Epoch [318/500]      | Train: Loss 0.10637, R2 0.96994, RMSE 0.32530                     | Test: Loss 1.15981, R2 0.70750, RMSE 1.02736\n",
      "Epoch [319/500]      | Train: Loss 0.10265, R2 0.97082, RMSE 0.31980                     | Test: Loss 1.10366, R2 0.72251, RMSE 1.02003\n",
      "Epoch [320/500]      | Train: Loss 0.10578, R2 0.97010, RMSE 0.32447                     | Test: Loss 0.91050, R2 0.74975, RMSE 0.94723\n",
      "Epoch [321/500]      | Train: Loss 0.10356, R2 0.97067, RMSE 0.32088                     | Test: Loss 0.93510, R2 0.72744, RMSE 0.95722\n",
      "Epoch [322/500]      | Train: Loss 0.10371, R2 0.97083, RMSE 0.32149                     | Test: Loss 0.88390, R2 0.75825, RMSE 0.92407\n",
      "Epoch [323/500]      | Train: Loss 0.10154, R2 0.97125, RMSE 0.31808                     | Test: Loss 0.87500, R2 0.75976, RMSE 0.92397\n",
      "Epoch [324/500]      | Train: Loss 0.10032, R2 0.97160, RMSE 0.31620                     | Test: Loss 0.88108, R2 0.75440, RMSE 0.92438\n",
      "Epoch [325/500]      | Train: Loss 0.10675, R2 0.97006, RMSE 0.32594                     | Test: Loss 0.93156, R2 0.73900, RMSE 0.95880\n",
      "Epoch [326/500]      | Train: Loss 0.10368, R2 0.97065, RMSE 0.32114                     | Test: Loss 0.90207, R2 0.74931, RMSE 0.94133\n",
      "Epoch [327/500]      | Train: Loss 0.09986, R2 0.97189, RMSE 0.31529                     | Test: Loss 0.89049, R2 0.75301, RMSE 0.93044\n",
      "Epoch [328/500]      | Train: Loss 0.10164, R2 0.97122, RMSE 0.31825                     | Test: Loss 0.91136, R2 0.75486, RMSE 0.94732\n",
      "Epoch [329/500]      | Train: Loss 0.09708, R2 0.97267, RMSE 0.31103                     | Test: Loss 0.93707, R2 0.74254, RMSE 0.96264\n",
      "Epoch [330/500]      | Train: Loss 0.09779, R2 0.97241, RMSE 0.31199                     | Test: Loss 0.89299, R2 0.75387, RMSE 0.93444\n",
      "Epoch [331/500]      | Train: Loss 0.09949, R2 0.97209, RMSE 0.31472                     | Test: Loss 0.87342, R2 0.76108, RMSE 0.92392\n",
      "Epoch [332/500]      | Train: Loss 0.10032, R2 0.97173, RMSE 0.31596                     | Test: Loss 0.90443, R2 0.75221, RMSE 0.93985\n",
      "Epoch [333/500]      | Train: Loss 0.09948, R2 0.97152, RMSE 0.31484                     | Test: Loss 1.11255, R2 0.73668, RMSE 1.01476\n",
      "Epoch [334/500]      | Train: Loss 0.10081, R2 0.97162, RMSE 0.31694                     | Test: Loss 1.00464, R2 0.71659, RMSE 0.98405\n",
      "Epoch [335/500]      | Train: Loss 0.09684, R2 0.97267, RMSE 0.31051                     | Test: Loss 0.88572, R2 0.75257, RMSE 0.92551\n",
      "Epoch [336/500]      | Train: Loss 0.09886, R2 0.97210, RMSE 0.31377                     | Test: Loss 0.89120, R2 0.74937, RMSE 0.93486\n",
      "Epoch [337/500]      | Train: Loss 0.09797, R2 0.97252, RMSE 0.31197                     | Test: Loss 0.89514, R2 0.75297, RMSE 0.94075\n",
      "Epoch [338/500]      | Train: Loss 0.09761, R2 0.97243, RMSE 0.31182                     | Test: Loss 0.91551, R2 0.74768, RMSE 0.95267\n",
      "Epoch [339/500]      | Train: Loss 0.09772, R2 0.97246, RMSE 0.31186                     | Test: Loss 0.92558, R2 0.70534, RMSE 0.95791\n",
      "Epoch [340/500]      | Train: Loss 0.09563, R2 0.97292, RMSE 0.30845                     | Test: Loss 0.88912, R2 0.75672, RMSE 0.93712\n",
      "Epoch [341/500]      | Train: Loss 0.09688, R2 0.97288, RMSE 0.31063                     | Test: Loss 0.89094, R2 0.75454, RMSE 0.93533\n",
      "Epoch [342/500]      | Train: Loss 0.09569, R2 0.97267, RMSE 0.30870                     | Test: Loss 1.09449, R2 0.71923, RMSE 1.01058\n",
      "Epoch [343/500]      | Train: Loss 0.09669, R2 0.97281, RMSE 0.30993                     | Test: Loss 0.96961, R2 0.74353, RMSE 0.97294\n",
      "Epoch [344/500]      | Train: Loss 0.09067, R2 0.97455, RMSE 0.30058                     | Test: Loss 0.93151, R2 0.75042, RMSE 0.95309\n",
      "Epoch [345/500]      | Train: Loss 0.09518, R2 0.97306, RMSE 0.30760                     | Test: Loss 0.88086, R2 0.74651, RMSE 0.92772\n",
      "Epoch [346/500]      | Train: Loss 0.09363, R2 0.97329, RMSE 0.30537                     | Test: Loss 0.93682, R2 0.71282, RMSE 0.96020\n",
      "Epoch [347/500]      | Train: Loss 0.09147, R2 0.97415, RMSE 0.30194                     | Test: Loss 0.88951, R2 0.75711, RMSE 0.93494\n",
      "Epoch [348/500]      | Train: Loss 0.09322, R2 0.97361, RMSE 0.30465                     | Test: Loss 0.90768, R2 0.71555, RMSE 0.94888\n",
      "Epoch [349/500]      | Train: Loss 0.09122, R2 0.97404, RMSE 0.30155                     | Test: Loss 0.96071, R2 0.75333, RMSE 0.96810\n",
      "Epoch [350/500]      | Train: Loss 0.09084, R2 0.97444, RMSE 0.30069                     | Test: Loss 0.92243, R2 0.74192, RMSE 0.95659\n",
      "Epoch [351/500]      | Train: Loss 0.09557, R2 0.97314, RMSE 0.30836                     | Test: Loss 0.88426, R2 0.74345, RMSE 0.93217\n",
      "Epoch [352/500]      | Train: Loss 0.09335, R2 0.97355, RMSE 0.30465                     | Test: Loss 0.89633, R2 0.74932, RMSE 0.93786\n",
      "Epoch [353/500]      | Train: Loss 0.09011, R2 0.97453, RMSE 0.29968                     | Test: Loss 0.95750, R2 0.73303, RMSE 0.97114\n",
      "Epoch [354/500]      | Train: Loss 0.09417, R2 0.97295, RMSE 0.30627                     | Test: Loss 0.89275, R2 0.75698, RMSE 0.93566\n",
      "Epoch [355/500]      | Train: Loss 0.09564, R2 0.97304, RMSE 0.30845                     | Test: Loss 1.29147, R2 0.70093, RMSE 1.05321\n",
      "Epoch [356/500]      | Train: Loss 0.08967, R2 0.97482, RMSE 0.29855                     | Test: Loss 0.90906, R2 0.74537, RMSE 0.94683\n",
      "Epoch [357/500]      | Train: Loss 0.09427, R2 0.97367, RMSE 0.30642                     | Test: Loss 1.14460, R2 0.71962, RMSE 1.02705\n",
      "Epoch [358/500]      | Train: Loss 0.08976, R2 0.97448, RMSE 0.29892                     | Test: Loss 0.87886, R2 0.75649, RMSE 0.92409\n",
      "Epoch [359/500]      | Train: Loss 0.08980, R2 0.97486, RMSE 0.29880                     | Test: Loss 1.27601, R2 0.71398, RMSE 1.04280\n",
      "Epoch [360/500]      | Train: Loss 0.09088, R2 0.97450, RMSE 0.30076                     | Test: Loss 0.90516, R2 0.73643, RMSE 0.94409\n",
      "Epoch [361/500]      | Train: Loss 0.08827, R2 0.97508, RMSE 0.29619                     | Test: Loss 0.89380, R2 0.75004, RMSE 0.94063\n",
      "Epoch [362/500]      | Train: Loss 0.09021, R2 0.97458, RMSE 0.29946                     | Test: Loss 0.90616, R2 0.74374, RMSE 0.94269\n",
      "Epoch [363/500]      | Train: Loss 0.08966, R2 0.97474, RMSE 0.29856                     | Test: Loss 0.92440, R2 0.74138, RMSE 0.95529\n",
      "Epoch [364/500]      | Train: Loss 0.09155, R2 0.97409, RMSE 0.30156                     | Test: Loss 0.89579, R2 0.75981, RMSE 0.93804\n",
      "Epoch [365/500]      | Train: Loss 0.08792, R2 0.97517, RMSE 0.29603                     | Test: Loss 0.90286, R2 0.74065, RMSE 0.94315\n",
      "Epoch [366/500]      | Train: Loss 0.08791, R2 0.97522, RMSE 0.29585                     | Test: Loss 0.89000, R2 0.75660, RMSE 0.93407\n",
      "Epoch [367/500]      | Train: Loss 0.08579, R2 0.97565, RMSE 0.29230                     | Test: Loss 0.91644, R2 0.74995, RMSE 0.95326\n",
      "Epoch [368/500]      | Train: Loss 0.08787, R2 0.97530, RMSE 0.29569                     | Test: Loss 0.89921, R2 0.74676, RMSE 0.93952\n",
      "Epoch [369/500]      | Train: Loss 0.08731, R2 0.97515, RMSE 0.29494                     | Test: Loss 0.85838, R2 0.76172, RMSE 0.91739\n",
      "Epoch [370/500]      | Train: Loss 0.08665, R2 0.97555, RMSE 0.29365                     | Test: Loss 0.90785, R2 0.73508, RMSE 0.94673\n",
      "Epoch [371/500]      | Train: Loss 0.08775, R2 0.97507, RMSE 0.29574                     | Test: Loss 1.23337, R2 0.71849, RMSE 1.03445\n",
      "Epoch [372/500]      | Train: Loss 0.08949, R2 0.97497, RMSE 0.29833                     | Test: Loss 0.90467, R2 0.75443, RMSE 0.94690\n",
      "Epoch [373/500]      | Train: Loss 0.08647, R2 0.97554, RMSE 0.29346                     | Test: Loss 0.90941, R2 0.72978, RMSE 0.94816\n",
      "Epoch [374/500]      | Train: Loss 0.08680, R2 0.97553, RMSE 0.29372                     | Test: Loss 0.88537, R2 0.76589, RMSE 0.92888\n",
      "Epoch [375/500]      | Train: Loss 0.08823, R2 0.97514, RMSE 0.29637                     | Test: Loss 0.90407, R2 0.76058, RMSE 0.93889\n",
      "Epoch [376/500]      | Train: Loss 0.08322, R2 0.97665, RMSE 0.28791                     | Test: Loss 0.97459, R2 0.71637, RMSE 0.97215\n",
      "Epoch [377/500]      | Train: Loss 0.08577, R2 0.97581, RMSE 0.29213                     | Test: Loss 0.88157, R2 0.74984, RMSE 0.93363\n",
      "Epoch [378/500]      | Train: Loss 0.08216, R2 0.97714, RMSE 0.28591                     | Test: Loss 0.87156, R2 0.75924, RMSE 0.92094\n",
      "Epoch [379/500]      | Train: Loss 0.08620, R2 0.97576, RMSE 0.29289                     | Test: Loss 1.14684, R2 0.72442, RMSE 1.02173\n",
      "Epoch [380/500]      | Train: Loss 0.08470, R2 0.97601, RMSE 0.29020                     | Test: Loss 0.91974, R2 0.74949, RMSE 0.95131\n",
      "Epoch [381/500]      | Train: Loss 0.08252, R2 0.97677, RMSE 0.28658                     | Test: Loss 0.87363, R2 0.75996, RMSE 0.92052\n",
      "Epoch [382/500]      | Train: Loss 0.08429, R2 0.97621, RMSE 0.28961                     | Test: Loss 0.93165, R2 0.75401, RMSE 0.95409\n",
      "Epoch [383/500]      | Train: Loss 0.08410, R2 0.97630, RMSE 0.28899                     | Test: Loss 0.88145, R2 0.75220, RMSE 0.92918\n",
      "Epoch [384/500]      | Train: Loss 0.08560, R2 0.97552, RMSE 0.29167                     | Test: Loss 0.94099, R2 0.75027, RMSE 0.95921\n",
      "Epoch [385/500]      | Train: Loss 0.08179, R2 0.97711, RMSE 0.28541                     | Test: Loss 0.90996, R2 0.75863, RMSE 0.94498\n",
      "Epoch [386/500]      | Train: Loss 0.08115, R2 0.97725, RMSE 0.28423                     | Test: Loss 0.88848, R2 0.76357, RMSE 0.93412\n",
      "Epoch [387/500]      | Train: Loss 0.08147, R2 0.97718, RMSE 0.28483                     | Test: Loss 0.95604, R2 0.74472, RMSE 0.96770\n",
      "Epoch [388/500]      | Train: Loss 0.08761, R2 0.97548, RMSE 0.29512                     | Test: Loss 0.91450, R2 0.72563, RMSE 0.94642\n",
      "Epoch [389/500]      | Train: Loss 0.08308, R2 0.97682, RMSE 0.28747                     | Test: Loss 0.92652, R2 0.71542, RMSE 0.95543\n",
      "Epoch [390/500]      | Train: Loss 0.08351, R2 0.97634, RMSE 0.28836                     | Test: Loss 0.87220, R2 0.75772, RMSE 0.92318\n",
      "Epoch [391/500]      | Train: Loss 0.08120, R2 0.97705, RMSE 0.28430                     | Test: Loss 0.88973, R2 0.75026, RMSE 0.93140\n",
      "Epoch [392/500]      | Train: Loss 0.08546, R2 0.97572, RMSE 0.29162                     | Test: Loss 0.87628, R2 0.75024, RMSE 0.92628\n",
      "Epoch [393/500]      | Train: Loss 0.08612, R2 0.97582, RMSE 0.29262                     | Test: Loss 0.88320, R2 0.75449, RMSE 0.93024\n",
      "Epoch [394/500]      | Train: Loss 0.08319, R2 0.97671, RMSE 0.28792                     | Test: Loss 0.87668, R2 0.75521, RMSE 0.92183\n",
      "Epoch [395/500]      | Train: Loss 0.07772, R2 0.97785, RMSE 0.27810                     | Test: Loss 0.87594, R2 0.74365, RMSE 0.92415\n",
      "Epoch [396/500]      | Train: Loss 0.07721, R2 0.97802, RMSE 0.27751                     | Test: Loss 0.93670, R2 0.74659, RMSE 0.95922\n",
      "Epoch [397/500]      | Train: Loss 0.08587, R2 0.97591, RMSE 0.29222                     | Test: Loss 0.87286, R2 0.75121, RMSE 0.92119\n",
      "Epoch [398/500]      | Train: Loss 0.08407, R2 0.97625, RMSE 0.28921                     | Test: Loss 0.89701, R2 0.75662, RMSE 0.93837\n",
      "Epoch [399/500]      | Train: Loss 0.08597, R2 0.97579, RMSE 0.29247                     | Test: Loss 0.98824, R2 0.72247, RMSE 0.97676\n",
      "Epoch [400/500]      | Train: Loss 0.07776, R2 0.97808, RMSE 0.27817                     | Test: Loss 0.86000, R2 0.76826, RMSE 0.90621\n",
      "Epoch [401/500]      | Train: Loss 0.07889, R2 0.97767, RMSE 0.28017                     | Test: Loss 0.86073, R2 0.76819, RMSE 0.91566\n",
      "Epoch [402/500]      | Train: Loss 0.07850, R2 0.97765, RMSE 0.27933                     | Test: Loss 0.90911, R2 0.73746, RMSE 0.94868\n",
      "Epoch [403/500]      | Train: Loss 0.08104, R2 0.97694, RMSE 0.28403                     | Test: Loss 0.86184, R2 0.76121, RMSE 0.91503\n",
      "Epoch [404/500]      | Train: Loss 0.07819, R2 0.97777, RMSE 0.27901                     | Test: Loss 0.88541, R2 0.75064, RMSE 0.93052\n",
      "Epoch [405/500]      | Train: Loss 0.08150, R2 0.97720, RMSE 0.28454                     | Test: Loss 0.89464, R2 0.75842, RMSE 0.93989\n",
      "Epoch [406/500]      | Train: Loss 0.07641, R2 0.97848, RMSE 0.27559                     | Test: Loss 0.97261, R2 0.73444, RMSE 0.96923\n",
      "Epoch [407/500]      | Train: Loss 0.08118, R2 0.97717, RMSE 0.28429                     | Test: Loss 0.87395, R2 0.76149, RMSE 0.92652\n",
      "Epoch [408/500]      | Train: Loss 0.08206, R2 0.97670, RMSE 0.28579                     | Test: Loss 0.86893, R2 0.76006, RMSE 0.92120\n",
      "Epoch [409/500]      | Train: Loss 0.08034, R2 0.97721, RMSE 0.28282                     | Test: Loss 0.93590, R2 0.76210, RMSE 0.95515\n",
      "Epoch [410/500]      | Train: Loss 0.07902, R2 0.97780, RMSE 0.28046                     | Test: Loss 0.91524, R2 0.75505, RMSE 0.95204\n",
      "Epoch [411/500]      | Train: Loss 0.08336, R2 0.97643, RMSE 0.28784                     | Test: Loss 0.87028, R2 0.75816, RMSE 0.91827\n",
      "Epoch [412/500]      | Train: Loss 0.07848, R2 0.97788, RMSE 0.27960                     | Test: Loss 0.88005, R2 0.76364, RMSE 0.92776\n",
      "Epoch [413/500]      | Train: Loss 0.07711, R2 0.97844, RMSE 0.27692                     | Test: Loss 0.94399, R2 0.70245, RMSE 0.96083\n",
      "Epoch [414/500]      | Train: Loss 0.07723, R2 0.97819, RMSE 0.27732                     | Test: Loss 0.95255, R2 0.74780, RMSE 0.96475\n",
      "Epoch [415/500]      | Train: Loss 0.07779, R2 0.97813, RMSE 0.27806                     | Test: Loss 0.86681, R2 0.75321, RMSE 0.92022\n",
      "Epoch [416/500]      | Train: Loss 0.07877, R2 0.97792, RMSE 0.27972                     | Test: Loss 0.93564, R2 0.75573, RMSE 0.96159\n",
      "Epoch [417/500]      | Train: Loss 0.08546, R2 0.97581, RMSE 0.29153                     | Test: Loss 0.95169, R2 0.75209, RMSE 0.96509\n",
      "Epoch [418/500]      | Train: Loss 0.07555, R2 0.97878, RMSE 0.27427                     | Test: Loss 0.88618, R2 0.75790, RMSE 0.93488\n",
      "Epoch [419/500]      | Train: Loss 0.07685, R2 0.97833, RMSE 0.27657                     | Test: Loss 0.92235, R2 0.74658, RMSE 0.95037\n",
      "Epoch [420/500]      | Train: Loss 0.07598, R2 0.97857, RMSE 0.27517                     | Test: Loss 0.89503, R2 0.75602, RMSE 0.93860\n",
      "Epoch [421/500]      | Train: Loss 0.07558, R2 0.97863, RMSE 0.27443                     | Test: Loss 0.88559, R2 0.75941, RMSE 0.92635\n",
      "Epoch [422/500]      | Train: Loss 0.07437, R2 0.97915, RMSE 0.27223                     | Test: Loss 0.94122, R2 0.74820, RMSE 0.95776\n",
      "Epoch [423/500]      | Train: Loss 0.08009, R2 0.97761, RMSE 0.28213                     | Test: Loss 0.87947, R2 0.75313, RMSE 0.92951\n",
      "Epoch [424/500]      | Train: Loss 0.07898, R2 0.97780, RMSE 0.28026                     | Test: Loss 0.89587, R2 0.75894, RMSE 0.93705\n",
      "Epoch [425/500]      | Train: Loss 0.07447, R2 0.97874, RMSE 0.27233                     | Test: Loss 0.87804, R2 0.75631, RMSE 0.92816\n",
      "Epoch [426/500]      | Train: Loss 0.07957, R2 0.97755, RMSE 0.28141                     | Test: Loss 0.88900, R2 0.75053, RMSE 0.93488\n",
      "Epoch [427/500]      | Train: Loss 0.07455, R2 0.97894, RMSE 0.27236                     | Test: Loss 0.91577, R2 0.74673, RMSE 0.94854\n",
      "Epoch [428/500]      | Train: Loss 0.07622, R2 0.97847, RMSE 0.27527                     | Test: Loss 0.88061, R2 0.74029, RMSE 0.93191\n",
      "Epoch [429/500]      | Train: Loss 0.07752, R2 0.97811, RMSE 0.27775                     | Test: Loss 0.88461, R2 0.76158, RMSE 0.93377\n",
      "Epoch [430/500]      | Train: Loss 0.07975, R2 0.97751, RMSE 0.28146                     | Test: Loss 0.89261, R2 0.76336, RMSE 0.93573\n",
      "Epoch [431/500]      | Train: Loss 0.07601, R2 0.97858, RMSE 0.27506                     | Test: Loss 0.88909, R2 0.74231, RMSE 0.93219\n",
      "Epoch [432/500]      | Train: Loss 0.07628, R2 0.97844, RMSE 0.27562                     | Test: Loss 0.90869, R2 0.74908, RMSE 0.94608\n",
      "Epoch [433/500]      | Train: Loss 0.07912, R2 0.97756, RMSE 0.28049                     | Test: Loss 0.91398, R2 0.70319, RMSE 0.94854\n",
      "Epoch [434/500]      | Train: Loss 0.07434, R2 0.97923, RMSE 0.27211                     | Test: Loss 0.89687, R2 0.75529, RMSE 0.93771\n",
      "Epoch [435/500]      | Train: Loss 0.07703, R2 0.97832, RMSE 0.27666                     | Test: Loss 0.84892, R2 0.76868, RMSE 0.90447\n",
      "Epoch [436/500]      | Train: Loss 0.07322, R2 0.97932, RMSE 0.27004                     | Test: Loss 0.85251, R2 0.76294, RMSE 0.91029\n",
      "Epoch [437/500]      | Train: Loss 0.07230, R2 0.97972, RMSE 0.26822                     | Test: Loss 0.91149, R2 0.74657, RMSE 0.94917\n",
      "Epoch [438/500]      | Train: Loss 0.07485, R2 0.97888, RMSE 0.27293                     | Test: Loss 0.91658, R2 0.75750, RMSE 0.94632\n",
      "Epoch [439/500]      | Train: Loss 0.07251, R2 0.97961, RMSE 0.26888                     | Test: Loss 0.88433, R2 0.75253, RMSE 0.93515\n",
      "Epoch [440/500]      | Train: Loss 0.07413, R2 0.97923, RMSE 0.27156                     | Test: Loss 0.86609, R2 0.76499, RMSE 0.91877\n",
      "Epoch [441/500]      | Train: Loss 0.07578, R2 0.97874, RMSE 0.27453                     | Test: Loss 0.87407, R2 0.76310, RMSE 0.91968\n",
      "Epoch [442/500]      | Train: Loss 0.07787, R2 0.97790, RMSE 0.27845                     | Test: Loss 0.94925, R2 0.74469, RMSE 0.96515\n",
      "Epoch [443/500]      | Train: Loss 0.07794, R2 0.97814, RMSE 0.27824                     | Test: Loss 0.87933, R2 0.73511, RMSE 0.93107\n",
      "Epoch [444/500]      | Train: Loss 0.07408, R2 0.97912, RMSE 0.27156                     | Test: Loss 0.90848, R2 0.75351, RMSE 0.94203\n",
      "Epoch [445/500]      | Train: Loss 0.07070, R2 0.98016, RMSE 0.26511                     | Test: Loss 0.89780, R2 0.75267, RMSE 0.93826\n",
      "Epoch [446/500]      | Train: Loss 0.07572, R2 0.97872, RMSE 0.27454                     | Test: Loss 0.90894, R2 0.75297, RMSE 0.94596\n",
      "Epoch [447/500]      | Train: Loss 0.07511, R2 0.97870, RMSE 0.27313                     | Test: Loss 0.87596, R2 0.76257, RMSE 0.92377\n",
      "Epoch [448/500]      | Train: Loss 0.07342, R2 0.97940, RMSE 0.27021                     | Test: Loss 0.86151, R2 0.75752, RMSE 0.91702\n",
      "Epoch [449/500]      | Train: Loss 0.07162, R2 0.97975, RMSE 0.26697                     | Test: Loss 0.89263, R2 0.75013, RMSE 0.93590\n",
      "Epoch [450/500]      | Train: Loss 0.07331, R2 0.97934, RMSE 0.27027                     | Test: Loss 0.85205, R2 0.77297, RMSE 0.90917\n",
      "Epoch [451/500]      | Train: Loss 0.07482, R2 0.97901, RMSE 0.27297                     | Test: Loss 0.85738, R2 0.76950, RMSE 0.91360\n",
      "Epoch [452/500]      | Train: Loss 0.07464, R2 0.97887, RMSE 0.27274                     | Test: Loss 0.85639, R2 0.76336, RMSE 0.91458\n",
      "Epoch [453/500]      | Train: Loss 0.07470, R2 0.97891, RMSE 0.27254                     | Test: Loss 0.89218, R2 0.75850, RMSE 0.93778\n",
      "Epoch [454/500]      | Train: Loss 0.07650, R2 0.97822, RMSE 0.27598                     | Test: Loss 0.88092, R2 0.74497, RMSE 0.92917\n",
      "Epoch [455/500]      | Train: Loss 0.07485, R2 0.97899, RMSE 0.27283                     | Test: Loss 0.85125, R2 0.76248, RMSE 0.90982\n",
      "Epoch [456/500]      | Train: Loss 0.07484, R2 0.97905, RMSE 0.27285                     | Test: Loss 0.86549, R2 0.75963, RMSE 0.91727\n",
      "Epoch [457/500]      | Train: Loss 0.07234, R2 0.97961, RMSE 0.26838                     | Test: Loss 0.85327, R2 0.77097, RMSE 0.91322\n",
      "Epoch [458/500]      | Train: Loss 0.07235, R2 0.97952, RMSE 0.26824                     | Test: Loss 0.89959, R2 0.76163, RMSE 0.93489\n",
      "Epoch [459/500]      | Train: Loss 0.07396, R2 0.97917, RMSE 0.27131                     | Test: Loss 0.93215, R2 0.75811, RMSE 0.95459\n",
      "Epoch [460/500]      | Train: Loss 0.07171, R2 0.97986, RMSE 0.26727                     | Test: Loss 0.88447, R2 0.72578, RMSE 0.93305\n",
      "Epoch [461/500]      | Train: Loss 0.07304, R2 0.97940, RMSE 0.26923                     | Test: Loss 0.95561, R2 0.70016, RMSE 0.96444\n",
      "Epoch [462/500]      | Train: Loss 0.07239, R2 0.97974, RMSE 0.26838                     | Test: Loss 0.84378, R2 0.77274, RMSE 0.90371\n",
      "Epoch [463/500]      | Train: Loss 0.07031, R2 0.98021, RMSE 0.26427                     | Test: Loss 0.89790, R2 0.76426, RMSE 0.93850\n",
      "Epoch [464/500]      | Train: Loss 0.07069, R2 0.98015, RMSE 0.26527                     | Test: Loss 0.89626, R2 0.75040, RMSE 0.93783\n",
      "Epoch [465/500]      | Train: Loss 0.07091, R2 0.98005, RMSE 0.26546                     | Test: Loss 0.93250, R2 0.74613, RMSE 0.95595\n",
      "Epoch [466/500]      | Train: Loss 0.07446, R2 0.97898, RMSE 0.27209                     | Test: Loss 0.94751, R2 0.72254, RMSE 0.96528\n",
      "Epoch [467/500]      | Train: Loss 0.07391, R2 0.97923, RMSE 0.27103                     | Test: Loss 0.95936, R2 0.74728, RMSE 0.96034\n",
      "Epoch [468/500]      | Train: Loss 0.06862, R2 0.98041, RMSE 0.26143                     | Test: Loss 0.86810, R2 0.74377, RMSE 0.92425\n",
      "Epoch [469/500]      | Train: Loss 0.07018, R2 0.98017, RMSE 0.26427                     | Test: Loss 0.84754, R2 0.76376, RMSE 0.90919\n",
      "Epoch [470/500]      | Train: Loss 0.06988, R2 0.98035, RMSE 0.26379                     | Test: Loss 0.86111, R2 0.76615, RMSE 0.92002\n",
      "Epoch [471/500]      | Train: Loss 0.07399, R2 0.97919, RMSE 0.27118                     | Test: Loss 0.86613, R2 0.76781, RMSE 0.92176\n",
      "Epoch [472/500]      | Train: Loss 0.06960, R2 0.98037, RMSE 0.26301                     | Test: Loss 0.87071, R2 0.75728, RMSE 0.92578\n",
      "Epoch [473/500]      | Train: Loss 0.07440, R2 0.97913, RMSE 0.27206                     | Test: Loss 0.85983, R2 0.76146, RMSE 0.91976\n",
      "Epoch [474/500]      | Train: Loss 0.07327, R2 0.97942, RMSE 0.26990                     | Test: Loss 0.86369, R2 0.75908, RMSE 0.91324\n",
      "Epoch [475/500]      | Train: Loss 0.06962, R2 0.98043, RMSE 0.26335                     | Test: Loss 0.87427, R2 0.75744, RMSE 0.92727\n",
      "Epoch [476/500]      | Train: Loss 0.07224, R2 0.97961, RMSE 0.26805                     | Test: Loss 0.87821, R2 0.76024, RMSE 0.92567\n",
      "Epoch [477/500]      | Train: Loss 0.07031, R2 0.98011, RMSE 0.26478                     | Test: Loss 0.84035, R2 0.77276, RMSE 0.90094\n",
      "Epoch [478/500]      | Train: Loss 0.06729, R2 0.98109, RMSE 0.25894                     | Test: Loss 0.86583, R2 0.76206, RMSE 0.91987\n",
      "Epoch [479/500]      | Train: Loss 0.07084, R2 0.98004, RMSE 0.26554                     | Test: Loss 0.89926, R2 0.74586, RMSE 0.94138\n",
      "Epoch [480/500]      | Train: Loss 0.07064, R2 0.98005, RMSE 0.26515                     | Test: Loss 0.84762, R2 0.76593, RMSE 0.90259\n",
      "Epoch [481/500]      | Train: Loss 0.07050, R2 0.98018, RMSE 0.26500                     | Test: Loss 0.86910, R2 0.76875, RMSE 0.92601\n",
      "Epoch [482/500]      | Train: Loss 0.06889, R2 0.98057, RMSE 0.26157                     | Test: Loss 0.85351, R2 0.75607, RMSE 0.91428\n",
      "Epoch [483/500]      | Train: Loss 0.07333, R2 0.97939, RMSE 0.26979                     | Test: Loss 0.86311, R2 0.75902, RMSE 0.91988\n",
      "Epoch [484/500]      | Train: Loss 0.06832, R2 0.98072, RMSE 0.26059                     | Test: Loss 0.85567, R2 0.77016, RMSE 0.91524\n",
      "Epoch [485/500]      | Train: Loss 0.06775, R2 0.98079, RMSE 0.25980                     | Test: Loss 0.85071, R2 0.76991, RMSE 0.91553\n",
      "Epoch [486/500]      | Train: Loss 0.06837, R2 0.98088, RMSE 0.26060                     | Test: Loss 0.89233, R2 0.74918, RMSE 0.93477\n",
      "Epoch [487/500]      | Train: Loss 0.06700, R2 0.98115, RMSE 0.25836                     | Test: Loss 0.86402, R2 0.76544, RMSE 0.92271\n",
      "Epoch [488/500]      | Train: Loss 0.07008, R2 0.98003, RMSE 0.26417                     | Test: Loss 0.87591, R2 0.77044, RMSE 0.92782\n",
      "Epoch [489/500]      | Train: Loss 0.06912, R2 0.98045, RMSE 0.26188                     | Test: Loss 0.99014, R2 0.74087, RMSE 0.97031\n",
      "Epoch [490/500]      | Train: Loss 0.06998, R2 0.98025, RMSE 0.26413                     | Test: Loss 0.88429, R2 0.74151, RMSE 0.93100\n",
      "Epoch [491/500]      | Train: Loss 0.07048, R2 0.98013, RMSE 0.26433                     | Test: Loss 0.87027, R2 0.75817, RMSE 0.92480\n",
      "Epoch [492/500]      | Train: Loss 0.06950, R2 0.98053, RMSE 0.26258                     | Test: Loss 0.87685, R2 0.75842, RMSE 0.92823\n",
      "Epoch [493/500]      | Train: Loss 0.06790, R2 0.98064, RMSE 0.26001                     | Test: Loss 0.86670, R2 0.75499, RMSE 0.92295\n",
      "Epoch [494/500]      | Train: Loss 0.06966, R2 0.98031, RMSE 0.26318                     | Test: Loss 0.93177, R2 0.75083, RMSE 0.95330\n",
      "Epoch [495/500]      | Train: Loss 0.07088, R2 0.98011, RMSE 0.26533                     | Test: Loss 0.85373, R2 0.76162, RMSE 0.91270\n",
      "Epoch [496/500]      | Train: Loss 0.06873, R2 0.98044, RMSE 0.26168                     | Test: Loss 0.86889, R2 0.74986, RMSE 0.92725\n",
      "Epoch [497/500]      | Train: Loss 0.06699, R2 0.98119, RMSE 0.25807                     | Test: Loss 0.87532, R2 0.75412, RMSE 0.92555\n",
      "Epoch [498/500]      | Train: Loss 0.06550, R2 0.98167, RMSE 0.25522                     | Test: Loss 0.85403, R2 0.75592, RMSE 0.91764\n",
      "Epoch [499/500]      | Train: Loss 0.06617, R2 0.98145, RMSE 0.25667                     | Test: Loss 0.90186, R2 0.76086, RMSE 0.93861\n",
      "Epoch [500/500]      | Train: Loss 0.07032, R2 0.98019, RMSE 0.26429                     | Test: Loss 0.86292, R2 0.74032, RMSE 0.92488\n",
      "Best rmse 0.8597500049150907\n",
      "100 epochs of training and evaluation took, 4009.081051557\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class LossFunction(nn.Module):\n",
    "    def __init__(self, num_tasks):\n",
    "        super(LossFunction, self).__init__()\n",
    "        self.num_tasks = num_tasks\n",
    "\n",
    "        self.loss_fns = [nn.MSELoss() for x in range(num_tasks)] \n",
    "\n",
    "    def forward(self, predictions, labels_task1):\n",
    "\n",
    "        #task 1\n",
    "        target = labels_task1\n",
    "        prediction = predictions\n",
    "        loss_fn = self.loss_fns[0]\n",
    "\n",
    "        task_loss = loss_fn(prediction, target)\n",
    "        \n",
    "        return task_loss\n",
    "    \n",
    "model = CATTransformer(rff_on=True,\n",
    "                       sigma=2,\n",
    "                       embed_size=160,\n",
    "                       heads=5,\n",
    "                       forward_expansion=8,\n",
    "                       pre_norm_on=False,\n",
    "                       mlp_scale_classification=8,\n",
    "                       decoder_dropout=0,\n",
    "                       classification_dropout=0.1,\n",
    "                       targets_classes=target_classes,\n",
    "                       n_cont=len(cont_columns),\n",
    "                       embedding_scheme='rff',\n",
    "                       linear_on=True).to(device_in_use)\n",
    "\n",
    "def format_metric(value): # Used to format the metrics output\n",
    "    return f\"{value:.5f}\"\n",
    "\n",
    "loss_functions = LossFunction(1)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr = 0.0001) # Maybe try messing around with optimizers. try other torch optimizers with different configurations.\n",
    "epochs = 500 #Set the number of epochs\n",
    "\n",
    "train_losses = []\n",
    "train_rmse = [] \n",
    "test_losses = []\n",
    "test_rmse = []  \n",
    "\n",
    "\n",
    "#Time will be recorded for all 100 epochs - This means the results will not be comparable to Xgboost but that is ok, we will only compare between transformer models who will also train for 100 epochs\n",
    "start_time = time.process_time()\n",
    "\n",
    "for t in range(epochs):\n",
    "    train_loss, r2_train, rmse_train = train(train_dataloader, model, loss_functions, optimizer, device_in_use=device_in_use)\n",
    "    test_loss, r2_test, rmse_test = test(test_dataloader, model, loss_functions, device_in_use=device_in_use)\n",
    "    train_losses.append(train_loss)\n",
    "    train_rmse.append(rmse_train)\n",
    "    # train_accuracies_2.append(train_accuracy_2)\n",
    "    # train_recalls.append(train_recall) \n",
    "    # train_f1_scores.append(train_f1)\n",
    "    test_losses.append(test_loss)\n",
    "    test_rmse.append(rmse_test)\n",
    "    # test_accuracies_2.append(test_accuracy_2)\n",
    "    # test_recalls.append(test_recall)\n",
    "    # test_f1_scores.append(test_f1)\n",
    "    # Formatting for easier reading\n",
    "    epoch_str = f\"Epoch [{t+1:2}/{epochs}]\"\n",
    "    train_metrics = f\"Train: Loss {format_metric(train_loss)}, R2 {format_metric(r2_train)}, RMSE {format_metric(rmse_train)}\"\n",
    "    test_metrics = f\"Test: Loss {format_metric(test_loss)}, R2 {format_metric(r2_test)}, RMSE {format_metric(rmse_test)}\"\n",
    "    print(f\"{epoch_str:20} | {train_metrics:65} | {test_metrics}\")\n",
    "\n",
    "total_time = time.process_time() - start_time\n",
    "\n",
    "# Plotting the loss curves\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, epochs+1), train_losses, label='Train Loss')\n",
    "plt.plot(range(1, epochs+1), [l for l in test_losses], label='Test Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Test Loss Curve')\n",
    "plt.legend()\n",
    "\n",
    "best_index = test_rmse.index(min(test_rmse))\n",
    "print(f\"Best rmse {test_rmse[best_index]}\")\n",
    "print(f\"100 epochs of training and evaluation took, {total_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainable Normal RFF to each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [ 1/500]       | Train: Loss 4.27183, R2 -0.19991, RMSE 2.03119                    | Test: Loss 3.77196, R2 0.06696, RMSE 1.91859\n",
      "Epoch [ 2/500]       | Train: Loss 2.25029, R2 0.37574, RMSE 1.48036                     | Test: Loss 1.35305, R2 0.63957, RMSE 1.15715\n",
      "Epoch [ 3/500]       | Train: Loss 1.23340, R2 0.65738, RMSE 1.10532                     | Test: Loss 0.98349, R2 0.71469, RMSE 0.98770\n",
      "Epoch [ 4/500]       | Train: Loss 1.07147, R2 0.70161, RMSE 1.03156                     | Test: Loss 0.95717, R2 0.74145, RMSE 0.97040\n",
      "Epoch [ 5/500]       | Train: Loss 1.04676, R2 0.70913, RMSE 1.01833                     | Test: Loss 0.87034, R2 0.75532, RMSE 0.92381\n",
      "Epoch [ 6/500]       | Train: Loss 0.97048, R2 0.72996, RMSE 0.98103                     | Test: Loss 0.88331, R2 0.76479, RMSE 0.93290\n",
      "Epoch [ 7/500]       | Train: Loss 0.95535, R2 0.73241, RMSE 0.97269                     | Test: Loss 0.91279, R2 0.73509, RMSE 0.94849\n",
      "Epoch [ 8/500]       | Train: Loss 0.92933, R2 0.73889, RMSE 0.96030                     | Test: Loss 0.85611, R2 0.77538, RMSE 0.91705\n",
      "Epoch [ 9/500]       | Train: Loss 0.92455, R2 0.74265, RMSE 0.95676                     | Test: Loss 0.84095, R2 0.75921, RMSE 0.91066\n",
      "Epoch [10/500]       | Train: Loss 0.93067, R2 0.73790, RMSE 0.96079                     | Test: Loss 0.92214, R2 0.75220, RMSE 0.95380\n",
      "Epoch [11/500]       | Train: Loss 0.90291, R2 0.74886, RMSE 0.94480                     | Test: Loss 0.84076, R2 0.77257, RMSE 0.91094\n",
      "Epoch [12/500]       | Train: Loss 0.91946, R2 0.74298, RMSE 0.95536                     | Test: Loss 0.82607, R2 0.77213, RMSE 0.90443\n",
      "Epoch [13/500]       | Train: Loss 0.90216, R2 0.74754, RMSE 0.94600                     | Test: Loss 0.88826, R2 0.77254, RMSE 0.93454\n",
      "Epoch [14/500]       | Train: Loss 0.88898, R2 0.74984, RMSE 0.93977                     | Test: Loss 0.83422, R2 0.76677, RMSE 0.90704\n",
      "Epoch [15/500]       | Train: Loss 0.88761, R2 0.75132, RMSE 0.93902                     | Test: Loss 0.83480, R2 0.77653, RMSE 0.91072\n",
      "Epoch [16/500]       | Train: Loss 0.86841, R2 0.75522, RMSE 0.92840                     | Test: Loss 0.81802, R2 0.77476, RMSE 0.89877\n",
      "Epoch [17/500]       | Train: Loss 0.87126, R2 0.75458, RMSE 0.93060                     | Test: Loss 0.88452, R2 0.76288, RMSE 0.93345\n",
      "Epoch [18/500]       | Train: Loss 0.87582, R2 0.75587, RMSE 0.93221                     | Test: Loss 0.81349, R2 0.76590, RMSE 0.89882\n",
      "Epoch [19/500]       | Train: Loss 0.86700, R2 0.75750, RMSE 0.92772                     | Test: Loss 0.81905, R2 0.76706, RMSE 0.89782\n",
      "Epoch [20/500]       | Train: Loss 0.85860, R2 0.76051, RMSE 0.92347                     | Test: Loss 0.80950, R2 0.77857, RMSE 0.89202\n",
      "Epoch [21/500]       | Train: Loss 0.85640, R2 0.76014, RMSE 0.92136                     | Test: Loss 0.78071, R2 0.78749, RMSE 0.87265\n",
      "Epoch [22/500]       | Train: Loss 0.84604, R2 0.76239, RMSE 0.91536                     | Test: Loss 0.80196, R2 0.77434, RMSE 0.89132\n",
      "Epoch [23/500]       | Train: Loss 0.86725, R2 0.75672, RMSE 0.92829                     | Test: Loss 0.85958, R2 0.77212, RMSE 0.92386\n",
      "Epoch [24/500]       | Train: Loss 0.83914, R2 0.76172, RMSE 0.91280                     | Test: Loss 0.78780, R2 0.77974, RMSE 0.88191\n",
      "Epoch [25/500]       | Train: Loss 0.84869, R2 0.76184, RMSE 0.91773                     | Test: Loss 0.83494, R2 0.76739, RMSE 0.90372\n",
      "Epoch [26/500]       | Train: Loss 0.85080, R2 0.76261, RMSE 0.91917                     | Test: Loss 0.80252, R2 0.78708, RMSE 0.88794\n",
      "Epoch [27/500]       | Train: Loss 0.83992, R2 0.76339, RMSE 0.91296                     | Test: Loss 0.80940, R2 0.77345, RMSE 0.89562\n",
      "Epoch [28/500]       | Train: Loss 0.84116, R2 0.76355, RMSE 0.91411                     | Test: Loss 0.82589, R2 0.76250, RMSE 0.90128\n",
      "Epoch [29/500]       | Train: Loss 0.83647, R2 0.76723, RMSE 0.91115                     | Test: Loss 0.80030, R2 0.78338, RMSE 0.88638\n",
      "Epoch [30/500]       | Train: Loss 0.83478, R2 0.76639, RMSE 0.90971                     | Test: Loss 0.81069, R2 0.77450, RMSE 0.89303\n",
      "Epoch [31/500]       | Train: Loss 0.83393, R2 0.76547, RMSE 0.91040                     | Test: Loss 0.79858, R2 0.78816, RMSE 0.88876\n",
      "Epoch [32/500]       | Train: Loss 0.82574, R2 0.76859, RMSE 0.90513                     | Test: Loss 0.78016, R2 0.78431, RMSE 0.87182\n",
      "Epoch [33/500]       | Train: Loss 0.81629, R2 0.77245, RMSE 0.89970                     | Test: Loss 0.77717, R2 0.78568, RMSE 0.87267\n",
      "Epoch [34/500]       | Train: Loss 0.81083, R2 0.77226, RMSE 0.89828                     | Test: Loss 0.80115, R2 0.77352, RMSE 0.88852\n",
      "Epoch [35/500]       | Train: Loss 0.81731, R2 0.76872, RMSE 0.90026                     | Test: Loss 0.76875, R2 0.78712, RMSE 0.86792\n",
      "Epoch [36/500]       | Train: Loss 0.79603, R2 0.77786, RMSE 0.88753                     | Test: Loss 0.77156, R2 0.78027, RMSE 0.87275\n",
      "Epoch [37/500]       | Train: Loss 0.81120, R2 0.77237, RMSE 0.89760                     | Test: Loss 0.78711, R2 0.78401, RMSE 0.88071\n",
      "Epoch [38/500]       | Train: Loss 0.79847, R2 0.77593, RMSE 0.89091                     | Test: Loss 0.82830, R2 0.78785, RMSE 0.90335\n",
      "Epoch [39/500]       | Train: Loss 0.80790, R2 0.77327, RMSE 0.89540                     | Test: Loss 0.80173, R2 0.75389, RMSE 0.89074\n",
      "Epoch [40/500]       | Train: Loss 0.78887, R2 0.77774, RMSE 0.88403                     | Test: Loss 0.87134, R2 0.73700, RMSE 0.91773\n",
      "Epoch [41/500]       | Train: Loss 0.79386, R2 0.77609, RMSE 0.88803                     | Test: Loss 0.78277, R2 0.77068, RMSE 0.88185\n",
      "Epoch [42/500]       | Train: Loss 0.82558, R2 0.76773, RMSE 0.90507                     | Test: Loss 0.88838, R2 0.74342, RMSE 0.93083\n",
      "Epoch [43/500]       | Train: Loss 0.80341, R2 0.77525, RMSE 0.89239                     | Test: Loss 0.79458, R2 0.77805, RMSE 0.88775\n",
      "Epoch [44/500]       | Train: Loss 0.78814, R2 0.77980, RMSE 0.88426                     | Test: Loss 0.79579, R2 0.77696, RMSE 0.87961\n",
      "Epoch [45/500]       | Train: Loss 0.77820, R2 0.78194, RMSE 0.87904                     | Test: Loss 0.79448, R2 0.78432, RMSE 0.88415\n",
      "Epoch [46/500]       | Train: Loss 0.78636, R2 0.78050, RMSE 0.88406                     | Test: Loss 0.77066, R2 0.78807, RMSE 0.86995\n",
      "Epoch [47/500]       | Train: Loss 0.79457, R2 0.77751, RMSE 0.88736                     | Test: Loss 0.80114, R2 0.78301, RMSE 0.88786\n",
      "Epoch [48/500]       | Train: Loss 0.78629, R2 0.78038, RMSE 0.88392                     | Test: Loss 0.79845, R2 0.78269, RMSE 0.88713\n",
      "Epoch [49/500]       | Train: Loss 0.78992, R2 0.77841, RMSE 0.88517                     | Test: Loss 0.88961, R2 0.76897, RMSE 0.92642\n",
      "Epoch [50/500]       | Train: Loss 0.77529, R2 0.78351, RMSE 0.87717                     | Test: Loss 0.82372, R2 0.77885, RMSE 0.90212\n",
      "Epoch [51/500]       | Train: Loss 0.77326, R2 0.78284, RMSE 0.87612                     | Test: Loss 0.81578, R2 0.74518, RMSE 0.89514\n",
      "Epoch [52/500]       | Train: Loss 0.78122, R2 0.77973, RMSE 0.88141                     | Test: Loss 0.76571, R2 0.78539, RMSE 0.86672\n",
      "Epoch [53/500]       | Train: Loss 0.76260, R2 0.78770, RMSE 0.86915                     | Test: Loss 0.85138, R2 0.77598, RMSE 0.91233\n",
      "Epoch [54/500]       | Train: Loss 0.78630, R2 0.77835, RMSE 0.88405                     | Test: Loss 0.81881, R2 0.76936, RMSE 0.89652\n",
      "Epoch [55/500]       | Train: Loss 0.76731, R2 0.78367, RMSE 0.87143                     | Test: Loss 0.77907, R2 0.78356, RMSE 0.87495\n",
      "Epoch [56/500]       | Train: Loss 0.76463, R2 0.78535, RMSE 0.87165                     | Test: Loss 0.79492, R2 0.78129, RMSE 0.88641\n",
      "Epoch [57/500]       | Train: Loss 0.75286, R2 0.78715, RMSE 0.86456                     | Test: Loss 0.86534, R2 0.75901, RMSE 0.92373\n",
      "Epoch [58/500]       | Train: Loss 0.76189, R2 0.78762, RMSE 0.86983                     | Test: Loss 0.81544, R2 0.77451, RMSE 0.89784\n",
      "Epoch [59/500]       | Train: Loss 0.77079, R2 0.78435, RMSE 0.87446                     | Test: Loss 0.81790, R2 0.77109, RMSE 0.89336\n",
      "Epoch [60/500]       | Train: Loss 0.76478, R2 0.78644, RMSE 0.87062                     | Test: Loss 0.77050, R2 0.79085, RMSE 0.87062\n",
      "Epoch [61/500]       | Train: Loss 0.75150, R2 0.79054, RMSE 0.86321                     | Test: Loss 0.77419, R2 0.78391, RMSE 0.87489\n",
      "Epoch [62/500]       | Train: Loss 0.74509, R2 0.79059, RMSE 0.85969                     | Test: Loss 0.77496, R2 0.77800, RMSE 0.87328\n",
      "Epoch [63/500]       | Train: Loss 0.74966, R2 0.79004, RMSE 0.86307                     | Test: Loss 0.88564, R2 0.77685, RMSE 0.92650\n",
      "Epoch [64/500]       | Train: Loss 0.74946, R2 0.78766, RMSE 0.86243                     | Test: Loss 0.77614, R2 0.78833, RMSE 0.87002\n",
      "Epoch [65/500]       | Train: Loss 0.74807, R2 0.79072, RMSE 0.86085                     | Test: Loss 0.99895, R2 0.70351, RMSE 0.95545\n",
      "Epoch [66/500]       | Train: Loss 0.75300, R2 0.79023, RMSE 0.86320                     | Test: Loss 0.80929, R2 0.75299, RMSE 0.89512\n",
      "Epoch [67/500]       | Train: Loss 0.75471, R2 0.78749, RMSE 0.86552                     | Test: Loss 0.76669, R2 0.79309, RMSE 0.86643\n",
      "Epoch [68/500]       | Train: Loss 0.74415, R2 0.79057, RMSE 0.85895                     | Test: Loss 0.73652, R2 0.78262, RMSE 0.84790\n",
      "Epoch [69/500]       | Train: Loss 0.73626, R2 0.79229, RMSE 0.85570                     | Test: Loss 0.77344, R2 0.79082, RMSE 0.87369\n",
      "Epoch [70/500]       | Train: Loss 0.74398, R2 0.79022, RMSE 0.85827                     | Test: Loss 0.82348, R2 0.78796, RMSE 0.90114\n",
      "Epoch [71/500]       | Train: Loss 0.73451, R2 0.79263, RMSE 0.85350                     | Test: Loss 0.79522, R2 0.78874, RMSE 0.88643\n",
      "Epoch [72/500]       | Train: Loss 0.73690, R2 0.79465, RMSE 0.85441                     | Test: Loss 0.77060, R2 0.77385, RMSE 0.86663\n",
      "Epoch [73/500]       | Train: Loss 0.74197, R2 0.79147, RMSE 0.85798                     | Test: Loss 0.80533, R2 0.77441, RMSE 0.88656\n",
      "Epoch [74/500]       | Train: Loss 0.72463, R2 0.79630, RMSE 0.84854                     | Test: Loss 0.77998, R2 0.77443, RMSE 0.87666\n",
      "Epoch [75/500]       | Train: Loss 0.71912, R2 0.79907, RMSE 0.84507                     | Test: Loss 0.75435, R2 0.79406, RMSE 0.86129\n",
      "Epoch [76/500]       | Train: Loss 0.72896, R2 0.79500, RMSE 0.85072                     | Test: Loss 0.73981, R2 0.78673, RMSE 0.84988\n",
      "Epoch [77/500]       | Train: Loss 0.73517, R2 0.79508, RMSE 0.85313                     | Test: Loss 0.84019, R2 0.77445, RMSE 0.91007\n",
      "Epoch [78/500]       | Train: Loss 0.71725, R2 0.80055, RMSE 0.84415                     | Test: Loss 0.77887, R2 0.78997, RMSE 0.87696\n",
      "Epoch [79/500]       | Train: Loss 0.71172, R2 0.80065, RMSE 0.84099                     | Test: Loss 0.75935, R2 0.76457, RMSE 0.86344\n",
      "Epoch [80/500]       | Train: Loss 0.71792, R2 0.79817, RMSE 0.84405                     | Test: Loss 0.93975, R2 0.70398, RMSE 0.93528\n",
      "Epoch [81/500]       | Train: Loss 0.71768, R2 0.79783, RMSE 0.84432                     | Test: Loss 0.77676, R2 0.78334, RMSE 0.87759\n",
      "Epoch [82/500]       | Train: Loss 0.70925, R2 0.80140, RMSE 0.83898                     | Test: Loss 0.80042, R2 0.76917, RMSE 0.88838\n",
      "Epoch [83/500]       | Train: Loss 0.71495, R2 0.80096, RMSE 0.84249                     | Test: Loss 0.82405, R2 0.76528, RMSE 0.90296\n",
      "Epoch [84/500]       | Train: Loss 0.71937, R2 0.79993, RMSE 0.84469                     | Test: Loss 0.78379, R2 0.77390, RMSE 0.87922\n",
      "Epoch [85/500]       | Train: Loss 0.71722, R2 0.79836, RMSE 0.84327                     | Test: Loss 0.77160, R2 0.77998, RMSE 0.87100\n",
      "Epoch [86/500]       | Train: Loss 0.70365, R2 0.80218, RMSE 0.83517                     | Test: Loss 0.74115, R2 0.78384, RMSE 0.85398\n",
      "Epoch [87/500]       | Train: Loss 0.70767, R2 0.80207, RMSE 0.83863                     | Test: Loss 0.78369, R2 0.78393, RMSE 0.87821\n",
      "Epoch [88/500]       | Train: Loss 0.70625, R2 0.80184, RMSE 0.83845                     | Test: Loss 0.75610, R2 0.79103, RMSE 0.86158\n",
      "Epoch [89/500]       | Train: Loss 0.69512, R2 0.80528, RMSE 0.83102                     | Test: Loss 0.78587, R2 0.77766, RMSE 0.88048\n",
      "Epoch [90/500]       | Train: Loss 0.69706, R2 0.80573, RMSE 0.83163                     | Test: Loss 0.74988, R2 0.78973, RMSE 0.85704\n",
      "Epoch [91/500]       | Train: Loss 0.69173, R2 0.80551, RMSE 0.82782                     | Test: Loss 0.83181, R2 0.78101, RMSE 0.90415\n",
      "Epoch [92/500]       | Train: Loss 0.70923, R2 0.79977, RMSE 0.83854                     | Test: Loss 0.85061, R2 0.76826, RMSE 0.91589\n",
      "Epoch [93/500]       | Train: Loss 0.73412, R2 0.79442, RMSE 0.85265                     | Test: Loss 0.74082, R2 0.79795, RMSE 0.84455\n",
      "Epoch [94/500]       | Train: Loss 0.69016, R2 0.80663, RMSE 0.82779                     | Test: Loss 0.82603, R2 0.77797, RMSE 0.90224\n",
      "Epoch [95/500]       | Train: Loss 0.68699, R2 0.80757, RMSE 0.82539                     | Test: Loss 0.76638, R2 0.79822, RMSE 0.86810\n",
      "Epoch [96/500]       | Train: Loss 0.68473, R2 0.80742, RMSE 0.82496                     | Test: Loss 0.73149, R2 0.79064, RMSE 0.84089\n",
      "Epoch [97/500]       | Train: Loss 0.69590, R2 0.80517, RMSE 0.83163                     | Test: Loss 0.75841, R2 0.79868, RMSE 0.86095\n",
      "Epoch [98/500]       | Train: Loss 0.69107, R2 0.80678, RMSE 0.82762                     | Test: Loss 0.76910, R2 0.78468, RMSE 0.87282\n",
      "Epoch [99/500]       | Train: Loss 0.68345, R2 0.80963, RMSE 0.82428                     | Test: Loss 0.77369, R2 0.79602, RMSE 0.87080\n",
      "Epoch [100/500]      | Train: Loss 0.68113, R2 0.80634, RMSE 0.82289                     | Test: Loss 0.73656, R2 0.79997, RMSE 0.84415\n",
      "Epoch [101/500]      | Train: Loss 0.68469, R2 0.80920, RMSE 0.82403                     | Test: Loss 0.75963, R2 0.77984, RMSE 0.85986\n",
      "Epoch [102/500]      | Train: Loss 0.69165, R2 0.80643, RMSE 0.82874                     | Test: Loss 0.82201, R2 0.78408, RMSE 0.89889\n",
      "Epoch [103/500]      | Train: Loss 0.68848, R2 0.80599, RMSE 0.82671                     | Test: Loss 0.76970, R2 0.78829, RMSE 0.87022\n",
      "Epoch [104/500]      | Train: Loss 0.69056, R2 0.80481, RMSE 0.82832                     | Test: Loss 0.81597, R2 0.77699, RMSE 0.89546\n",
      "Epoch [105/500]      | Train: Loss 0.68257, R2 0.80892, RMSE 0.82310                     | Test: Loss 0.77057, R2 0.78360, RMSE 0.86962\n",
      "Epoch [106/500]      | Train: Loss 0.66864, R2 0.81044, RMSE 0.81531                     | Test: Loss 0.82017, R2 0.78406, RMSE 0.89360\n",
      "Epoch [107/500]      | Train: Loss 0.66558, R2 0.81176, RMSE 0.81198                     | Test: Loss 0.82278, R2 0.79451, RMSE 0.89801\n",
      "Epoch [108/500]      | Train: Loss 0.66660, R2 0.81263, RMSE 0.81397                     | Test: Loss 0.77054, R2 0.77748, RMSE 0.86597\n",
      "Epoch [109/500]      | Train: Loss 0.65901, R2 0.81519, RMSE 0.80823                     | Test: Loss 0.81226, R2 0.77142, RMSE 0.89607\n",
      "Epoch [110/500]      | Train: Loss 0.67361, R2 0.81013, RMSE 0.81806                     | Test: Loss 0.76501, R2 0.78677, RMSE 0.86335\n",
      "Epoch [111/500]      | Train: Loss 0.66114, R2 0.81522, RMSE 0.81050                     | Test: Loss 0.81135, R2 0.78129, RMSE 0.89445\n",
      "Epoch [112/500]      | Train: Loss 0.65848, R2 0.81389, RMSE 0.80863                     | Test: Loss 0.79882, R2 0.77881, RMSE 0.87972\n",
      "Epoch [113/500]      | Train: Loss 0.64699, R2 0.81908, RMSE 0.80081                     | Test: Loss 0.77475, R2 0.79207, RMSE 0.86914\n",
      "Epoch [114/500]      | Train: Loss 0.64958, R2 0.81677, RMSE 0.80342                     | Test: Loss 0.78046, R2 0.78285, RMSE 0.87093\n",
      "Epoch [115/500]      | Train: Loss 0.67320, R2 0.81215, RMSE 0.81580                     | Test: Loss 0.79519, R2 0.77796, RMSE 0.87991\n",
      "Epoch [116/500]      | Train: Loss 0.64885, R2 0.81655, RMSE 0.80261                     | Test: Loss 0.99181, R2 0.76051, RMSE 0.95528\n",
      "Epoch [117/500]      | Train: Loss 0.63681, R2 0.82106, RMSE 0.79526                     | Test: Loss 0.74894, R2 0.78219, RMSE 0.85578\n",
      "Epoch [118/500]      | Train: Loss 0.63400, R2 0.82188, RMSE 0.79419                     | Test: Loss 0.75998, R2 0.78750, RMSE 0.86435\n",
      "Epoch [119/500]      | Train: Loss 0.65925, R2 0.81515, RMSE 0.80947                     | Test: Loss 0.77316, R2 0.78550, RMSE 0.86960\n",
      "Epoch [120/500]      | Train: Loss 0.64446, R2 0.81797, RMSE 0.79978                     | Test: Loss 0.82703, R2 0.75960, RMSE 0.90434\n",
      "Epoch [121/500]      | Train: Loss 0.66688, R2 0.81410, RMSE 0.81340                     | Test: Loss 0.77287, R2 0.79217, RMSE 0.87130\n",
      "Epoch [122/500]      | Train: Loss 0.64868, R2 0.81821, RMSE 0.80247                     | Test: Loss 0.79248, R2 0.78223, RMSE 0.87579\n",
      "Epoch [123/500]      | Train: Loss 0.64794, R2 0.81624, RMSE 0.80203                     | Test: Loss 0.81780, R2 0.77051, RMSE 0.89523\n",
      "Epoch [124/500]      | Train: Loss 0.62489, R2 0.82510, RMSE 0.78823                     | Test: Loss 0.77604, R2 0.78126, RMSE 0.87493\n",
      "Epoch [125/500]      | Train: Loss 0.64019, R2 0.81882, RMSE 0.79789                     | Test: Loss 0.83721, R2 0.77413, RMSE 0.90439\n",
      "Epoch [126/500]      | Train: Loss 0.63314, R2 0.82128, RMSE 0.79339                     | Test: Loss 0.78730, R2 0.78421, RMSE 0.87779\n",
      "Epoch [127/500]      | Train: Loss 0.63951, R2 0.82027, RMSE 0.79630                     | Test: Loss 0.77848, R2 0.78895, RMSE 0.87403\n",
      "Epoch [128/500]      | Train: Loss 0.65032, R2 0.81715, RMSE 0.80349                     | Test: Loss 0.79038, R2 0.78357, RMSE 0.88321\n",
      "Epoch [129/500]      | Train: Loss 0.62597, R2 0.82558, RMSE 0.78778                     | Test: Loss 0.75466, R2 0.78917, RMSE 0.86232\n",
      "Epoch [130/500]      | Train: Loss 0.62550, R2 0.82390, RMSE 0.78794                     | Test: Loss 0.84605, R2 0.78179, RMSE 0.90806\n",
      "Epoch [131/500]      | Train: Loss 0.61281, R2 0.82750, RMSE 0.78049                     | Test: Loss 0.82281, R2 0.77935, RMSE 0.89704\n",
      "Epoch [132/500]      | Train: Loss 0.61440, R2 0.82715, RMSE 0.78071                     | Test: Loss 0.78190, R2 0.78677, RMSE 0.87780\n",
      "Epoch [133/500]      | Train: Loss 0.61083, R2 0.82857, RMSE 0.77905                     | Test: Loss 0.81162, R2 0.78287, RMSE 0.88942\n",
      "Epoch [134/500]      | Train: Loss 0.59797, R2 0.83261, RMSE 0.76980                     | Test: Loss 0.78259, R2 0.78452, RMSE 0.87130\n",
      "Epoch [135/500]      | Train: Loss 0.61403, R2 0.82700, RMSE 0.78084                     | Test: Loss 0.76567, R2 0.78392, RMSE 0.86427\n",
      "Epoch [136/500]      | Train: Loss 0.60513, R2 0.82911, RMSE 0.77542                     | Test: Loss 0.80560, R2 0.78956, RMSE 0.88339\n",
      "Epoch [137/500]      | Train: Loss 0.60200, R2 0.83200, RMSE 0.77297                     | Test: Loss 0.76304, R2 0.79387, RMSE 0.85096\n",
      "Epoch [138/500]      | Train: Loss 0.59977, R2 0.83125, RMSE 0.77124                     | Test: Loss 0.78072, R2 0.78125, RMSE 0.87062\n",
      "Epoch [139/500]      | Train: Loss 0.60525, R2 0.83186, RMSE 0.77525                     | Test: Loss 0.74485, R2 0.79026, RMSE 0.84989\n",
      "Epoch [140/500]      | Train: Loss 0.60566, R2 0.82977, RMSE 0.77540                     | Test: Loss 0.94574, R2 0.73380, RMSE 0.93642\n",
      "Epoch [141/500]      | Train: Loss 0.60552, R2 0.82923, RMSE 0.77632                     | Test: Loss 0.81698, R2 0.77396, RMSE 0.89497\n",
      "Epoch [142/500]      | Train: Loss 0.60475, R2 0.83069, RMSE 0.77504                     | Test: Loss 0.76782, R2 0.78191, RMSE 0.86810\n",
      "Epoch [143/500]      | Train: Loss 0.59530, R2 0.83243, RMSE 0.76937                     | Test: Loss 0.81414, R2 0.78972, RMSE 0.88835\n",
      "Epoch [144/500]      | Train: Loss 0.59151, R2 0.83315, RMSE 0.76657                     | Test: Loss 0.76605, R2 0.78345, RMSE 0.86799\n",
      "Epoch [145/500]      | Train: Loss 0.59398, R2 0.83470, RMSE 0.76798                     | Test: Loss 0.76977, R2 0.78006, RMSE 0.86706\n",
      "Epoch [146/500]      | Train: Loss 0.58421, R2 0.83548, RMSE 0.76217                     | Test: Loss 0.76496, R2 0.79073, RMSE 0.85912\n",
      "Epoch [147/500]      | Train: Loss 0.60431, R2 0.83175, RMSE 0.77441                     | Test: Loss 0.75371, R2 0.77825, RMSE 0.85622\n",
      "Epoch [148/500]      | Train: Loss 0.57765, R2 0.83644, RMSE 0.75800                     | Test: Loss 0.74726, R2 0.78876, RMSE 0.85242\n",
      "Epoch [149/500]      | Train: Loss 0.57985, R2 0.83791, RMSE 0.75899                     | Test: Loss 0.80935, R2 0.76786, RMSE 0.89526\n",
      "Epoch [150/500]      | Train: Loss 0.57205, R2 0.83954, RMSE 0.75393                     | Test: Loss 0.75513, R2 0.79546, RMSE 0.85685\n",
      "Epoch [151/500]      | Train: Loss 0.57707, R2 0.83584, RMSE 0.75767                     | Test: Loss 0.75582, R2 0.79433, RMSE 0.85324\n",
      "Epoch [152/500]      | Train: Loss 0.57541, R2 0.83695, RMSE 0.75661                     | Test: Loss 0.80551, R2 0.77730, RMSE 0.88704\n",
      "Epoch [153/500]      | Train: Loss 0.57518, R2 0.83896, RMSE 0.75613                     | Test: Loss 0.86604, R2 0.78165, RMSE 0.90969\n",
      "Epoch [154/500]      | Train: Loss 0.58103, R2 0.83746, RMSE 0.76038                     | Test: Loss 0.76842, R2 0.78222, RMSE 0.86832\n",
      "Epoch [155/500]      | Train: Loss 0.55873, R2 0.84304, RMSE 0.74546                     | Test: Loss 0.80871, R2 0.78123, RMSE 0.88666\n",
      "Epoch [156/500]      | Train: Loss 0.57275, R2 0.83854, RMSE 0.75387                     | Test: Loss 0.79997, R2 0.77487, RMSE 0.88868\n",
      "Epoch [157/500]      | Train: Loss 0.56884, R2 0.84193, RMSE 0.75115                     | Test: Loss 0.78475, R2 0.76531, RMSE 0.87585\n",
      "Epoch [158/500]      | Train: Loss 0.55854, R2 0.84234, RMSE 0.74480                     | Test: Loss 0.79823, R2 0.77870, RMSE 0.88283\n",
      "Epoch [159/500]      | Train: Loss 0.55397, R2 0.84436, RMSE 0.74171                     | Test: Loss 0.79174, R2 0.78367, RMSE 0.88269\n",
      "Epoch [160/500]      | Train: Loss 0.55785, R2 0.84225, RMSE 0.74498                     | Test: Loss 0.78889, R2 0.78548, RMSE 0.87669\n",
      "Epoch [161/500]      | Train: Loss 0.56819, R2 0.84045, RMSE 0.75158                     | Test: Loss 0.75467, R2 0.79596, RMSE 0.85284\n",
      "Epoch [162/500]      | Train: Loss 0.56575, R2 0.83976, RMSE 0.74937                     | Test: Loss 0.76413, R2 0.78143, RMSE 0.85688\n",
      "Epoch [163/500]      | Train: Loss 0.55659, R2 0.84385, RMSE 0.74348                     | Test: Loss 0.77627, R2 0.79003, RMSE 0.86985\n",
      "Epoch [164/500]      | Train: Loss 0.56583, R2 0.84126, RMSE 0.74952                     | Test: Loss 0.79164, R2 0.77721, RMSE 0.87863\n",
      "Epoch [165/500]      | Train: Loss 0.55979, R2 0.84333, RMSE 0.74640                     | Test: Loss 0.83684, R2 0.77489, RMSE 0.89834\n",
      "Epoch [166/500]      | Train: Loss 0.55346, R2 0.84515, RMSE 0.74162                     | Test: Loss 0.75602, R2 0.78995, RMSE 0.85471\n",
      "Epoch [167/500]      | Train: Loss 0.54404, R2 0.84625, RMSE 0.73540                     | Test: Loss 0.79724, R2 0.77267, RMSE 0.88183\n",
      "Epoch [168/500]      | Train: Loss 0.54224, R2 0.84863, RMSE 0.73395                     | Test: Loss 0.82974, R2 0.77680, RMSE 0.90436\n",
      "Epoch [169/500]      | Train: Loss 0.54529, R2 0.84452, RMSE 0.73648                     | Test: Loss 0.79640, R2 0.77902, RMSE 0.88339\n",
      "Epoch [170/500]      | Train: Loss 0.53447, R2 0.85006, RMSE 0.72943                     | Test: Loss 0.78227, R2 0.78358, RMSE 0.86586\n",
      "Epoch [171/500]      | Train: Loss 0.53620, R2 0.85026, RMSE 0.73020                     | Test: Loss 0.77138, R2 0.78812, RMSE 0.86730\n",
      "Epoch [172/500]      | Train: Loss 0.53914, R2 0.84731, RMSE 0.73287                     | Test: Loss 0.77682, R2 0.78913, RMSE 0.85934\n",
      "Epoch [173/500]      | Train: Loss 0.52382, R2 0.85188, RMSE 0.72185                     | Test: Loss 0.78961, R2 0.79272, RMSE 0.87579\n",
      "Epoch [174/500]      | Train: Loss 0.53710, R2 0.84910, RMSE 0.73085                     | Test: Loss 0.78709, R2 0.78492, RMSE 0.87116\n",
      "Epoch [175/500]      | Train: Loss 0.52311, R2 0.85353, RMSE 0.72116                     | Test: Loss 0.86007, R2 0.78687, RMSE 0.90777\n",
      "Epoch [176/500]      | Train: Loss 0.51677, R2 0.85473, RMSE 0.71621                     | Test: Loss 0.80329, R2 0.77036, RMSE 0.88898\n",
      "Epoch [177/500]      | Train: Loss 0.52764, R2 0.85041, RMSE 0.72453                     | Test: Loss 0.77338, R2 0.78694, RMSE 0.86953\n",
      "Epoch [178/500]      | Train: Loss 0.52500, R2 0.85241, RMSE 0.72210                     | Test: Loss 0.77411, R2 0.78716, RMSE 0.86671\n",
      "Epoch [179/500]      | Train: Loss 0.51588, R2 0.85558, RMSE 0.71621                     | Test: Loss 0.85704, R2 0.76283, RMSE 0.91398\n",
      "Epoch [180/500]      | Train: Loss 0.53102, R2 0.85033, RMSE 0.72654                     | Test: Loss 0.79348, R2 0.79048, RMSE 0.87847\n",
      "Epoch [181/500]      | Train: Loss 0.50998, R2 0.85555, RMSE 0.71239                     | Test: Loss 0.88408, R2 0.76220, RMSE 0.92923\n",
      "Epoch [182/500]      | Train: Loss 0.51429, R2 0.85575, RMSE 0.71544                     | Test: Loss 0.78524, R2 0.78484, RMSE 0.87081\n",
      "Epoch [183/500]      | Train: Loss 0.50949, R2 0.85716, RMSE 0.71175                     | Test: Loss 0.77080, R2 0.78035, RMSE 0.86622\n",
      "Epoch [184/500]      | Train: Loss 0.51131, R2 0.85731, RMSE 0.71293                     | Test: Loss 0.75689, R2 0.78897, RMSE 0.84889\n",
      "Epoch [185/500]      | Train: Loss 0.51159, R2 0.85539, RMSE 0.71266                     | Test: Loss 0.79003, R2 0.78882, RMSE 0.87511\n",
      "Epoch [186/500]      | Train: Loss 0.50504, R2 0.85810, RMSE 0.70832                     | Test: Loss 0.78293, R2 0.78363, RMSE 0.86706\n",
      "Epoch [187/500]      | Train: Loss 0.50126, R2 0.85975, RMSE 0.70626                     | Test: Loss 0.81504, R2 0.78400, RMSE 0.89445\n",
      "Epoch [188/500]      | Train: Loss 0.50870, R2 0.85683, RMSE 0.71131                     | Test: Loss 0.87887, R2 0.76444, RMSE 0.92248\n",
      "Epoch [189/500]      | Train: Loss 0.49710, R2 0.86074, RMSE 0.70256                     | Test: Loss 0.80514, R2 0.77076, RMSE 0.88219\n",
      "Epoch [190/500]      | Train: Loss 0.49681, R2 0.86036, RMSE 0.70335                     | Test: Loss 0.78381, R2 0.78546, RMSE 0.87103\n",
      "Epoch [191/500]      | Train: Loss 0.50842, R2 0.85638, RMSE 0.71160                     | Test: Loss 0.79166, R2 0.78568, RMSE 0.87335\n",
      "Epoch [192/500]      | Train: Loss 0.49086, R2 0.86194, RMSE 0.69865                     | Test: Loss 0.85246, R2 0.76957, RMSE 0.90831\n",
      "Epoch [193/500]      | Train: Loss 0.48443, R2 0.86263, RMSE 0.69401                     | Test: Loss 0.79628, R2 0.78179, RMSE 0.87858\n",
      "Epoch [194/500]      | Train: Loss 0.49358, R2 0.86091, RMSE 0.70095                     | Test: Loss 0.91266, R2 0.69657, RMSE 0.93532\n",
      "Epoch [195/500]      | Train: Loss 0.49737, R2 0.86037, RMSE 0.70359                     | Test: Loss 0.79817, R2 0.75373, RMSE 0.88233\n",
      "Epoch [196/500]      | Train: Loss 0.48490, R2 0.86266, RMSE 0.69426                     | Test: Loss 0.78733, R2 0.78150, RMSE 0.87656\n",
      "Epoch [197/500]      | Train: Loss 0.47773, R2 0.86696, RMSE 0.68901                     | Test: Loss 1.14376, R2 0.74419, RMSE 0.98276\n",
      "Epoch [198/500]      | Train: Loss 0.46988, R2 0.86829, RMSE 0.68385                     | Test: Loss 0.79205, R2 0.78415, RMSE 0.87861\n",
      "Epoch [199/500]      | Train: Loss 0.46691, R2 0.86902, RMSE 0.68125                     | Test: Loss 0.77496, R2 0.78110, RMSE 0.87028\n",
      "Epoch [200/500]      | Train: Loss 0.47144, R2 0.86743, RMSE 0.68457                     | Test: Loss 0.80532, R2 0.76444, RMSE 0.89046\n",
      "Epoch [201/500]      | Train: Loss 0.47862, R2 0.86470, RMSE 0.69012                     | Test: Loss 0.79898, R2 0.76271, RMSE 0.88577\n",
      "Epoch [202/500]      | Train: Loss 0.47332, R2 0.86580, RMSE 0.68568                     | Test: Loss 0.80063, R2 0.78261, RMSE 0.88436\n",
      "Epoch [203/500]      | Train: Loss 0.46117, R2 0.86974, RMSE 0.67760                     | Test: Loss 0.84586, R2 0.76642, RMSE 0.90507\n",
      "Epoch [204/500]      | Train: Loss 0.47866, R2 0.86603, RMSE 0.69018                     | Test: Loss 0.77172, R2 0.77354, RMSE 0.86967\n",
      "Epoch [205/500]      | Train: Loss 0.47740, R2 0.86543, RMSE 0.68967                     | Test: Loss 0.81637, R2 0.78442, RMSE 0.89324\n",
      "Epoch [206/500]      | Train: Loss 0.49076, R2 0.85962, RMSE 0.69857                     | Test: Loss 0.82008, R2 0.77080, RMSE 0.89740\n",
      "Epoch [207/500]      | Train: Loss 0.45924, R2 0.87039, RMSE 0.67639                     | Test: Loss 0.80376, R2 0.75919, RMSE 0.88151\n",
      "Epoch [208/500]      | Train: Loss 0.45071, R2 0.87386, RMSE 0.67002                     | Test: Loss 0.80881, R2 0.77832, RMSE 0.88983\n",
      "Epoch [209/500]      | Train: Loss 0.45095, R2 0.87154, RMSE 0.66947                     | Test: Loss 0.81749, R2 0.77088, RMSE 0.89578\n",
      "Epoch [210/500]      | Train: Loss 0.45334, R2 0.87219, RMSE 0.67226                     | Test: Loss 0.77320, R2 0.78753, RMSE 0.86148\n",
      "Epoch [211/500]      | Train: Loss 0.45303, R2 0.87342, RMSE 0.67103                     | Test: Loss 1.09199, R2 0.75147, RMSE 0.97560\n",
      "Epoch [212/500]      | Train: Loss 0.46313, R2 0.86949, RMSE 0.67922                     | Test: Loss 0.85066, R2 0.75282, RMSE 0.91341\n",
      "Epoch [213/500]      | Train: Loss 0.46092, R2 0.86942, RMSE 0.67726                     | Test: Loss 0.85784, R2 0.76807, RMSE 0.91379\n",
      "Epoch [214/500]      | Train: Loss 0.44931, R2 0.87377, RMSE 0.66839                     | Test: Loss 0.92345, R2 0.76684, RMSE 0.94277\n",
      "Epoch [215/500]      | Train: Loss 0.46078, R2 0.87010, RMSE 0.67706                     | Test: Loss 0.79928, R2 0.78566, RMSE 0.88488\n",
      "Epoch [216/500]      | Train: Loss 0.43997, R2 0.87637, RMSE 0.66196                     | Test: Loss 0.81753, R2 0.77782, RMSE 0.89726\n",
      "Epoch [217/500]      | Train: Loss 0.45441, R2 0.87189, RMSE 0.67257                     | Test: Loss 1.16966, R2 0.76373, RMSE 0.99016\n",
      "Epoch [218/500]      | Train: Loss 0.43091, R2 0.87864, RMSE 0.65473                     | Test: Loss 0.82223, R2 0.77514, RMSE 0.89728\n",
      "Epoch [219/500]      | Train: Loss 0.43647, R2 0.87708, RMSE 0.65899                     | Test: Loss 0.82461, R2 0.77073, RMSE 0.89856\n",
      "Epoch [220/500]      | Train: Loss 0.44090, R2 0.87665, RMSE 0.66258                     | Test: Loss 0.79563, R2 0.78421, RMSE 0.87830\n",
      "Epoch [221/500]      | Train: Loss 0.43057, R2 0.87874, RMSE 0.65462                     | Test: Loss 0.79601, R2 0.78272, RMSE 0.87735\n",
      "Epoch [222/500]      | Train: Loss 0.43104, R2 0.87893, RMSE 0.65487                     | Test: Loss 0.79432, R2 0.78460, RMSE 0.88118\n",
      "Epoch [223/500]      | Train: Loss 0.41675, R2 0.88264, RMSE 0.64407                     | Test: Loss 0.80883, R2 0.78178, RMSE 0.89374\n",
      "Epoch [224/500]      | Train: Loss 0.42721, R2 0.87930, RMSE 0.65196                     | Test: Loss 0.82499, R2 0.76384, RMSE 0.89957\n",
      "Epoch [225/500]      | Train: Loss 0.42515, R2 0.88078, RMSE 0.65010                     | Test: Loss 0.80510, R2 0.78075, RMSE 0.88645\n",
      "Epoch [226/500]      | Train: Loss 0.42037, R2 0.88082, RMSE 0.64682                     | Test: Loss 0.79187, R2 0.77523, RMSE 0.87939\n",
      "Epoch [227/500]      | Train: Loss 0.41514, R2 0.88329, RMSE 0.64297                     | Test: Loss 0.83981, R2 0.77778, RMSE 0.90843\n",
      "Epoch [228/500]      | Train: Loss 0.43123, R2 0.87885, RMSE 0.65469                     | Test: Loss 0.84119, R2 0.77411, RMSE 0.90932\n",
      "Epoch [229/500]      | Train: Loss 0.41061, R2 0.88422, RMSE 0.63914                     | Test: Loss 0.79689, R2 0.78207, RMSE 0.87749\n",
      "Epoch [230/500]      | Train: Loss 0.41563, R2 0.88333, RMSE 0.64322                     | Test: Loss 0.80588, R2 0.77872, RMSE 0.88424\n",
      "Epoch [231/500]      | Train: Loss 0.40496, R2 0.88440, RMSE 0.63467                     | Test: Loss 0.88081, R2 0.76207, RMSE 0.92703\n",
      "Epoch [232/500]      | Train: Loss 0.40317, R2 0.88700, RMSE 0.63337                     | Test: Loss 0.83210, R2 0.77166, RMSE 0.90419\n",
      "Epoch [233/500]      | Train: Loss 0.40488, R2 0.88539, RMSE 0.63510                     | Test: Loss 0.79140, R2 0.77448, RMSE 0.88024\n",
      "Epoch [234/500]      | Train: Loss 0.39518, R2 0.88908, RMSE 0.62672                     | Test: Loss 1.04905, R2 0.65752, RMSE 0.96875\n",
      "Epoch [235/500]      | Train: Loss 0.41051, R2 0.88499, RMSE 0.63900                     | Test: Loss 0.79210, R2 0.78493, RMSE 0.87563\n",
      "Epoch [236/500]      | Train: Loss 0.40631, R2 0.88554, RMSE 0.63621                     | Test: Loss 0.82410, R2 0.78082, RMSE 0.90147\n",
      "Epoch [237/500]      | Train: Loss 0.39708, R2 0.88776, RMSE 0.62873                     | Test: Loss 0.84071, R2 0.76669, RMSE 0.90558\n",
      "Epoch [238/500]      | Train: Loss 0.39141, R2 0.89008, RMSE 0.62411                     | Test: Loss 0.78789, R2 0.78714, RMSE 0.87571\n",
      "Epoch [239/500]      | Train: Loss 0.39446, R2 0.88837, RMSE 0.62659                     | Test: Loss 0.84044, R2 0.77746, RMSE 0.91055\n",
      "Epoch [240/500]      | Train: Loss 0.38896, R2 0.88934, RMSE 0.62228                     | Test: Loss 0.80367, R2 0.78162, RMSE 0.88510\n",
      "Epoch [241/500]      | Train: Loss 0.39235, R2 0.88949, RMSE 0.62515                     | Test: Loss 0.85532, R2 0.76772, RMSE 0.91642\n",
      "Epoch [242/500]      | Train: Loss 0.40041, R2 0.88764, RMSE 0.63155                     | Test: Loss 0.86671, R2 0.76121, RMSE 0.92652\n",
      "Epoch [243/500]      | Train: Loss 0.39228, R2 0.88950, RMSE 0.62469                     | Test: Loss 0.78777, R2 0.78103, RMSE 0.87619\n",
      "Epoch [244/500]      | Train: Loss 0.37812, R2 0.89240, RMSE 0.61339                     | Test: Loss 0.92709, R2 0.65257, RMSE 0.94156\n",
      "Epoch [245/500]      | Train: Loss 0.37992, R2 0.89308, RMSE 0.61506                     | Test: Loss 0.80132, R2 0.77834, RMSE 0.88366\n",
      "Epoch [246/500]      | Train: Loss 0.38262, R2 0.89072, RMSE 0.61726                     | Test: Loss 0.85062, R2 0.76758, RMSE 0.90510\n",
      "Epoch [247/500]      | Train: Loss 0.37781, R2 0.89367, RMSE 0.61343                     | Test: Loss 0.79728, R2 0.76718, RMSE 0.87716\n",
      "Epoch [248/500]      | Train: Loss 0.37336, R2 0.89395, RMSE 0.60989                     | Test: Loss 0.79675, R2 0.78531, RMSE 0.88210\n",
      "Epoch [249/500]      | Train: Loss 0.37012, R2 0.89486, RMSE 0.60719                     | Test: Loss 0.78318, R2 0.78051, RMSE 0.87051\n",
      "Epoch [250/500]      | Train: Loss 0.37552, R2 0.89470, RMSE 0.61154                     | Test: Loss 0.79942, R2 0.77639, RMSE 0.88189\n",
      "Epoch [251/500]      | Train: Loss 0.37327, R2 0.89567, RMSE 0.61006                     | Test: Loss 0.86065, R2 0.76815, RMSE 0.91787\n",
      "Epoch [252/500]      | Train: Loss 0.36746, R2 0.89575, RMSE 0.60495                     | Test: Loss 0.79844, R2 0.78161, RMSE 0.88160\n",
      "Epoch [253/500]      | Train: Loss 0.36845, R2 0.89708, RMSE 0.60512                     | Test: Loss 0.84105, R2 0.76758, RMSE 0.90880\n",
      "Epoch [254/500]      | Train: Loss 0.36737, R2 0.89567, RMSE 0.60496                     | Test: Loss 0.80574, R2 0.77287, RMSE 0.88255\n",
      "Epoch [255/500]      | Train: Loss 0.36058, R2 0.89914, RMSE 0.59960                     | Test: Loss 0.85099, R2 0.77718, RMSE 0.91365\n",
      "Epoch [256/500]      | Train: Loss 0.36645, R2 0.89588, RMSE 0.60396                     | Test: Loss 0.81375, R2 0.77668, RMSE 0.89768\n",
      "Epoch [257/500]      | Train: Loss 0.36003, R2 0.89755, RMSE 0.59898                     | Test: Loss 0.89779, R2 0.76148, RMSE 0.93457\n",
      "Epoch [258/500]      | Train: Loss 0.35776, R2 0.89850, RMSE 0.59721                     | Test: Loss 0.81976, R2 0.76114, RMSE 0.89996\n",
      "Epoch [259/500]      | Train: Loss 0.35906, R2 0.89871, RMSE 0.59812                     | Test: Loss 0.82574, R2 0.77062, RMSE 0.90142\n",
      "Epoch [260/500]      | Train: Loss 0.36472, R2 0.89636, RMSE 0.60261                     | Test: Loss 0.81619, R2 0.77500, RMSE 0.89808\n",
      "Epoch [261/500]      | Train: Loss 0.35905, R2 0.89935, RMSE 0.59687                     | Test: Loss 0.83328, R2 0.75899, RMSE 0.90170\n",
      "Epoch [262/500]      | Train: Loss 0.36073, R2 0.89809, RMSE 0.59863                     | Test: Loss 0.87157, R2 0.76677, RMSE 0.92540\n",
      "Epoch [263/500]      | Train: Loss 0.35600, R2 0.90002, RMSE 0.59576                     | Test: Loss 0.91903, R2 0.76765, RMSE 0.93759\n",
      "Epoch [264/500]      | Train: Loss 0.34934, R2 0.90066, RMSE 0.58925                     | Test: Loss 0.90740, R2 0.76122, RMSE 0.93785\n",
      "Epoch [265/500]      | Train: Loss 0.34567, R2 0.90280, RMSE 0.58639                     | Test: Loss 0.86490, R2 0.76391, RMSE 0.91772\n",
      "Epoch [266/500]      | Train: Loss 0.34464, R2 0.90201, RMSE 0.58593                     | Test: Loss 0.83506, R2 0.77170, RMSE 0.90010\n",
      "Epoch [267/500]      | Train: Loss 0.34405, R2 0.90348, RMSE 0.58477                     | Test: Loss 0.87502, R2 0.76083, RMSE 0.92639\n",
      "Epoch [268/500]      | Train: Loss 0.34216, R2 0.90297, RMSE 0.58376                     | Test: Loss 0.84294, R2 0.77626, RMSE 0.91004\n",
      "Epoch [269/500]      | Train: Loss 0.33843, R2 0.90435, RMSE 0.58005                     | Test: Loss 0.79461, R2 0.78131, RMSE 0.88276\n",
      "Epoch [270/500]      | Train: Loss 0.33848, R2 0.90500, RMSE 0.58066                     | Test: Loss 0.81767, R2 0.77144, RMSE 0.89108\n",
      "Epoch [271/500]      | Train: Loss 0.33714, R2 0.90457, RMSE 0.57962                     | Test: Loss 0.81321, R2 0.77736, RMSE 0.88719\n",
      "Epoch [272/500]      | Train: Loss 0.34361, R2 0.90341, RMSE 0.58498                     | Test: Loss 0.80336, R2 0.77949, RMSE 0.88636\n",
      "Epoch [273/500]      | Train: Loss 0.33648, R2 0.90571, RMSE 0.57884                     | Test: Loss 0.82851, R2 0.78414, RMSE 0.90472\n",
      "Epoch [274/500]      | Train: Loss 0.32374, R2 0.90862, RMSE 0.56747                     | Test: Loss 0.83487, R2 0.77241, RMSE 0.90297\n",
      "Epoch [275/500]      | Train: Loss 0.33796, R2 0.90413, RMSE 0.57971                     | Test: Loss 0.84736, R2 0.76620, RMSE 0.90810\n",
      "Epoch [276/500]      | Train: Loss 0.32901, R2 0.90725, RMSE 0.57283                     | Test: Loss 0.82542, R2 0.77327, RMSE 0.90200\n",
      "Epoch [277/500]      | Train: Loss 0.32578, R2 0.90747, RMSE 0.56945                     | Test: Loss 0.84283, R2 0.76247, RMSE 0.91034\n",
      "Epoch [278/500]      | Train: Loss 0.32259, R2 0.90926, RMSE 0.56687                     | Test: Loss 0.82371, R2 0.75459, RMSE 0.89958\n",
      "Epoch [279/500]      | Train: Loss 0.32174, R2 0.90844, RMSE 0.56609                     | Test: Loss 0.79393, R2 0.77581, RMSE 0.88001\n",
      "Epoch [280/500]      | Train: Loss 0.31417, R2 0.91065, RMSE 0.55947                     | Test: Loss 0.86014, R2 0.76540, RMSE 0.91995\n",
      "Epoch [281/500]      | Train: Loss 0.32347, R2 0.90861, RMSE 0.56756                     | Test: Loss 0.98230, R2 0.71784, RMSE 0.96046\n",
      "Epoch [282/500]      | Train: Loss 0.31256, R2 0.91240, RMSE 0.55815                     | Test: Loss 0.93218, R2 0.73481, RMSE 0.95383\n",
      "Epoch [283/500]      | Train: Loss 0.32348, R2 0.90811, RMSE 0.56779                     | Test: Loss 0.85659, R2 0.77472, RMSE 0.91387\n",
      "Epoch [284/500]      | Train: Loss 0.32559, R2 0.90848, RMSE 0.56938                     | Test: Loss 0.85987, R2 0.76500, RMSE 0.91467\n",
      "Epoch [285/500]      | Train: Loss 0.31904, R2 0.91045, RMSE 0.56398                     | Test: Loss 0.87172, R2 0.75197, RMSE 0.92377\n",
      "Epoch [286/500]      | Train: Loss 0.31869, R2 0.91054, RMSE 0.56310                     | Test: Loss 0.85703, R2 0.76049, RMSE 0.92070\n",
      "Epoch [287/500]      | Train: Loss 0.31695, R2 0.91102, RMSE 0.56194                     | Test: Loss 0.82677, R2 0.77820, RMSE 0.89296\n",
      "Epoch [288/500]      | Train: Loss 0.31650, R2 0.91021, RMSE 0.56143                     | Test: Loss 0.87290, R2 0.77677, RMSE 0.92745\n",
      "Epoch [289/500]      | Train: Loss 0.30581, R2 0.91375, RMSE 0.55193                     | Test: Loss 0.84503, R2 0.76720, RMSE 0.91179\n",
      "Epoch [290/500]      | Train: Loss 0.31128, R2 0.91254, RMSE 0.55693                     | Test: Loss 0.95933, R2 0.75166, RMSE 0.95283\n",
      "Epoch [291/500]      | Train: Loss 0.30823, R2 0.91262, RMSE 0.55385                     | Test: Loss 0.84351, R2 0.76205, RMSE 0.91446\n",
      "Epoch [292/500]      | Train: Loss 0.30594, R2 0.91311, RMSE 0.55203                     | Test: Loss 0.83168, R2 0.75653, RMSE 0.90270\n",
      "Epoch [293/500]      | Train: Loss 0.30905, R2 0.91307, RMSE 0.55491                     | Test: Loss 0.85176, R2 0.75452, RMSE 0.91403\n",
      "Epoch [294/500]      | Train: Loss 0.30451, R2 0.91385, RMSE 0.55062                     | Test: Loss 0.85116, R2 0.77414, RMSE 0.91645\n",
      "Epoch [295/500]      | Train: Loss 0.30382, R2 0.91485, RMSE 0.55009                     | Test: Loss 0.85539, R2 0.76692, RMSE 0.91580\n",
      "Epoch [296/500]      | Train: Loss 0.29676, R2 0.91557, RMSE 0.54363                     | Test: Loss 0.82582, R2 0.76606, RMSE 0.89895\n",
      "Epoch [297/500]      | Train: Loss 0.29136, R2 0.91810, RMSE 0.53872                     | Test: Loss 0.81588, R2 0.76402, RMSE 0.89770\n",
      "Epoch [298/500]      | Train: Loss 0.29147, R2 0.91799, RMSE 0.53857                     | Test: Loss 0.84059, R2 0.77050, RMSE 0.90878\n",
      "Epoch [299/500]      | Train: Loss 0.29972, R2 0.91427, RMSE 0.54657                     | Test: Loss 0.92181, R2 0.73390, RMSE 0.94179\n",
      "Epoch [300/500]      | Train: Loss 0.29940, R2 0.91561, RMSE 0.54570                     | Test: Loss 0.80415, R2 0.78080, RMSE 0.88072\n",
      "Epoch [301/500]      | Train: Loss 0.28960, R2 0.91845, RMSE 0.53698                     | Test: Loss 0.86926, R2 0.76064, RMSE 0.92293\n",
      "Epoch [302/500]      | Train: Loss 0.28927, R2 0.91856, RMSE 0.53697                     | Test: Loss 0.94573, R2 0.75177, RMSE 0.95926\n",
      "Epoch [303/500]      | Train: Loss 0.28883, R2 0.91834, RMSE 0.53599                     | Test: Loss 0.82621, R2 0.77600, RMSE 0.89503\n",
      "Epoch [304/500]      | Train: Loss 0.29123, R2 0.91760, RMSE 0.53843                     | Test: Loss 0.85847, R2 0.76621, RMSE 0.92191\n",
      "Epoch [305/500]      | Train: Loss 0.29255, R2 0.91666, RMSE 0.53952                     | Test: Loss 0.83787, R2 0.76806, RMSE 0.91166\n",
      "Epoch [306/500]      | Train: Loss 0.28368, R2 0.91992, RMSE 0.53188                     | Test: Loss 0.87225, R2 0.75675, RMSE 0.92789\n",
      "Epoch [307/500]      | Train: Loss 0.28392, R2 0.92029, RMSE 0.53154                     | Test: Loss 0.82581, R2 0.77744, RMSE 0.89587\n",
      "Epoch [308/500]      | Train: Loss 0.28205, R2 0.91955, RMSE 0.53011                     | Test: Loss 0.83495, R2 0.76757, RMSE 0.90506\n",
      "Epoch [309/500]      | Train: Loss 0.27791, R2 0.92189, RMSE 0.52592                     | Test: Loss 0.86291, R2 0.74298, RMSE 0.92131\n",
      "Epoch [310/500]      | Train: Loss 0.27533, R2 0.92244, RMSE 0.52360                     | Test: Loss 0.83155, R2 0.75989, RMSE 0.90543\n",
      "Epoch [311/500]      | Train: Loss 0.28022, R2 0.92008, RMSE 0.52834                     | Test: Loss 0.93227, R2 0.75387, RMSE 0.95417\n",
      "Epoch [312/500]      | Train: Loss 0.28113, R2 0.92125, RMSE 0.52912                     | Test: Loss 0.85121, R2 0.75646, RMSE 0.91611\n",
      "Epoch [313/500]      | Train: Loss 0.27752, R2 0.92126, RMSE 0.52599                     | Test: Loss 0.82651, R2 0.76586, RMSE 0.90035\n",
      "Epoch [314/500]      | Train: Loss 0.27284, R2 0.92185, RMSE 0.52157                     | Test: Loss 0.98802, R2 0.75945, RMSE 0.97064\n",
      "Epoch [315/500]      | Train: Loss 0.27176, R2 0.92338, RMSE 0.52051                     | Test: Loss 0.86106, R2 0.74940, RMSE 0.92131\n",
      "Epoch [316/500]      | Train: Loss 0.28022, R2 0.92112, RMSE 0.52846                     | Test: Loss 0.81948, R2 0.76935, RMSE 0.89837\n",
      "Epoch [317/500]      | Train: Loss 0.26366, R2 0.92480, RMSE 0.51235                     | Test: Loss 0.81905, R2 0.78278, RMSE 0.88965\n",
      "Epoch [318/500]      | Train: Loss 0.26632, R2 0.92363, RMSE 0.51481                     | Test: Loss 0.79845, R2 0.77875, RMSE 0.88183\n",
      "Epoch [319/500]      | Train: Loss 0.25804, R2 0.92719, RMSE 0.50697                     | Test: Loss 0.88747, R2 0.75717, RMSE 0.93123\n",
      "Epoch [320/500]      | Train: Loss 0.25824, R2 0.92652, RMSE 0.50670                     | Test: Loss 0.89600, R2 0.75579, RMSE 0.94048\n",
      "Epoch [321/500]      | Train: Loss 0.26176, R2 0.92623, RMSE 0.51041                     | Test: Loss 0.84236, R2 0.76468, RMSE 0.91239\n",
      "Epoch [322/500]      | Train: Loss 0.26770, R2 0.92404, RMSE 0.51661                     | Test: Loss 0.86300, R2 0.75737, RMSE 0.92487\n",
      "Epoch [323/500]      | Train: Loss 0.25761, R2 0.92735, RMSE 0.50649                     | Test: Loss 0.83132, R2 0.75835, RMSE 0.90403\n",
      "Epoch [324/500]      | Train: Loss 0.26150, R2 0.92595, RMSE 0.51017                     | Test: Loss 0.84207, R2 0.77038, RMSE 0.90351\n",
      "Epoch [325/500]      | Train: Loss 0.25170, R2 0.92809, RMSE 0.50083                     | Test: Loss 0.83397, R2 0.76237, RMSE 0.90243\n",
      "Epoch [326/500]      | Train: Loss 0.26224, R2 0.92616, RMSE 0.51111                     | Test: Loss 0.85736, R2 0.76680, RMSE 0.91930\n",
      "Epoch [327/500]      | Train: Loss 0.26330, R2 0.92586, RMSE 0.51207                     | Test: Loss 0.87652, R2 0.76172, RMSE 0.92817\n",
      "Epoch [328/500]      | Train: Loss 0.25940, R2 0.92696, RMSE 0.50810                     | Test: Loss 0.82993, R2 0.76374, RMSE 0.89666\n",
      "Epoch [329/500]      | Train: Loss 0.24884, R2 0.92963, RMSE 0.49781                     | Test: Loss 0.85894, R2 0.77483, RMSE 0.91921\n",
      "Epoch [330/500]      | Train: Loss 0.25234, R2 0.92753, RMSE 0.50124                     | Test: Loss 0.82840, R2 0.77817, RMSE 0.89843\n",
      "Epoch [331/500]      | Train: Loss 0.25754, R2 0.92764, RMSE 0.50625                     | Test: Loss 0.85147, R2 0.75645, RMSE 0.91448\n",
      "Epoch [332/500]      | Train: Loss 0.24587, R2 0.93083, RMSE 0.49483                     | Test: Loss 0.89625, R2 0.76198, RMSE 0.93822\n",
      "Epoch [333/500]      | Train: Loss 0.24230, R2 0.93156, RMSE 0.49151                     | Test: Loss 0.86136, R2 0.75766, RMSE 0.92034\n",
      "Epoch [334/500]      | Train: Loss 0.24532, R2 0.93056, RMSE 0.49413                     | Test: Loss 0.90953, R2 0.74973, RMSE 0.94576\n",
      "Epoch [335/500]      | Train: Loss 0.24187, R2 0.93157, RMSE 0.49070                     | Test: Loss 0.83437, R2 0.75955, RMSE 0.90341\n",
      "Epoch [336/500]      | Train: Loss 0.24660, R2 0.93029, RMSE 0.49589                     | Test: Loss 0.85066, R2 0.76845, RMSE 0.91516\n",
      "Epoch [337/500]      | Train: Loss 0.24705, R2 0.92947, RMSE 0.49648                     | Test: Loss 0.84669, R2 0.76294, RMSE 0.91248\n",
      "Epoch [338/500]      | Train: Loss 0.23937, R2 0.93208, RMSE 0.48812                     | Test: Loss 0.83879, R2 0.76334, RMSE 0.90307\n",
      "Epoch [339/500]      | Train: Loss 0.24268, R2 0.93175, RMSE 0.49188                     | Test: Loss 0.89692, R2 0.76163, RMSE 0.93886\n",
      "Epoch [340/500]      | Train: Loss 0.24653, R2 0.92981, RMSE 0.49596                     | Test: Loss 0.91727, R2 0.75941, RMSE 0.94991\n",
      "Epoch [341/500]      | Train: Loss 0.24388, R2 0.93127, RMSE 0.49254                     | Test: Loss 0.87420, R2 0.75005, RMSE 0.93184\n",
      "Epoch [342/500]      | Train: Loss 0.24426, R2 0.93160, RMSE 0.49306                     | Test: Loss 1.00869, R2 0.73439, RMSE 0.97862\n",
      "Epoch [343/500]      | Train: Loss 0.23466, R2 0.93402, RMSE 0.48333                     | Test: Loss 0.87979, R2 0.75887, RMSE 0.93327\n",
      "Epoch [344/500]      | Train: Loss 0.24114, R2 0.93231, RMSE 0.49020                     | Test: Loss 0.94958, R2 0.75086, RMSE 0.96375\n",
      "Epoch [345/500]      | Train: Loss 0.24139, R2 0.93195, RMSE 0.49036                     | Test: Loss 0.85573, R2 0.73718, RMSE 0.91922\n",
      "Epoch [346/500]      | Train: Loss 0.23111, R2 0.93537, RMSE 0.47954                     | Test: Loss 0.93274, R2 0.75485, RMSE 0.94493\n",
      "Epoch [347/500]      | Train: Loss 0.23000, R2 0.93501, RMSE 0.47842                     | Test: Loss 0.84797, R2 0.76083, RMSE 0.91428\n",
      "Epoch [348/500]      | Train: Loss 0.22730, R2 0.93548, RMSE 0.47592                     | Test: Loss 0.86152, R2 0.76569, RMSE 0.92078\n",
      "Epoch [349/500]      | Train: Loss 0.23833, R2 0.93252, RMSE 0.48712                     | Test: Loss 0.85039, R2 0.73271, RMSE 0.91162\n",
      "Epoch [350/500]      | Train: Loss 0.23607, R2 0.93254, RMSE 0.48471                     | Test: Loss 0.87338, R2 0.76192, RMSE 0.92559\n",
      "Epoch [351/500]      | Train: Loss 0.23184, R2 0.93411, RMSE 0.48041                     | Test: Loss 0.84486, R2 0.77428, RMSE 0.90771\n",
      "Epoch [352/500]      | Train: Loss 0.22273, R2 0.93669, RMSE 0.47115                     | Test: Loss 0.85412, R2 0.75832, RMSE 0.91720\n",
      "Epoch [353/500]      | Train: Loss 0.22635, R2 0.93624, RMSE 0.47469                     | Test: Loss 0.83871, R2 0.77116, RMSE 0.90341\n",
      "Epoch [354/500]      | Train: Loss 0.22175, R2 0.93767, RMSE 0.47009                     | Test: Loss 0.89664, R2 0.76906, RMSE 0.93685\n",
      "Epoch [355/500]      | Train: Loss 0.22160, R2 0.93688, RMSE 0.46957                     | Test: Loss 0.86860, R2 0.75542, RMSE 0.92631\n",
      "Epoch [356/500]      | Train: Loss 0.21953, R2 0.93746, RMSE 0.46769                     | Test: Loss 0.91439, R2 0.75325, RMSE 0.94878\n",
      "Epoch [357/500]      | Train: Loss 0.22417, R2 0.93629, RMSE 0.47254                     | Test: Loss 0.91744, R2 0.75689, RMSE 0.94708\n",
      "Epoch [358/500]      | Train: Loss 0.21600, R2 0.93894, RMSE 0.46388                     | Test: Loss 0.86664, R2 0.75566, RMSE 0.92396\n",
      "Epoch [359/500]      | Train: Loss 0.22355, R2 0.93739, RMSE 0.47180                     | Test: Loss 0.85148, R2 0.76434, RMSE 0.91684\n",
      "Epoch [360/500]      | Train: Loss 0.21744, R2 0.93909, RMSE 0.46488                     | Test: Loss 0.90264, R2 0.76100, RMSE 0.94522\n",
      "Epoch [361/500]      | Train: Loss 0.21561, R2 0.93861, RMSE 0.46386                     | Test: Loss 0.84244, R2 0.76353, RMSE 0.90790\n",
      "Epoch [362/500]      | Train: Loss 0.22125, R2 0.93775, RMSE 0.46942                     | Test: Loss 0.85986, R2 0.76571, RMSE 0.91893\n",
      "Epoch [363/500]      | Train: Loss 0.22466, R2 0.93657, RMSE 0.47284                     | Test: Loss 0.87850, R2 0.75584, RMSE 0.92833\n",
      "Epoch [364/500]      | Train: Loss 0.21569, R2 0.93888, RMSE 0.46354                     | Test: Loss 0.83709, R2 0.76819, RMSE 0.90520\n",
      "Epoch [365/500]      | Train: Loss 0.21394, R2 0.93995, RMSE 0.46145                     | Test: Loss 0.86027, R2 0.76872, RMSE 0.92135\n",
      "Epoch [366/500]      | Train: Loss 0.21521, R2 0.93904, RMSE 0.46323                     | Test: Loss 0.87042, R2 0.75278, RMSE 0.92814\n",
      "Epoch [367/500]      | Train: Loss 0.20643, R2 0.94181, RMSE 0.45344                     | Test: Loss 0.89664, R2 0.76057, RMSE 0.93958\n",
      "Epoch [368/500]      | Train: Loss 0.21158, R2 0.94084, RMSE 0.45907                     | Test: Loss 0.85439, R2 0.76796, RMSE 0.91227\n",
      "Epoch [369/500]      | Train: Loss 0.21159, R2 0.94013, RMSE 0.45936                     | Test: Loss 0.84112, R2 0.77158, RMSE 0.90709\n",
      "Epoch [370/500]      | Train: Loss 0.20491, R2 0.94098, RMSE 0.45202                     | Test: Loss 0.86851, R2 0.74916, RMSE 0.92528\n",
      "Epoch [371/500]      | Train: Loss 0.20751, R2 0.94181, RMSE 0.45411                     | Test: Loss 0.87363, R2 0.74466, RMSE 0.92710\n",
      "Epoch [372/500]      | Train: Loss 0.20660, R2 0.94193, RMSE 0.45360                     | Test: Loss 0.87145, R2 0.76737, RMSE 0.92105\n",
      "Epoch [373/500]      | Train: Loss 0.20575, R2 0.94152, RMSE 0.45301                     | Test: Loss 0.89602, R2 0.76173, RMSE 0.94476\n",
      "Epoch [374/500]      | Train: Loss 0.20724, R2 0.94183, RMSE 0.45454                     | Test: Loss 0.85569, R2 0.76667, RMSE 0.91820\n",
      "Epoch [375/500]      | Train: Loss 0.19888, R2 0.94329, RMSE 0.44504                     | Test: Loss 0.90969, R2 0.74980, RMSE 0.94510\n",
      "Epoch [376/500]      | Train: Loss 0.19428, R2 0.94547, RMSE 0.43995                     | Test: Loss 0.86210, R2 0.76579, RMSE 0.92231\n",
      "Epoch [377/500]      | Train: Loss 0.19809, R2 0.94400, RMSE 0.44429                     | Test: Loss 0.86267, R2 0.76509, RMSE 0.91810\n",
      "Epoch [378/500]      | Train: Loss 0.20129, R2 0.94307, RMSE 0.44786                     | Test: Loss 0.91039, R2 0.75892, RMSE 0.94829\n",
      "Epoch [379/500]      | Train: Loss 0.20306, R2 0.94201, RMSE 0.44992                     | Test: Loss 0.86819, R2 0.76199, RMSE 0.92302\n",
      "Epoch [380/500]      | Train: Loss 0.19901, R2 0.94344, RMSE 0.44512                     | Test: Loss 0.82811, R2 0.77196, RMSE 0.89913\n",
      "Epoch [381/500]      | Train: Loss 0.19515, R2 0.94440, RMSE 0.44091                     | Test: Loss 0.86140, R2 0.76362, RMSE 0.91818\n",
      "Epoch [382/500]      | Train: Loss 0.19863, R2 0.94400, RMSE 0.44467                     | Test: Loss 0.90156, R2 0.74137, RMSE 0.94481\n",
      "Epoch [383/500]      | Train: Loss 0.19663, R2 0.94437, RMSE 0.44232                     | Test: Loss 0.86620, R2 0.76689, RMSE 0.92436\n",
      "Epoch [384/500]      | Train: Loss 0.19461, R2 0.94510, RMSE 0.44029                     | Test: Loss 0.93769, R2 0.75576, RMSE 0.95763\n",
      "Epoch [385/500]      | Train: Loss 0.19855, R2 0.94346, RMSE 0.44451                     | Test: Loss 0.96044, R2 0.75771, RMSE 0.96749\n",
      "Epoch [386/500]      | Train: Loss 0.19340, R2 0.94516, RMSE 0.43877                     | Test: Loss 0.92099, R2 0.74076, RMSE 0.95148\n",
      "Epoch [387/500]      | Train: Loss 0.20109, R2 0.94316, RMSE 0.44758                     | Test: Loss 0.85868, R2 0.75055, RMSE 0.91768\n",
      "Epoch [388/500]      | Train: Loss 0.19549, R2 0.94412, RMSE 0.44147                     | Test: Loss 0.87019, R2 0.76139, RMSE 0.92510\n",
      "Epoch [389/500]      | Train: Loss 0.19981, R2 0.94351, RMSE 0.44599                     | Test: Loss 0.89136, R2 0.75283, RMSE 0.93867\n",
      "Epoch [390/500]      | Train: Loss 0.19515, R2 0.94475, RMSE 0.44119                     | Test: Loss 0.85374, R2 0.76769, RMSE 0.91916\n",
      "Epoch [391/500]      | Train: Loss 0.19326, R2 0.94517, RMSE 0.43860                     | Test: Loss 0.86576, R2 0.75155, RMSE 0.91952\n",
      "Epoch [392/500]      | Train: Loss 0.18301, R2 0.94859, RMSE 0.42703                     | Test: Loss 0.85631, R2 0.76229, RMSE 0.91480\n",
      "Epoch [393/500]      | Train: Loss 0.18960, R2 0.94562, RMSE 0.43469                     | Test: Loss 1.15989, R2 0.65443, RMSE 1.01798\n",
      "Epoch [394/500]      | Train: Loss 0.18935, R2 0.94666, RMSE 0.43418                     | Test: Loss 0.82579, R2 0.77080, RMSE 0.89390\n",
      "Epoch [395/500]      | Train: Loss 0.19019, R2 0.94687, RMSE 0.43503                     | Test: Loss 0.86367, R2 0.75701, RMSE 0.91951\n",
      "Epoch [396/500]      | Train: Loss 0.19034, R2 0.94584, RMSE 0.43565                     | Test: Loss 0.84596, R2 0.77153, RMSE 0.90750\n",
      "Epoch [397/500]      | Train: Loss 0.18483, R2 0.94779, RMSE 0.42879                     | Test: Loss 0.85408, R2 0.76475, RMSE 0.91558\n",
      "Epoch [398/500]      | Train: Loss 0.18481, R2 0.94650, RMSE 0.42913                     | Test: Loss 0.90706, R2 0.75692, RMSE 0.94914\n",
      "Epoch [399/500]      | Train: Loss 0.17986, R2 0.94872, RMSE 0.42344                     | Test: Loss 0.87476, R2 0.75712, RMSE 0.93192\n",
      "Epoch [400/500]      | Train: Loss 0.18053, R2 0.94887, RMSE 0.42411                     | Test: Loss 0.87369, R2 0.76128, RMSE 0.92645\n",
      "Epoch [401/500]      | Train: Loss 0.18370, R2 0.94821, RMSE 0.42774                     | Test: Loss 0.84392, R2 0.76114, RMSE 0.90706\n",
      "Epoch [402/500]      | Train: Loss 0.18086, R2 0.94888, RMSE 0.42445                     | Test: Loss 0.87209, R2 0.76819, RMSE 0.92592\n",
      "Epoch [403/500]      | Train: Loss 0.17749, R2 0.94973, RMSE 0.42060                     | Test: Loss 0.84411, R2 0.76648, RMSE 0.91192\n",
      "Epoch [404/500]      | Train: Loss 0.18475, R2 0.94785, RMSE 0.42857                     | Test: Loss 0.90716, R2 0.75487, RMSE 0.94724\n",
      "Epoch [405/500]      | Train: Loss 0.17995, R2 0.94942, RMSE 0.42345                     | Test: Loss 0.84311, R2 0.77250, RMSE 0.91239\n",
      "Epoch [406/500]      | Train: Loss 0.17701, R2 0.94991, RMSE 0.41977                     | Test: Loss 0.85356, R2 0.76596, RMSE 0.91301\n",
      "Epoch [407/500]      | Train: Loss 0.17679, R2 0.94998, RMSE 0.41952                     | Test: Loss 0.85970, R2 0.76328, RMSE 0.91894\n",
      "Epoch [408/500]      | Train: Loss 0.17534, R2 0.95059, RMSE 0.41790                     | Test: Loss 0.85281, R2 0.76729, RMSE 0.91205\n",
      "Epoch [409/500]      | Train: Loss 0.17069, R2 0.95151, RMSE 0.41241                     | Test: Loss 0.86295, R2 0.76123, RMSE 0.92226\n",
      "Epoch [410/500]      | Train: Loss 0.17641, R2 0.94984, RMSE 0.41918                     | Test: Loss 1.17817, R2 0.72467, RMSE 1.02481\n",
      "Epoch [411/500]      | Train: Loss 0.19039, R2 0.94629, RMSE 0.43486                     | Test: Loss 0.86426, R2 0.75840, RMSE 0.92138\n",
      "Epoch [412/500]      | Train: Loss 0.18342, R2 0.94817, RMSE 0.42671                     | Test: Loss 1.14565, R2 0.73327, RMSE 1.01335\n",
      "Epoch [413/500]      | Train: Loss 0.17122, R2 0.95151, RMSE 0.41315                     | Test: Loss 0.83927, R2 0.76565, RMSE 0.90430\n",
      "Epoch [414/500]      | Train: Loss 0.17064, R2 0.95227, RMSE 0.41204                     | Test: Loss 0.88041, R2 0.73759, RMSE 0.92516\n",
      "Epoch [415/500]      | Train: Loss 0.16979, R2 0.95181, RMSE 0.41131                     | Test: Loss 0.85104, R2 0.76664, RMSE 0.90840\n",
      "Epoch [416/500]      | Train: Loss 0.17335, R2 0.95071, RMSE 0.41557                     | Test: Loss 0.86203, R2 0.76696, RMSE 0.92047\n",
      "Epoch [417/500]      | Train: Loss 0.17154, R2 0.95153, RMSE 0.41322                     | Test: Loss 0.86489, R2 0.76373, RMSE 0.91854\n",
      "Epoch [418/500]      | Train: Loss 0.16404, R2 0.95313, RMSE 0.40400                     | Test: Loss 0.88300, R2 0.74935, RMSE 0.93439\n",
      "Epoch [419/500]      | Train: Loss 0.16512, R2 0.95362, RMSE 0.40546                     | Test: Loss 0.89223, R2 0.75291, RMSE 0.94098\n",
      "Epoch [420/500]      | Train: Loss 0.16251, R2 0.95402, RMSE 0.40252                     | Test: Loss 0.87659, R2 0.76361, RMSE 0.92491\n",
      "Epoch [421/500]      | Train: Loss 0.16645, R2 0.95311, RMSE 0.40743                     | Test: Loss 0.85367, R2 0.76446, RMSE 0.90939\n",
      "Epoch [422/500]      | Train: Loss 0.16503, R2 0.95372, RMSE 0.40528                     | Test: Loss 0.85236, R2 0.76734, RMSE 0.91019\n",
      "Epoch [423/500]      | Train: Loss 0.17199, R2 0.95125, RMSE 0.41389                     | Test: Loss 0.98552, R2 0.74529, RMSE 0.97499\n",
      "Epoch [424/500]      | Train: Loss 0.16401, R2 0.95319, RMSE 0.40434                     | Test: Loss 0.84253, R2 0.77359, RMSE 0.91205\n",
      "Epoch [425/500]      | Train: Loss 0.16034, R2 0.95475, RMSE 0.39979                     | Test: Loss 0.87928, R2 0.76052, RMSE 0.93077\n",
      "Epoch [426/500]      | Train: Loss 0.16862, R2 0.95263, RMSE 0.40978                     | Test: Loss 0.89775, R2 0.76853, RMSE 0.93931\n",
      "Epoch [427/500]      | Train: Loss 0.17657, R2 0.95012, RMSE 0.41889                     | Test: Loss 0.86877, R2 0.75125, RMSE 0.92270\n",
      "Epoch [428/500]      | Train: Loss 0.17275, R2 0.95075, RMSE 0.41419                     | Test: Loss 0.89157, R2 0.75094, RMSE 0.93606\n",
      "Epoch [429/500]      | Train: Loss 0.15898, R2 0.95515, RMSE 0.39771                     | Test: Loss 0.86221, R2 0.76065, RMSE 0.92221\n",
      "Epoch [430/500]      | Train: Loss 0.16333, R2 0.95389, RMSE 0.40302                     | Test: Loss 0.85301, R2 0.75156, RMSE 0.91465\n",
      "Epoch [431/500]      | Train: Loss 0.16015, R2 0.95460, RMSE 0.39933                     | Test: Loss 0.85044, R2 0.75792, RMSE 0.91035\n",
      "Epoch [432/500]      | Train: Loss 0.15900, R2 0.95462, RMSE 0.39784                     | Test: Loss 0.88563, R2 0.75046, RMSE 0.93766\n",
      "Epoch [433/500]      | Train: Loss 0.16003, R2 0.95474, RMSE 0.39921                     | Test: Loss 0.87026, R2 0.75778, RMSE 0.92712\n",
      "Epoch [434/500]      | Train: Loss 0.15532, R2 0.95535, RMSE 0.39337                     | Test: Loss 0.86401, R2 0.75732, RMSE 0.92122\n",
      "Epoch [435/500]      | Train: Loss 0.15963, R2 0.95498, RMSE 0.39865                     | Test: Loss 0.86209, R2 0.74838, RMSE 0.92259\n",
      "Epoch [436/500]      | Train: Loss 0.15747, R2 0.95542, RMSE 0.39611                     | Test: Loss 0.89986, R2 0.74330, RMSE 0.94342\n",
      "Epoch [437/500]      | Train: Loss 0.15894, R2 0.95486, RMSE 0.39774                     | Test: Loss 1.02126, R2 0.67593, RMSE 0.98311\n",
      "Epoch [438/500]      | Train: Loss 0.15491, R2 0.95613, RMSE 0.39258                     | Test: Loss 0.88884, R2 0.74727, RMSE 0.93627\n",
      "Epoch [439/500]      | Train: Loss 0.15385, R2 0.95622, RMSE 0.39138                     | Test: Loss 0.86285, R2 0.75886, RMSE 0.91992\n",
      "Epoch [440/500]      | Train: Loss 0.15321, R2 0.95702, RMSE 0.39051                     | Test: Loss 0.87783, R2 0.75591, RMSE 0.93077\n",
      "Epoch [441/500]      | Train: Loss 0.15282, R2 0.95691, RMSE 0.38995                     | Test: Loss 0.86559, R2 0.76487, RMSE 0.92170\n",
      "Epoch [442/500]      | Train: Loss 0.15324, R2 0.95673, RMSE 0.39046                     | Test: Loss 0.87924, R2 0.76059, RMSE 0.93070\n",
      "Epoch [443/500]      | Train: Loss 0.15513, R2 0.95593, RMSE 0.39297                     | Test: Loss 0.86826, R2 0.75579, RMSE 0.91753\n",
      "Epoch [444/500]      | Train: Loss 0.15478, R2 0.95630, RMSE 0.39260                     | Test: Loss 0.88304, R2 0.75393, RMSE 0.93179\n",
      "Epoch [445/500]      | Train: Loss 0.15705, R2 0.95566, RMSE 0.39542                     | Test: Loss 0.92560, R2 0.74837, RMSE 0.95409\n",
      "Epoch [446/500]      | Train: Loss 0.15246, R2 0.95712, RMSE 0.38957                     | Test: Loss 0.88903, R2 0.76422, RMSE 0.93561\n",
      "Epoch [447/500]      | Train: Loss 0.15009, R2 0.95771, RMSE 0.38654                     | Test: Loss 0.86523, R2 0.76043, RMSE 0.92414\n",
      "Epoch [448/500]      | Train: Loss 0.14660, R2 0.95863, RMSE 0.38213                     | Test: Loss 0.87436, R2 0.75789, RMSE 0.92138\n",
      "Epoch [449/500]      | Train: Loss 0.15945, R2 0.95473, RMSE 0.39845                     | Test: Loss 0.87859, R2 0.76011, RMSE 0.93120\n",
      "Epoch [450/500]      | Train: Loss 0.15751, R2 0.95579, RMSE 0.39573                     | Test: Loss 0.90007, R2 0.75523, RMSE 0.93784\n",
      "Epoch [451/500]      | Train: Loss 0.15403, R2 0.95651, RMSE 0.39151                     | Test: Loss 0.89210, R2 0.73959, RMSE 0.93704\n",
      "Epoch [452/500]      | Train: Loss 0.14875, R2 0.95841, RMSE 0.38487                     | Test: Loss 0.87825, R2 0.75262, RMSE 0.92605\n",
      "Epoch [453/500]      | Train: Loss 0.14782, R2 0.95834, RMSE 0.38369                     | Test: Loss 0.88688, R2 0.75420, RMSE 0.93338\n",
      "Epoch [454/500]      | Train: Loss 0.14194, R2 0.95993, RMSE 0.37622                     | Test: Loss 0.90430, R2 0.74099, RMSE 0.94646\n",
      "Epoch [455/500]      | Train: Loss 0.14160, R2 0.95986, RMSE 0.37555                     | Test: Loss 0.85807, R2 0.76210, RMSE 0.92211\n",
      "Epoch [456/500]      | Train: Loss 0.14276, R2 0.95975, RMSE 0.37724                     | Test: Loss 0.88865, R2 0.74795, RMSE 0.93346\n",
      "Epoch [457/500]      | Train: Loss 0.14465, R2 0.95898, RMSE 0.37963                     | Test: Loss 0.87542, R2 0.76062, RMSE 0.92631\n",
      "Epoch [458/500]      | Train: Loss 0.13845, R2 0.96067, RMSE 0.37137                     | Test: Loss 0.89429, R2 0.74969, RMSE 0.93995\n",
      "Epoch [459/500]      | Train: Loss 0.14729, R2 0.95872, RMSE 0.38278                     | Test: Loss 0.87211, R2 0.76347, RMSE 0.92609\n",
      "Epoch [460/500]      | Train: Loss 0.14535, R2 0.95878, RMSE 0.38065                     | Test: Loss 0.87347, R2 0.73483, RMSE 0.92492\n",
      "Epoch [461/500]      | Train: Loss 0.15025, R2 0.95783, RMSE 0.38624                     | Test: Loss 0.93597, R2 0.75529, RMSE 0.95836\n",
      "Epoch [462/500]      | Train: Loss 0.14100, R2 0.96028, RMSE 0.37490                     | Test: Loss 0.92618, R2 0.75146, RMSE 0.95251\n",
      "Epoch [463/500]      | Train: Loss 0.14365, R2 0.95958, RMSE 0.37823                     | Test: Loss 1.12177, R2 0.72149, RMSE 1.00588\n",
      "Epoch [464/500]      | Train: Loss 0.14101, R2 0.96009, RMSE 0.37465                     | Test: Loss 0.86367, R2 0.76750, RMSE 0.92247\n",
      "Epoch [465/500]      | Train: Loss 0.14922, R2 0.95801, RMSE 0.38520                     | Test: Loss 0.88388, R2 0.76092, RMSE 0.92447\n",
      "Epoch [466/500]      | Train: Loss 0.14022, R2 0.96012, RMSE 0.37375                     | Test: Loss 0.86502, R2 0.75796, RMSE 0.92001\n",
      "Epoch [467/500]      | Train: Loss 0.13892, R2 0.96070, RMSE 0.37174                     | Test: Loss 0.91011, R2 0.75318, RMSE 0.94550\n",
      "Epoch [468/500]      | Train: Loss 0.14135, R2 0.96001, RMSE 0.37517                     | Test: Loss 0.86593, R2 0.76180, RMSE 0.92485\n",
      "Epoch [469/500]      | Train: Loss 0.14103, R2 0.96016, RMSE 0.37467                     | Test: Loss 0.88380, R2 0.74781, RMSE 0.92762\n",
      "Epoch [470/500]      | Train: Loss 0.13920, R2 0.96066, RMSE 0.37227                     | Test: Loss 0.86829, R2 0.76149, RMSE 0.92504\n",
      "Epoch [471/500]      | Train: Loss 0.13940, R2 0.96067, RMSE 0.37266                     | Test: Loss 0.89956, R2 0.74853, RMSE 0.94183\n",
      "Epoch [472/500]      | Train: Loss 0.14092, R2 0.96032, RMSE 0.37439                     | Test: Loss 0.85402, R2 0.76224, RMSE 0.91170\n",
      "Epoch [473/500]      | Train: Loss 0.14027, R2 0.96039, RMSE 0.37377                     | Test: Loss 0.89318, R2 0.75387, RMSE 0.94058\n",
      "Epoch [474/500]      | Train: Loss 0.14103, R2 0.96006, RMSE 0.37479                     | Test: Loss 0.84639, R2 0.76144, RMSE 0.91081\n",
      "Epoch [475/500]      | Train: Loss 0.13728, R2 0.96080, RMSE 0.36975                     | Test: Loss 0.87675, R2 0.75786, RMSE 0.93093\n",
      "Epoch [476/500]      | Train: Loss 0.13799, R2 0.96142, RMSE 0.37054                     | Test: Loss 0.89994, R2 0.75420, RMSE 0.94341\n",
      "Epoch [477/500]      | Train: Loss 0.13330, R2 0.96252, RMSE 0.36406                     | Test: Loss 0.90002, R2 0.76078, RMSE 0.94058\n",
      "Epoch [478/500]      | Train: Loss 0.13475, R2 0.96189, RMSE 0.36639                     | Test: Loss 0.89181, R2 0.74823, RMSE 0.93823\n",
      "Epoch [479/500]      | Train: Loss 0.13289, R2 0.96236, RMSE 0.36368                     | Test: Loss 0.85820, R2 0.76297, RMSE 0.92079\n",
      "Epoch [480/500]      | Train: Loss 0.13432, R2 0.96211, RMSE 0.36585                     | Test: Loss 0.88634, R2 0.75853, RMSE 0.93637\n",
      "Epoch [481/500]      | Train: Loss 0.13322, R2 0.96228, RMSE 0.36429                     | Test: Loss 0.86459, R2 0.76520, RMSE 0.91732\n",
      "Epoch [482/500]      | Train: Loss 0.13526, R2 0.96183, RMSE 0.36711                     | Test: Loss 0.85490, R2 0.75244, RMSE 0.91510\n",
      "Epoch [483/500]      | Train: Loss 0.12896, R2 0.96370, RMSE 0.35813                     | Test: Loss 0.92330, R2 0.74800, RMSE 0.95189\n",
      "Epoch [484/500]      | Train: Loss 0.12760, R2 0.96438, RMSE 0.35650                     | Test: Loss 0.86764, R2 0.76159, RMSE 0.92137\n",
      "Epoch [485/500]      | Train: Loss 0.13324, R2 0.96239, RMSE 0.36421                     | Test: Loss 0.93236, R2 0.75879, RMSE 0.95082\n",
      "Epoch [486/500]      | Train: Loss 0.13362, R2 0.96187, RMSE 0.36501                     | Test: Loss 0.88023, R2 0.75556, RMSE 0.93299\n",
      "Epoch [487/500]      | Train: Loss 0.13650, R2 0.96188, RMSE 0.36857                     | Test: Loss 0.89411, R2 0.76018, RMSE 0.93982\n",
      "Epoch [488/500]      | Train: Loss 0.13492, R2 0.96197, RMSE 0.36630                     | Test: Loss 0.89427, R2 0.74966, RMSE 0.93982\n",
      "Epoch [489/500]      | Train: Loss 0.13617, R2 0.96145, RMSE 0.36825                     | Test: Loss 0.88388, R2 0.76019, RMSE 0.93560\n",
      "Epoch [490/500]      | Train: Loss 0.12775, R2 0.96412, RMSE 0.35668                     | Test: Loss 0.86748, R2 0.76311, RMSE 0.92254\n",
      "Epoch [491/500]      | Train: Loss 0.13863, R2 0.96088, RMSE 0.37093                     | Test: Loss 0.86184, R2 0.76103, RMSE 0.92161\n",
      "Epoch [492/500]      | Train: Loss 0.13451, R2 0.96237, RMSE 0.36581                     | Test: Loss 1.14524, R2 0.52302, RMSE 1.01819\n",
      "Epoch [493/500]      | Train: Loss 0.13291, R2 0.96203, RMSE 0.36383                     | Test: Loss 0.89617, R2 0.75769, RMSE 0.93933\n",
      "Epoch [494/500]      | Train: Loss 0.13534, R2 0.96217, RMSE 0.36712                     | Test: Loss 0.90342, R2 0.75299, RMSE 0.94023\n",
      "Epoch [495/500]      | Train: Loss 0.12466, R2 0.96476, RMSE 0.35248                     | Test: Loss 0.87524, R2 0.75472, RMSE 0.92868\n",
      "Epoch [496/500]      | Train: Loss 0.12420, R2 0.96492, RMSE 0.35146                     | Test: Loss 0.89793, R2 0.75758, RMSE 0.93969\n",
      "Epoch [497/500]      | Train: Loss 0.12843, R2 0.96380, RMSE 0.35781                     | Test: Loss 0.95729, R2 0.75058, RMSE 0.96704\n",
      "Epoch [498/500]      | Train: Loss 0.12312, R2 0.96493, RMSE 0.34995                     | Test: Loss 0.96811, R2 0.71993, RMSE 0.97359\n",
      "Epoch [499/500]      | Train: Loss 0.12570, R2 0.96462, RMSE 0.35356                     | Test: Loss 0.85143, R2 0.76296, RMSE 0.91456\n",
      "Epoch [500/500]      | Train: Loss 0.12273, R2 0.96554, RMSE 0.34965                     | Test: Loss 0.87712, R2 0.76351, RMSE 0.93379\n",
      "Best rmse 0.840886441560892\n",
      "500 epochs of training and evaluation took, 4293.201329894\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class LossFunction(nn.Module):\n",
    "    def __init__(self, num_tasks):\n",
    "        super(LossFunction, self).__init__()\n",
    "        self.num_tasks = num_tasks\n",
    "\n",
    "        self.loss_fns = [nn.MSELoss() for x in range(num_tasks)] \n",
    "\n",
    "    def forward(self, predictions, labels_task1):\n",
    "\n",
    "        #task 1\n",
    "        target = labels_task1\n",
    "        prediction = predictions\n",
    "        loss_fn = self.loss_fns[0]\n",
    "\n",
    "        task_loss = loss_fn(prediction, target)\n",
    "        \n",
    "        return task_loss\n",
    "    \n",
    "model = CATTransformer(rff_on=True,\n",
    "                       sigma=2,\n",
    "                       embed_size=160,\n",
    "                       heads=5,\n",
    "                       forward_expansion=8,\n",
    "                       pre_norm_on=False,\n",
    "                       mlp_scale_classification=8,\n",
    "                    \n",
    "                       decoder_dropout=0,\n",
    "                       classification_dropout=0.1,\n",
    "                       targets_classes=target_classes,\n",
    "                       n_cont=len(cont_columns),\n",
    "                     \n",
    "                       embedding_scheme='normal_periodic',\n",
    "                       trainable=True,\n",
    "                       linear_on=True).to(device_in_use)\n",
    "\n",
    "def format_metric(value): # Used to format the metrics output\n",
    "    return f\"{value:.5f}\"\n",
    "\n",
    "loss_functions = LossFunction(1)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr = 0.0001) # Maybe try messing around with optimizers. try other torch optimizers with different configurations.\n",
    "epochs = 500 #Set the number of epochs\n",
    "\n",
    "train_losses = []\n",
    "train_rmse = [] \n",
    "test_losses = []\n",
    "test_rmse = []  \n",
    "\n",
    "\n",
    "#Time will be recorded for all 100 epochs - This means the results will not be comparable to Xgboost but that is ok, we will only compare between transformer models who will also train for 100 epochs\n",
    "start_time = time.process_time()\n",
    "\n",
    "for t in range(epochs):\n",
    "    train_loss, r2_train, rmse_train = train(train_dataloader, model, loss_functions, optimizer, device_in_use=device_in_use)\n",
    "    test_loss, r2_test, rmse_test = test(test_dataloader, model, loss_functions, device_in_use=device_in_use)\n",
    "    train_losses.append(train_loss)\n",
    "    train_rmse.append(rmse_train)\n",
    "    # train_accuracies_2.append(train_accuracy_2)\n",
    "    # train_recalls.append(train_recall) \n",
    "    # train_f1_scores.append(train_f1)\n",
    "    test_losses.append(test_loss)\n",
    "    test_rmse.append(rmse_test)\n",
    "    # test_accuracies_2.append(test_accuracy_2)\n",
    "    # test_recalls.append(test_recall)\n",
    "    # test_f1_scores.append(test_f1)\n",
    "    # Formatting for easier reading\n",
    "    epoch_str = f\"Epoch [{t+1:2}/{epochs}]\"\n",
    "    train_metrics = f\"Train: Loss {format_metric(train_loss)}, R2 {format_metric(r2_train)}, RMSE {format_metric(rmse_train)}\"\n",
    "    test_metrics = f\"Test: Loss {format_metric(test_loss)}, R2 {format_metric(r2_test)}, RMSE {format_metric(rmse_test)}\"\n",
    "    print(f\"{epoch_str:20} | {train_metrics:65} | {test_metrics}\")\n",
    "\n",
    "total_time = time.process_time() - start_time\n",
    "\n",
    "# Plotting the loss curves\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, epochs+1), train_losses, label='Train Loss')\n",
    "plt.plot(range(1, epochs+1), [l for l in test_losses], label='Test Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Test Loss Curve')\n",
    "plt.legend()\n",
    "\n",
    "best_index = test_rmse.index(min(test_rmse))\n",
    "print(f\"Best rmse {test_rmse[best_index]}\")\n",
    "print(f\"500 epochs of training and evaluation took, {total_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainable Log-Linear RFF to each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [ 1/500]       | Train: Loss 4.35443, R2 -0.23209, RMSE 2.05037                    | Test: Loss 3.37220, R2 0.06031, RMSE 1.82614\n",
      "Epoch [ 2/500]       | Train: Loss 2.32185, R2 0.34885, RMSE 1.50168                     | Test: Loss 1.31692, R2 0.63307, RMSE 1.13827\n",
      "Epoch [ 3/500]       | Train: Loss 1.35995, R2 0.62208, RMSE 1.15915                     | Test: Loss 1.12094, R2 0.69188, RMSE 1.05168\n",
      "Epoch [ 4/500]       | Train: Loss 1.30032, R2 0.63755, RMSE 1.13367                     | Test: Loss 1.08198, R2 0.69993, RMSE 1.03442\n",
      "Epoch [ 5/500]       | Train: Loss 1.18831, R2 0.66907, RMSE 1.08376                     | Test: Loss 1.00995, R2 0.72547, RMSE 1.00111\n",
      "Epoch [ 6/500]       | Train: Loss 1.09686, R2 0.69482, RMSE 1.04319                     | Test: Loss 0.94033, R2 0.74072, RMSE 0.96346\n",
      "Epoch [ 7/500]       | Train: Loss 1.04560, R2 0.70843, RMSE 1.01796                     | Test: Loss 0.88818, R2 0.75363, RMSE 0.93223\n",
      "Epoch [ 8/500]       | Train: Loss 1.05859, R2 0.70341, RMSE 1.02602                     | Test: Loss 1.01860, R2 0.71985, RMSE 1.00440\n",
      "Epoch [ 9/500]       | Train: Loss 1.03258, R2 0.71255, RMSE 1.01137                     | Test: Loss 0.89151, R2 0.76432, RMSE 0.94147\n",
      "Epoch [10/500]       | Train: Loss 0.99657, R2 0.72015, RMSE 0.99242                     | Test: Loss 0.85279, R2 0.75843, RMSE 0.91838\n",
      "Epoch [11/500]       | Train: Loss 0.97299, R2 0.72765, RMSE 0.98143                     | Test: Loss 0.86079, R2 0.76736, RMSE 0.92334\n",
      "Epoch [12/500]       | Train: Loss 0.97457, R2 0.72674, RMSE 0.98160                     | Test: Loss 0.84433, R2 0.77346, RMSE 0.91236\n",
      "Epoch [13/500]       | Train: Loss 0.96046, R2 0.73286, RMSE 0.97522                     | Test: Loss 0.82786, R2 0.76949, RMSE 0.90003\n",
      "Epoch [14/500]       | Train: Loss 0.97153, R2 0.73000, RMSE 0.98103                     | Test: Loss 0.88172, R2 0.76095, RMSE 0.93186\n",
      "Epoch [15/500]       | Train: Loss 0.96200, R2 0.73270, RMSE 0.97644                     | Test: Loss 0.88654, R2 0.74984, RMSE 0.93466\n",
      "Epoch [16/500]       | Train: Loss 0.95245, R2 0.73560, RMSE 0.97148                     | Test: Loss 0.83945, R2 0.76927, RMSE 0.90893\n",
      "Epoch [17/500]       | Train: Loss 0.94937, R2 0.73616, RMSE 0.97019                     | Test: Loss 0.83607, R2 0.76394, RMSE 0.91002\n",
      "Epoch [18/500]       | Train: Loss 0.92199, R2 0.74129, RMSE 0.95583                     | Test: Loss 0.89411, R2 0.75361, RMSE 0.94030\n",
      "Epoch [19/500]       | Train: Loss 0.94856, R2 0.73506, RMSE 0.96866                     | Test: Loss 0.86708, R2 0.76879, RMSE 0.92768\n",
      "Epoch [20/500]       | Train: Loss 0.94463, R2 0.73637, RMSE 0.96655                     | Test: Loss 0.82365, R2 0.76251, RMSE 0.89850\n",
      "Epoch [21/500]       | Train: Loss 0.93614, R2 0.74014, RMSE 0.96092                     | Test: Loss 0.85667, R2 0.76102, RMSE 0.92046\n",
      "Epoch [22/500]       | Train: Loss 0.92308, R2 0.74095, RMSE 0.95652                     | Test: Loss 0.80565, R2 0.78195, RMSE 0.89263\n",
      "Epoch [23/500]       | Train: Loss 0.91320, R2 0.74483, RMSE 0.95200                     | Test: Loss 0.89220, R2 0.75634, RMSE 0.93721\n",
      "Epoch [24/500]       | Train: Loss 0.90286, R2 0.74779, RMSE 0.94554                     | Test: Loss 0.84869, R2 0.78028, RMSE 0.91449\n",
      "Epoch [25/500]       | Train: Loss 0.88840, R2 0.75215, RMSE 0.93868                     | Test: Loss 0.85435, R2 0.74643, RMSE 0.91959\n",
      "Epoch [26/500]       | Train: Loss 0.90757, R2 0.74777, RMSE 0.94701                     | Test: Loss 0.82447, R2 0.76895, RMSE 0.90545\n",
      "Epoch [27/500]       | Train: Loss 0.91015, R2 0.74600, RMSE 0.94929                     | Test: Loss 0.84889, R2 0.77010, RMSE 0.91405\n",
      "Epoch [28/500]       | Train: Loss 0.87990, R2 0.75419, RMSE 0.93351                     | Test: Loss 0.77618, R2 0.79194, RMSE 0.87619\n",
      "Epoch [29/500]       | Train: Loss 0.88270, R2 0.75355, RMSE 0.93637                     | Test: Loss 0.80542, R2 0.78070, RMSE 0.88786\n",
      "Epoch [30/500]       | Train: Loss 0.85954, R2 0.76104, RMSE 0.92164                     | Test: Loss 0.80299, R2 0.77439, RMSE 0.89239\n",
      "Epoch [31/500]       | Train: Loss 0.86468, R2 0.75753, RMSE 0.92684                     | Test: Loss 0.82677, R2 0.77375, RMSE 0.90437\n",
      "Epoch [32/500]       | Train: Loss 0.85891, R2 0.76105, RMSE 0.92269                     | Test: Loss 0.80488, R2 0.77683, RMSE 0.88985\n",
      "Epoch [33/500]       | Train: Loss 0.84641, R2 0.76370, RMSE 0.91643                     | Test: Loss 0.81996, R2 0.77889, RMSE 0.90080\n",
      "Epoch [34/500]       | Train: Loss 0.84550, R2 0.76240, RMSE 0.91525                     | Test: Loss 0.83427, R2 0.76703, RMSE 0.91001\n",
      "Epoch [35/500]       | Train: Loss 0.86891, R2 0.75815, RMSE 0.92815                     | Test: Loss 0.76382, R2 0.78879, RMSE 0.86389\n",
      "Epoch [36/500]       | Train: Loss 0.82595, R2 0.76739, RMSE 0.90503                     | Test: Loss 0.79809, R2 0.78952, RMSE 0.88922\n",
      "Epoch [37/500]       | Train: Loss 0.85510, R2 0.76295, RMSE 0.92057                     | Test: Loss 0.79195, R2 0.76835, RMSE 0.88413\n",
      "Epoch [38/500]       | Train: Loss 0.83670, R2 0.76337, RMSE 0.91058                     | Test: Loss 0.78707, R2 0.79230, RMSE 0.87962\n",
      "Epoch [39/500]       | Train: Loss 0.83683, R2 0.76752, RMSE 0.90981                     | Test: Loss 0.77361, R2 0.76779, RMSE 0.87255\n",
      "Epoch [40/500]       | Train: Loss 0.82374, R2 0.76800, RMSE 0.90415                     | Test: Loss 0.77068, R2 0.78911, RMSE 0.87238\n",
      "Epoch [41/500]       | Train: Loss 0.81051, R2 0.77246, RMSE 0.89708                     | Test: Loss 0.77872, R2 0.78804, RMSE 0.87663\n",
      "Epoch [42/500]       | Train: Loss 0.81610, R2 0.77243, RMSE 0.89933                     | Test: Loss 0.78464, R2 0.78659, RMSE 0.88131\n",
      "Epoch [43/500]       | Train: Loss 0.81999, R2 0.76964, RMSE 0.90163                     | Test: Loss 0.82170, R2 0.77886, RMSE 0.90264\n",
      "Epoch [44/500]       | Train: Loss 0.82133, R2 0.76947, RMSE 0.90180                     | Test: Loss 0.76569, R2 0.78596, RMSE 0.87180\n",
      "Epoch [45/500]       | Train: Loss 0.82934, R2 0.76821, RMSE 0.90743                     | Test: Loss 0.78399, R2 0.78802, RMSE 0.88051\n",
      "Epoch [46/500]       | Train: Loss 0.81597, R2 0.77200, RMSE 0.90022                     | Test: Loss 0.81708, R2 0.78156, RMSE 0.89822\n",
      "Epoch [47/500]       | Train: Loss 0.81531, R2 0.77259, RMSE 0.89862                     | Test: Loss 0.77398, R2 0.78769, RMSE 0.87487\n",
      "Epoch [48/500]       | Train: Loss 0.80932, R2 0.77320, RMSE 0.89568                     | Test: Loss 0.74988, R2 0.77913, RMSE 0.85604\n",
      "Epoch [49/500]       | Train: Loss 0.80009, R2 0.77619, RMSE 0.89013                     | Test: Loss 0.77051, R2 0.79282, RMSE 0.87333\n",
      "Epoch [50/500]       | Train: Loss 0.79593, R2 0.77678, RMSE 0.88882                     | Test: Loss 0.75869, R2 0.80050, RMSE 0.85999\n",
      "Epoch [51/500]       | Train: Loss 0.80355, R2 0.77606, RMSE 0.89168                     | Test: Loss 0.75634, R2 0.78226, RMSE 0.86065\n",
      "Epoch [52/500]       | Train: Loss 0.79046, R2 0.77917, RMSE 0.88534                     | Test: Loss 0.77481, R2 0.78908, RMSE 0.87540\n",
      "Epoch [53/500]       | Train: Loss 0.79596, R2 0.77696, RMSE 0.88860                     | Test: Loss 0.77223, R2 0.78919, RMSE 0.87678\n",
      "Epoch [54/500]       | Train: Loss 0.78313, R2 0.78206, RMSE 0.88043                     | Test: Loss 0.78638, R2 0.79152, RMSE 0.87940\n",
      "Epoch [55/500]       | Train: Loss 0.79363, R2 0.77875, RMSE 0.88713                     | Test: Loss 0.74897, R2 0.79933, RMSE 0.85045\n",
      "Epoch [56/500]       | Train: Loss 0.78256, R2 0.78154, RMSE 0.88020                     | Test: Loss 0.83818, R2 0.75275, RMSE 0.90868\n",
      "Epoch [57/500]       | Train: Loss 0.77531, R2 0.78330, RMSE 0.87726                     | Test: Loss 0.79672, R2 0.78044, RMSE 0.88706\n",
      "Epoch [58/500]       | Train: Loss 0.76939, R2 0.78403, RMSE 0.87379                     | Test: Loss 0.75427, R2 0.78987, RMSE 0.86460\n",
      "Epoch [59/500]       | Train: Loss 0.77449, R2 0.78318, RMSE 0.87645                     | Test: Loss 0.77427, R2 0.77524, RMSE 0.87337\n",
      "Epoch [60/500]       | Train: Loss 0.77921, R2 0.78228, RMSE 0.87943                     | Test: Loss 0.74938, R2 0.79446, RMSE 0.85460\n",
      "Epoch [61/500]       | Train: Loss 0.76160, R2 0.78707, RMSE 0.86820                     | Test: Loss 0.73842, R2 0.79227, RMSE 0.85127\n",
      "Epoch [62/500]       | Train: Loss 0.76215, R2 0.78753, RMSE 0.86953                     | Test: Loss 0.74841, R2 0.79516, RMSE 0.85901\n",
      "Epoch [63/500]       | Train: Loss 0.75468, R2 0.78959, RMSE 0.86463                     | Test: Loss 0.77033, R2 0.78292, RMSE 0.87104\n",
      "Epoch [64/500]       | Train: Loss 0.75740, R2 0.78801, RMSE 0.86624                     | Test: Loss 0.74041, R2 0.79661, RMSE 0.84901\n",
      "Epoch [65/500]       | Train: Loss 0.76995, R2 0.78461, RMSE 0.87358                     | Test: Loss 0.76087, R2 0.78569, RMSE 0.86730\n",
      "Epoch [66/500]       | Train: Loss 0.75728, R2 0.78825, RMSE 0.86564                     | Test: Loss 0.73780, R2 0.79102, RMSE 0.85158\n",
      "Epoch [67/500]       | Train: Loss 0.75914, R2 0.78888, RMSE 0.86653                     | Test: Loss 0.76155, R2 0.79199, RMSE 0.86540\n",
      "Epoch [68/500]       | Train: Loss 0.75401, R2 0.79047, RMSE 0.86389                     | Test: Loss 0.80231, R2 0.78600, RMSE 0.89151\n",
      "Epoch [69/500]       | Train: Loss 0.74641, R2 0.78889, RMSE 0.86115                     | Test: Loss 0.79417, R2 0.77715, RMSE 0.88550\n",
      "Epoch [70/500]       | Train: Loss 0.75656, R2 0.78792, RMSE 0.86586                     | Test: Loss 0.77462, R2 0.78458, RMSE 0.86958\n",
      "Epoch [71/500]       | Train: Loss 0.75787, R2 0.78682, RMSE 0.86664                     | Test: Loss 0.75802, R2 0.79172, RMSE 0.86115\n",
      "Epoch [72/500]       | Train: Loss 0.74524, R2 0.79043, RMSE 0.86065                     | Test: Loss 0.88723, R2 0.76830, RMSE 0.93721\n",
      "Epoch [73/500]       | Train: Loss 0.75597, R2 0.78933, RMSE 0.86650                     | Test: Loss 0.76519, R2 0.79233, RMSE 0.86842\n",
      "Epoch [74/500]       | Train: Loss 0.73395, R2 0.79461, RMSE 0.85226                     | Test: Loss 0.79325, R2 0.78565, RMSE 0.88533\n",
      "Epoch [75/500]       | Train: Loss 0.73570, R2 0.79364, RMSE 0.85472                     | Test: Loss 0.78638, R2 0.78992, RMSE 0.87717\n",
      "Epoch [76/500]       | Train: Loss 0.74219, R2 0.79005, RMSE 0.85792                     | Test: Loss 0.79895, R2 0.77245, RMSE 0.89027\n",
      "Epoch [77/500]       | Train: Loss 0.74779, R2 0.78994, RMSE 0.86171                     | Test: Loss 0.75427, R2 0.78863, RMSE 0.85476\n",
      "Epoch [78/500]       | Train: Loss 0.71571, R2 0.79909, RMSE 0.84292                     | Test: Loss 0.73946, R2 0.79295, RMSE 0.85313\n",
      "Epoch [79/500]       | Train: Loss 0.73623, R2 0.79303, RMSE 0.85539                     | Test: Loss 0.73345, R2 0.79927, RMSE 0.84704\n",
      "Epoch [80/500]       | Train: Loss 0.70821, R2 0.80116, RMSE 0.83860                     | Test: Loss 0.79162, R2 0.77875, RMSE 0.88054\n",
      "Epoch [81/500]       | Train: Loss 0.71645, R2 0.79781, RMSE 0.84340                     | Test: Loss 0.78220, R2 0.78238, RMSE 0.87838\n",
      "Epoch [82/500]       | Train: Loss 0.72399, R2 0.79798, RMSE 0.84776                     | Test: Loss 0.76209, R2 0.79104, RMSE 0.86703\n",
      "Epoch [83/500]       | Train: Loss 0.71395, R2 0.79927, RMSE 0.84150                     | Test: Loss 0.76596, R2 0.79152, RMSE 0.86712\n",
      "Epoch [84/500]       | Train: Loss 0.72552, R2 0.79577, RMSE 0.84897                     | Test: Loss 0.75811, R2 0.80229, RMSE 0.86189\n",
      "Epoch [85/500]       | Train: Loss 0.70570, R2 0.80310, RMSE 0.83483                     | Test: Loss 0.85235, R2 0.77355, RMSE 0.90619\n",
      "Epoch [86/500]       | Train: Loss 0.70793, R2 0.80095, RMSE 0.83881                     | Test: Loss 0.73010, R2 0.80090, RMSE 0.84746\n",
      "Epoch [87/500]       | Train: Loss 0.69869, R2 0.80123, RMSE 0.83256                     | Test: Loss 0.72938, R2 0.80336, RMSE 0.84498\n",
      "Epoch [88/500]       | Train: Loss 0.68836, R2 0.80610, RMSE 0.82707                     | Test: Loss 0.80330, R2 0.79629, RMSE 0.88694\n",
      "Epoch [89/500]       | Train: Loss 0.70754, R2 0.80292, RMSE 0.83833                     | Test: Loss 0.80612, R2 0.78500, RMSE 0.88982\n",
      "Epoch [90/500]       | Train: Loss 0.68443, R2 0.80783, RMSE 0.82474                     | Test: Loss 0.78890, R2 0.79379, RMSE 0.88029\n",
      "Epoch [91/500]       | Train: Loss 0.69205, R2 0.80806, RMSE 0.82802                     | Test: Loss 0.80271, R2 0.77383, RMSE 0.89020\n",
      "Epoch [92/500]       | Train: Loss 0.68418, R2 0.80858, RMSE 0.82441                     | Test: Loss 0.76276, R2 0.79074, RMSE 0.86451\n",
      "Epoch [93/500]       | Train: Loss 0.69546, R2 0.80562, RMSE 0.83052                     | Test: Loss 0.72626, R2 0.80061, RMSE 0.83587\n",
      "Epoch [94/500]       | Train: Loss 0.69132, R2 0.80594, RMSE 0.82797                     | Test: Loss 0.79099, R2 0.78549, RMSE 0.88408\n",
      "Epoch [95/500]       | Train: Loss 0.67950, R2 0.80993, RMSE 0.82155                     | Test: Loss 0.83505, R2 0.77166, RMSE 0.89375\n",
      "Epoch [96/500]       | Train: Loss 0.66962, R2 0.81380, RMSE 0.81545                     | Test: Loss 0.71576, R2 0.80486, RMSE 0.83544\n",
      "Epoch [97/500]       | Train: Loss 0.67775, R2 0.80870, RMSE 0.82029                     | Test: Loss 0.80168, R2 0.78142, RMSE 0.88714\n",
      "Epoch [98/500]       | Train: Loss 0.66362, R2 0.81489, RMSE 0.81214                     | Test: Loss 0.73180, R2 0.79854, RMSE 0.85079\n",
      "Epoch [99/500]       | Train: Loss 0.67994, R2 0.80981, RMSE 0.82157                     | Test: Loss 0.77791, R2 0.77683, RMSE 0.87351\n",
      "Epoch [100/500]      | Train: Loss 0.67183, R2 0.81185, RMSE 0.81674                     | Test: Loss 0.74022, R2 0.76889, RMSE 0.85505\n",
      "Epoch [101/500]      | Train: Loss 0.66897, R2 0.81343, RMSE 0.81566                     | Test: Loss 0.74792, R2 0.75981, RMSE 0.85984\n",
      "Epoch [102/500]      | Train: Loss 0.67240, R2 0.81201, RMSE 0.81725                     | Test: Loss 0.79319, R2 0.78504, RMSE 0.88661\n",
      "Epoch [103/500]      | Train: Loss 0.66149, R2 0.81409, RMSE 0.81158                     | Test: Loss 0.75153, R2 0.78912, RMSE 0.85701\n",
      "Epoch [104/500]      | Train: Loss 0.66780, R2 0.81342, RMSE 0.81470                     | Test: Loss 0.74367, R2 0.80386, RMSE 0.85597\n",
      "Epoch [105/500]      | Train: Loss 0.67038, R2 0.81125, RMSE 0.81650                     | Test: Loss 0.74752, R2 0.78877, RMSE 0.85714\n",
      "Epoch [106/500]      | Train: Loss 0.65119, R2 0.81708, RMSE 0.80506                     | Test: Loss 0.89126, R2 0.75088, RMSE 0.92322\n",
      "Epoch [107/500]      | Train: Loss 0.65572, R2 0.81591, RMSE 0.80702                     | Test: Loss 0.80012, R2 0.78177, RMSE 0.88780\n",
      "Epoch [108/500]      | Train: Loss 0.65730, R2 0.81582, RMSE 0.80840                     | Test: Loss 0.76161, R2 0.78079, RMSE 0.86346\n",
      "Epoch [109/500]      | Train: Loss 0.64320, R2 0.81987, RMSE 0.79881                     | Test: Loss 0.75647, R2 0.76796, RMSE 0.86580\n",
      "Epoch [110/500]      | Train: Loss 0.63978, R2 0.81900, RMSE 0.79762                     | Test: Loss 0.75592, R2 0.78845, RMSE 0.86302\n",
      "Epoch [111/500]      | Train: Loss 0.64249, R2 0.82034, RMSE 0.79855                     | Test: Loss 0.75913, R2 0.77147, RMSE 0.86498\n",
      "Epoch [112/500]      | Train: Loss 0.66915, R2 0.81129, RMSE 0.81438                     | Test: Loss 0.80175, R2 0.74682, RMSE 0.89049\n",
      "Epoch [113/500]      | Train: Loss 0.65620, R2 0.81591, RMSE 0.80689                     | Test: Loss 0.75836, R2 0.78472, RMSE 0.86114\n",
      "Epoch [114/500]      | Train: Loss 0.65812, R2 0.81431, RMSE 0.80809                     | Test: Loss 0.74853, R2 0.78852, RMSE 0.85762\n",
      "Epoch [115/500]      | Train: Loss 0.62820, R2 0.82285, RMSE 0.78974                     | Test: Loss 0.71225, R2 0.79982, RMSE 0.83216\n",
      "Epoch [116/500]      | Train: Loss 0.63831, R2 0.81985, RMSE 0.79635                     | Test: Loss 0.71642, R2 0.80297, RMSE 0.83746\n",
      "Epoch [117/500]      | Train: Loss 0.63165, R2 0.82277, RMSE 0.79239                     | Test: Loss 0.70308, R2 0.81005, RMSE 0.82852\n",
      "Epoch [118/500]      | Train: Loss 0.62583, R2 0.82382, RMSE 0.78848                     | Test: Loss 0.73251, R2 0.79175, RMSE 0.84439\n",
      "Epoch [119/500]      | Train: Loss 0.62980, R2 0.82287, RMSE 0.79061                     | Test: Loss 0.71126, R2 0.80105, RMSE 0.83710\n",
      "Epoch [120/500]      | Train: Loss 0.61735, R2 0.82842, RMSE 0.78271                     | Test: Loss 0.71456, R2 0.80138, RMSE 0.83895\n",
      "Epoch [121/500]      | Train: Loss 0.61303, R2 0.82877, RMSE 0.77984                     | Test: Loss 0.72864, R2 0.79141, RMSE 0.84404\n",
      "Epoch [122/500]      | Train: Loss 0.62556, R2 0.82376, RMSE 0.78877                     | Test: Loss 0.74697, R2 0.79320, RMSE 0.85898\n",
      "Epoch [123/500]      | Train: Loss 0.60860, R2 0.82937, RMSE 0.77800                     | Test: Loss 0.74334, R2 0.79164, RMSE 0.85631\n",
      "Epoch [124/500]      | Train: Loss 0.60877, R2 0.82903, RMSE 0.77845                     | Test: Loss 0.74801, R2 0.79256, RMSE 0.85559\n",
      "Epoch [125/500]      | Train: Loss 0.59707, R2 0.83154, RMSE 0.77078                     | Test: Loss 0.70678, R2 0.80402, RMSE 0.83332\n",
      "Epoch [126/500]      | Train: Loss 0.62755, R2 0.82393, RMSE 0.78886                     | Test: Loss 0.72448, R2 0.79882, RMSE 0.84511\n",
      "Epoch [127/500]      | Train: Loss 0.59925, R2 0.83109, RMSE 0.77138                     | Test: Loss 0.75112, R2 0.78868, RMSE 0.85897\n",
      "Epoch [128/500]      | Train: Loss 0.61061, R2 0.82821, RMSE 0.77975                     | Test: Loss 0.71940, R2 0.79367, RMSE 0.83698\n",
      "Epoch [129/500]      | Train: Loss 0.60683, R2 0.83060, RMSE 0.77622                     | Test: Loss 0.73955, R2 0.79612, RMSE 0.85011\n",
      "Epoch [130/500]      | Train: Loss 0.60639, R2 0.82872, RMSE 0.77600                     | Test: Loss 0.73009, R2 0.79791, RMSE 0.84525\n",
      "Epoch [131/500]      | Train: Loss 0.59335, R2 0.83448, RMSE 0.76733                     | Test: Loss 0.82114, R2 0.78004, RMSE 0.89895\n",
      "Epoch [132/500]      | Train: Loss 0.60882, R2 0.82930, RMSE 0.77731                     | Test: Loss 0.82673, R2 0.76955, RMSE 0.89830\n",
      "Epoch [133/500]      | Train: Loss 0.60389, R2 0.83121, RMSE 0.77533                     | Test: Loss 0.77765, R2 0.78404, RMSE 0.87781\n",
      "Epoch [134/500]      | Train: Loss 0.59990, R2 0.83199, RMSE 0.77248                     | Test: Loss 0.74770, R2 0.80470, RMSE 0.85739\n",
      "Epoch [135/500]      | Train: Loss 0.57891, R2 0.83651, RMSE 0.75848                     | Test: Loss 0.73684, R2 0.79033, RMSE 0.85063\n",
      "Epoch [136/500]      | Train: Loss 0.59998, R2 0.83240, RMSE 0.77172                     | Test: Loss 0.71916, R2 0.79442, RMSE 0.83977\n",
      "Epoch [137/500]      | Train: Loss 0.59571, R2 0.83310, RMSE 0.76918                     | Test: Loss 0.75635, R2 0.79347, RMSE 0.86369\n",
      "Epoch [138/500]      | Train: Loss 0.58598, R2 0.83502, RMSE 0.76338                     | Test: Loss 0.77799, R2 0.79902, RMSE 0.87407\n",
      "Epoch [139/500]      | Train: Loss 0.58087, R2 0.83816, RMSE 0.75926                     | Test: Loss 0.75346, R2 0.79495, RMSE 0.86122\n",
      "Epoch [140/500]      | Train: Loss 0.58070, R2 0.83787, RMSE 0.76040                     | Test: Loss 0.77078, R2 0.80397, RMSE 0.86859\n",
      "Epoch [141/500]      | Train: Loss 0.59988, R2 0.83199, RMSE 0.77280                     | Test: Loss 0.72765, R2 0.80072, RMSE 0.84629\n",
      "Epoch [142/500]      | Train: Loss 0.57633, R2 0.83746, RMSE 0.75772                     | Test: Loss 0.71092, R2 0.79480, RMSE 0.83678\n",
      "Epoch [143/500]      | Train: Loss 0.58765, R2 0.83482, RMSE 0.76436                     | Test: Loss 0.72469, R2 0.79076, RMSE 0.84608\n",
      "Epoch [144/500]      | Train: Loss 0.57470, R2 0.83800, RMSE 0.75569                     | Test: Loss 0.70879, R2 0.79249, RMSE 0.83386\n",
      "Epoch [145/500]      | Train: Loss 0.56791, R2 0.84106, RMSE 0.75185                     | Test: Loss 0.70930, R2 0.79842, RMSE 0.83680\n",
      "Epoch [146/500]      | Train: Loss 0.56872, R2 0.84086, RMSE 0.75148                     | Test: Loss 0.72036, R2 0.80273, RMSE 0.84237\n",
      "Epoch [147/500]      | Train: Loss 0.56819, R2 0.84046, RMSE 0.75185                     | Test: Loss 0.71450, R2 0.79886, RMSE 0.83835\n",
      "Epoch [148/500]      | Train: Loss 0.56184, R2 0.84165, RMSE 0.74736                     | Test: Loss 0.72629, R2 0.80115, RMSE 0.84777\n",
      "Epoch [149/500]      | Train: Loss 0.55543, R2 0.84414, RMSE 0.74283                     | Test: Loss 0.72891, R2 0.79847, RMSE 0.85013\n",
      "Epoch [150/500]      | Train: Loss 0.57563, R2 0.83747, RMSE 0.75605                     | Test: Loss 0.76870, R2 0.80472, RMSE 0.86775\n",
      "Epoch [151/500]      | Train: Loss 0.56714, R2 0.84176, RMSE 0.75109                     | Test: Loss 0.70992, R2 0.79478, RMSE 0.83718\n",
      "Epoch [152/500]      | Train: Loss 0.57646, R2 0.83650, RMSE 0.75752                     | Test: Loss 0.77374, R2 0.77376, RMSE 0.86568\n",
      "Epoch [153/500]      | Train: Loss 0.57317, R2 0.83899, RMSE 0.75415                     | Test: Loss 0.72987, R2 0.80152, RMSE 0.84661\n",
      "Epoch [154/500]      | Train: Loss 0.54396, R2 0.84789, RMSE 0.73522                     | Test: Loss 0.74821, R2 0.78533, RMSE 0.85879\n",
      "Epoch [155/500]      | Train: Loss 0.55779, R2 0.84126, RMSE 0.74487                     | Test: Loss 0.74306, R2 0.78670, RMSE 0.85561\n",
      "Epoch [156/500]      | Train: Loss 0.55912, R2 0.84314, RMSE 0.74548                     | Test: Loss 0.73006, R2 0.79694, RMSE 0.84544\n",
      "Epoch [157/500]      | Train: Loss 0.54058, R2 0.84768, RMSE 0.73272                     | Test: Loss 0.73188, R2 0.80235, RMSE 0.84708\n",
      "Epoch [158/500]      | Train: Loss 0.55713, R2 0.84405, RMSE 0.74417                     | Test: Loss 0.72515, R2 0.79369, RMSE 0.83672\n",
      "Epoch [159/500]      | Train: Loss 0.54776, R2 0.84671, RMSE 0.73782                     | Test: Loss 0.72061, R2 0.80733, RMSE 0.84477\n",
      "Epoch [160/500]      | Train: Loss 0.53997, R2 0.84845, RMSE 0.73255                     | Test: Loss 0.72685, R2 0.80015, RMSE 0.84735\n",
      "Epoch [161/500]      | Train: Loss 0.55651, R2 0.84287, RMSE 0.74328                     | Test: Loss 0.73963, R2 0.80687, RMSE 0.85211\n",
      "Epoch [162/500]      | Train: Loss 0.53570, R2 0.84944, RMSE 0.72872                     | Test: Loss 0.73757, R2 0.79177, RMSE 0.85169\n",
      "Epoch [163/500]      | Train: Loss 0.53616, R2 0.84958, RMSE 0.73025                     | Test: Loss 0.75551, R2 0.77127, RMSE 0.86288\n",
      "Epoch [164/500]      | Train: Loss 0.53287, R2 0.84910, RMSE 0.72787                     | Test: Loss 0.72802, R2 0.79530, RMSE 0.85042\n",
      "Epoch [165/500]      | Train: Loss 0.53186, R2 0.84934, RMSE 0.72733                     | Test: Loss 0.74336, R2 0.78292, RMSE 0.85600\n",
      "Epoch [166/500]      | Train: Loss 0.53043, R2 0.84929, RMSE 0.72535                     | Test: Loss 0.74645, R2 0.79934, RMSE 0.85773\n",
      "Epoch [167/500]      | Train: Loss 0.52219, R2 0.85234, RMSE 0.72088                     | Test: Loss 0.71219, R2 0.80894, RMSE 0.83544\n",
      "Epoch [168/500]      | Train: Loss 0.53510, R2 0.84980, RMSE 0.72887                     | Test: Loss 0.74609, R2 0.79363, RMSE 0.84850\n",
      "Epoch [169/500]      | Train: Loss 0.52287, R2 0.85286, RMSE 0.72117                     | Test: Loss 0.73984, R2 0.80176, RMSE 0.85530\n",
      "Epoch [170/500]      | Train: Loss 0.52409, R2 0.85285, RMSE 0.72143                     | Test: Loss 0.70179, R2 0.79913, RMSE 0.82596\n",
      "Epoch [171/500]      | Train: Loss 0.51469, R2 0.85506, RMSE 0.71548                     | Test: Loss 0.73568, R2 0.79030, RMSE 0.85271\n",
      "Epoch [172/500]      | Train: Loss 0.52889, R2 0.85111, RMSE 0.72504                     | Test: Loss 0.69843, R2 0.80781, RMSE 0.82464\n",
      "Epoch [173/500]      | Train: Loss 0.51884, R2 0.85428, RMSE 0.71826                     | Test: Loss 0.88519, R2 0.76707, RMSE 0.92263\n",
      "Epoch [174/500]      | Train: Loss 0.53041, R2 0.85064, RMSE 0.72619                     | Test: Loss 0.74358, R2 0.79048, RMSE 0.85976\n",
      "Epoch [175/500]      | Train: Loss 0.51887, R2 0.85302, RMSE 0.71779                     | Test: Loss 0.69743, R2 0.80686, RMSE 0.82347\n",
      "Epoch [176/500]      | Train: Loss 0.51848, R2 0.85309, RMSE 0.71811                     | Test: Loss 0.76487, R2 0.77494, RMSE 0.86530\n",
      "Epoch [177/500]      | Train: Loss 0.50877, R2 0.85778, RMSE 0.71086                     | Test: Loss 0.77600, R2 0.79786, RMSE 0.87219\n",
      "Epoch [178/500]      | Train: Loss 0.50834, R2 0.85673, RMSE 0.71066                     | Test: Loss 0.69028, R2 0.81355, RMSE 0.81446\n",
      "Epoch [179/500]      | Train: Loss 0.50876, R2 0.85689, RMSE 0.71082                     | Test: Loss 0.72048, R2 0.79519, RMSE 0.84233\n",
      "Epoch [180/500]      | Train: Loss 0.49536, R2 0.86118, RMSE 0.70214                     | Test: Loss 0.70430, R2 0.80565, RMSE 0.82470\n",
      "Epoch [181/500]      | Train: Loss 0.50244, R2 0.85807, RMSE 0.70736                     | Test: Loss 0.70474, R2 0.80440, RMSE 0.83260\n",
      "Epoch [182/500]      | Train: Loss 0.49832, R2 0.86043, RMSE 0.70437                     | Test: Loss 0.74733, R2 0.78925, RMSE 0.85829\n",
      "Epoch [183/500]      | Train: Loss 0.50958, R2 0.85513, RMSE 0.71126                     | Test: Loss 0.71044, R2 0.80653, RMSE 0.83193\n",
      "Epoch [184/500]      | Train: Loss 0.50684, R2 0.85791, RMSE 0.70978                     | Test: Loss 0.71881, R2 0.80573, RMSE 0.84075\n",
      "Epoch [185/500]      | Train: Loss 0.49018, R2 0.86109, RMSE 0.69785                     | Test: Loss 0.73198, R2 0.79585, RMSE 0.85283\n",
      "Epoch [186/500]      | Train: Loss 0.47912, R2 0.86543, RMSE 0.69045                     | Test: Loss 0.75612, R2 0.77250, RMSE 0.86183\n",
      "Epoch [187/500]      | Train: Loss 0.49156, R2 0.86044, RMSE 0.69973                     | Test: Loss 0.69612, R2 0.80875, RMSE 0.82267\n",
      "Epoch [188/500]      | Train: Loss 0.48525, R2 0.86370, RMSE 0.69475                     | Test: Loss 0.72504, R2 0.79744, RMSE 0.84497\n",
      "Epoch [189/500]      | Train: Loss 0.49479, R2 0.86094, RMSE 0.70169                     | Test: Loss 0.73866, R2 0.75841, RMSE 0.85097\n",
      "Epoch [190/500]      | Train: Loss 0.49413, R2 0.86108, RMSE 0.70085                     | Test: Loss 0.74335, R2 0.78866, RMSE 0.85437\n",
      "Epoch [191/500]      | Train: Loss 0.49510, R2 0.86098, RMSE 0.70232                     | Test: Loss 0.76028, R2 0.78443, RMSE 0.86504\n",
      "Epoch [192/500]      | Train: Loss 0.48959, R2 0.86247, RMSE 0.69776                     | Test: Loss 0.74407, R2 0.79658, RMSE 0.85826\n",
      "Epoch [193/500]      | Train: Loss 0.48864, R2 0.86286, RMSE 0.69720                     | Test: Loss 0.71510, R2 0.80532, RMSE 0.84120\n",
      "Epoch [194/500]      | Train: Loss 0.46709, R2 0.86720, RMSE 0.68224                     | Test: Loss 0.70719, R2 0.79398, RMSE 0.82905\n",
      "Epoch [195/500]      | Train: Loss 0.47978, R2 0.86422, RMSE 0.69081                     | Test: Loss 0.82993, R2 0.78696, RMSE 0.90124\n",
      "Epoch [196/500]      | Train: Loss 0.48243, R2 0.86458, RMSE 0.69272                     | Test: Loss 0.75519, R2 0.78453, RMSE 0.86378\n",
      "Epoch [197/500]      | Train: Loss 0.48011, R2 0.86505, RMSE 0.69099                     | Test: Loss 0.74995, R2 0.79939, RMSE 0.86091\n",
      "Epoch [198/500]      | Train: Loss 0.47554, R2 0.86588, RMSE 0.68796                     | Test: Loss 0.74083, R2 0.79914, RMSE 0.85492\n",
      "Epoch [199/500]      | Train: Loss 0.48084, R2 0.86482, RMSE 0.69156                     | Test: Loss 0.71666, R2 0.80410, RMSE 0.83941\n",
      "Epoch [200/500]      | Train: Loss 0.47498, R2 0.86583, RMSE 0.68732                     | Test: Loss 0.80089, R2 0.78094, RMSE 0.89007\n",
      "Epoch [201/500]      | Train: Loss 0.47893, R2 0.86457, RMSE 0.69039                     | Test: Loss 0.72754, R2 0.79260, RMSE 0.84394\n",
      "Epoch [202/500]      | Train: Loss 0.47081, R2 0.86708, RMSE 0.68452                     | Test: Loss 0.87416, R2 0.76666, RMSE 0.90390\n",
      "Epoch [203/500]      | Train: Loss 0.47854, R2 0.86605, RMSE 0.68990                     | Test: Loss 0.71412, R2 0.80303, RMSE 0.83810\n",
      "Epoch [204/500]      | Train: Loss 0.46528, R2 0.86938, RMSE 0.68012                     | Test: Loss 0.77049, R2 0.78590, RMSE 0.86869\n",
      "Epoch [205/500]      | Train: Loss 0.45930, R2 0.87044, RMSE 0.67553                     | Test: Loss 0.73848, R2 0.79697, RMSE 0.84712\n",
      "Epoch [206/500]      | Train: Loss 0.45997, R2 0.87106, RMSE 0.67678                     | Test: Loss 0.72265, R2 0.79627, RMSE 0.84213\n",
      "Epoch [207/500]      | Train: Loss 0.46009, R2 0.87115, RMSE 0.67663                     | Test: Loss 0.72171, R2 0.79890, RMSE 0.84378\n",
      "Epoch [208/500]      | Train: Loss 0.45076, R2 0.87305, RMSE 0.66957                     | Test: Loss 0.75217, R2 0.79002, RMSE 0.86468\n",
      "Epoch [209/500]      | Train: Loss 0.45620, R2 0.87159, RMSE 0.67380                     | Test: Loss 0.73513, R2 0.79226, RMSE 0.84898\n",
      "Epoch [210/500]      | Train: Loss 0.45873, R2 0.86943, RMSE 0.67557                     | Test: Loss 0.74617, R2 0.79721, RMSE 0.85924\n",
      "Epoch [211/500]      | Train: Loss 0.44641, R2 0.87482, RMSE 0.66708                     | Test: Loss 0.76106, R2 0.79307, RMSE 0.86844\n",
      "Epoch [212/500]      | Train: Loss 0.45498, R2 0.87252, RMSE 0.67302                     | Test: Loss 0.77791, R2 0.78830, RMSE 0.87509\n",
      "Epoch [213/500]      | Train: Loss 0.45170, R2 0.87292, RMSE 0.67071                     | Test: Loss 0.70615, R2 0.80917, RMSE 0.82872\n",
      "Epoch [214/500]      | Train: Loss 0.44355, R2 0.87485, RMSE 0.66481                     | Test: Loss 0.72329, R2 0.78904, RMSE 0.84436\n",
      "Epoch [215/500]      | Train: Loss 0.44215, R2 0.87436, RMSE 0.66347                     | Test: Loss 0.75197, R2 0.77233, RMSE 0.86104\n",
      "Epoch [216/500]      | Train: Loss 0.44930, R2 0.87422, RMSE 0.66880                     | Test: Loss 0.75615, R2 0.79425, RMSE 0.86384\n",
      "Epoch [217/500]      | Train: Loss 0.43593, R2 0.87738, RMSE 0.65814                     | Test: Loss 0.72192, R2 0.80208, RMSE 0.83567\n",
      "Epoch [218/500]      | Train: Loss 0.44176, R2 0.87574, RMSE 0.66286                     | Test: Loss 0.74535, R2 0.79988, RMSE 0.85537\n",
      "Epoch [219/500]      | Train: Loss 0.43820, R2 0.87711, RMSE 0.66054                     | Test: Loss 0.71243, R2 0.80307, RMSE 0.82892\n",
      "Epoch [220/500]      | Train: Loss 0.43804, R2 0.87565, RMSE 0.66045                     | Test: Loss 0.72535, R2 0.78373, RMSE 0.84562\n",
      "Epoch [221/500]      | Train: Loss 0.44042, R2 0.87578, RMSE 0.66222                     | Test: Loss 0.77743, R2 0.77237, RMSE 0.87583\n",
      "Epoch [222/500]      | Train: Loss 0.42395, R2 0.88140, RMSE 0.64903                     | Test: Loss 0.77861, R2 0.77985, RMSE 0.87495\n",
      "Epoch [223/500]      | Train: Loss 0.42956, R2 0.87874, RMSE 0.65425                     | Test: Loss 0.72812, R2 0.80079, RMSE 0.84508\n",
      "Epoch [224/500]      | Train: Loss 0.42994, R2 0.87914, RMSE 0.65397                     | Test: Loss 0.76357, R2 0.79598, RMSE 0.87007\n",
      "Epoch [225/500]      | Train: Loss 0.43391, R2 0.87800, RMSE 0.65691                     | Test: Loss 0.74981, R2 0.80048, RMSE 0.85888\n",
      "Epoch [226/500]      | Train: Loss 0.43425, R2 0.87849, RMSE 0.65739                     | Test: Loss 0.79946, R2 0.77942, RMSE 0.88596\n",
      "Epoch [227/500]      | Train: Loss 0.42705, R2 0.87945, RMSE 0.65227                     | Test: Loss 0.80375, R2 0.73747, RMSE 0.88881\n",
      "Epoch [228/500]      | Train: Loss 0.43116, R2 0.87829, RMSE 0.65555                     | Test: Loss 0.73812, R2 0.79311, RMSE 0.85360\n",
      "Epoch [229/500]      | Train: Loss 0.41104, R2 0.88450, RMSE 0.63946                     | Test: Loss 0.72780, R2 0.80342, RMSE 0.84537\n",
      "Epoch [230/500]      | Train: Loss 0.42969, R2 0.87926, RMSE 0.65373                     | Test: Loss 0.75282, R2 0.79616, RMSE 0.85594\n",
      "Epoch [231/500]      | Train: Loss 0.42682, R2 0.87964, RMSE 0.65206                     | Test: Loss 0.75255, R2 0.78508, RMSE 0.85717\n",
      "Epoch [232/500]      | Train: Loss 0.41375, R2 0.88288, RMSE 0.64201                     | Test: Loss 0.76854, R2 0.79518, RMSE 0.87188\n",
      "Epoch [233/500]      | Train: Loss 0.40955, R2 0.88386, RMSE 0.63855                     | Test: Loss 0.75147, R2 0.79249, RMSE 0.85719\n",
      "Epoch [234/500]      | Train: Loss 0.40598, R2 0.88546, RMSE 0.63620                     | Test: Loss 0.77403, R2 0.79434, RMSE 0.87242\n",
      "Epoch [235/500]      | Train: Loss 0.42497, R2 0.88070, RMSE 0.65052                     | Test: Loss 0.77455, R2 0.78735, RMSE 0.87433\n",
      "Epoch [236/500]      | Train: Loss 0.39812, R2 0.88757, RMSE 0.62931                     | Test: Loss 0.76963, R2 0.78397, RMSE 0.86903\n",
      "Epoch [237/500]      | Train: Loss 0.40320, R2 0.88655, RMSE 0.63350                     | Test: Loss 0.77290, R2 0.78979, RMSE 0.87601\n",
      "Epoch [238/500]      | Train: Loss 0.40305, R2 0.88657, RMSE 0.63331                     | Test: Loss 0.80054, R2 0.79468, RMSE 0.88585\n",
      "Epoch [239/500]      | Train: Loss 0.40572, R2 0.88487, RMSE 0.63530                     | Test: Loss 0.73429, R2 0.79517, RMSE 0.85279\n",
      "Epoch [240/500]      | Train: Loss 0.40231, R2 0.88723, RMSE 0.63352                     | Test: Loss 0.71958, R2 0.80044, RMSE 0.83930\n",
      "Epoch [241/500]      | Train: Loss 0.40691, R2 0.88483, RMSE 0.63673                     | Test: Loss 0.76038, R2 0.79497, RMSE 0.86661\n",
      "Epoch [242/500]      | Train: Loss 0.40436, R2 0.88600, RMSE 0.63427                     | Test: Loss 0.75281, R2 0.76400, RMSE 0.86172\n",
      "Epoch [243/500]      | Train: Loss 0.40326, R2 0.88687, RMSE 0.63370                     | Test: Loss 0.76540, R2 0.78345, RMSE 0.86171\n",
      "Epoch [244/500]      | Train: Loss 0.38698, R2 0.89056, RMSE 0.62070                     | Test: Loss 0.77921, R2 0.78299, RMSE 0.87221\n",
      "Epoch [245/500]      | Train: Loss 0.40205, R2 0.88656, RMSE 0.63267                     | Test: Loss 0.79798, R2 0.79218, RMSE 0.88539\n",
      "Epoch [246/500]      | Train: Loss 0.39534, R2 0.88851, RMSE 0.62737                     | Test: Loss 0.83241, R2 0.79219, RMSE 0.90109\n",
      "Epoch [247/500]      | Train: Loss 0.40016, R2 0.88621, RMSE 0.63154                     | Test: Loss 0.79792, R2 0.78620, RMSE 0.88272\n",
      "Epoch [248/500]      | Train: Loss 0.39034, R2 0.89057, RMSE 0.62320                     | Test: Loss 0.73855, R2 0.79006, RMSE 0.84910\n",
      "Epoch [249/500]      | Train: Loss 0.38319, R2 0.89152, RMSE 0.61704                     | Test: Loss 0.73226, R2 0.80072, RMSE 0.84775\n",
      "Epoch [250/500]      | Train: Loss 0.38976, R2 0.88962, RMSE 0.62261                     | Test: Loss 0.72497, R2 0.78636, RMSE 0.84455\n",
      "Epoch [251/500]      | Train: Loss 0.37973, R2 0.89278, RMSE 0.61463                     | Test: Loss 0.73842, R2 0.78648, RMSE 0.85247\n",
      "Epoch [252/500]      | Train: Loss 0.38287, R2 0.89150, RMSE 0.61788                     | Test: Loss 0.74065, R2 0.79673, RMSE 0.85733\n",
      "Epoch [253/500]      | Train: Loss 0.37491, R2 0.89474, RMSE 0.61101                     | Test: Loss 0.77351, R2 0.78354, RMSE 0.87428\n",
      "Epoch [254/500]      | Train: Loss 0.38132, R2 0.89268, RMSE 0.61567                     | Test: Loss 0.76315, R2 0.78916, RMSE 0.86654\n",
      "Epoch [255/500]      | Train: Loss 0.37810, R2 0.89355, RMSE 0.61421                     | Test: Loss 0.76856, R2 0.76898, RMSE 0.87003\n",
      "Epoch [256/500]      | Train: Loss 0.37673, R2 0.89427, RMSE 0.61264                     | Test: Loss 0.74728, R2 0.79971, RMSE 0.86024\n",
      "Epoch [257/500]      | Train: Loss 0.37663, R2 0.89300, RMSE 0.61226                     | Test: Loss 0.83932, R2 0.78321, RMSE 0.90389\n",
      "Epoch [258/500]      | Train: Loss 0.38299, R2 0.89164, RMSE 0.61754                     | Test: Loss 0.73303, R2 0.79907, RMSE 0.84953\n",
      "Epoch [259/500]      | Train: Loss 0.37553, R2 0.89496, RMSE 0.61127                     | Test: Loss 0.75323, R2 0.78836, RMSE 0.85751\n",
      "Epoch [260/500]      | Train: Loss 0.37696, R2 0.89339, RMSE 0.61295                     | Test: Loss 0.75878, R2 0.78880, RMSE 0.86223\n",
      "Epoch [261/500]      | Train: Loss 0.37623, R2 0.89469, RMSE 0.61166                     | Test: Loss 0.73666, R2 0.79971, RMSE 0.84952\n",
      "Epoch [262/500]      | Train: Loss 0.38553, R2 0.89197, RMSE 0.61958                     | Test: Loss 0.76117, R2 0.78386, RMSE 0.86621\n",
      "Epoch [263/500]      | Train: Loss 0.37688, R2 0.89337, RMSE 0.61318                     | Test: Loss 0.76471, R2 0.79139, RMSE 0.86436\n",
      "Epoch [264/500]      | Train: Loss 0.38005, R2 0.89362, RMSE 0.61472                     | Test: Loss 0.77697, R2 0.78920, RMSE 0.87766\n",
      "Epoch [265/500]      | Train: Loss 0.38430, R2 0.89208, RMSE 0.61815                     | Test: Loss 0.74263, R2 0.79733, RMSE 0.85237\n",
      "Epoch [266/500]      | Train: Loss 0.35590, R2 0.90023, RMSE 0.59570                     | Test: Loss 0.75511, R2 0.79212, RMSE 0.85878\n",
      "Epoch [267/500]      | Train: Loss 0.37153, R2 0.89616, RMSE 0.60781                     | Test: Loss 0.77073, R2 0.78713, RMSE 0.87080\n",
      "Epoch [268/500]      | Train: Loss 0.36170, R2 0.89881, RMSE 0.59989                     | Test: Loss 0.73527, R2 0.77632, RMSE 0.85014\n",
      "Epoch [269/500]      | Train: Loss 0.36415, R2 0.89756, RMSE 0.60209                     | Test: Loss 0.74697, R2 0.79467, RMSE 0.85187\n",
      "Epoch [270/500]      | Train: Loss 0.36602, R2 0.89658, RMSE 0.60325                     | Test: Loss 0.76093, R2 0.78733, RMSE 0.86493\n",
      "Epoch [271/500]      | Train: Loss 0.36003, R2 0.89744, RMSE 0.59888                     | Test: Loss 0.78857, R2 0.78351, RMSE 0.88133\n",
      "Epoch [272/500]      | Train: Loss 0.35441, R2 0.90044, RMSE 0.59417                     | Test: Loss 0.73545, R2 0.80097, RMSE 0.84820\n",
      "Epoch [273/500]      | Train: Loss 0.35180, R2 0.90027, RMSE 0.59204                     | Test: Loss 0.77625, R2 0.78295, RMSE 0.87622\n",
      "Epoch [274/500]      | Train: Loss 0.35184, R2 0.90045, RMSE 0.59192                     | Test: Loss 0.75575, R2 0.79243, RMSE 0.86511\n",
      "Epoch [275/500]      | Train: Loss 0.34868, R2 0.90171, RMSE 0.58950                     | Test: Loss 0.74803, R2 0.77541, RMSE 0.85730\n",
      "Epoch [276/500]      | Train: Loss 0.34805, R2 0.90188, RMSE 0.58889                     | Test: Loss 0.73324, R2 0.79681, RMSE 0.84841\n",
      "Epoch [277/500]      | Train: Loss 0.34200, R2 0.90281, RMSE 0.58393                     | Test: Loss 0.75620, R2 0.79375, RMSE 0.85770\n",
      "Epoch [278/500]      | Train: Loss 0.34990, R2 0.90146, RMSE 0.59031                     | Test: Loss 0.78427, R2 0.78076, RMSE 0.88049\n",
      "Epoch [279/500]      | Train: Loss 0.34150, R2 0.90383, RMSE 0.58337                     | Test: Loss 0.75828, R2 0.78806, RMSE 0.86300\n",
      "Epoch [280/500]      | Train: Loss 0.34406, R2 0.90266, RMSE 0.58546                     | Test: Loss 0.76951, R2 0.78999, RMSE 0.86952\n",
      "Epoch [281/500]      | Train: Loss 0.34805, R2 0.90274, RMSE 0.58821                     | Test: Loss 0.76104, R2 0.79105, RMSE 0.86204\n",
      "Epoch [282/500]      | Train: Loss 0.34963, R2 0.90144, RMSE 0.59001                     | Test: Loss 0.74067, R2 0.79878, RMSE 0.85265\n",
      "Epoch [283/500]      | Train: Loss 0.34438, R2 0.90266, RMSE 0.58564                     | Test: Loss 0.77099, R2 0.78486, RMSE 0.87284\n",
      "Epoch [284/500]      | Train: Loss 0.34286, R2 0.90395, RMSE 0.58426                     | Test: Loss 0.82471, R2 0.73515, RMSE 0.90046\n",
      "Epoch [285/500]      | Train: Loss 0.34332, R2 0.90283, RMSE 0.58484                     | Test: Loss 0.73827, R2 0.78999, RMSE 0.85024\n",
      "Epoch [286/500]      | Train: Loss 0.33877, R2 0.90450, RMSE 0.58107                     | Test: Loss 0.75342, R2 0.79253, RMSE 0.85774\n",
      "Epoch [287/500]      | Train: Loss 0.34064, R2 0.90353, RMSE 0.58216                     | Test: Loss 0.77014, R2 0.79246, RMSE 0.86375\n",
      "Epoch [288/500]      | Train: Loss 0.33208, R2 0.90612, RMSE 0.57509                     | Test: Loss 0.75822, R2 0.79767, RMSE 0.86460\n",
      "Epoch [289/500]      | Train: Loss 0.32810, R2 0.90770, RMSE 0.57154                     | Test: Loss 0.74393, R2 0.79610, RMSE 0.85345\n",
      "Epoch [290/500]      | Train: Loss 0.32497, R2 0.90759, RMSE 0.56891                     | Test: Loss 0.77096, R2 0.78881, RMSE 0.86958\n",
      "Epoch [291/500]      | Train: Loss 0.32858, R2 0.90750, RMSE 0.57170                     | Test: Loss 0.80226, R2 0.77739, RMSE 0.89054\n",
      "Epoch [292/500]      | Train: Loss 0.32611, R2 0.90769, RMSE 0.56981                     | Test: Loss 0.76505, R2 0.77925, RMSE 0.86918\n",
      "Epoch [293/500]      | Train: Loss 0.33749, R2 0.90522, RMSE 0.57953                     | Test: Loss 0.77123, R2 0.79336, RMSE 0.87208\n",
      "Epoch [294/500]      | Train: Loss 0.33298, R2 0.90609, RMSE 0.57576                     | Test: Loss 0.82745, R2 0.76187, RMSE 0.89954\n",
      "Epoch [295/500]      | Train: Loss 0.32721, R2 0.90740, RMSE 0.57095                     | Test: Loss 0.79379, R2 0.79141, RMSE 0.88079\n",
      "Epoch [296/500]      | Train: Loss 0.32674, R2 0.90667, RMSE 0.57045                     | Test: Loss 0.78557, R2 0.76448, RMSE 0.87972\n",
      "Epoch [297/500]      | Train: Loss 0.32228, R2 0.90828, RMSE 0.56647                     | Test: Loss 0.94406, R2 0.73489, RMSE 0.93540\n",
      "Epoch [298/500]      | Train: Loss 0.31696, R2 0.91081, RMSE 0.56220                     | Test: Loss 0.77009, R2 0.77071, RMSE 0.87073\n",
      "Epoch [299/500]      | Train: Loss 0.31996, R2 0.90944, RMSE 0.56395                     | Test: Loss 0.76686, R2 0.78203, RMSE 0.86571\n",
      "Epoch [300/500]      | Train: Loss 0.32783, R2 0.90751, RMSE 0.57186                     | Test: Loss 0.78301, R2 0.78445, RMSE 0.87730\n",
      "Epoch [301/500]      | Train: Loss 0.31932, R2 0.90919, RMSE 0.56424                     | Test: Loss 0.77506, R2 0.78135, RMSE 0.87770\n",
      "Epoch [302/500]      | Train: Loss 0.31724, R2 0.91039, RMSE 0.56201                     | Test: Loss 0.76103, R2 0.77789, RMSE 0.86394\n",
      "Epoch [303/500]      | Train: Loss 0.32217, R2 0.90924, RMSE 0.56675                     | Test: Loss 0.81367, R2 0.77484, RMSE 0.89235\n",
      "Epoch [304/500]      | Train: Loss 0.31745, R2 0.90982, RMSE 0.56208                     | Test: Loss 0.74702, R2 0.79390, RMSE 0.85640\n",
      "Epoch [305/500]      | Train: Loss 0.31409, R2 0.91126, RMSE 0.55922                     | Test: Loss 0.88925, R2 0.71524, RMSE 0.91850\n",
      "Epoch [306/500]      | Train: Loss 0.31824, R2 0.91033, RMSE 0.56265                     | Test: Loss 0.72375, R2 0.79999, RMSE 0.84069\n",
      "Epoch [307/500]      | Train: Loss 0.32989, R2 0.90700, RMSE 0.57274                     | Test: Loss 0.83289, R2 0.77149, RMSE 0.90343\n",
      "Epoch [308/500]      | Train: Loss 0.31069, R2 0.91258, RMSE 0.55609                     | Test: Loss 0.77296, R2 0.77849, RMSE 0.87476\n",
      "Epoch [309/500]      | Train: Loss 0.31579, R2 0.91138, RMSE 0.56097                     | Test: Loss 0.75069, R2 0.78941, RMSE 0.86002\n",
      "Epoch [310/500]      | Train: Loss 0.31518, R2 0.91183, RMSE 0.56018                     | Test: Loss 0.77823, R2 0.78547, RMSE 0.87228\n",
      "Epoch [311/500]      | Train: Loss 0.30336, R2 0.91439, RMSE 0.54989                     | Test: Loss 0.73041, R2 0.79745, RMSE 0.84217\n",
      "Epoch [312/500]      | Train: Loss 0.31387, R2 0.91139, RMSE 0.55911                     | Test: Loss 0.77353, R2 0.77799, RMSE 0.87254\n",
      "Epoch [313/500]      | Train: Loss 0.29924, R2 0.91573, RMSE 0.54627                     | Test: Loss 0.81282, R2 0.76399, RMSE 0.89646\n",
      "Epoch [314/500]      | Train: Loss 0.31426, R2 0.91176, RMSE 0.55935                     | Test: Loss 0.76136, R2 0.79367, RMSE 0.86235\n",
      "Epoch [315/500]      | Train: Loss 0.30594, R2 0.91347, RMSE 0.55196                     | Test: Loss 0.80575, R2 0.78743, RMSE 0.89324\n",
      "Epoch [316/500]      | Train: Loss 0.30389, R2 0.91470, RMSE 0.55034                     | Test: Loss 0.76024, R2 0.78314, RMSE 0.86534\n",
      "Epoch [317/500]      | Train: Loss 0.30039, R2 0.91481, RMSE 0.54708                     | Test: Loss 0.80423, R2 0.78906, RMSE 0.89062\n",
      "Epoch [318/500]      | Train: Loss 0.29570, R2 0.91636, RMSE 0.54252                     | Test: Loss 0.77199, R2 0.78003, RMSE 0.87181\n",
      "Epoch [319/500]      | Train: Loss 0.29282, R2 0.91765, RMSE 0.53998                     | Test: Loss 0.75511, R2 0.78698, RMSE 0.85938\n",
      "Epoch [320/500]      | Train: Loss 0.29860, R2 0.91499, RMSE 0.54498                     | Test: Loss 0.76185, R2 0.78743, RMSE 0.86397\n",
      "Epoch [321/500]      | Train: Loss 0.28949, R2 0.91798, RMSE 0.53732                     | Test: Loss 0.79032, R2 0.76130, RMSE 0.88029\n",
      "Epoch [322/500]      | Train: Loss 0.29845, R2 0.91546, RMSE 0.54505                     | Test: Loss 0.80260, R2 0.78802, RMSE 0.88538\n",
      "Epoch [323/500]      | Train: Loss 0.29926, R2 0.91536, RMSE 0.54624                     | Test: Loss 0.76634, R2 0.78577, RMSE 0.86796\n",
      "Epoch [324/500]      | Train: Loss 0.29669, R2 0.91631, RMSE 0.54371                     | Test: Loss 0.76478, R2 0.79096, RMSE 0.86573\n",
      "Epoch [325/500]      | Train: Loss 0.29932, R2 0.91484, RMSE 0.54561                     | Test: Loss 0.76425, R2 0.79256, RMSE 0.86637\n",
      "Epoch [326/500]      | Train: Loss 0.29645, R2 0.91619, RMSE 0.54325                     | Test: Loss 0.77910, R2 0.78730, RMSE 0.87026\n",
      "Epoch [327/500]      | Train: Loss 0.29122, R2 0.91733, RMSE 0.53865                     | Test: Loss 0.75076, R2 0.79410, RMSE 0.85685\n",
      "Epoch [328/500]      | Train: Loss 0.29599, R2 0.91680, RMSE 0.54322                     | Test: Loss 0.82352, R2 0.75110, RMSE 0.89961\n",
      "Epoch [329/500]      | Train: Loss 0.29198, R2 0.91764, RMSE 0.53916                     | Test: Loss 0.81052, R2 0.76986, RMSE 0.89283\n",
      "Epoch [330/500]      | Train: Loss 0.28693, R2 0.91935, RMSE 0.53467                     | Test: Loss 0.77629, R2 0.78122, RMSE 0.87791\n",
      "Epoch [331/500]      | Train: Loss 0.29055, R2 0.91844, RMSE 0.53770                     | Test: Loss 0.77915, R2 0.77984, RMSE 0.87507\n",
      "Epoch [332/500]      | Train: Loss 0.28921, R2 0.91855, RMSE 0.53683                     | Test: Loss 0.78354, R2 0.78290, RMSE 0.87616\n",
      "Epoch [333/500]      | Train: Loss 0.30038, R2 0.91493, RMSE 0.54692                     | Test: Loss 0.80814, R2 0.77699, RMSE 0.89325\n",
      "Epoch [334/500]      | Train: Loss 0.29410, R2 0.91670, RMSE 0.54121                     | Test: Loss 0.89677, R2 0.77727, RMSE 0.92678\n",
      "Epoch [335/500]      | Train: Loss 0.28218, R2 0.92040, RMSE 0.52996                     | Test: Loss 0.85606, R2 0.77213, RMSE 0.91171\n",
      "Epoch [336/500]      | Train: Loss 0.27668, R2 0.92151, RMSE 0.52497                     | Test: Loss 0.80120, R2 0.76776, RMSE 0.88607\n",
      "Epoch [337/500]      | Train: Loss 0.28613, R2 0.91898, RMSE 0.53357                     | Test: Loss 0.76845, R2 0.79388, RMSE 0.86100\n",
      "Epoch [338/500]      | Train: Loss 0.27736, R2 0.92237, RMSE 0.52555                     | Test: Loss 0.76514, R2 0.79234, RMSE 0.86858\n",
      "Epoch [339/500]      | Train: Loss 0.28233, R2 0.91956, RMSE 0.53005                     | Test: Loss 0.79628, R2 0.76130, RMSE 0.88870\n",
      "Epoch [340/500]      | Train: Loss 0.28257, R2 0.91944, RMSE 0.53057                     | Test: Loss 0.78198, R2 0.78546, RMSE 0.87777\n",
      "Epoch [341/500]      | Train: Loss 0.28193, R2 0.91961, RMSE 0.53026                     | Test: Loss 0.76446, R2 0.77549, RMSE 0.86706\n",
      "Epoch [342/500]      | Train: Loss 0.27586, R2 0.92253, RMSE 0.52370                     | Test: Loss 0.75511, R2 0.79733, RMSE 0.86339\n",
      "Epoch [343/500]      | Train: Loss 0.27620, R2 0.92214, RMSE 0.52464                     | Test: Loss 0.77434, R2 0.78527, RMSE 0.86960\n",
      "Epoch [344/500]      | Train: Loss 0.27960, R2 0.92119, RMSE 0.52765                     | Test: Loss 0.77491, R2 0.78398, RMSE 0.87342\n",
      "Epoch [345/500]      | Train: Loss 0.27417, R2 0.92215, RMSE 0.52233                     | Test: Loss 0.78932, R2 0.78282, RMSE 0.88203\n",
      "Epoch [346/500]      | Train: Loss 0.27840, R2 0.92044, RMSE 0.52678                     | Test: Loss 0.77463, R2 0.77975, RMSE 0.87592\n",
      "Epoch [347/500]      | Train: Loss 0.28271, R2 0.92002, RMSE 0.52990                     | Test: Loss 0.75353, R2 0.79768, RMSE 0.85869\n",
      "Epoch [348/500]      | Train: Loss 0.27987, R2 0.92048, RMSE 0.52829                     | Test: Loss 0.77524, R2 0.77980, RMSE 0.87204\n",
      "Epoch [349/500]      | Train: Loss 0.27913, R2 0.92118, RMSE 0.52730                     | Test: Loss 0.78591, R2 0.77670, RMSE 0.88075\n",
      "Epoch [350/500]      | Train: Loss 0.26746, R2 0.92460, RMSE 0.51598                     | Test: Loss 0.80642, R2 0.76338, RMSE 0.89006\n",
      "Epoch [351/500]      | Train: Loss 0.26892, R2 0.92395, RMSE 0.51774                     | Test: Loss 0.78098, R2 0.78289, RMSE 0.87299\n",
      "Epoch [352/500]      | Train: Loss 0.27955, R2 0.92061, RMSE 0.52808                     | Test: Loss 0.78148, R2 0.78138, RMSE 0.87907\n",
      "Epoch [353/500]      | Train: Loss 0.27145, R2 0.92314, RMSE 0.52011                     | Test: Loss 0.79801, R2 0.76864, RMSE 0.88555\n",
      "Epoch [354/500]      | Train: Loss 0.26749, R2 0.92394, RMSE 0.51652                     | Test: Loss 0.76906, R2 0.79250, RMSE 0.87044\n",
      "Epoch [355/500]      | Train: Loss 0.26059, R2 0.92491, RMSE 0.50940                     | Test: Loss 0.81280, R2 0.77822, RMSE 0.89575\n",
      "Epoch [356/500]      | Train: Loss 0.26635, R2 0.92525, RMSE 0.51535                     | Test: Loss 0.76852, R2 0.79201, RMSE 0.86728\n",
      "Epoch [357/500]      | Train: Loss 0.26298, R2 0.92584, RMSE 0.51217                     | Test: Loss 0.77990, R2 0.77800, RMSE 0.87542\n",
      "Epoch [358/500]      | Train: Loss 0.27149, R2 0.92295, RMSE 0.52023                     | Test: Loss 0.79086, R2 0.76246, RMSE 0.88138\n",
      "Epoch [359/500]      | Train: Loss 0.27666, R2 0.92176, RMSE 0.52476                     | Test: Loss 0.80040, R2 0.78010, RMSE 0.88597\n",
      "Epoch [360/500]      | Train: Loss 0.26419, R2 0.92529, RMSE 0.51295                     | Test: Loss 0.79171, R2 0.78566, RMSE 0.88531\n",
      "Epoch [361/500]      | Train: Loss 0.25967, R2 0.92666, RMSE 0.50826                     | Test: Loss 0.82134, R2 0.75327, RMSE 0.90052\n",
      "Epoch [362/500]      | Train: Loss 0.26865, R2 0.92399, RMSE 0.51698                     | Test: Loss 0.78527, R2 0.76027, RMSE 0.87958\n",
      "Epoch [363/500]      | Train: Loss 0.25966, R2 0.92673, RMSE 0.50867                     | Test: Loss 0.82061, R2 0.76348, RMSE 0.89323\n",
      "Epoch [364/500]      | Train: Loss 0.25958, R2 0.92701, RMSE 0.50868                     | Test: Loss 0.79228, R2 0.78317, RMSE 0.87601\n",
      "Epoch [365/500]      | Train: Loss 0.25430, R2 0.92793, RMSE 0.50334                     | Test: Loss 0.92587, R2 0.76459, RMSE 0.95010\n",
      "Epoch [366/500]      | Train: Loss 0.26055, R2 0.92640, RMSE 0.50916                     | Test: Loss 0.75951, R2 0.79060, RMSE 0.86216\n",
      "Epoch [367/500]      | Train: Loss 0.25905, R2 0.92709, RMSE 0.50812                     | Test: Loss 0.76662, R2 0.78792, RMSE 0.86459\n",
      "Epoch [368/500]      | Train: Loss 0.26146, R2 0.92443, RMSE 0.51015                     | Test: Loss 0.78990, R2 0.77356, RMSE 0.88053\n",
      "Epoch [369/500]      | Train: Loss 0.26414, R2 0.92523, RMSE 0.51298                     | Test: Loss 0.79728, R2 0.78070, RMSE 0.88644\n",
      "Epoch [370/500]      | Train: Loss 0.25717, R2 0.92705, RMSE 0.50617                     | Test: Loss 0.78717, R2 0.78366, RMSE 0.87498\n",
      "Epoch [371/500]      | Train: Loss 0.25563, R2 0.92778, RMSE 0.50480                     | Test: Loss 0.75138, R2 0.79244, RMSE 0.85580\n",
      "Epoch [372/500]      | Train: Loss 0.25423, R2 0.92819, RMSE 0.50323                     | Test: Loss 0.76100, R2 0.78517, RMSE 0.85876\n",
      "Epoch [373/500]      | Train: Loss 0.24755, R2 0.92940, RMSE 0.49651                     | Test: Loss 0.78581, R2 0.78041, RMSE 0.87993\n",
      "Epoch [374/500]      | Train: Loss 0.24601, R2 0.93077, RMSE 0.49491                     | Test: Loss 0.81645, R2 0.78206, RMSE 0.89859\n",
      "Epoch [375/500]      | Train: Loss 0.24647, R2 0.92982, RMSE 0.49537                     | Test: Loss 0.80955, R2 0.75845, RMSE 0.89513\n",
      "Epoch [376/500]      | Train: Loss 0.25243, R2 0.92880, RMSE 0.50112                     | Test: Loss 0.78707, R2 0.76345, RMSE 0.88036\n",
      "Epoch [377/500]      | Train: Loss 0.25059, R2 0.92934, RMSE 0.49960                     | Test: Loss 0.79150, R2 0.78529, RMSE 0.88187\n",
      "Epoch [378/500]      | Train: Loss 0.25144, R2 0.92845, RMSE 0.50028                     | Test: Loss 0.78533, R2 0.78214, RMSE 0.88102\n",
      "Epoch [379/500]      | Train: Loss 0.25096, R2 0.92880, RMSE 0.50024                     | Test: Loss 1.12762, R2 0.75234, RMSE 0.98688\n",
      "Epoch [380/500]      | Train: Loss 0.24768, R2 0.92976, RMSE 0.49677                     | Test: Loss 0.81335, R2 0.77298, RMSE 0.89111\n",
      "Epoch [381/500]      | Train: Loss 0.25530, R2 0.92673, RMSE 0.50432                     | Test: Loss 0.79301, R2 0.75393, RMSE 0.88410\n",
      "Epoch [382/500]      | Train: Loss 0.25452, R2 0.92738, RMSE 0.50309                     | Test: Loss 0.81654, R2 0.77354, RMSE 0.89320\n",
      "Epoch [383/500]      | Train: Loss 0.24613, R2 0.93016, RMSE 0.49511                     | Test: Loss 0.90570, R2 0.77728, RMSE 0.93284\n",
      "Epoch [384/500]      | Train: Loss 0.24325, R2 0.93149, RMSE 0.49217                     | Test: Loss 0.82228, R2 0.75240, RMSE 0.89929\n",
      "Epoch [385/500]      | Train: Loss 0.23869, R2 0.93283, RMSE 0.48773                     | Test: Loss 0.79146, R2 0.77940, RMSE 0.88481\n",
      "Epoch [386/500]      | Train: Loss 0.24190, R2 0.93172, RMSE 0.49099                     | Test: Loss 0.80294, R2 0.77724, RMSE 0.88932\n",
      "Epoch [387/500]      | Train: Loss 0.23579, R2 0.93305, RMSE 0.48487                     | Test: Loss 0.78041, R2 0.77767, RMSE 0.87317\n",
      "Epoch [388/500]      | Train: Loss 0.23939, R2 0.93263, RMSE 0.48780                     | Test: Loss 0.84681, R2 0.78215, RMSE 0.91185\n",
      "Epoch [389/500]      | Train: Loss 0.23736, R2 0.93242, RMSE 0.48629                     | Test: Loss 0.81205, R2 0.78163, RMSE 0.89643\n",
      "Epoch [390/500]      | Train: Loss 0.24862, R2 0.92986, RMSE 0.49749                     | Test: Loss 0.79808, R2 0.77642, RMSE 0.88590\n",
      "Epoch [391/500]      | Train: Loss 0.23760, R2 0.93253, RMSE 0.48652                     | Test: Loss 0.81248, R2 0.77916, RMSE 0.89325\n",
      "Epoch [392/500]      | Train: Loss 0.23818, R2 0.93285, RMSE 0.48709                     | Test: Loss 0.79206, R2 0.77172, RMSE 0.88101\n",
      "Epoch [393/500]      | Train: Loss 0.24412, R2 0.93051, RMSE 0.49301                     | Test: Loss 0.79197, R2 0.78766, RMSE 0.88487\n",
      "Epoch [394/500]      | Train: Loss 0.24056, R2 0.93199, RMSE 0.48939                     | Test: Loss 0.80939, R2 0.78013, RMSE 0.89390\n",
      "Epoch [395/500]      | Train: Loss 0.23625, R2 0.93349, RMSE 0.48505                     | Test: Loss 0.80601, R2 0.78153, RMSE 0.88917\n",
      "Epoch [396/500]      | Train: Loss 0.23372, R2 0.93392, RMSE 0.48285                     | Test: Loss 0.83279, R2 0.77552, RMSE 0.90731\n",
      "Epoch [397/500]      | Train: Loss 0.22798, R2 0.93597, RMSE 0.47654                     | Test: Loss 0.80805, R2 0.78118, RMSE 0.89501\n",
      "Epoch [398/500]      | Train: Loss 0.22772, R2 0.93569, RMSE 0.47650                     | Test: Loss 0.79881, R2 0.77181, RMSE 0.88124\n",
      "Epoch [399/500]      | Train: Loss 0.23017, R2 0.93510, RMSE 0.47870                     | Test: Loss 0.79902, R2 0.78426, RMSE 0.88648\n",
      "Epoch [400/500]      | Train: Loss 0.23045, R2 0.93524, RMSE 0.47918                     | Test: Loss 0.85029, R2 0.78132, RMSE 0.91160\n",
      "Epoch [401/500]      | Train: Loss 0.23104, R2 0.93411, RMSE 0.47980                     | Test: Loss 0.83909, R2 0.77727, RMSE 0.91099\n",
      "Epoch [402/500]      | Train: Loss 0.23090, R2 0.93400, RMSE 0.47949                     | Test: Loss 0.80763, R2 0.76993, RMSE 0.89064\n",
      "Epoch [403/500]      | Train: Loss 0.23089, R2 0.93453, RMSE 0.47958                     | Test: Loss 0.78464, R2 0.78194, RMSE 0.88156\n",
      "Epoch [404/500]      | Train: Loss 0.22495, R2 0.93641, RMSE 0.47320                     | Test: Loss 0.79055, R2 0.78475, RMSE 0.88278\n",
      "Epoch [405/500]      | Train: Loss 0.22687, R2 0.93552, RMSE 0.47546                     | Test: Loss 0.80211, R2 0.78092, RMSE 0.88878\n",
      "Epoch [406/500]      | Train: Loss 0.22490, R2 0.93622, RMSE 0.47345                     | Test: Loss 0.79130, R2 0.77765, RMSE 0.88480\n",
      "Epoch [407/500]      | Train: Loss 0.22365, R2 0.93672, RMSE 0.47215                     | Test: Loss 0.78211, R2 0.78413, RMSE 0.87289\n",
      "Epoch [408/500]      | Train: Loss 0.22251, R2 0.93660, RMSE 0.47085                     | Test: Loss 0.81715, R2 0.74062, RMSE 0.89557\n",
      "Epoch [409/500]      | Train: Loss 0.22429, R2 0.93616, RMSE 0.47302                     | Test: Loss 0.77718, R2 0.78484, RMSE 0.86729\n",
      "Epoch [410/500]      | Train: Loss 0.22851, R2 0.93491, RMSE 0.47733                     | Test: Loss 0.86959, R2 0.75905, RMSE 0.92371\n",
      "Epoch [411/500]      | Train: Loss 0.22185, R2 0.93756, RMSE 0.47020                     | Test: Loss 0.95092, R2 0.74948, RMSE 0.95050\n",
      "Epoch [412/500]      | Train: Loss 0.22356, R2 0.93601, RMSE 0.47191                     | Test: Loss 0.81436, R2 0.77020, RMSE 0.89510\n",
      "Epoch [413/500]      | Train: Loss 0.21740, R2 0.93803, RMSE 0.46557                     | Test: Loss 0.82483, R2 0.75610, RMSE 0.90060\n",
      "Epoch [414/500]      | Train: Loss 0.23139, R2 0.93405, RMSE 0.47995                     | Test: Loss 0.94509, R2 0.76808, RMSE 0.94117\n",
      "Epoch [415/500]      | Train: Loss 0.22747, R2 0.93515, RMSE 0.47617                     | Test: Loss 0.79365, R2 0.77427, RMSE 0.88240\n",
      "Epoch [416/500]      | Train: Loss 0.22099, R2 0.93784, RMSE 0.46885                     | Test: Loss 0.77153, R2 0.78361, RMSE 0.86491\n",
      "Epoch [417/500]      | Train: Loss 0.22237, R2 0.93653, RMSE 0.47079                     | Test: Loss 0.80769, R2 0.77973, RMSE 0.89488\n",
      "Epoch [418/500]      | Train: Loss 0.22311, R2 0.93693, RMSE 0.47153                     | Test: Loss 0.84105, R2 0.77328, RMSE 0.90817\n",
      "Epoch [419/500]      | Train: Loss 0.21588, R2 0.93886, RMSE 0.46405                     | Test: Loss 0.80556, R2 0.76652, RMSE 0.89391\n",
      "Epoch [420/500]      | Train: Loss 0.22006, R2 0.93808, RMSE 0.46840                     | Test: Loss 0.80208, R2 0.77508, RMSE 0.88653\n",
      "Epoch [421/500]      | Train: Loss 0.21641, R2 0.93855, RMSE 0.46435                     | Test: Loss 0.80523, R2 0.78413, RMSE 0.89145\n",
      "Epoch [422/500]      | Train: Loss 0.22120, R2 0.93738, RMSE 0.46907                     | Test: Loss 0.81356, R2 0.76853, RMSE 0.89024\n",
      "Epoch [423/500]      | Train: Loss 0.21497, R2 0.93900, RMSE 0.46265                     | Test: Loss 0.76952, R2 0.77840, RMSE 0.87268\n",
      "Epoch [424/500]      | Train: Loss 0.21318, R2 0.93970, RMSE 0.46091                     | Test: Loss 0.78062, R2 0.78099, RMSE 0.87659\n",
      "Epoch [425/500]      | Train: Loss 0.21450, R2 0.93918, RMSE 0.46225                     | Test: Loss 0.77822, R2 0.78679, RMSE 0.87078\n",
      "Epoch [426/500]      | Train: Loss 0.21298, R2 0.93957, RMSE 0.46045                     | Test: Loss 0.78883, R2 0.77990, RMSE 0.88274\n",
      "Epoch [427/500]      | Train: Loss 0.22210, R2 0.93706, RMSE 0.46983                     | Test: Loss 0.82834, R2 0.77455, RMSE 0.90293\n",
      "Epoch [428/500]      | Train: Loss 0.21976, R2 0.93809, RMSE 0.46754                     | Test: Loss 0.82277, R2 0.76643, RMSE 0.90139\n",
      "Epoch [429/500]      | Train: Loss 0.21223, R2 0.94038, RMSE 0.45973                     | Test: Loss 0.78905, R2 0.78904, RMSE 0.87989\n",
      "Epoch [430/500]      | Train: Loss 0.20737, R2 0.94078, RMSE 0.45474                     | Test: Loss 0.89496, R2 0.76471, RMSE 0.93191\n",
      "Epoch [431/500]      | Train: Loss 0.20400, R2 0.94277, RMSE 0.45071                     | Test: Loss 0.81951, R2 0.78179, RMSE 0.89858\n",
      "Epoch [432/500]      | Train: Loss 0.20831, R2 0.94108, RMSE 0.45548                     | Test: Loss 0.85793, R2 0.75631, RMSE 0.91598\n",
      "Epoch [433/500]      | Train: Loss 0.21336, R2 0.93963, RMSE 0.46098                     | Test: Loss 0.88295, R2 0.75105, RMSE 0.92979\n",
      "Epoch [434/500]      | Train: Loss 0.21320, R2 0.93969, RMSE 0.46108                     | Test: Loss 0.83965, R2 0.75527, RMSE 0.91193\n",
      "Epoch [435/500]      | Train: Loss 0.21076, R2 0.94036, RMSE 0.45796                     | Test: Loss 0.84980, R2 0.78184, RMSE 0.91539\n",
      "Epoch [436/500]      | Train: Loss 0.20144, R2 0.94299, RMSE 0.44797                     | Test: Loss 0.78875, R2 0.78367, RMSE 0.88271\n",
      "Epoch [437/500]      | Train: Loss 0.20942, R2 0.94038, RMSE 0.45685                     | Test: Loss 0.83625, R2 0.76880, RMSE 0.91009\n",
      "Epoch [438/500]      | Train: Loss 0.21491, R2 0.93898, RMSE 0.46261                     | Test: Loss 0.77705, R2 0.77988, RMSE 0.87073\n",
      "Epoch [439/500]      | Train: Loss 0.20972, R2 0.94088, RMSE 0.45731                     | Test: Loss 0.77808, R2 0.79064, RMSE 0.87194\n",
      "Epoch [440/500]      | Train: Loss 0.19898, R2 0.94383, RMSE 0.44537                     | Test: Loss 0.79541, R2 0.77633, RMSE 0.88062\n",
      "Epoch [441/500]      | Train: Loss 0.20464, R2 0.94258, RMSE 0.45152                     | Test: Loss 0.85243, R2 0.76936, RMSE 0.91579\n",
      "Epoch [442/500]      | Train: Loss 0.20687, R2 0.94164, RMSE 0.45403                     | Test: Loss 0.82146, R2 0.77315, RMSE 0.90100\n",
      "Epoch [443/500]      | Train: Loss 0.20818, R2 0.94053, RMSE 0.45534                     | Test: Loss 0.83938, R2 0.75591, RMSE 0.91223\n",
      "Epoch [444/500]      | Train: Loss 0.20558, R2 0.94239, RMSE 0.45239                     | Test: Loss 0.79360, R2 0.78026, RMSE 0.87886\n",
      "Epoch [445/500]      | Train: Loss 0.20393, R2 0.94250, RMSE 0.45095                     | Test: Loss 0.79056, R2 0.77389, RMSE 0.88043\n",
      "Epoch [446/500]      | Train: Loss 0.21049, R2 0.94110, RMSE 0.45781                     | Test: Loss 0.81725, R2 0.77670, RMSE 0.89765\n",
      "Epoch [447/500]      | Train: Loss 0.21258, R2 0.93997, RMSE 0.46001                     | Test: Loss 0.79734, R2 0.77500, RMSE 0.88362\n",
      "Epoch [448/500]      | Train: Loss 0.20527, R2 0.94178, RMSE 0.45221                     | Test: Loss 0.82280, R2 0.77696, RMSE 0.90279\n",
      "Epoch [449/500]      | Train: Loss 0.19284, R2 0.94536, RMSE 0.43846                     | Test: Loss 0.77976, R2 0.78879, RMSE 0.87302\n",
      "Epoch [450/500]      | Train: Loss 0.19636, R2 0.94445, RMSE 0.44197                     | Test: Loss 0.83732, R2 0.77728, RMSE 0.90638\n",
      "Epoch [451/500]      | Train: Loss 0.19604, R2 0.94490, RMSE 0.44207                     | Test: Loss 0.83281, R2 0.74251, RMSE 0.90883\n",
      "Epoch [452/500]      | Train: Loss 0.19478, R2 0.94492, RMSE 0.44045                     | Test: Loss 0.86043, R2 0.75443, RMSE 0.91929\n",
      "Epoch [453/500]      | Train: Loss 0.19444, R2 0.94465, RMSE 0.44010                     | Test: Loss 0.80994, R2 0.76867, RMSE 0.88715\n",
      "Epoch [454/500]      | Train: Loss 0.19984, R2 0.94368, RMSE 0.44646                     | Test: Loss 0.81660, R2 0.77019, RMSE 0.89597\n",
      "Epoch [455/500]      | Train: Loss 0.19783, R2 0.94354, RMSE 0.44356                     | Test: Loss 0.83493, R2 0.77662, RMSE 0.90987\n",
      "Epoch [456/500]      | Train: Loss 0.19672, R2 0.94433, RMSE 0.44250                     | Test: Loss 0.88738, R2 0.77456, RMSE 0.92614\n",
      "Epoch [457/500]      | Train: Loss 0.19356, R2 0.94574, RMSE 0.43881                     | Test: Loss 0.82175, R2 0.76479, RMSE 0.89872\n",
      "Epoch [458/500]      | Train: Loss 0.19232, R2 0.94504, RMSE 0.43784                     | Test: Loss 0.81447, R2 0.77614, RMSE 0.89606\n",
      "Epoch [459/500]      | Train: Loss 0.19110, R2 0.94611, RMSE 0.43662                     | Test: Loss 0.88269, R2 0.77079, RMSE 0.92703\n",
      "Epoch [460/500]      | Train: Loss 0.19144, R2 0.94506, RMSE 0.43689                     | Test: Loss 0.82326, R2 0.77295, RMSE 0.89660\n",
      "Epoch [461/500]      | Train: Loss 0.19360, R2 0.94514, RMSE 0.43938                     | Test: Loss 0.80254, R2 0.77716, RMSE 0.88682\n",
      "Epoch [462/500]      | Train: Loss 0.19258, R2 0.94560, RMSE 0.43809                     | Test: Loss 0.78226, R2 0.78731, RMSE 0.87276\n",
      "Epoch [463/500]      | Train: Loss 0.19454, R2 0.94526, RMSE 0.44021                     | Test: Loss 0.87215, R2 0.77156, RMSE 0.92359\n",
      "Epoch [464/500]      | Train: Loss 0.19091, R2 0.94602, RMSE 0.43589                     | Test: Loss 0.78727, R2 0.77129, RMSE 0.87818\n",
      "Epoch [465/500]      | Train: Loss 0.19204, R2 0.94569, RMSE 0.43735                     | Test: Loss 0.88813, R2 0.73697, RMSE 0.93467\n",
      "Epoch [466/500]      | Train: Loss 0.18768, R2 0.94680, RMSE 0.43221                     | Test: Loss 0.82051, R2 0.77001, RMSE 0.89853\n",
      "Epoch [467/500]      | Train: Loss 0.18667, R2 0.94730, RMSE 0.43104                     | Test: Loss 0.85327, R2 0.75755, RMSE 0.91491\n",
      "Epoch [468/500]      | Train: Loss 0.18698, R2 0.94673, RMSE 0.43158                     | Test: Loss 0.79953, R2 0.77995, RMSE 0.88604\n",
      "Epoch [469/500]      | Train: Loss 0.19093, R2 0.94585, RMSE 0.43611                     | Test: Loss 0.79854, R2 0.77526, RMSE 0.88321\n",
      "Epoch [470/500]      | Train: Loss 0.18874, R2 0.94641, RMSE 0.43353                     | Test: Loss 0.79707, R2 0.78374, RMSE 0.88011\n",
      "Epoch [471/500]      | Train: Loss 0.18914, R2 0.94660, RMSE 0.43429                     | Test: Loss 0.82910, R2 0.77631, RMSE 0.90500\n",
      "Epoch [472/500]      | Train: Loss 0.19125, R2 0.94592, RMSE 0.43646                     | Test: Loss 0.80555, R2 0.77349, RMSE 0.89144\n",
      "Epoch [473/500]      | Train: Loss 0.18769, R2 0.94673, RMSE 0.43234                     | Test: Loss 0.81257, R2 0.77962, RMSE 0.89746\n",
      "Epoch [474/500]      | Train: Loss 0.18900, R2 0.94641, RMSE 0.43387                     | Test: Loss 0.84004, R2 0.77651, RMSE 0.90912\n",
      "Epoch [475/500]      | Train: Loss 0.18899, R2 0.94677, RMSE 0.43380                     | Test: Loss 0.83213, R2 0.77847, RMSE 0.90943\n",
      "Epoch [476/500]      | Train: Loss 0.18017, R2 0.94902, RMSE 0.42392                     | Test: Loss 0.89650, R2 0.76327, RMSE 0.93281\n",
      "Epoch [477/500]      | Train: Loss 0.18838, R2 0.94626, RMSE 0.43282                     | Test: Loss 0.81512, R2 0.75120, RMSE 0.89423\n",
      "Epoch [478/500]      | Train: Loss 0.18023, R2 0.94980, RMSE 0.42344                     | Test: Loss 0.84970, R2 0.76910, RMSE 0.91335\n",
      "Epoch [479/500]      | Train: Loss 0.19159, R2 0.94590, RMSE 0.43655                     | Test: Loss 0.80073, R2 0.77885, RMSE 0.88724\n",
      "Epoch [480/500]      | Train: Loss 0.18807, R2 0.94709, RMSE 0.43262                     | Test: Loss 0.79183, R2 0.78600, RMSE 0.87939\n",
      "Epoch [481/500]      | Train: Loss 0.17884, R2 0.94955, RMSE 0.42209                     | Test: Loss 0.79455, R2 0.77397, RMSE 0.88233\n",
      "Epoch [482/500]      | Train: Loss 0.18351, R2 0.94824, RMSE 0.42748                     | Test: Loss 0.80857, R2 0.77960, RMSE 0.89244\n",
      "Epoch [483/500]      | Train: Loss 0.17800, R2 0.94944, RMSE 0.42111                     | Test: Loss 0.80160, R2 0.77874, RMSE 0.89086\n",
      "Epoch [484/500]      | Train: Loss 0.17877, R2 0.94918, RMSE 0.42217                     | Test: Loss 0.81482, R2 0.77677, RMSE 0.89633\n",
      "Epoch [485/500]      | Train: Loss 0.18082, R2 0.94877, RMSE 0.42416                     | Test: Loss 0.83381, R2 0.76358, RMSE 0.90901\n",
      "Epoch [486/500]      | Train: Loss 0.18489, R2 0.94709, RMSE 0.42918                     | Test: Loss 0.80964, R2 0.77196, RMSE 0.89230\n",
      "Epoch [487/500]      | Train: Loss 0.17421, R2 0.95068, RMSE 0.41673                     | Test: Loss 0.83984, R2 0.76066, RMSE 0.91189\n",
      "Epoch [488/500]      | Train: Loss 0.17866, R2 0.94960, RMSE 0.42160                     | Test: Loss 0.80163, R2 0.78033, RMSE 0.88903\n",
      "Epoch [489/500]      | Train: Loss 0.17979, R2 0.94909, RMSE 0.42338                     | Test: Loss 0.81763, R2 0.76390, RMSE 0.89887\n",
      "Epoch [490/500]      | Train: Loss 0.17289, R2 0.95154, RMSE 0.41508                     | Test: Loss 0.87779, R2 0.76806, RMSE 0.92871\n",
      "Epoch [491/500]      | Train: Loss 0.18397, R2 0.94834, RMSE 0.42787                     | Test: Loss 0.98945, R2 0.73262, RMSE 0.96883\n",
      "Epoch [492/500]      | Train: Loss 0.17616, R2 0.95018, RMSE 0.41900                     | Test: Loss 0.82731, R2 0.76748, RMSE 0.90370\n",
      "Epoch [493/500]      | Train: Loss 0.18434, R2 0.94781, RMSE 0.42844                     | Test: Loss 0.80989, R2 0.77591, RMSE 0.89239\n",
      "Epoch [494/500]      | Train: Loss 0.17616, R2 0.95049, RMSE 0.41896                     | Test: Loss 0.82075, R2 0.76998, RMSE 0.90160\n",
      "Epoch [495/500]      | Train: Loss 0.17491, R2 0.95041, RMSE 0.41754                     | Test: Loss 0.81260, R2 0.78283, RMSE 0.89571\n",
      "Epoch [496/500]      | Train: Loss 0.17649, R2 0.94985, RMSE 0.41940                     | Test: Loss 0.85295, R2 0.73895, RMSE 0.91813\n",
      "Epoch [497/500]      | Train: Loss 0.17870, R2 0.94927, RMSE 0.42211                     | Test: Loss 0.83598, R2 0.78008, RMSE 0.91024\n",
      "Epoch [498/500]      | Train: Loss 0.17407, R2 0.95089, RMSE 0.41606                     | Test: Loss 0.82440, R2 0.77960, RMSE 0.90038\n",
      "Epoch [499/500]      | Train: Loss 0.17702, R2 0.95007, RMSE 0.41998                     | Test: Loss 0.82313, R2 0.75957, RMSE 0.90264\n",
      "Epoch [500/500]      | Train: Loss 0.18010, R2 0.94901, RMSE 0.42355                     | Test: Loss 0.85022, R2 0.76852, RMSE 0.91416\n",
      "Best rmse 0.8144613343935746\n",
      "100 epochs of training and evaluation took, 4250.534773039999\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class LossFunction(nn.Module):\n",
    "    def __init__(self, num_tasks):\n",
    "        super(LossFunction, self).__init__()\n",
    "        self.num_tasks = num_tasks\n",
    "\n",
    "        self.loss_fns = [nn.MSELoss() for x in range(num_tasks)] \n",
    "\n",
    "    def forward(self, predictions, labels_task1):\n",
    "\n",
    "        #task 1\n",
    "        target = labels_task1\n",
    "        prediction = predictions\n",
    "        loss_fn = self.loss_fns[0]\n",
    "\n",
    "        task_loss = loss_fn(prediction, target)\n",
    "        \n",
    "        return task_loss\n",
    "    \n",
    "model = CATTransformer(rff_on=True,\n",
    "                       sigma=2,\n",
    "                       embed_size=160,\n",
    "                       heads=5,\n",
    "                       forward_expansion=8,\n",
    "                       pre_norm_on=False,\n",
    "                       mlp_scale_classification=8,\n",
    "                  \n",
    "                       decoder_dropout=0,\n",
    "                       classification_dropout=0.1,\n",
    "                       targets_classes=target_classes,\n",
    "                       n_cont=len(cont_columns),\n",
    "                     \n",
    "                       embedding_scheme='log-linear_periodic',\n",
    "                       trainable=True,\n",
    "                       linear_on=True).to(device_in_use)\n",
    "\n",
    "def format_metric(value): # Used to format the metrics output\n",
    "    return f\"{value:.5f}\"\n",
    "\n",
    "loss_functions = LossFunction(1)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr = 0.0001) # Maybe try messing around with optimizers. try other torch optimizers with different configurations.\n",
    "epochs = 500 #Set the number of epochs\n",
    "\n",
    "train_losses = []\n",
    "train_rmse = [] \n",
    "test_losses = []\n",
    "test_rmse = []  \n",
    "\n",
    "\n",
    "#Time will be recorded for all 100 epochs - This means the results will not be comparable to Xgboost but that is ok, we will only compare between transformer models who will also train for 100 epochs\n",
    "start_time = time.process_time()\n",
    "\n",
    "for t in range(epochs):\n",
    "    train_loss, r2_train, rmse_train = train(train_dataloader, model, loss_functions, optimizer, device_in_use=device_in_use)\n",
    "    test_loss, r2_test, rmse_test = test(test_dataloader, model, loss_functions, device_in_use=device_in_use)\n",
    "    train_losses.append(train_loss)\n",
    "    train_rmse.append(rmse_train)\n",
    "    # train_accuracies_2.append(train_accuracy_2)\n",
    "    # train_recalls.append(train_recall) \n",
    "    # train_f1_scores.append(train_f1)\n",
    "    test_losses.append(test_loss)\n",
    "    test_rmse.append(rmse_test)\n",
    "    # test_accuracies_2.append(test_accuracy_2)\n",
    "    # test_recalls.append(test_recall)\n",
    "    # test_f1_scores.append(test_f1)\n",
    "    # Formatting for easier reading\n",
    "    epoch_str = f\"Epoch [{t+1:2}/{epochs}]\"\n",
    "    train_metrics = f\"Train: Loss {format_metric(train_loss)}, R2 {format_metric(r2_train)}, RMSE {format_metric(rmse_train)}\"\n",
    "    test_metrics = f\"Test: Loss {format_metric(test_loss)}, R2 {format_metric(r2_test)}, RMSE {format_metric(rmse_test)}\"\n",
    "    print(f\"{epoch_str:20} | {train_metrics:65} | {test_metrics}\")\n",
    "\n",
    "total_time = time.process_time() - start_time\n",
    "\n",
    "# Plotting the loss curves\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, epochs+1), train_losses, label='Train Loss')\n",
    "plt.plot(range(1, epochs+1), [l for l in test_losses], label='Test Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Test Loss Curve')\n",
    "plt.legend()\n",
    "\n",
    "best_index = test_rmse.index(min(test_rmse))\n",
    "print(f\"Best rmse {test_rmse[best_index]}\")\n",
    "print(f\"100 epochs of training and evaluation took, {total_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log-Linear RFF to each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [ 1/500]       | Train: Loss 4.27046, R2 -0.18312, RMSE 2.02449                    | Test: Loss 3.20598, R2 0.13351, RMSE 1.78079\n",
      "Epoch [ 2/500]       | Train: Loss 1.99810, R2 0.44766, RMSE 1.39470                     | Test: Loss 1.25262, R2 0.65747, RMSE 1.11108\n",
      "Epoch [ 3/500]       | Train: Loss 1.37158, R2 0.61754, RMSE 1.16594                     | Test: Loss 1.17078, R2 0.68181, RMSE 1.07299\n",
      "Epoch [ 4/500]       | Train: Loss 1.23509, R2 0.65803, RMSE 1.10366                     | Test: Loss 1.04044, R2 0.71702, RMSE 1.01491\n",
      "Epoch [ 5/500]       | Train: Loss 1.13456, R2 0.68244, RMSE 1.06051                     | Test: Loss 0.93105, R2 0.74011, RMSE 0.95724\n",
      "Epoch [ 6/500]       | Train: Loss 1.06011, R2 0.70569, RMSE 1.02410                     | Test: Loss 0.94710, R2 0.74667, RMSE 0.96847\n",
      "Epoch [ 7/500]       | Train: Loss 1.03389, R2 0.71293, RMSE 1.01135                     | Test: Loss 0.94099, R2 0.75480, RMSE 0.96471\n",
      "Epoch [ 8/500]       | Train: Loss 1.01601, R2 0.71737, RMSE 1.00217                     | Test: Loss 0.92339, R2 0.73274, RMSE 0.95209\n",
      "Epoch [ 9/500]       | Train: Loss 1.01524, R2 0.71619, RMSE 1.00201                     | Test: Loss 0.91413, R2 0.74926, RMSE 0.94993\n",
      "Epoch [10/500]       | Train: Loss 0.99716, R2 0.72109, RMSE 0.99464                     | Test: Loss 0.88899, R2 0.76009, RMSE 0.93676\n",
      "Epoch [11/500]       | Train: Loss 0.96718, R2 0.72970, RMSE 0.97796                     | Test: Loss 0.88290, R2 0.76543, RMSE 0.93515\n",
      "Epoch [12/500]       | Train: Loss 0.98107, R2 0.72575, RMSE 0.98472                     | Test: Loss 0.91322, R2 0.75007, RMSE 0.95025\n",
      "Epoch [13/500]       | Train: Loss 0.96273, R2 0.72986, RMSE 0.97688                     | Test: Loss 0.91182, R2 0.74644, RMSE 0.94870\n",
      "Epoch [14/500]       | Train: Loss 0.96084, R2 0.73014, RMSE 0.97590                     | Test: Loss 0.85925, R2 0.76573, RMSE 0.92060\n",
      "Epoch [15/500]       | Train: Loss 0.94163, R2 0.73632, RMSE 0.96571                     | Test: Loss 0.82308, R2 0.77242, RMSE 0.89962\n",
      "Epoch [16/500]       | Train: Loss 0.94206, R2 0.73551, RMSE 0.96709                     | Test: Loss 0.84142, R2 0.77622, RMSE 0.91139\n",
      "Epoch [17/500]       | Train: Loss 0.95478, R2 0.73398, RMSE 0.97203                     | Test: Loss 0.85449, R2 0.75755, RMSE 0.91426\n",
      "Epoch [18/500]       | Train: Loss 0.93309, R2 0.73915, RMSE 0.96133                     | Test: Loss 0.82767, R2 0.76461, RMSE 0.90468\n",
      "Epoch [19/500]       | Train: Loss 0.92133, R2 0.74303, RMSE 0.95615                     | Test: Loss 0.96321, R2 0.74998, RMSE 0.97719\n",
      "Epoch [20/500]       | Train: Loss 0.94572, R2 0.73631, RMSE 0.96797                     | Test: Loss 0.84526, R2 0.77663, RMSE 0.91292\n",
      "Epoch [21/500]       | Train: Loss 0.91807, R2 0.74349, RMSE 0.95365                     | Test: Loss 0.80284, R2 0.77922, RMSE 0.88957\n",
      "Epoch [22/500]       | Train: Loss 0.90992, R2 0.74586, RMSE 0.95100                     | Test: Loss 0.84262, R2 0.76704, RMSE 0.91316\n",
      "Epoch [23/500]       | Train: Loss 0.89203, R2 0.75100, RMSE 0.93995                     | Test: Loss 0.82833, R2 0.75930, RMSE 0.90629\n",
      "Epoch [24/500]       | Train: Loss 0.90096, R2 0.74855, RMSE 0.94525                     | Test: Loss 0.81353, R2 0.78094, RMSE 0.89730\n",
      "Epoch [25/500]       | Train: Loss 0.88789, R2 0.75169, RMSE 0.93623                     | Test: Loss 0.91518, R2 0.74372, RMSE 0.95267\n",
      "Epoch [26/500]       | Train: Loss 0.92117, R2 0.74418, RMSE 0.95327                     | Test: Loss 0.80807, R2 0.77517, RMSE 0.89250\n",
      "Epoch [27/500]       | Train: Loss 0.89176, R2 0.75210, RMSE 0.93976                     | Test: Loss 0.78155, R2 0.78887, RMSE 0.87803\n",
      "Epoch [28/500]       | Train: Loss 0.88028, R2 0.75270, RMSE 0.93377                     | Test: Loss 0.79923, R2 0.78805, RMSE 0.88946\n",
      "Epoch [29/500]       | Train: Loss 0.87530, R2 0.75592, RMSE 0.93098                     | Test: Loss 0.77822, R2 0.78150, RMSE 0.87524\n",
      "Epoch [30/500]       | Train: Loss 0.86448, R2 0.75747, RMSE 0.92419                     | Test: Loss 0.81034, R2 0.77492, RMSE 0.89481\n",
      "Epoch [31/500]       | Train: Loss 0.86239, R2 0.75758, RMSE 0.92437                     | Test: Loss 0.78102, R2 0.78817, RMSE 0.87629\n",
      "Epoch [32/500]       | Train: Loss 0.87552, R2 0.75359, RMSE 0.93011                     | Test: Loss 0.77010, R2 0.78837, RMSE 0.86837\n",
      "Epoch [33/500]       | Train: Loss 0.86954, R2 0.75701, RMSE 0.92801                     | Test: Loss 0.80474, R2 0.78136, RMSE 0.88963\n",
      "Epoch [34/500]       | Train: Loss 0.84841, R2 0.76356, RMSE 0.91704                     | Test: Loss 0.78611, R2 0.78549, RMSE 0.88060\n",
      "Epoch [35/500]       | Train: Loss 0.85685, R2 0.75975, RMSE 0.92191                     | Test: Loss 0.82414, R2 0.76149, RMSE 0.90007\n",
      "Epoch [36/500]       | Train: Loss 0.83187, R2 0.76693, RMSE 0.90733                     | Test: Loss 0.87580, R2 0.75482, RMSE 0.92802\n",
      "Epoch [37/500]       | Train: Loss 0.85561, R2 0.76145, RMSE 0.92081                     | Test: Loss 0.78373, R2 0.78457, RMSE 0.87693\n",
      "Epoch [38/500]       | Train: Loss 0.84036, R2 0.76537, RMSE 0.91128                     | Test: Loss 0.79104, R2 0.78400, RMSE 0.88494\n",
      "Epoch [39/500]       | Train: Loss 0.83704, R2 0.76701, RMSE 0.90964                     | Test: Loss 0.81687, R2 0.76937, RMSE 0.89872\n",
      "Epoch [40/500]       | Train: Loss 0.85263, R2 0.76239, RMSE 0.91907                     | Test: Loss 0.77002, R2 0.78221, RMSE 0.86937\n",
      "Epoch [41/500]       | Train: Loss 0.81115, R2 0.77290, RMSE 0.89752                     | Test: Loss 0.81665, R2 0.78262, RMSE 0.89886\n",
      "Epoch [42/500]       | Train: Loss 0.83414, R2 0.76594, RMSE 0.90920                     | Test: Loss 0.81591, R2 0.74563, RMSE 0.89440\n",
      "Epoch [43/500]       | Train: Loss 0.86283, R2 0.75787, RMSE 0.92578                     | Test: Loss 0.78761, R2 0.77949, RMSE 0.87695\n",
      "Epoch [44/500]       | Train: Loss 0.82927, R2 0.76857, RMSE 0.90578                     | Test: Loss 1.11021, R2 0.75547, RMSE 0.97508\n",
      "Epoch [45/500]       | Train: Loss 0.81083, R2 0.77426, RMSE 0.89704                     | Test: Loss 0.77352, R2 0.78058, RMSE 0.87594\n",
      "Epoch [46/500]       | Train: Loss 0.79986, R2 0.77536, RMSE 0.89167                     | Test: Loss 0.79676, R2 0.78770, RMSE 0.88554\n",
      "Epoch [47/500]       | Train: Loss 0.80019, R2 0.77534, RMSE 0.89089                     | Test: Loss 0.77908, R2 0.79019, RMSE 0.87305\n",
      "Epoch [48/500]       | Train: Loss 0.81270, R2 0.77344, RMSE 0.89719                     | Test: Loss 0.77899, R2 0.78996, RMSE 0.87625\n",
      "Epoch [49/500]       | Train: Loss 0.80252, R2 0.77508, RMSE 0.89185                     | Test: Loss 0.75260, R2 0.79660, RMSE 0.85510\n",
      "Epoch [50/500]       | Train: Loss 0.79198, R2 0.77947, RMSE 0.88675                     | Test: Loss 0.77092, R2 0.77667, RMSE 0.87399\n",
      "Epoch [51/500]       | Train: Loss 0.78798, R2 0.77825, RMSE 0.88417                     | Test: Loss 0.75869, R2 0.75949, RMSE 0.86404\n",
      "Epoch [52/500]       | Train: Loss 0.80691, R2 0.77355, RMSE 0.89495                     | Test: Loss 0.75085, R2 0.79551, RMSE 0.85332\n",
      "Epoch [53/500]       | Train: Loss 0.78513, R2 0.78073, RMSE 0.88198                     | Test: Loss 0.83271, R2 0.78817, RMSE 0.89958\n",
      "Epoch [54/500]       | Train: Loss 0.79417, R2 0.77597, RMSE 0.88788                     | Test: Loss 0.79237, R2 0.77654, RMSE 0.88223\n",
      "Epoch [55/500]       | Train: Loss 0.79011, R2 0.77815, RMSE 0.88589                     | Test: Loss 0.80004, R2 0.78337, RMSE 0.89086\n",
      "Epoch [56/500]       | Train: Loss 0.79388, R2 0.77638, RMSE 0.88618                     | Test: Loss 0.78633, R2 0.72212, RMSE 0.88174\n",
      "Epoch [57/500]       | Train: Loss 0.77308, R2 0.78212, RMSE 0.87623                     | Test: Loss 0.75526, R2 0.79710, RMSE 0.86096\n",
      "Epoch [58/500]       | Train: Loss 0.76906, R2 0.78293, RMSE 0.87256                     | Test: Loss 0.79371, R2 0.78185, RMSE 0.88513\n",
      "Epoch [59/500]       | Train: Loss 0.76614, R2 0.78597, RMSE 0.87157                     | Test: Loss 0.76245, R2 0.79604, RMSE 0.86704\n",
      "Epoch [60/500]       | Train: Loss 0.76316, R2 0.78652, RMSE 0.86987                     | Test: Loss 0.78428, R2 0.78459, RMSE 0.88028\n",
      "Epoch [61/500]       | Train: Loss 0.76282, R2 0.78677, RMSE 0.87023                     | Test: Loss 0.74962, R2 0.79360, RMSE 0.85378\n",
      "Epoch [62/500]       | Train: Loss 0.76104, R2 0.78707, RMSE 0.86892                     | Test: Loss 0.75490, R2 0.78630, RMSE 0.85541\n",
      "Epoch [63/500]       | Train: Loss 0.75326, R2 0.78857, RMSE 0.86449                     | Test: Loss 0.77459, R2 0.77990, RMSE 0.87082\n",
      "Epoch [64/500]       | Train: Loss 0.75901, R2 0.78814, RMSE 0.86805                     | Test: Loss 0.74709, R2 0.79166, RMSE 0.85725\n",
      "Epoch [65/500]       | Train: Loss 0.74377, R2 0.79050, RMSE 0.85894                     | Test: Loss 0.72760, R2 0.79716, RMSE 0.84497\n",
      "Epoch [66/500]       | Train: Loss 0.74608, R2 0.79158, RMSE 0.85850                     | Test: Loss 0.72146, R2 0.80134, RMSE 0.84003\n",
      "Epoch [67/500]       | Train: Loss 0.75718, R2 0.78893, RMSE 0.86719                     | Test: Loss 0.76379, R2 0.78597, RMSE 0.86923\n",
      "Epoch [68/500]       | Train: Loss 0.73283, R2 0.79390, RMSE 0.85265                     | Test: Loss 0.72988, R2 0.80016, RMSE 0.85063\n",
      "Epoch [69/500]       | Train: Loss 0.73147, R2 0.79679, RMSE 0.85208                     | Test: Loss 0.76222, R2 0.77449, RMSE 0.86510\n",
      "Epoch [70/500]       | Train: Loss 0.74543, R2 0.79202, RMSE 0.85891                     | Test: Loss 0.74388, R2 0.79781, RMSE 0.85530\n",
      "Epoch [71/500]       | Train: Loss 0.72592, R2 0.79673, RMSE 0.84882                     | Test: Loss 0.85411, R2 0.77618, RMSE 0.91521\n",
      "Epoch [72/500]       | Train: Loss 0.73917, R2 0.79171, RMSE 0.85675                     | Test: Loss 0.72828, R2 0.80540, RMSE 0.84204\n",
      "Epoch [73/500]       | Train: Loss 0.72820, R2 0.79593, RMSE 0.84985                     | Test: Loss 0.74716, R2 0.78534, RMSE 0.86177\n",
      "Epoch [74/500]       | Train: Loss 0.71652, R2 0.79896, RMSE 0.84322                     | Test: Loss 0.73037, R2 0.79299, RMSE 0.84614\n",
      "Epoch [75/500]       | Train: Loss 0.72655, R2 0.79690, RMSE 0.84928                     | Test: Loss 0.73931, R2 0.79758, RMSE 0.85384\n",
      "Epoch [76/500]       | Train: Loss 0.71979, R2 0.79787, RMSE 0.84550                     | Test: Loss 0.70537, R2 0.80447, RMSE 0.83217\n",
      "Epoch [77/500]       | Train: Loss 0.71498, R2 0.79979, RMSE 0.84263                     | Test: Loss 0.74910, R2 0.79895, RMSE 0.85448\n",
      "Epoch [78/500]       | Train: Loss 0.72829, R2 0.79775, RMSE 0.85048                     | Test: Loss 0.70219, R2 0.80875, RMSE 0.82751\n",
      "Epoch [79/500]       | Train: Loss 0.71598, R2 0.80135, RMSE 0.84284                     | Test: Loss 0.71546, R2 0.80530, RMSE 0.83492\n",
      "Epoch [80/500]       | Train: Loss 0.71219, R2 0.79953, RMSE 0.84108                     | Test: Loss 0.71330, R2 0.80269, RMSE 0.83472\n",
      "Epoch [81/500]       | Train: Loss 0.72753, R2 0.79700, RMSE 0.84994                     | Test: Loss 0.76596, R2 0.80036, RMSE 0.86693\n",
      "Epoch [82/500]       | Train: Loss 0.70664, R2 0.80290, RMSE 0.83702                     | Test: Loss 0.71496, R2 0.80701, RMSE 0.83681\n",
      "Epoch [83/500]       | Train: Loss 0.68558, R2 0.80699, RMSE 0.82515                     | Test: Loss 0.74889, R2 0.77736, RMSE 0.85640\n",
      "Epoch [84/500]       | Train: Loss 0.68433, R2 0.80907, RMSE 0.82410                     | Test: Loss 0.73576, R2 0.80461, RMSE 0.85100\n",
      "Epoch [85/500]       | Train: Loss 0.68961, R2 0.80677, RMSE 0.82719                     | Test: Loss 0.75787, R2 0.79466, RMSE 0.86617\n",
      "Epoch [86/500]       | Train: Loss 0.69205, R2 0.80606, RMSE 0.82907                     | Test: Loss 0.70819, R2 0.80352, RMSE 0.83319\n",
      "Epoch [87/500]       | Train: Loss 0.67807, R2 0.80816, RMSE 0.81995                     | Test: Loss 0.73317, R2 0.79583, RMSE 0.84762\n",
      "Epoch [88/500]       | Train: Loss 0.67542, R2 0.81094, RMSE 0.81948                     | Test: Loss 0.69900, R2 0.80882, RMSE 0.82205\n",
      "Epoch [89/500]       | Train: Loss 0.67217, R2 0.81280, RMSE 0.81680                     | Test: Loss 0.72647, R2 0.80377, RMSE 0.84769\n",
      "Epoch [90/500]       | Train: Loss 0.68061, R2 0.80791, RMSE 0.82177                     | Test: Loss 0.84759, R2 0.78637, RMSE 0.90383\n",
      "Epoch [91/500]       | Train: Loss 0.68176, R2 0.80908, RMSE 0.82219                     | Test: Loss 0.73325, R2 0.79053, RMSE 0.85274\n",
      "Epoch [92/500]       | Train: Loss 0.66928, R2 0.81236, RMSE 0.81481                     | Test: Loss 0.74892, R2 0.79357, RMSE 0.85669\n",
      "Epoch [93/500]       | Train: Loss 0.66840, R2 0.81082, RMSE 0.81497                     | Test: Loss 0.72379, R2 0.80155, RMSE 0.84023\n",
      "Epoch [94/500]       | Train: Loss 0.66179, R2 0.81490, RMSE 0.81111                     | Test: Loss 0.73082, R2 0.80042, RMSE 0.84523\n",
      "Epoch [95/500]       | Train: Loss 0.65848, R2 0.81487, RMSE 0.80806                     | Test: Loss 0.71052, R2 0.80050, RMSE 0.83219\n",
      "Epoch [96/500]       | Train: Loss 0.65464, R2 0.81602, RMSE 0.80693                     | Test: Loss 0.77733, R2 0.79446, RMSE 0.87219\n",
      "Epoch [97/500]       | Train: Loss 0.64438, R2 0.82028, RMSE 0.79921                     | Test: Loss 0.72462, R2 0.79931, RMSE 0.84234\n",
      "Epoch [98/500]       | Train: Loss 0.64966, R2 0.81758, RMSE 0.80338                     | Test: Loss 0.73445, R2 0.79802, RMSE 0.84505\n",
      "Epoch [99/500]       | Train: Loss 0.63932, R2 0.81963, RMSE 0.79685                     | Test: Loss 0.75920, R2 0.79563, RMSE 0.86485\n",
      "Epoch [100/500]      | Train: Loss 0.64739, R2 0.81838, RMSE 0.80245                     | Test: Loss 0.73962, R2 0.79217, RMSE 0.85377\n",
      "Epoch [101/500]      | Train: Loss 0.64568, R2 0.81806, RMSE 0.80046                     | Test: Loss 0.70357, R2 0.81357, RMSE 0.82759\n",
      "Epoch [102/500]      | Train: Loss 0.62923, R2 0.82204, RMSE 0.78993                     | Test: Loss 0.68488, R2 0.80800, RMSE 0.81783\n",
      "Epoch [103/500]      | Train: Loss 0.62142, R2 0.82525, RMSE 0.78549                     | Test: Loss 0.71940, R2 0.79010, RMSE 0.84184\n",
      "Epoch [104/500]      | Train: Loss 0.62249, R2 0.82487, RMSE 0.78705                     | Test: Loss 0.74323, R2 0.79953, RMSE 0.85331\n",
      "Epoch [105/500]      | Train: Loss 0.62543, R2 0.82543, RMSE 0.78836                     | Test: Loss 0.70747, R2 0.80877, RMSE 0.82564\n",
      "Epoch [106/500]      | Train: Loss 0.63060, R2 0.82341, RMSE 0.79119                     | Test: Loss 0.71346, R2 0.80699, RMSE 0.83770\n",
      "Epoch [107/500]      | Train: Loss 0.63844, R2 0.81959, RMSE 0.79657                     | Test: Loss 0.70457, R2 0.80256, RMSE 0.83291\n",
      "Epoch [108/500]      | Train: Loss 0.62646, R2 0.82437, RMSE 0.78913                     | Test: Loss 0.70745, R2 0.80898, RMSE 0.83227\n",
      "Epoch [109/500]      | Train: Loss 0.62580, R2 0.82353, RMSE 0.78859                     | Test: Loss 0.73667, R2 0.78770, RMSE 0.85236\n",
      "Epoch [110/500]      | Train: Loss 0.62504, R2 0.82483, RMSE 0.78774                     | Test: Loss 0.72993, R2 0.79197, RMSE 0.84797\n",
      "Epoch [111/500]      | Train: Loss 0.61707, R2 0.82803, RMSE 0.78278                     | Test: Loss 0.71401, R2 0.80427, RMSE 0.83426\n",
      "Epoch [112/500]      | Train: Loss 0.59997, R2 0.82938, RMSE 0.77255                     | Test: Loss 0.69154, R2 0.80920, RMSE 0.82429\n",
      "Epoch [113/500]      | Train: Loss 0.59707, R2 0.83129, RMSE 0.77036                     | Test: Loss 0.73247, R2 0.79825, RMSE 0.84608\n",
      "Epoch [114/500]      | Train: Loss 0.59317, R2 0.83313, RMSE 0.76741                     | Test: Loss 0.69867, R2 0.80339, RMSE 0.82540\n",
      "Epoch [115/500]      | Train: Loss 0.60617, R2 0.83067, RMSE 0.77602                     | Test: Loss 0.73535, R2 0.79959, RMSE 0.84899\n",
      "Epoch [116/500]      | Train: Loss 0.59454, R2 0.83166, RMSE 0.76844                     | Test: Loss 0.70769, R2 0.80867, RMSE 0.83241\n",
      "Epoch [117/500]      | Train: Loss 0.59426, R2 0.83272, RMSE 0.76805                     | Test: Loss 0.68808, R2 0.80354, RMSE 0.82032\n",
      "Epoch [118/500]      | Train: Loss 0.59452, R2 0.83244, RMSE 0.76797                     | Test: Loss 0.70439, R2 0.80220, RMSE 0.83582\n",
      "Epoch [119/500]      | Train: Loss 0.58979, R2 0.83482, RMSE 0.76561                     | Test: Loss 0.74319, R2 0.80928, RMSE 0.85274\n",
      "Epoch [120/500]      | Train: Loss 0.58005, R2 0.83741, RMSE 0.75989                     | Test: Loss 0.69513, R2 0.80083, RMSE 0.82463\n",
      "Epoch [121/500]      | Train: Loss 0.60263, R2 0.82932, RMSE 0.77330                     | Test: Loss 0.73614, R2 0.79265, RMSE 0.85238\n",
      "Epoch [122/500]      | Train: Loss 0.58398, R2 0.83655, RMSE 0.76219                     | Test: Loss 0.72498, R2 0.80036, RMSE 0.84454\n",
      "Epoch [123/500]      | Train: Loss 0.58390, R2 0.83682, RMSE 0.76180                     | Test: Loss 0.72870, R2 0.79972, RMSE 0.84786\n",
      "Epoch [124/500]      | Train: Loss 0.56633, R2 0.83967, RMSE 0.75072                     | Test: Loss 0.75476, R2 0.79810, RMSE 0.86151\n",
      "Epoch [125/500]      | Train: Loss 0.57548, R2 0.83724, RMSE 0.75591                     | Test: Loss 0.72936, R2 0.79102, RMSE 0.84739\n",
      "Epoch [126/500]      | Train: Loss 0.58418, R2 0.83607, RMSE 0.76237                     | Test: Loss 0.70180, R2 0.79616, RMSE 0.82607\n",
      "Epoch [127/500]      | Train: Loss 0.58370, R2 0.83559, RMSE 0.76145                     | Test: Loss 0.70072, R2 0.80748, RMSE 0.83227\n",
      "Epoch [128/500]      | Train: Loss 0.58066, R2 0.83637, RMSE 0.75956                     | Test: Loss 0.69753, R2 0.81228, RMSE 0.82548\n",
      "Epoch [129/500]      | Train: Loss 0.55677, R2 0.84374, RMSE 0.74400                     | Test: Loss 0.71676, R2 0.78969, RMSE 0.84216\n",
      "Epoch [130/500]      | Train: Loss 0.56212, R2 0.84037, RMSE 0.74791                     | Test: Loss 0.73121, R2 0.80531, RMSE 0.85036\n",
      "Epoch [131/500]      | Train: Loss 0.56761, R2 0.84108, RMSE 0.75116                     | Test: Loss 0.79832, R2 0.79969, RMSE 0.87968\n",
      "Epoch [132/500]      | Train: Loss 0.55955, R2 0.84286, RMSE 0.74577                     | Test: Loss 0.78248, R2 0.77478, RMSE 0.87832\n",
      "Epoch [133/500]      | Train: Loss 0.56444, R2 0.84115, RMSE 0.74777                     | Test: Loss 0.71718, R2 0.80066, RMSE 0.84177\n",
      "Epoch [134/500]      | Train: Loss 0.55278, R2 0.84394, RMSE 0.74198                     | Test: Loss 0.67759, R2 0.81034, RMSE 0.81197\n",
      "Epoch [135/500]      | Train: Loss 0.55811, R2 0.84364, RMSE 0.74488                     | Test: Loss 0.76722, R2 0.79562, RMSE 0.86353\n",
      "Epoch [136/500]      | Train: Loss 0.55360, R2 0.84477, RMSE 0.74164                     | Test: Loss 0.77010, R2 0.79846, RMSE 0.86985\n",
      "Epoch [137/500]      | Train: Loss 0.55534, R2 0.84283, RMSE 0.74238                     | Test: Loss 0.73618, R2 0.79442, RMSE 0.84390\n",
      "Epoch [138/500]      | Train: Loss 0.55686, R2 0.84378, RMSE 0.74352                     | Test: Loss 0.88980, R2 0.78859, RMSE 0.91392\n",
      "Epoch [139/500]      | Train: Loss 0.54859, R2 0.84396, RMSE 0.73888                     | Test: Loss 0.72754, R2 0.79498, RMSE 0.84655\n",
      "Epoch [140/500]      | Train: Loss 0.53667, R2 0.84868, RMSE 0.73090                     | Test: Loss 0.72468, R2 0.79779, RMSE 0.84365\n",
      "Epoch [141/500]      | Train: Loss 0.54180, R2 0.84782, RMSE 0.73301                     | Test: Loss 0.70486, R2 0.80269, RMSE 0.83358\n",
      "Epoch [142/500]      | Train: Loss 0.53361, R2 0.85042, RMSE 0.72869                     | Test: Loss 0.74271, R2 0.77908, RMSE 0.85547\n",
      "Epoch [143/500]      | Train: Loss 0.52695, R2 0.85265, RMSE 0.72319                     | Test: Loss 0.69592, R2 0.80538, RMSE 0.82363\n",
      "Epoch [144/500]      | Train: Loss 0.53388, R2 0.84973, RMSE 0.72805                     | Test: Loss 0.74134, R2 0.79153, RMSE 0.85126\n",
      "Epoch [145/500]      | Train: Loss 0.54464, R2 0.84699, RMSE 0.73498                     | Test: Loss 0.74445, R2 0.79290, RMSE 0.85277\n",
      "Epoch [146/500]      | Train: Loss 0.54416, R2 0.84602, RMSE 0.73496                     | Test: Loss 0.74477, R2 0.79484, RMSE 0.85610\n",
      "Epoch [147/500]      | Train: Loss 0.54215, R2 0.84729, RMSE 0.73315                     | Test: Loss 0.70679, R2 0.80099, RMSE 0.83350\n",
      "Epoch [148/500]      | Train: Loss 0.53251, R2 0.84912, RMSE 0.72787                     | Test: Loss 0.71732, R2 0.80557, RMSE 0.84050\n",
      "Epoch [149/500]      | Train: Loss 0.53777, R2 0.84788, RMSE 0.73179                     | Test: Loss 0.74010, R2 0.80018, RMSE 0.85013\n",
      "Epoch [150/500]      | Train: Loss 0.52136, R2 0.85198, RMSE 0.72037                     | Test: Loss 0.69217, R2 0.81026, RMSE 0.82404\n",
      "Epoch [151/500]      | Train: Loss 0.52301, R2 0.85289, RMSE 0.72074                     | Test: Loss 0.72781, R2 0.79409, RMSE 0.84506\n",
      "Epoch [152/500]      | Train: Loss 0.51973, R2 0.85416, RMSE 0.71913                     | Test: Loss 0.76286, R2 0.79639, RMSE 0.86726\n",
      "Epoch [153/500]      | Train: Loss 0.51924, R2 0.85371, RMSE 0.71780                     | Test: Loss 0.68210, R2 0.81617, RMSE 0.81799\n",
      "Epoch [154/500]      | Train: Loss 0.50647, R2 0.85867, RMSE 0.70894                     | Test: Loss 0.74156, R2 0.79578, RMSE 0.85063\n",
      "Epoch [155/500]      | Train: Loss 0.51755, R2 0.85419, RMSE 0.71758                     | Test: Loss 0.78428, R2 0.77978, RMSE 0.87998\n",
      "Epoch [156/500]      | Train: Loss 0.53782, R2 0.84905, RMSE 0.73084                     | Test: Loss 0.71872, R2 0.79934, RMSE 0.84006\n",
      "Epoch [157/500]      | Train: Loss 0.51172, R2 0.85582, RMSE 0.71368                     | Test: Loss 0.71050, R2 0.80526, RMSE 0.82848\n",
      "Epoch [158/500]      | Train: Loss 0.51764, R2 0.85353, RMSE 0.71688                     | Test: Loss 0.73150, R2 0.79921, RMSE 0.84832\n",
      "Epoch [159/500]      | Train: Loss 0.51268, R2 0.85478, RMSE 0.71400                     | Test: Loss 0.71324, R2 0.80353, RMSE 0.83680\n",
      "Epoch [160/500]      | Train: Loss 0.52631, R2 0.85255, RMSE 0.72234                     | Test: Loss 0.74196, R2 0.80022, RMSE 0.85290\n",
      "Epoch [161/500]      | Train: Loss 0.50453, R2 0.85762, RMSE 0.70729                     | Test: Loss 0.70471, R2 0.79992, RMSE 0.83469\n",
      "Epoch [162/500]      | Train: Loss 0.49809, R2 0.85929, RMSE 0.70358                     | Test: Loss 0.71791, R2 0.80043, RMSE 0.83941\n",
      "Epoch [163/500]      | Train: Loss 0.49384, R2 0.86098, RMSE 0.70070                     | Test: Loss 0.72934, R2 0.77987, RMSE 0.84848\n",
      "Epoch [164/500]      | Train: Loss 0.49696, R2 0.86042, RMSE 0.70308                     | Test: Loss 0.73527, R2 0.79678, RMSE 0.85194\n",
      "Epoch [165/500]      | Train: Loss 0.50637, R2 0.85682, RMSE 0.70956                     | Test: Loss 0.73183, R2 0.78643, RMSE 0.84529\n",
      "Epoch [166/500]      | Train: Loss 0.49298, R2 0.86178, RMSE 0.69991                     | Test: Loss 0.70732, R2 0.79628, RMSE 0.83549\n",
      "Epoch [167/500]      | Train: Loss 0.50473, R2 0.85991, RMSE 0.70749                     | Test: Loss 0.77061, R2 0.80073, RMSE 0.86817\n",
      "Epoch [168/500]      | Train: Loss 0.49537, R2 0.86100, RMSE 0.70265                     | Test: Loss 0.69607, R2 0.81026, RMSE 0.82692\n",
      "Epoch [169/500]      | Train: Loss 0.48322, R2 0.86445, RMSE 0.69330                     | Test: Loss 0.72709, R2 0.79768, RMSE 0.84192\n",
      "Epoch [170/500]      | Train: Loss 0.48723, R2 0.86247, RMSE 0.69589                     | Test: Loss 0.81231, R2 0.78645, RMSE 0.88498\n",
      "Epoch [171/500]      | Train: Loss 0.48072, R2 0.86443, RMSE 0.69140                     | Test: Loss 0.69073, R2 0.80104, RMSE 0.82113\n",
      "Epoch [172/500]      | Train: Loss 0.48420, R2 0.86360, RMSE 0.69429                     | Test: Loss 0.69853, R2 0.80434, RMSE 0.83133\n",
      "Epoch [173/500]      | Train: Loss 0.48516, R2 0.86463, RMSE 0.69428                     | Test: Loss 0.78204, R2 0.79362, RMSE 0.87581\n",
      "Epoch [174/500]      | Train: Loss 0.48860, R2 0.86229, RMSE 0.69727                     | Test: Loss 0.70334, R2 0.80787, RMSE 0.83155\n",
      "Epoch [175/500]      | Train: Loss 0.46900, R2 0.86770, RMSE 0.68370                     | Test: Loss 0.68022, R2 0.80608, RMSE 0.81140\n",
      "Epoch [176/500]      | Train: Loss 0.49501, R2 0.86155, RMSE 0.70020                     | Test: Loss 0.71187, R2 0.80156, RMSE 0.83388\n",
      "Epoch [177/500]      | Train: Loss 0.48657, R2 0.86120, RMSE 0.69558                     | Test: Loss 0.77031, R2 0.79285, RMSE 0.87182\n",
      "Epoch [178/500]      | Train: Loss 0.48099, R2 0.86390, RMSE 0.69174                     | Test: Loss 0.79234, R2 0.78158, RMSE 0.87354\n",
      "Epoch [179/500]      | Train: Loss 0.46312, R2 0.87035, RMSE 0.67912                     | Test: Loss 0.68085, R2 0.81460, RMSE 0.81419\n",
      "Epoch [180/500]      | Train: Loss 0.47328, R2 0.86674, RMSE 0.68638                     | Test: Loss 0.74366, R2 0.80358, RMSE 0.85430\n",
      "Epoch [181/500]      | Train: Loss 0.46741, R2 0.86793, RMSE 0.68206                     | Test: Loss 0.70110, R2 0.79586, RMSE 0.83251\n",
      "Epoch [182/500]      | Train: Loss 0.46531, R2 0.86830, RMSE 0.68050                     | Test: Loss 0.73743, R2 0.79893, RMSE 0.84952\n",
      "Epoch [183/500]      | Train: Loss 0.46513, R2 0.86985, RMSE 0.68035                     | Test: Loss 0.70972, R2 0.80681, RMSE 0.83428\n",
      "Epoch [184/500]      | Train: Loss 0.46011, R2 0.87107, RMSE 0.67629                     | Test: Loss 0.74114, R2 0.79721, RMSE 0.85034\n",
      "Epoch [185/500]      | Train: Loss 0.46359, R2 0.87007, RMSE 0.67912                     | Test: Loss 0.70993, R2 0.80306, RMSE 0.83774\n",
      "Epoch [186/500]      | Train: Loss 0.48823, R2 0.86168, RMSE 0.69752                     | Test: Loss 0.72220, R2 0.79071, RMSE 0.84314\n",
      "Epoch [187/500]      | Train: Loss 0.45557, R2 0.87132, RMSE 0.67348                     | Test: Loss 0.74486, R2 0.78833, RMSE 0.85672\n",
      "Epoch [188/500]      | Train: Loss 0.45704, R2 0.87137, RMSE 0.67464                     | Test: Loss 0.77132, R2 0.78623, RMSE 0.86996\n",
      "Epoch [189/500]      | Train: Loss 0.45907, R2 0.87080, RMSE 0.67635                     | Test: Loss 0.72611, R2 0.80451, RMSE 0.84335\n",
      "Epoch [190/500]      | Train: Loss 0.44448, R2 0.87517, RMSE 0.66496                     | Test: Loss 0.81199, R2 0.79417, RMSE 0.87861\n",
      "Epoch [191/500]      | Train: Loss 0.45052, R2 0.87138, RMSE 0.66991                     | Test: Loss 0.74832, R2 0.79644, RMSE 0.85930\n",
      "Epoch [192/500]      | Train: Loss 0.45301, R2 0.87168, RMSE 0.67065                     | Test: Loss 0.72947, R2 0.79053, RMSE 0.84658\n",
      "Epoch [193/500]      | Train: Loss 0.44663, R2 0.87461, RMSE 0.66632                     | Test: Loss 0.70374, R2 0.80667, RMSE 0.83278\n",
      "Epoch [194/500]      | Train: Loss 0.44636, R2 0.87400, RMSE 0.66634                     | Test: Loss 1.03217, R2 0.63210, RMSE 0.94520\n",
      "Epoch [195/500]      | Train: Loss 0.43871, R2 0.87502, RMSE 0.66091                     | Test: Loss 0.73021, R2 0.79570, RMSE 0.84523\n",
      "Epoch [196/500]      | Train: Loss 0.43208, R2 0.87658, RMSE 0.65591                     | Test: Loss 0.73278, R2 0.79444, RMSE 0.84621\n",
      "Epoch [197/500]      | Train: Loss 0.43324, R2 0.87843, RMSE 0.65681                     | Test: Loss 0.70971, R2 0.80381, RMSE 0.83362\n",
      "Epoch [198/500]      | Train: Loss 0.44767, R2 0.87401, RMSE 0.66768                     | Test: Loss 0.76367, R2 0.79826, RMSE 0.87029\n",
      "Epoch [199/500]      | Train: Loss 0.43684, R2 0.87791, RMSE 0.65899                     | Test: Loss 0.76767, R2 0.77148, RMSE 0.86945\n",
      "Epoch [200/500]      | Train: Loss 0.43632, R2 0.87764, RMSE 0.65897                     | Test: Loss 0.76576, R2 0.79427, RMSE 0.86898\n",
      "Epoch [201/500]      | Train: Loss 0.42912, R2 0.87922, RMSE 0.65345                     | Test: Loss 0.72661, R2 0.79774, RMSE 0.84502\n",
      "Epoch [202/500]      | Train: Loss 0.43695, R2 0.87692, RMSE 0.65975                     | Test: Loss 0.76039, R2 0.79055, RMSE 0.86721\n",
      "Epoch [203/500]      | Train: Loss 0.42754, R2 0.88020, RMSE 0.65188                     | Test: Loss 0.73494, R2 0.80201, RMSE 0.85004\n",
      "Epoch [204/500]      | Train: Loss 0.42812, R2 0.87913, RMSE 0.65298                     | Test: Loss 0.74831, R2 0.79761, RMSE 0.85866\n",
      "Epoch [205/500]      | Train: Loss 0.43680, R2 0.87595, RMSE 0.65915                     | Test: Loss 0.71533, R2 0.80442, RMSE 0.84096\n",
      "Epoch [206/500]      | Train: Loss 0.42460, R2 0.87993, RMSE 0.65013                     | Test: Loss 0.92515, R2 0.76937, RMSE 0.93213\n",
      "Epoch [207/500]      | Train: Loss 0.43764, R2 0.87709, RMSE 0.65965                     | Test: Loss 0.77893, R2 0.78239, RMSE 0.87209\n",
      "Epoch [208/500]      | Train: Loss 0.43455, R2 0.87801, RMSE 0.65710                     | Test: Loss 0.85780, R2 0.76685, RMSE 0.90533\n",
      "Epoch [209/500]      | Train: Loss 0.42471, R2 0.87926, RMSE 0.65032                     | Test: Loss 0.73174, R2 0.78461, RMSE 0.85191\n",
      "Epoch [210/500]      | Train: Loss 0.42708, R2 0.88085, RMSE 0.65231                     | Test: Loss 0.72566, R2 0.78691, RMSE 0.84573\n",
      "Epoch [211/500]      | Train: Loss 0.41690, R2 0.88241, RMSE 0.64425                     | Test: Loss 0.75104, R2 0.79037, RMSE 0.85959\n",
      "Epoch [212/500]      | Train: Loss 0.42403, R2 0.87815, RMSE 0.64980                     | Test: Loss 0.72140, R2 0.80844, RMSE 0.83660\n",
      "Epoch [213/500]      | Train: Loss 0.41588, R2 0.88340, RMSE 0.64354                     | Test: Loss 0.73129, R2 0.80027, RMSE 0.84656\n",
      "Epoch [214/500]      | Train: Loss 0.41981, R2 0.88100, RMSE 0.64616                     | Test: Loss 0.77100, R2 0.79029, RMSE 0.87164\n",
      "Epoch [215/500]      | Train: Loss 0.41354, R2 0.88316, RMSE 0.64141                     | Test: Loss 0.73106, R2 0.78898, RMSE 0.84532\n",
      "Epoch [216/500]      | Train: Loss 0.41107, R2 0.88492, RMSE 0.64015                     | Test: Loss 0.71761, R2 0.81054, RMSE 0.83651\n",
      "Epoch [217/500]      | Train: Loss 0.41006, R2 0.88448, RMSE 0.63892                     | Test: Loss 0.78343, R2 0.78304, RMSE 0.87337\n",
      "Epoch [218/500]      | Train: Loss 0.40805, R2 0.88531, RMSE 0.63723                     | Test: Loss 0.74200, R2 0.79851, RMSE 0.84449\n",
      "Epoch [219/500]      | Train: Loss 0.40673, R2 0.88402, RMSE 0.63635                     | Test: Loss 0.77536, R2 0.78667, RMSE 0.87025\n",
      "Epoch [220/500]      | Train: Loss 0.40473, R2 0.88684, RMSE 0.63467                     | Test: Loss 0.73498, R2 0.80342, RMSE 0.85036\n",
      "Epoch [221/500]      | Train: Loss 0.41625, R2 0.88304, RMSE 0.64285                     | Test: Loss 0.71183, R2 0.80423, RMSE 0.82396\n",
      "Epoch [222/500]      | Train: Loss 0.40916, R2 0.88448, RMSE 0.63836                     | Test: Loss 0.76970, R2 0.76636, RMSE 0.86948\n",
      "Epoch [223/500]      | Train: Loss 0.41198, R2 0.88402, RMSE 0.64000                     | Test: Loss 0.75438, R2 0.76671, RMSE 0.86219\n",
      "Epoch [224/500]      | Train: Loss 0.39684, R2 0.88806, RMSE 0.62852                     | Test: Loss 0.86754, R2 0.68432, RMSE 0.91733\n",
      "Epoch [225/500]      | Train: Loss 0.39709, R2 0.88865, RMSE 0.62854                     | Test: Loss 0.72110, R2 0.79289, RMSE 0.84181\n",
      "Epoch [226/500]      | Train: Loss 0.39740, R2 0.88742, RMSE 0.62899                     | Test: Loss 0.71769, R2 0.79635, RMSE 0.84133\n",
      "Epoch [227/500]      | Train: Loss 0.38935, R2 0.88996, RMSE 0.62260                     | Test: Loss 0.73362, R2 0.79417, RMSE 0.84068\n",
      "Epoch [228/500]      | Train: Loss 0.38901, R2 0.88986, RMSE 0.62257                     | Test: Loss 0.78963, R2 0.78444, RMSE 0.88106\n",
      "Epoch [229/500]      | Train: Loss 0.39096, R2 0.89078, RMSE 0.62315                     | Test: Loss 0.77961, R2 0.77510, RMSE 0.87723\n",
      "Epoch [230/500]      | Train: Loss 0.40279, R2 0.88630, RMSE 0.63316                     | Test: Loss 0.72464, R2 0.80668, RMSE 0.84335\n",
      "Epoch [231/500]      | Train: Loss 0.39246, R2 0.88912, RMSE 0.62468                     | Test: Loss 0.74402, R2 0.79799, RMSE 0.85634\n",
      "Epoch [232/500]      | Train: Loss 0.38579, R2 0.89197, RMSE 0.61953                     | Test: Loss 0.79010, R2 0.77945, RMSE 0.87899\n",
      "Epoch [233/500]      | Train: Loss 0.38055, R2 0.89292, RMSE 0.61601                     | Test: Loss 0.73049, R2 0.79647, RMSE 0.84313\n",
      "Epoch [234/500]      | Train: Loss 0.39181, R2 0.88983, RMSE 0.62484                     | Test: Loss 0.74441, R2 0.79576, RMSE 0.85851\n",
      "Epoch [235/500]      | Train: Loss 0.38841, R2 0.89052, RMSE 0.62157                     | Test: Loss 0.76206, R2 0.79459, RMSE 0.86100\n",
      "Epoch [236/500]      | Train: Loss 0.40813, R2 0.88515, RMSE 0.63724                     | Test: Loss 0.76039, R2 0.80045, RMSE 0.86573\n",
      "Epoch [237/500]      | Train: Loss 0.38489, R2 0.89165, RMSE 0.61912                     | Test: Loss 0.77650, R2 0.80300, RMSE 0.86800\n",
      "Epoch [238/500]      | Train: Loss 0.37689, R2 0.89299, RMSE 0.61286                     | Test: Loss 0.75488, R2 0.79336, RMSE 0.85857\n",
      "Epoch [239/500]      | Train: Loss 0.37746, R2 0.89329, RMSE 0.61304                     | Test: Loss 0.72069, R2 0.80390, RMSE 0.83640\n",
      "Epoch [240/500]      | Train: Loss 0.38646, R2 0.89128, RMSE 0.61976                     | Test: Loss 0.76243, R2 0.78703, RMSE 0.86490\n",
      "Epoch [241/500]      | Train: Loss 0.38143, R2 0.89236, RMSE 0.61625                     | Test: Loss 0.77897, R2 0.78183, RMSE 0.87645\n",
      "Epoch [242/500]      | Train: Loss 0.37662, R2 0.89376, RMSE 0.61268                     | Test: Loss 0.71570, R2 0.79894, RMSE 0.83758\n",
      "Epoch [243/500]      | Train: Loss 0.37931, R2 0.89250, RMSE 0.61447                     | Test: Loss 0.77438, R2 0.78900, RMSE 0.87647\n",
      "Epoch [244/500]      | Train: Loss 0.37784, R2 0.89292, RMSE 0.61312                     | Test: Loss 0.75511, R2 0.79533, RMSE 0.86441\n",
      "Epoch [245/500]      | Train: Loss 0.37743, R2 0.89367, RMSE 0.61303                     | Test: Loss 0.76264, R2 0.78234, RMSE 0.86886\n",
      "Epoch [246/500]      | Train: Loss 0.36312, R2 0.89719, RMSE 0.60153                     | Test: Loss 0.83579, R2 0.74041, RMSE 0.89803\n",
      "Epoch [247/500]      | Train: Loss 0.36361, R2 0.89709, RMSE 0.60175                     | Test: Loss 0.85965, R2 0.77628, RMSE 0.91202\n",
      "Epoch [248/500]      | Train: Loss 0.36599, R2 0.89605, RMSE 0.60392                     | Test: Loss 0.75599, R2 0.79174, RMSE 0.86629\n",
      "Epoch [249/500]      | Train: Loss 0.36134, R2 0.89804, RMSE 0.59998                     | Test: Loss 0.78240, R2 0.79245, RMSE 0.87805\n",
      "Epoch [250/500]      | Train: Loss 0.36608, R2 0.89683, RMSE 0.60368                     | Test: Loss 0.74697, R2 0.79004, RMSE 0.85233\n",
      "Epoch [251/500]      | Train: Loss 0.35963, R2 0.89864, RMSE 0.59823                     | Test: Loss 0.77028, R2 0.79217, RMSE 0.86520\n",
      "Epoch [252/500]      | Train: Loss 0.37321, R2 0.89445, RMSE 0.61008                     | Test: Loss 0.79409, R2 0.77908, RMSE 0.88690\n",
      "Epoch [253/500]      | Train: Loss 0.37595, R2 0.89386, RMSE 0.61141                     | Test: Loss 0.74606, R2 0.78220, RMSE 0.85209\n",
      "Epoch [254/500]      | Train: Loss 0.35135, R2 0.90118, RMSE 0.59163                     | Test: Loss 0.83276, R2 0.74408, RMSE 0.89745\n",
      "Epoch [255/500]      | Train: Loss 0.35974, R2 0.89821, RMSE 0.59826                     | Test: Loss 0.82946, R2 0.77448, RMSE 0.90092\n",
      "Epoch [256/500]      | Train: Loss 0.36424, R2 0.89721, RMSE 0.60172                     | Test: Loss 0.75117, R2 0.79249, RMSE 0.85921\n",
      "Epoch [257/500]      | Train: Loss 0.35305, R2 0.89989, RMSE 0.59318                     | Test: Loss 0.74729, R2 0.78121, RMSE 0.86073\n",
      "Epoch [258/500]      | Train: Loss 0.36083, R2 0.89812, RMSE 0.59924                     | Test: Loss 0.76075, R2 0.79562, RMSE 0.86611\n",
      "Epoch [259/500]      | Train: Loss 0.35567, R2 0.90045, RMSE 0.59472                     | Test: Loss 0.75680, R2 0.80103, RMSE 0.85924\n",
      "Epoch [260/500]      | Train: Loss 0.36461, R2 0.89697, RMSE 0.60219                     | Test: Loss 0.76100, R2 0.79335, RMSE 0.86872\n",
      "Epoch [261/500]      | Train: Loss 0.35449, R2 0.89941, RMSE 0.59359                     | Test: Loss 0.86052, R2 0.76799, RMSE 0.91012\n",
      "Epoch [262/500]      | Train: Loss 0.36387, R2 0.89755, RMSE 0.60145                     | Test: Loss 0.77181, R2 0.77828, RMSE 0.87132\n",
      "Epoch [263/500]      | Train: Loss 0.35247, R2 0.90086, RMSE 0.59249                     | Test: Loss 0.74595, R2 0.79428, RMSE 0.85737\n",
      "Epoch [264/500]      | Train: Loss 0.34956, R2 0.90037, RMSE 0.59023                     | Test: Loss 0.75850, R2 0.79723, RMSE 0.86314\n",
      "Epoch [265/500]      | Train: Loss 0.34424, R2 0.90308, RMSE 0.58585                     | Test: Loss 0.73949, R2 0.80400, RMSE 0.84975\n",
      "Epoch [266/500]      | Train: Loss 0.35061, R2 0.90166, RMSE 0.59090                     | Test: Loss 0.77085, R2 0.79134, RMSE 0.86179\n",
      "Epoch [267/500]      | Train: Loss 0.34396, R2 0.90194, RMSE 0.58526                     | Test: Loss 0.78360, R2 0.79053, RMSE 0.87513\n",
      "Epoch [268/500]      | Train: Loss 0.33459, R2 0.90541, RMSE 0.57687                     | Test: Loss 0.74399, R2 0.79734, RMSE 0.85037\n",
      "Epoch [269/500]      | Train: Loss 0.33391, R2 0.90531, RMSE 0.57708                     | Test: Loss 0.74228, R2 0.79506, RMSE 0.85014\n",
      "Epoch [270/500]      | Train: Loss 0.34492, R2 0.90351, RMSE 0.58614                     | Test: Loss 0.78761, R2 0.76524, RMSE 0.88127\n",
      "Epoch [271/500]      | Train: Loss 0.34100, R2 0.90321, RMSE 0.58229                     | Test: Loss 0.82080, R2 0.77037, RMSE 0.89792\n",
      "Epoch [272/500]      | Train: Loss 0.33532, R2 0.90551, RMSE 0.57797                     | Test: Loss 0.78272, R2 0.79365, RMSE 0.87873\n",
      "Epoch [273/500]      | Train: Loss 0.33461, R2 0.90506, RMSE 0.57741                     | Test: Loss 0.76562, R2 0.78565, RMSE 0.86545\n",
      "Epoch [274/500]      | Train: Loss 0.34292, R2 0.90241, RMSE 0.58429                     | Test: Loss 0.74512, R2 0.79367, RMSE 0.85399\n",
      "Epoch [275/500]      | Train: Loss 0.33270, R2 0.90630, RMSE 0.57564                     | Test: Loss 0.77750, R2 0.78720, RMSE 0.87054\n",
      "Epoch [276/500]      | Train: Loss 0.33379, R2 0.90525, RMSE 0.57681                     | Test: Loss 0.77882, R2 0.79128, RMSE 0.87649\n",
      "Epoch [277/500]      | Train: Loss 0.33840, R2 0.90463, RMSE 0.58019                     | Test: Loss 0.81977, R2 0.76437, RMSE 0.90139\n",
      "Epoch [278/500]      | Train: Loss 0.34558, R2 0.90276, RMSE 0.58652                     | Test: Loss 0.78347, R2 0.78871, RMSE 0.87991\n",
      "Epoch [279/500]      | Train: Loss 0.33664, R2 0.90457, RMSE 0.57927                     | Test: Loss 0.77318, R2 0.78561, RMSE 0.86676\n",
      "Epoch [280/500]      | Train: Loss 0.32548, R2 0.90782, RMSE 0.56920                     | Test: Loss 0.77592, R2 0.78861, RMSE 0.87296\n",
      "Epoch [281/500]      | Train: Loss 0.32638, R2 0.90790, RMSE 0.56989                     | Test: Loss 0.76262, R2 0.79204, RMSE 0.86466\n",
      "Epoch [282/500]      | Train: Loss 0.33622, R2 0.90541, RMSE 0.57856                     | Test: Loss 0.77225, R2 0.78991, RMSE 0.87145\n",
      "Epoch [283/500]      | Train: Loss 0.33371, R2 0.90582, RMSE 0.57683                     | Test: Loss 0.75803, R2 0.79533, RMSE 0.86782\n",
      "Epoch [284/500]      | Train: Loss 0.32897, R2 0.90763, RMSE 0.57250                     | Test: Loss 0.79008, R2 0.78453, RMSE 0.88433\n",
      "Epoch [285/500]      | Train: Loss 0.32963, R2 0.90667, RMSE 0.57274                     | Test: Loss 0.78966, R2 0.77409, RMSE 0.88308\n",
      "Epoch [286/500]      | Train: Loss 0.32559, R2 0.90818, RMSE 0.56935                     | Test: Loss 0.77431, R2 0.78670, RMSE 0.87051\n",
      "Epoch [287/500]      | Train: Loss 0.33819, R2 0.90512, RMSE 0.58025                     | Test: Loss 0.79806, R2 0.76904, RMSE 0.88912\n",
      "Epoch [288/500]      | Train: Loss 0.30993, R2 0.91199, RMSE 0.55573                     | Test: Loss 0.73937, R2 0.78875, RMSE 0.85188\n",
      "Epoch [289/500]      | Train: Loss 0.32179, R2 0.90993, RMSE 0.56601                     | Test: Loss 0.80344, R2 0.78157, RMSE 0.88798\n",
      "Epoch [290/500]      | Train: Loss 0.32746, R2 0.90778, RMSE 0.57117                     | Test: Loss 0.74105, R2 0.79528, RMSE 0.84495\n",
      "Epoch [291/500]      | Train: Loss 0.31599, R2 0.91049, RMSE 0.56115                     | Test: Loss 0.76421, R2 0.78834, RMSE 0.86743\n",
      "Epoch [292/500]      | Train: Loss 0.33096, R2 0.90644, RMSE 0.57410                     | Test: Loss 0.76521, R2 0.78014, RMSE 0.86852\n",
      "Epoch [293/500]      | Train: Loss 0.31892, R2 0.90990, RMSE 0.56356                     | Test: Loss 0.80738, R2 0.77181, RMSE 0.89109\n",
      "Epoch [294/500]      | Train: Loss 0.31510, R2 0.91139, RMSE 0.56042                     | Test: Loss 0.80032, R2 0.78909, RMSE 0.89123\n",
      "Epoch [295/500]      | Train: Loss 0.31163, R2 0.91186, RMSE 0.55713                     | Test: Loss 0.75688, R2 0.79572, RMSE 0.85908\n",
      "Epoch [296/500]      | Train: Loss 0.30411, R2 0.91455, RMSE 0.55050                     | Test: Loss 0.77367, R2 0.78464, RMSE 0.86791\n",
      "Epoch [297/500]      | Train: Loss 0.33301, R2 0.90664, RMSE 0.57556                     | Test: Loss 0.76721, R2 0.78111, RMSE 0.86501\n",
      "Epoch [298/500]      | Train: Loss 0.31962, R2 0.90947, RMSE 0.56423                     | Test: Loss 0.77695, R2 0.78031, RMSE 0.86978\n",
      "Epoch [299/500]      | Train: Loss 0.31228, R2 0.91178, RMSE 0.55764                     | Test: Loss 0.87082, R2 0.77322, RMSE 0.92136\n",
      "Epoch [300/500]      | Train: Loss 0.31491, R2 0.91154, RMSE 0.55974                     | Test: Loss 0.82272, R2 0.76145, RMSE 0.90115\n",
      "Epoch [301/500]      | Train: Loss 0.30995, R2 0.91358, RMSE 0.55545                     | Test: Loss 0.77639, R2 0.79238, RMSE 0.87385\n",
      "Epoch [302/500]      | Train: Loss 0.30708, R2 0.91342, RMSE 0.55300                     | Test: Loss 0.82717, R2 0.76577, RMSE 0.90198\n",
      "Epoch [303/500]      | Train: Loss 0.31582, R2 0.91057, RMSE 0.56080                     | Test: Loss 0.81983, R2 0.76001, RMSE 0.89834\n",
      "Epoch [304/500]      | Train: Loss 0.31452, R2 0.91150, RMSE 0.55952                     | Test: Loss 0.84478, R2 0.78712, RMSE 0.90386\n",
      "Epoch [305/500]      | Train: Loss 0.30697, R2 0.91260, RMSE 0.55312                     | Test: Loss 0.77523, R2 0.79448, RMSE 0.86029\n",
      "Epoch [306/500]      | Train: Loss 0.30429, R2 0.91341, RMSE 0.55045                     | Test: Loss 0.78154, R2 0.76778, RMSE 0.87099\n",
      "Epoch [307/500]      | Train: Loss 0.30455, R2 0.91442, RMSE 0.55032                     | Test: Loss 0.82941, R2 0.78539, RMSE 0.89999\n",
      "Epoch [308/500]      | Train: Loss 0.30182, R2 0.91488, RMSE 0.54839                     | Test: Loss 0.78419, R2 0.78756, RMSE 0.88072\n",
      "Epoch [309/500]      | Train: Loss 0.30527, R2 0.91395, RMSE 0.55141                     | Test: Loss 0.78804, R2 0.77877, RMSE 0.87853\n",
      "Epoch [310/500]      | Train: Loss 0.31355, R2 0.91169, RMSE 0.55898                     | Test: Loss 0.75775, R2 0.79437, RMSE 0.86030\n",
      "Epoch [311/500]      | Train: Loss 0.30718, R2 0.91334, RMSE 0.55301                     | Test: Loss 0.80792, R2 0.74844, RMSE 0.88861\n",
      "Epoch [312/500]      | Train: Loss 0.29203, R2 0.91752, RMSE 0.53916                     | Test: Loss 0.76739, R2 0.79036, RMSE 0.86507\n",
      "Epoch [313/500]      | Train: Loss 0.29360, R2 0.91764, RMSE 0.54084                     | Test: Loss 0.80514, R2 0.77524, RMSE 0.89277\n",
      "Epoch [314/500]      | Train: Loss 0.30569, R2 0.91381, RMSE 0.55157                     | Test: Loss 0.77896, R2 0.78839, RMSE 0.87613\n",
      "Epoch [315/500]      | Train: Loss 0.29247, R2 0.91760, RMSE 0.53982                     | Test: Loss 0.79162, R2 0.78650, RMSE 0.88005\n",
      "Epoch [316/500]      | Train: Loss 0.28838, R2 0.91878, RMSE 0.53592                     | Test: Loss 0.79448, R2 0.78753, RMSE 0.88414\n",
      "Epoch [317/500]      | Train: Loss 0.29720, R2 0.91599, RMSE 0.54403                     | Test: Loss 0.79343, R2 0.77534, RMSE 0.88193\n",
      "Epoch [318/500]      | Train: Loss 0.30064, R2 0.91555, RMSE 0.54732                     | Test: Loss 0.78911, R2 0.78253, RMSE 0.87923\n",
      "Epoch [319/500]      | Train: Loss 0.29268, R2 0.91700, RMSE 0.53977                     | Test: Loss 0.79545, R2 0.78240, RMSE 0.88481\n",
      "Epoch [320/500]      | Train: Loss 0.29104, R2 0.91685, RMSE 0.53845                     | Test: Loss 0.88103, R2 0.77278, RMSE 0.93021\n",
      "Epoch [321/500]      | Train: Loss 0.28725, R2 0.91925, RMSE 0.53466                     | Test: Loss 0.78678, R2 0.78473, RMSE 0.87861\n",
      "Epoch [322/500]      | Train: Loss 0.28854, R2 0.91881, RMSE 0.53635                     | Test: Loss 0.78512, R2 0.78059, RMSE 0.87765\n",
      "Epoch [323/500]      | Train: Loss 0.28362, R2 0.91921, RMSE 0.53125                     | Test: Loss 0.81698, R2 0.77488, RMSE 0.89782\n",
      "Epoch [324/500]      | Train: Loss 0.29175, R2 0.91825, RMSE 0.53924                     | Test: Loss 0.84507, R2 0.76909, RMSE 0.91619\n",
      "Epoch [325/500]      | Train: Loss 0.28940, R2 0.91786, RMSE 0.53711                     | Test: Loss 0.77326, R2 0.78658, RMSE 0.86963\n",
      "Epoch [326/500]      | Train: Loss 0.28864, R2 0.91907, RMSE 0.53583                     | Test: Loss 0.80760, R2 0.76410, RMSE 0.89076\n",
      "Epoch [327/500]      | Train: Loss 0.27537, R2 0.92185, RMSE 0.52397                     | Test: Loss 0.76763, R2 0.79394, RMSE 0.86383\n",
      "Epoch [328/500]      | Train: Loss 0.28223, R2 0.92072, RMSE 0.53032                     | Test: Loss 0.79694, R2 0.77249, RMSE 0.88369\n",
      "Epoch [329/500]      | Train: Loss 0.28704, R2 0.91883, RMSE 0.53480                     | Test: Loss 0.82209, R2 0.77617, RMSE 0.89809\n",
      "Epoch [330/500]      | Train: Loss 0.28732, R2 0.91838, RMSE 0.53492                     | Test: Loss 0.79756, R2 0.78390, RMSE 0.88142\n",
      "Epoch [331/500]      | Train: Loss 0.28108, R2 0.92023, RMSE 0.52911                     | Test: Loss 0.78054, R2 0.78595, RMSE 0.87630\n",
      "Epoch [332/500]      | Train: Loss 0.27345, R2 0.92258, RMSE 0.52222                     | Test: Loss 0.77849, R2 0.78943, RMSE 0.87078\n",
      "Epoch [333/500]      | Train: Loss 0.27802, R2 0.92019, RMSE 0.52628                     | Test: Loss 0.80706, R2 0.78382, RMSE 0.89423\n",
      "Epoch [334/500]      | Train: Loss 0.28079, R2 0.92112, RMSE 0.52868                     | Test: Loss 0.79663, R2 0.77361, RMSE 0.88392\n",
      "Epoch [335/500]      | Train: Loss 0.27235, R2 0.92215, RMSE 0.52116                     | Test: Loss 0.90548, R2 0.75349, RMSE 0.93575\n",
      "Epoch [336/500]      | Train: Loss 0.27325, R2 0.92290, RMSE 0.52182                     | Test: Loss 0.77048, R2 0.78406, RMSE 0.86664\n",
      "Epoch [337/500]      | Train: Loss 0.27735, R2 0.92176, RMSE 0.52532                     | Test: Loss 0.81031, R2 0.77144, RMSE 0.89239\n",
      "Epoch [338/500]      | Train: Loss 0.27359, R2 0.92190, RMSE 0.52242                     | Test: Loss 0.80450, R2 0.78683, RMSE 0.88239\n",
      "Epoch [339/500]      | Train: Loss 0.28037, R2 0.92012, RMSE 0.52802                     | Test: Loss 0.80437, R2 0.78266, RMSE 0.88550\n",
      "Epoch [340/500]      | Train: Loss 0.26952, R2 0.92279, RMSE 0.51797                     | Test: Loss 0.79210, R2 0.78813, RMSE 0.88077\n",
      "Epoch [341/500]      | Train: Loss 0.26817, R2 0.92408, RMSE 0.51673                     | Test: Loss 0.79858, R2 0.78212, RMSE 0.88447\n",
      "Epoch [342/500]      | Train: Loss 0.27058, R2 0.92387, RMSE 0.51931                     | Test: Loss 0.82500, R2 0.77435, RMSE 0.89825\n",
      "Epoch [343/500]      | Train: Loss 0.27930, R2 0.92050, RMSE 0.52771                     | Test: Loss 0.86979, R2 0.76929, RMSE 0.92437\n",
      "Epoch [344/500]      | Train: Loss 0.26216, R2 0.92621, RMSE 0.51111                     | Test: Loss 0.82139, R2 0.76787, RMSE 0.90008\n",
      "Epoch [345/500]      | Train: Loss 0.26638, R2 0.92420, RMSE 0.51487                     | Test: Loss 0.81937, R2 0.74264, RMSE 0.89964\n",
      "Epoch [346/500]      | Train: Loss 0.26640, R2 0.92501, RMSE 0.51545                     | Test: Loss 0.80981, R2 0.78001, RMSE 0.89643\n",
      "Epoch [347/500]      | Train: Loss 0.26569, R2 0.92470, RMSE 0.51453                     | Test: Loss 0.80667, R2 0.77047, RMSE 0.88665\n",
      "Epoch [348/500]      | Train: Loss 0.26083, R2 0.92722, RMSE 0.51000                     | Test: Loss 0.86854, R2 0.76673, RMSE 0.92601\n",
      "Epoch [349/500]      | Train: Loss 0.26690, R2 0.92478, RMSE 0.51547                     | Test: Loss 0.78192, R2 0.78372, RMSE 0.86958\n",
      "Epoch [350/500]      | Train: Loss 0.26225, R2 0.92588, RMSE 0.51108                     | Test: Loss 0.78967, R2 0.78613, RMSE 0.88381\n",
      "Epoch [351/500]      | Train: Loss 0.26590, R2 0.92489, RMSE 0.51460                     | Test: Loss 0.84745, R2 0.77547, RMSE 0.91193\n",
      "Epoch [352/500]      | Train: Loss 0.26300, R2 0.92609, RMSE 0.51199                     | Test: Loss 0.81368, R2 0.78139, RMSE 0.89556\n",
      "Epoch [353/500]      | Train: Loss 0.26628, R2 0.92442, RMSE 0.51501                     | Test: Loss 0.80389, R2 0.76581, RMSE 0.88763\n",
      "Epoch [354/500]      | Train: Loss 0.25854, R2 0.92702, RMSE 0.50726                     | Test: Loss 0.84084, R2 0.77754, RMSE 0.91187\n",
      "Epoch [355/500]      | Train: Loss 0.26523, R2 0.92531, RMSE 0.51392                     | Test: Loss 0.80344, R2 0.77628, RMSE 0.88904\n",
      "Epoch [356/500]      | Train: Loss 0.26126, R2 0.92626, RMSE 0.51006                     | Test: Loss 0.80400, R2 0.77795, RMSE 0.88566\n",
      "Epoch [357/500]      | Train: Loss 0.25904, R2 0.92668, RMSE 0.50815                     | Test: Loss 0.82227, R2 0.76547, RMSE 0.90053\n",
      "Epoch [358/500]      | Train: Loss 0.25961, R2 0.92661, RMSE 0.50861                     | Test: Loss 0.80459, R2 0.77537, RMSE 0.88432\n",
      "Epoch [359/500]      | Train: Loss 0.26295, R2 0.92629, RMSE 0.51165                     | Test: Loss 0.84935, R2 0.76515, RMSE 0.91430\n",
      "Epoch [360/500]      | Train: Loss 0.26267, R2 0.92600, RMSE 0.51163                     | Test: Loss 0.83075, R2 0.77004, RMSE 0.90221\n",
      "Epoch [361/500]      | Train: Loss 0.24717, R2 0.93012, RMSE 0.49623                     | Test: Loss 0.86521, R2 0.75885, RMSE 0.92236\n",
      "Epoch [362/500]      | Train: Loss 0.24918, R2 0.92929, RMSE 0.49814                     | Test: Loss 0.80721, R2 0.77204, RMSE 0.89025\n",
      "Epoch [363/500]      | Train: Loss 0.25390, R2 0.92776, RMSE 0.50281                     | Test: Loss 0.81104, R2 0.76906, RMSE 0.89356\n",
      "Epoch [364/500]      | Train: Loss 0.24545, R2 0.93096, RMSE 0.49425                     | Test: Loss 0.82178, R2 0.76072, RMSE 0.89965\n",
      "Epoch [365/500]      | Train: Loss 0.25016, R2 0.92933, RMSE 0.49911                     | Test: Loss 0.89860, R2 0.74499, RMSE 0.93776\n",
      "Epoch [366/500]      | Train: Loss 0.24738, R2 0.92959, RMSE 0.49637                     | Test: Loss 0.83213, R2 0.77238, RMSE 0.90405\n",
      "Epoch [367/500]      | Train: Loss 0.24706, R2 0.93013, RMSE 0.49594                     | Test: Loss 0.81085, R2 0.78002, RMSE 0.88654\n",
      "Epoch [368/500]      | Train: Loss 0.25422, R2 0.92772, RMSE 0.50306                     | Test: Loss 0.80603, R2 0.78412, RMSE 0.89198\n",
      "Epoch [369/500]      | Train: Loss 0.24396, R2 0.93014, RMSE 0.49293                     | Test: Loss 0.80961, R2 0.77648, RMSE 0.88977\n",
      "Epoch [370/500]      | Train: Loss 0.24801, R2 0.92961, RMSE 0.49730                     | Test: Loss 0.80679, R2 0.77675, RMSE 0.88924\n",
      "Epoch [371/500]      | Train: Loss 0.25176, R2 0.92846, RMSE 0.50080                     | Test: Loss 0.84654, R2 0.76395, RMSE 0.91156\n",
      "Epoch [372/500]      | Train: Loss 0.24871, R2 0.92909, RMSE 0.49820                     | Test: Loss 0.85209, R2 0.76961, RMSE 0.91805\n",
      "Epoch [373/500]      | Train: Loss 0.24244, R2 0.93118, RMSE 0.49164                     | Test: Loss 0.82607, R2 0.76163, RMSE 0.89791\n",
      "Epoch [374/500]      | Train: Loss 0.24690, R2 0.93087, RMSE 0.49558                     | Test: Loss 0.85249, R2 0.75475, RMSE 0.91838\n",
      "Epoch [375/500]      | Train: Loss 0.24000, R2 0.93237, RMSE 0.48879                     | Test: Loss 0.81507, R2 0.78039, RMSE 0.89380\n",
      "Epoch [376/500]      | Train: Loss 0.24669, R2 0.93019, RMSE 0.49572                     | Test: Loss 1.02058, R2 0.74812, RMSE 0.97208\n",
      "Epoch [377/500]      | Train: Loss 0.23938, R2 0.93201, RMSE 0.48823                     | Test: Loss 0.90366, R2 0.74406, RMSE 0.94100\n",
      "Epoch [378/500]      | Train: Loss 0.24086, R2 0.93125, RMSE 0.48978                     | Test: Loss 0.79748, R2 0.77563, RMSE 0.88112\n",
      "Epoch [379/500]      | Train: Loss 0.24087, R2 0.93142, RMSE 0.49005                     | Test: Loss 0.83058, R2 0.76839, RMSE 0.90584\n",
      "Epoch [380/500]      | Train: Loss 0.23722, R2 0.93298, RMSE 0.48606                     | Test: Loss 0.86448, R2 0.76097, RMSE 0.91787\n",
      "Epoch [381/500]      | Train: Loss 0.23480, R2 0.93247, RMSE 0.48360                     | Test: Loss 0.83742, R2 0.76133, RMSE 0.90719\n",
      "Epoch [382/500]      | Train: Loss 0.24351, R2 0.93153, RMSE 0.49219                     | Test: Loss 0.80706, R2 0.78205, RMSE 0.88786\n",
      "Epoch [383/500]      | Train: Loss 0.23665, R2 0.93298, RMSE 0.48524                     | Test: Loss 0.88067, R2 0.75116, RMSE 0.93422\n",
      "Epoch [384/500]      | Train: Loss 0.23712, R2 0.93352, RMSE 0.48605                     | Test: Loss 1.01815, R2 0.74180, RMSE 0.97427\n",
      "Epoch [385/500]      | Train: Loss 0.23984, R2 0.93227, RMSE 0.48882                     | Test: Loss 0.86627, R2 0.76643, RMSE 0.92523\n",
      "Epoch [386/500]      | Train: Loss 0.23077, R2 0.93503, RMSE 0.47983                     | Test: Loss 0.82549, R2 0.77426, RMSE 0.89671\n",
      "Epoch [387/500]      | Train: Loss 0.23321, R2 0.93393, RMSE 0.48179                     | Test: Loss 0.88550, R2 0.76381, RMSE 0.93372\n",
      "Epoch [388/500]      | Train: Loss 0.23454, R2 0.93411, RMSE 0.48324                     | Test: Loss 0.81884, R2 0.77329, RMSE 0.89217\n",
      "Epoch [389/500]      | Train: Loss 0.23996, R2 0.93244, RMSE 0.48899                     | Test: Loss 0.94875, R2 0.74678, RMSE 0.96068\n",
      "Epoch [390/500]      | Train: Loss 0.23802, R2 0.93206, RMSE 0.48687                     | Test: Loss 0.83199, R2 0.77502, RMSE 0.90169\n",
      "Epoch [391/500]      | Train: Loss 0.22685, R2 0.93610, RMSE 0.47555                     | Test: Loss 0.83125, R2 0.76460, RMSE 0.90531\n",
      "Epoch [392/500]      | Train: Loss 0.23583, R2 0.93289, RMSE 0.48474                     | Test: Loss 0.84750, R2 0.76169, RMSE 0.91179\n",
      "Epoch [393/500]      | Train: Loss 0.22958, R2 0.93594, RMSE 0.47821                     | Test: Loss 0.82368, R2 0.77163, RMSE 0.89799\n",
      "Epoch [394/500]      | Train: Loss 0.22763, R2 0.93593, RMSE 0.47652                     | Test: Loss 0.82345, R2 0.76432, RMSE 0.90409\n",
      "Epoch [395/500]      | Train: Loss 0.23015, R2 0.93487, RMSE 0.47900                     | Test: Loss 0.83372, R2 0.77044, RMSE 0.90516\n",
      "Epoch [396/500]      | Train: Loss 0.22646, R2 0.93578, RMSE 0.47482                     | Test: Loss 0.85132, R2 0.76541, RMSE 0.91663\n",
      "Epoch [397/500]      | Train: Loss 0.22819, R2 0.93559, RMSE 0.47682                     | Test: Loss 0.79345, R2 0.78464, RMSE 0.88057\n",
      "Epoch [398/500]      | Train: Loss 0.22153, R2 0.93732, RMSE 0.46967                     | Test: Loss 0.80720, R2 0.78237, RMSE 0.88747\n",
      "Epoch [399/500]      | Train: Loss 0.22392, R2 0.93660, RMSE 0.47229                     | Test: Loss 0.81701, R2 0.77538, RMSE 0.89913\n",
      "Epoch [400/500]      | Train: Loss 0.22466, R2 0.93632, RMSE 0.47318                     | Test: Loss 0.85636, R2 0.75802, RMSE 0.92076\n",
      "Epoch [401/500]      | Train: Loss 0.22732, R2 0.93574, RMSE 0.47567                     | Test: Loss 0.82560, R2 0.76893, RMSE 0.89943\n",
      "Epoch [402/500]      | Train: Loss 0.23109, R2 0.93469, RMSE 0.47939                     | Test: Loss 0.85092, R2 0.76558, RMSE 0.91646\n",
      "Epoch [403/500]      | Train: Loss 0.23685, R2 0.93292, RMSE 0.48595                     | Test: Loss 0.84955, R2 0.76803, RMSE 0.90877\n",
      "Epoch [404/500]      | Train: Loss 0.23179, R2 0.93507, RMSE 0.48027                     | Test: Loss 0.87427, R2 0.76618, RMSE 0.92860\n",
      "Epoch [405/500]      | Train: Loss 0.21876, R2 0.93855, RMSE 0.46712                     | Test: Loss 0.85684, R2 0.76029, RMSE 0.91800\n",
      "Epoch [406/500]      | Train: Loss 0.22099, R2 0.93734, RMSE 0.46911                     | Test: Loss 0.85424, R2 0.76826, RMSE 0.91955\n",
      "Epoch [407/500]      | Train: Loss 0.22211, R2 0.93674, RMSE 0.47055                     | Test: Loss 0.83987, R2 0.77425, RMSE 0.90597\n",
      "Epoch [408/500]      | Train: Loss 0.21780, R2 0.93798, RMSE 0.46588                     | Test: Loss 0.80285, R2 0.76689, RMSE 0.88837\n",
      "Epoch [409/500]      | Train: Loss 0.22225, R2 0.93741, RMSE 0.47048                     | Test: Loss 0.81303, R2 0.77471, RMSE 0.89527\n",
      "Epoch [410/500]      | Train: Loss 0.21957, R2 0.93860, RMSE 0.46772                     | Test: Loss 0.81927, R2 0.78307, RMSE 0.89419\n",
      "Epoch [411/500]      | Train: Loss 0.21397, R2 0.93977, RMSE 0.46169                     | Test: Loss 0.95514, R2 0.73782, RMSE 0.95726\n",
      "Epoch [412/500]      | Train: Loss 0.22129, R2 0.93779, RMSE 0.46958                     | Test: Loss 0.84757, R2 0.76400, RMSE 0.90856\n",
      "Epoch [413/500]      | Train: Loss 0.22208, R2 0.93718, RMSE 0.47027                     | Test: Loss 0.81508, R2 0.77015, RMSE 0.88872\n",
      "Epoch [414/500]      | Train: Loss 0.21277, R2 0.94014, RMSE 0.46040                     | Test: Loss 0.84985, R2 0.77521, RMSE 0.91554\n",
      "Epoch [415/500]      | Train: Loss 0.21290, R2 0.94018, RMSE 0.46055                     | Test: Loss 0.81736, R2 0.76122, RMSE 0.89120\n",
      "Epoch [416/500]      | Train: Loss 0.21087, R2 0.94067, RMSE 0.45859                     | Test: Loss 0.89418, R2 0.76503, RMSE 0.93776\n",
      "Epoch [417/500]      | Train: Loss 0.21125, R2 0.94067, RMSE 0.45887                     | Test: Loss 0.88203, R2 0.76124, RMSE 0.93056\n",
      "Epoch [418/500]      | Train: Loss 0.20761, R2 0.94054, RMSE 0.45483                     | Test: Loss 0.85877, R2 0.77053, RMSE 0.91951\n",
      "Epoch [419/500]      | Train: Loss 0.20510, R2 0.94208, RMSE 0.45179                     | Test: Loss 0.85200, R2 0.74299, RMSE 0.92067\n",
      "Epoch [420/500]      | Train: Loss 0.20457, R2 0.94235, RMSE 0.45153                     | Test: Loss 0.86240, R2 0.76057, RMSE 0.92392\n",
      "Epoch [421/500]      | Train: Loss 0.21558, R2 0.93944, RMSE 0.46335                     | Test: Loss 0.87508, R2 0.75926, RMSE 0.92888\n",
      "Epoch [422/500]      | Train: Loss 0.21378, R2 0.93891, RMSE 0.46186                     | Test: Loss 0.83764, R2 0.76133, RMSE 0.90201\n",
      "Epoch [423/500]      | Train: Loss 0.20983, R2 0.94122, RMSE 0.45694                     | Test: Loss 0.86105, R2 0.73515, RMSE 0.92146\n",
      "Epoch [424/500]      | Train: Loss 0.20385, R2 0.94283, RMSE 0.45046                     | Test: Loss 0.86411, R2 0.76370, RMSE 0.92222\n",
      "Epoch [425/500]      | Train: Loss 0.21976, R2 0.93819, RMSE 0.46755                     | Test: Loss 0.86369, R2 0.74787, RMSE 0.92521\n",
      "Epoch [426/500]      | Train: Loss 0.21004, R2 0.94032, RMSE 0.45732                     | Test: Loss 0.82839, R2 0.76250, RMSE 0.90318\n",
      "Epoch [427/500]      | Train: Loss 0.20049, R2 0.94334, RMSE 0.44710                     | Test: Loss 0.89551, R2 0.76003, RMSE 0.94117\n",
      "Epoch [428/500]      | Train: Loss 0.20805, R2 0.94122, RMSE 0.45532                     | Test: Loss 0.86659, R2 0.76578, RMSE 0.92360\n",
      "Epoch [429/500]      | Train: Loss 0.20313, R2 0.94268, RMSE 0.44979                     | Test: Loss 0.84037, R2 0.75953, RMSE 0.90597\n",
      "Epoch [430/500]      | Train: Loss 0.20158, R2 0.94374, RMSE 0.44816                     | Test: Loss 0.85247, R2 0.76026, RMSE 0.91713\n",
      "Epoch [431/500]      | Train: Loss 0.21078, R2 0.94114, RMSE 0.45816                     | Test: Loss 0.91059, R2 0.76846, RMSE 0.93931\n",
      "Epoch [432/500]      | Train: Loss 0.20350, R2 0.94206, RMSE 0.45016                     | Test: Loss 0.82911, R2 0.76479, RMSE 0.89743\n",
      "Epoch [433/500]      | Train: Loss 0.20192, R2 0.94277, RMSE 0.44845                     | Test: Loss 0.89344, R2 0.76347, RMSE 0.93112\n",
      "Epoch [434/500]      | Train: Loss 0.19835, R2 0.94343, RMSE 0.44452                     | Test: Loss 0.95619, R2 0.72444, RMSE 0.95764\n",
      "Epoch [435/500]      | Train: Loss 0.19713, R2 0.94479, RMSE 0.44275                     | Test: Loss 0.84672, R2 0.77027, RMSE 0.91211\n",
      "Epoch [436/500]      | Train: Loss 0.20226, R2 0.94267, RMSE 0.44839                     | Test: Loss 0.85663, R2 0.76737, RMSE 0.91425\n",
      "Epoch [437/500]      | Train: Loss 0.19765, R2 0.94450, RMSE 0.44386                     | Test: Loss 0.83650, R2 0.77239, RMSE 0.90480\n",
      "Epoch [438/500]      | Train: Loss 0.19852, R2 0.94386, RMSE 0.44461                     | Test: Loss 0.86136, R2 0.75158, RMSE 0.92482\n",
      "Epoch [439/500]      | Train: Loss 0.19735, R2 0.94415, RMSE 0.44361                     | Test: Loss 0.89142, R2 0.74852, RMSE 0.93896\n",
      "Epoch [440/500]      | Train: Loss 0.20105, R2 0.94342, RMSE 0.44761                     | Test: Loss 0.85143, R2 0.77059, RMSE 0.90747\n",
      "Epoch [441/500]      | Train: Loss 0.19463, R2 0.94523, RMSE 0.44033                     | Test: Loss 0.84425, R2 0.75930, RMSE 0.90989\n",
      "Epoch [442/500]      | Train: Loss 0.20264, R2 0.94307, RMSE 0.44882                     | Test: Loss 0.84028, R2 0.76339, RMSE 0.90824\n",
      "Epoch [443/500]      | Train: Loss 0.20041, R2 0.94293, RMSE 0.44700                     | Test: Loss 0.83342, R2 0.76593, RMSE 0.90094\n",
      "Epoch [444/500]      | Train: Loss 0.20464, R2 0.94206, RMSE 0.45123                     | Test: Loss 0.90733, R2 0.75156, RMSE 0.94390\n",
      "Epoch [445/500]      | Train: Loss 0.19276, R2 0.94543, RMSE 0.43796                     | Test: Loss 0.93942, R2 0.73934, RMSE 0.95144\n",
      "Epoch [446/500]      | Train: Loss 0.19591, R2 0.94431, RMSE 0.44157                     | Test: Loss 0.84294, R2 0.77234, RMSE 0.90979\n",
      "Epoch [447/500]      | Train: Loss 0.19350, R2 0.94545, RMSE 0.43912                     | Test: Loss 0.86029, R2 0.76532, RMSE 0.92039\n",
      "Epoch [448/500]      | Train: Loss 0.19837, R2 0.94419, RMSE 0.44472                     | Test: Loss 0.84139, R2 0.77425, RMSE 0.90953\n",
      "Epoch [449/500]      | Train: Loss 0.19816, R2 0.94427, RMSE 0.44393                     | Test: Loss 0.87305, R2 0.73798, RMSE 0.92573\n",
      "Epoch [450/500]      | Train: Loss 0.19351, R2 0.94533, RMSE 0.43862                     | Test: Loss 0.82819, R2 0.76374, RMSE 0.90258\n",
      "Epoch [451/500]      | Train: Loss 0.19410, R2 0.94512, RMSE 0.43949                     | Test: Loss 0.82921, R2 0.75792, RMSE 0.90107\n",
      "Epoch [452/500]      | Train: Loss 0.18782, R2 0.94715, RMSE 0.43283                     | Test: Loss 0.84362, R2 0.77723, RMSE 0.90994\n",
      "Epoch [453/500]      | Train: Loss 0.19112, R2 0.94637, RMSE 0.43621                     | Test: Loss 0.83071, R2 0.77372, RMSE 0.90289\n",
      "Epoch [454/500]      | Train: Loss 0.19359, R2 0.94544, RMSE 0.43902                     | Test: Loss 0.84992, R2 0.76376, RMSE 0.91162\n",
      "Epoch [455/500]      | Train: Loss 0.19136, R2 0.94528, RMSE 0.43659                     | Test: Loss 0.85032, R2 0.75649, RMSE 0.91196\n",
      "Epoch [456/500]      | Train: Loss 0.19387, R2 0.94535, RMSE 0.43953                     | Test: Loss 0.85727, R2 0.76289, RMSE 0.91980\n",
      "Epoch [457/500]      | Train: Loss 0.19118, R2 0.94613, RMSE 0.43606                     | Test: Loss 0.85937, R2 0.75990, RMSE 0.91919\n",
      "Epoch [458/500]      | Train: Loss 0.18571, R2 0.94789, RMSE 0.43037                     | Test: Loss 0.86013, R2 0.76221, RMSE 0.91783\n",
      "Epoch [459/500]      | Train: Loss 0.18889, R2 0.94707, RMSE 0.43369                     | Test: Loss 0.87901, R2 0.73529, RMSE 0.93216\n",
      "Epoch [460/500]      | Train: Loss 0.18945, R2 0.94667, RMSE 0.43432                     | Test: Loss 0.85442, R2 0.76885, RMSE 0.92040\n",
      "Epoch [461/500]      | Train: Loss 0.18837, R2 0.94708, RMSE 0.43300                     | Test: Loss 0.89389, R2 0.74265, RMSE 0.93893\n",
      "Epoch [462/500]      | Train: Loss 0.17987, R2 0.94911, RMSE 0.42327                     | Test: Loss 0.86049, R2 0.75844, RMSE 0.92007\n",
      "Epoch [463/500]      | Train: Loss 0.18211, R2 0.94840, RMSE 0.42591                     | Test: Loss 0.83555, R2 0.75769, RMSE 0.90513\n",
      "Epoch [464/500]      | Train: Loss 0.18751, R2 0.94661, RMSE 0.43200                     | Test: Loss 0.83552, R2 0.76639, RMSE 0.90205\n",
      "Epoch [465/500]      | Train: Loss 0.18899, R2 0.94687, RMSE 0.43399                     | Test: Loss 0.85023, R2 0.76777, RMSE 0.91395\n",
      "Epoch [466/500]      | Train: Loss 0.18850, R2 0.94653, RMSE 0.43321                     | Test: Loss 0.95252, R2 0.76259, RMSE 0.95266\n",
      "Epoch [467/500]      | Train: Loss 0.18258, R2 0.94839, RMSE 0.42660                     | Test: Loss 0.88294, R2 0.75173, RMSE 0.93066\n",
      "Epoch [468/500]      | Train: Loss 0.18395, R2 0.94761, RMSE 0.42818                     | Test: Loss 0.84177, R2 0.76750, RMSE 0.90773\n",
      "Epoch [469/500]      | Train: Loss 0.18766, R2 0.94742, RMSE 0.43218                     | Test: Loss 0.87695, R2 0.74764, RMSE 0.93047\n",
      "Epoch [470/500]      | Train: Loss 0.17852, R2 0.94957, RMSE 0.42172                     | Test: Loss 0.90206, R2 0.75965, RMSE 0.94219\n",
      "Epoch [471/500]      | Train: Loss 0.17994, R2 0.94885, RMSE 0.42348                     | Test: Loss 0.83838, R2 0.77313, RMSE 0.90485\n",
      "Epoch [472/500]      | Train: Loss 0.17583, R2 0.95077, RMSE 0.41852                     | Test: Loss 0.82529, R2 0.76488, RMSE 0.90070\n",
      "Epoch [473/500]      | Train: Loss 0.18156, R2 0.94864, RMSE 0.42560                     | Test: Loss 0.81453, R2 0.77331, RMSE 0.89610\n",
      "Epoch [474/500]      | Train: Loss 0.18370, R2 0.94790, RMSE 0.42774                     | Test: Loss 0.83743, R2 0.77178, RMSE 0.90789\n",
      "Epoch [475/500]      | Train: Loss 0.18443, R2 0.94792, RMSE 0.42882                     | Test: Loss 0.91812, R2 0.75394, RMSE 0.94828\n",
      "Epoch [476/500]      | Train: Loss 0.18704, R2 0.94719, RMSE 0.43146                     | Test: Loss 0.85571, R2 0.76539, RMSE 0.91390\n",
      "Epoch [477/500]      | Train: Loss 0.17798, R2 0.94979, RMSE 0.42114                     | Test: Loss 0.83327, R2 0.76675, RMSE 0.90491\n",
      "Epoch [478/500]      | Train: Loss 0.17936, R2 0.94878, RMSE 0.42276                     | Test: Loss 0.88407, R2 0.76171, RMSE 0.93516\n",
      "Epoch [479/500]      | Train: Loss 0.17941, R2 0.94925, RMSE 0.42270                     | Test: Loss 0.82633, R2 0.77238, RMSE 0.89638\n",
      "Epoch [480/500]      | Train: Loss 0.17776, R2 0.94981, RMSE 0.42080                     | Test: Loss 0.86101, R2 0.76098, RMSE 0.91641\n",
      "Epoch [481/500]      | Train: Loss 0.18337, R2 0.94768, RMSE 0.42731                     | Test: Loss 0.81411, R2 0.77813, RMSE 0.89092\n",
      "Epoch [482/500]      | Train: Loss 0.17909, R2 0.94979, RMSE 0.42243                     | Test: Loss 0.83261, R2 0.77003, RMSE 0.90366\n",
      "Epoch [483/500]      | Train: Loss 0.19061, R2 0.94568, RMSE 0.43542                     | Test: Loss 0.81704, R2 0.77763, RMSE 0.88873\n",
      "Epoch [484/500]      | Train: Loss 0.18010, R2 0.94927, RMSE 0.42371                     | Test: Loss 0.86869, R2 0.76229, RMSE 0.92658\n",
      "Epoch [485/500]      | Train: Loss 0.17870, R2 0.94945, RMSE 0.42166                     | Test: Loss 0.82211, R2 0.77314, RMSE 0.89098\n",
      "Epoch [486/500]      | Train: Loss 0.17192, R2 0.95156, RMSE 0.41393                     | Test: Loss 0.81777, R2 0.77436, RMSE 0.89806\n",
      "Epoch [487/500]      | Train: Loss 0.16971, R2 0.95241, RMSE 0.41112                     | Test: Loss 0.82696, R2 0.76744, RMSE 0.90076\n",
      "Epoch [488/500]      | Train: Loss 0.16616, R2 0.95319, RMSE 0.40667                     | Test: Loss 0.81164, R2 0.77451, RMSE 0.89053\n",
      "Epoch [489/500]      | Train: Loss 0.17130, R2 0.95164, RMSE 0.41299                     | Test: Loss 0.85208, R2 0.75454, RMSE 0.91493\n",
      "Epoch [490/500]      | Train: Loss 0.16717, R2 0.95247, RMSE 0.40772                     | Test: Loss 1.11754, R2 0.74726, RMSE 0.99926\n",
      "Epoch [491/500]      | Train: Loss 0.16490, R2 0.95358, RMSE 0.40536                     | Test: Loss 0.89329, R2 0.75958, RMSE 0.93459\n",
      "Epoch [492/500]      | Train: Loss 0.17171, R2 0.95138, RMSE 0.41383                     | Test: Loss 0.81041, R2 0.76878, RMSE 0.88890\n",
      "Epoch [493/500]      | Train: Loss 0.16954, R2 0.95219, RMSE 0.41099                     | Test: Loss 0.81953, R2 0.77608, RMSE 0.88770\n",
      "Epoch [494/500]      | Train: Loss 0.17743, R2 0.94986, RMSE 0.42026                     | Test: Loss 0.83849, R2 0.75608, RMSE 0.90512\n",
      "Epoch [495/500]      | Train: Loss 0.16790, R2 0.95268, RMSE 0.40915                     | Test: Loss 1.08691, R2 0.74214, RMSE 0.99187\n",
      "Epoch [496/500]      | Train: Loss 0.16478, R2 0.95354, RMSE 0.40515                     | Test: Loss 0.86449, R2 0.75688, RMSE 0.92333\n",
      "Epoch [497/500]      | Train: Loss 0.17397, R2 0.95059, RMSE 0.41619                     | Test: Loss 0.84086, R2 0.76973, RMSE 0.90250\n",
      "Epoch [498/500]      | Train: Loss 0.16877, R2 0.95229, RMSE 0.40992                     | Test: Loss 0.88295, R2 0.76962, RMSE 0.92756\n",
      "Epoch [499/500]      | Train: Loss 0.16173, R2 0.95395, RMSE 0.40087                     | Test: Loss 0.86442, R2 0.75895, RMSE 0.92071\n",
      "Epoch [500/500]      | Train: Loss 0.17111, R2 0.95138, RMSE 0.41269                     | Test: Loss 0.94130, R2 0.74853, RMSE 0.95861\n",
      "Best rmse 0.8113951132847712\n",
      "100 epochs of training and evaluation took, 4012.307692323\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class LossFunction(nn.Module):\n",
    "    def __init__(self, num_tasks):\n",
    "        super(LossFunction, self).__init__()\n",
    "        self.num_tasks = num_tasks\n",
    "\n",
    "        self.loss_fns = [nn.MSELoss() for x in range(num_tasks)] \n",
    "\n",
    "    def forward(self, predictions, labels_task1):\n",
    "\n",
    "        #task 1\n",
    "        target = labels_task1\n",
    "        prediction = predictions\n",
    "        loss_fn = self.loss_fns[0]\n",
    "\n",
    "        task_loss = loss_fn(prediction, target)\n",
    "        \n",
    "        return task_loss\n",
    "    \n",
    "model = CATTransformer(rff_on=True,\n",
    "                       sigma=2,\n",
    "                       embed_size=160,\n",
    "                       heads=5,\n",
    "                       forward_expansion=8,\n",
    "                       pre_norm_on=False,\n",
    "                       mlp_scale_classification=8,\n",
    "                   \n",
    "                       decoder_dropout=0,\n",
    "                       classification_dropout=0.1,\n",
    "                       targets_classes=target_classes,\n",
    "                       n_cont=len(cont_columns),\n",
    "              \n",
    "                       embedding_scheme='log-linear_periodic',\n",
    "                       trainable=False,\n",
    "                       linear_on=True).to(device_in_use)\n",
    "\n",
    "def format_metric(value): # Used to format the metrics output\n",
    "    return f\"{value:.5f}\"\n",
    "\n",
    "loss_functions = LossFunction(1)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr = 0.0001) # Maybe try messing around with optimizers. try other torch optimizers with different configurations.\n",
    "epochs = 500 #Set the number of epochs\n",
    "\n",
    "train_losses = []\n",
    "train_rmse = [] \n",
    "test_losses = []\n",
    "test_rmse = []  \n",
    "\n",
    "\n",
    "#Time will be recorded for all 100 epochs - This means the results will not be comparable to Xgboost but that is ok, we will only compare between transformer models who will also train for 100 epochs\n",
    "start_time = time.process_time()\n",
    "\n",
    "for t in range(epochs):\n",
    "    train_loss, r2_train, rmse_train = train(train_dataloader, model, loss_functions, optimizer, device_in_use=device_in_use)\n",
    "    test_loss, r2_test, rmse_test = test(test_dataloader, model, loss_functions, device_in_use=device_in_use)\n",
    "    train_losses.append(train_loss)\n",
    "    train_rmse.append(rmse_train)\n",
    "    # train_accuracies_2.append(train_accuracy_2)\n",
    "    # train_recalls.append(train_recall) \n",
    "    # train_f1_scores.append(train_f1)\n",
    "    test_losses.append(test_loss)\n",
    "    test_rmse.append(rmse_test)\n",
    "    # test_accuracies_2.append(test_accuracy_2)\n",
    "    # test_recalls.append(test_recall)\n",
    "    # test_f1_scores.append(test_f1)\n",
    "    # Formatting for easier reading\n",
    "    epoch_str = f\"Epoch [{t+1:2}/{epochs}]\"\n",
    "    train_metrics = f\"Train: Loss {format_metric(train_loss)}, R2 {format_metric(r2_train)}, RMSE {format_metric(rmse_train)}\"\n",
    "    test_metrics = f\"Test: Loss {format_metric(test_loss)}, R2 {format_metric(r2_test)}, RMSE {format_metric(rmse_test)}\"\n",
    "    print(f\"{epoch_str:20} | {train_metrics:65} | {test_metrics}\")\n",
    "\n",
    "total_time = time.process_time() - start_time\n",
    "\n",
    "# Plotting the loss curves\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, epochs+1), train_losses, label='Train Loss')\n",
    "plt.plot(range(1, epochs+1), [l for l in test_losses], label='Test Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Test Loss Curve')\n",
    "plt.legend()\n",
    "\n",
    "best_index = test_rmse.index(min(test_rmse))\n",
    "print(f\"Best rmse {test_rmse[best_index]}\")\n",
    "print(f\"100 epochs of training and evaluation took, {total_time}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
