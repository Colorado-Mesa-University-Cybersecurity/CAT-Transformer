{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\smbm2\\AppData\\Local\\miniconda3\\envs\\ml-env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from rff.layers import GaussianEncoding #pip install random-fourier-features-pytorch\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import os\n",
    "import optuna\n",
    "from optuna.trial import TrialState\n",
    "from tab_transformer_pytorch import FTTransformer, TabTransformer\n",
    "import sys\n",
    "import time\n",
    "from torch import Tensor\n",
    "from typing import Literal\n",
    "\n",
    "device_in_use='cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train = pd.read_csv('/home/cscadmin/CyberResearch/CAT-Transformer/datasets/higgs/train.csv')\n",
    "# df_test = pd.read_csv('/home/cscadmin/CyberResearch/CAT-Transformer/datasets/higgs/test.csv')\n",
    "# df_val = pd.read_csv('/home/cscadmin/CyberResearch/CAT-Transformer/datasets/higgs/validation.csv') #READ FROM RIGHT SPOT\n",
    "\n",
    "df_train = pd.read_csv(r'C:\\Users\\smbm2\\projects\\CAT-Transformer\\datasets\\california\\train.csv')\n",
    "df_test = pd.read_csv(r'C:\\Users\\smbm2\\projects\\CAT-Transformer\\datasets\\california\\test.csv')\n",
    "df_val = pd.read_csv(r'C:\\Users\\smbm2\\projects\\CAT-Transformer\\datasets\\california\\validation.csv') #READ FROM RIGHT SPOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9851]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cont_columns = [ 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup',\n",
    "       'Latitude', 'Longitude']\n",
    "target = ['MedInc']\n",
    "\n",
    "#CHECKING TO MAKE SURE YOUR LIST IS CORRECT (NO NEED TO TOUCH)\n",
    "yourlist = cont_columns + target\n",
    "yourlist.sort()\n",
    "oglist = list(df_train.columns)\n",
    "oglist.sort()\n",
    "\n",
    "assert(yourlist == oglist), \"You may of spelled feature name wrong or you forgot to put on of them in the list\"\n",
    "\n",
    "target_classes = [max(len(df_train[target].value_counts()), len(df_val[target].value_counts()),len(df_test[target].value_counts()))]\n",
    "print(target_classes)\n",
    "# Create a StandardScaler and fit it to the cont features\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(df_train[cont_columns])\n",
    "\n",
    "# Transform the training, test, and validation datasets\n",
    "df_train[cont_columns] = scaler.transform(df_train[cont_columns])\n",
    "df_test[cont_columns] = scaler.transform(df_test[cont_columns])\n",
    "df_val[cont_columns] = scaler.transform(df_val[cont_columns])\n",
    "\n",
    "class SingleTaskDataset(Dataset):\n",
    "    def __init__(self, df : pd.DataFrame, num_columns,task1_column):\n",
    "        self.n = df.shape[0]\n",
    "        \n",
    "        self.task1_labels = df[task1_column].astype(np.float32).values\n",
    "\n",
    "        self.num = df[num_columns].astype(np.float32).values\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Retrieve features and labels from the dataframe using column names\n",
    "        num_features = self.num[idx]\n",
    "        labels_task1 = self.task1_labels[idx]\n",
    "\n",
    "        return num_features, labels_task1\n",
    "\n",
    "#Wrapping in Dataset\n",
    "train_dataset = SingleTaskDataset(df_train, cont_columns, 'MedInc')\n",
    "val_dataset = SingleTaskDataset(df_val, cont_columns, 'MedInc')\n",
    "test_dataset = SingleTaskDataset(df_test, cont_columns, 'MedInc')\n",
    "\n",
    "#This is a hyperparameter that is not tuned. Maybe mess with what makes sense here\n",
    "#Also try looking to see what other papers have done\n",
    "batch_size = 256\n",
    "\n",
    "# Wrapping with DataLoader for easy batch extraction\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_size, heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        self.head_dim = embed_size // heads\n",
    "        assert(self.head_dim * heads == embed_size), \"Embed size needs to be div by heads\"\n",
    "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.keys =nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.fc_out = nn.Linear(heads*self.head_dim, embed_size)\n",
    "\n",
    "\n",
    "    def forward(self, values, keys, query):\n",
    "        N = query.shape[0]\n",
    "        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
    "\n",
    "        values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
    "        keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n",
    "        queries = query.reshape(N, query_len, self.heads, self.head_dim)\n",
    "\n",
    "        values = self.values(values)\n",
    "        keys = self.keys(keys)\n",
    "        queries = self.queries(queries)\n",
    "\n",
    "        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
    "\n",
    "        attention = torch.softmax(energy / (self.embed_size ** (1/2)), dim=3) #(batch_size, head_dim, #query_embeddings, #key_embeddings)\n",
    "\n",
    "        # Calculate simplified attention scores\n",
    "        avg_attention = attention.mean(dim=0)  # Average across batches\n",
    "        # print(\"batch average\", avg_attention.shape)\n",
    "        avg_attention = avg_attention.mean(dim=0).squeeze(dim=0)\n",
    "        # print(\"head average\", avg_attention.shape)\n",
    "\n",
    "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(N, query_len, self.heads*self.head_dim) #(batch_size, n_features, embed_size)\n",
    "        out = self.fc_out(out)\n",
    "\n",
    "        return out, avg_attention\n",
    "    \n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_size, heads, dropout, forward_expansion, pre_norm_on):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "\n",
    "        self.pre_norm_on = pre_norm_on\n",
    "        if self.pre_norm_on:\n",
    "            self.pre_norm = nn.LayerNorm(embed_size)\n",
    "        self.attention = MultiHeadAttention(embed_size, heads)\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "\n",
    "        self.feed_forward = nn.Sequential(nn.Linear(embed_size, forward_expansion*embed_size),\n",
    "                                          nn.ReLU(),\n",
    "                                          nn.Linear(forward_expansion*embed_size, embed_size)\n",
    "                                          )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self,value,key,query):\n",
    "        if self.pre_norm_on:\n",
    "            query = self.pre_norm(query)\n",
    "            key = self.pre_norm(key)\n",
    "            value = self.pre_norm(value)\n",
    "            \n",
    "        attention, avg_attention = self.attention(value, key, query)\n",
    "        x = self.dropout(self.norm1(attention + query))\n",
    "        forward = self.feed_forward(x)\n",
    "        out = self.dropout(self.norm2(forward + x))\n",
    "        return out, avg_attention\n",
    "    \n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, embed_size, heads, forward_expansion, dropout, pre_norm_on):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "\n",
    "        self.attention = MultiHeadAttention(embed_size, heads)\n",
    "        self.norm = nn.LayerNorm(embed_size)\n",
    "        self.transformer_block = TransformerBlock(embed_size, heads, dropout, forward_expansion, pre_norm_on)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, value, key):\n",
    "        out, avg_attention = self.transformer_block(value, key, x)\n",
    "\n",
    "        return out, avg_attention\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 embed_size,\n",
    "                 num_layers,\n",
    "                 heads,\n",
    "                 forward_expansion,\n",
    "                 decoder_dropout,\n",
    "                 pre_norm_on\n",
    "    ):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "                [\n",
    "                    DecoderBlock(\n",
    "                        embed_size,\n",
    "                        heads,\n",
    "                        dropout=decoder_dropout,\n",
    "                        forward_expansion=forward_expansion,\n",
    "                        pre_norm_on=pre_norm_on\n",
    "                    )\n",
    "                    for _ in range(num_layers)\n",
    "                ]\n",
    "            )\n",
    "        self.avg_attention = None\n",
    "\n",
    "    def forward(self, class_embed, context):\n",
    "        for layer in self.layers:\n",
    "            # x is the classification embedding (CLS Token)\n",
    "            # context are the feature embeddings that will be used as key and value\n",
    "            x, self.avg_attention = layer(class_embed, context, context)\n",
    "  \n",
    "        return x \n",
    "\n",
    "class EmbeddingsRFFforIndividualFeatures(nn.Module):\n",
    "    def __init__(self, sigma, embed_size, input_size, n_cont,  num_target_labels, rff_on, linear_on:bool):\n",
    "        super(EmbeddingsRFFforIndividualFeatures, self).__init__()\n",
    "\n",
    "        self.rff_on = rff_on\n",
    "\n",
    "        self.linear_on = linear_on\n",
    "\n",
    "        if self.rff_on:\n",
    "            self.rffs = nn.ModuleList([GaussianEncoding(sigma=sigma, input_size=input_size, encoded_size=embed_size//2) for _ in range(n_cont)])\n",
    "            self.mlp_in = embed_size\n",
    "        else:\n",
    "            self.mlp_in = input_size\n",
    "\n",
    "        if self.linear_on:\n",
    "            self.cont_embeddings = nn.ModuleList([nn.Linear(in_features=self.mlp_in, out_features=embed_size) for _ in range(n_cont)])\n",
    "\n",
    "        # Classifcation Embeddings for each target label\n",
    "        self.target_label_embeddings = nn.ModuleList([nn.Embedding(1, embed_size) for _ in range(num_target_labels)])\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(2) #(batch_size, n_features) -> (batch_size, n_features, 1)\n",
    "        rff_vectors = []\n",
    "        if self.rff_on:\n",
    "            for i, r in enumerate(self.rffs):\n",
    "                input = x[:,i,:]\n",
    "                out = r(input)\n",
    "                rff_vectors.append(out)\n",
    "\n",
    "        embeddings = []\n",
    "        if self.linear_on:\n",
    "            x = torch.stack(rff_vectors, dim=1)\n",
    "            for i, e in enumerate(self.cont_embeddings):\n",
    "                goin_in = x[:,i,:]\n",
    "                goin_out = e(goin_in)\n",
    "                embeddings.append(goin_out)\n",
    "        else:\n",
    "            embeddings = rff_vectors\n",
    "\n",
    "        target_label_embeddings_ = []\n",
    "        for e in self.target_label_embeddings:\n",
    "            input = torch.tensor([0], device=x.device)\n",
    "            temp = e(input)\n",
    "            temp = temp.repeat(x.size(0), 1)\n",
    "            tmep = temp.unsqueeze(1)\n",
    "            target_label_embeddings_.append(temp)\n",
    "\n",
    "        class_embeddings = torch.stack(target_label_embeddings_, dim=1)\n",
    "\n",
    "        context = torch.stack(embeddings, dim=1)\n",
    "\n",
    "\n",
    "        return class_embeddings, context\n",
    "\n",
    "class EmbeddingsRFFforAllFeatures(nn.Module):\n",
    "    def __init__(self, sigma, embed_size, input_size, n_cont, num_target_labels, rff_on, linear_on:bool):\n",
    "        super(EmbeddingsRFFforAllFeatures, self).__init__()\n",
    "\n",
    "        self.rff_on = rff_on\n",
    "        self.n_cont = n_cont\n",
    "        self.linear_on=linear_on\n",
    "\n",
    "        if self.rff_on:\n",
    "            self.rff = GaussianEncoding(sigma=sigma, input_size=input_size, encoded_size=embed_size//2) \n",
    "            self.mlp_in = embed_size\n",
    "        else:\n",
    "            self.mlp_in = input_size\n",
    "\n",
    "        if self.linear_on:\n",
    "            self.cont_embeddings = nn.ModuleList([nn.Linear(in_features=self.mlp_in, out_features=embed_size) for _ in range(n_cont)])\n",
    "\n",
    "        \n",
    "        # Classifcation Embeddings for each target label\n",
    "        self.target_label_embeddings = nn.ModuleList([nn.Embedding(1, embed_size) for _ in range(num_target_labels)])\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(2) #(batch_size, n_features) -> (batch_size, n_features, 1)\n",
    "        rff_vectors = []\n",
    "        if self.rff_on:\n",
    "            for i in range(self.n_cont):\n",
    "                input = x[:,i,:]\n",
    "                out = self.rff(input)\n",
    "                rff_vectors.append(out)\n",
    "        \n",
    "        embeddings = []\n",
    "        if self.linear_on:\n",
    "            x = torch.stack(rff_vectors, dim=1)\n",
    "            for i, e in enumerate(self.cont_embeddings):\n",
    "                goin_in = x[:,i,:]\n",
    "                goin_out = e(goin_in)\n",
    "                embeddings.append(goin_out)\n",
    "        else:\n",
    "            embeddings = rff_vectors\n",
    "\n",
    "        target_label_embeddings_ = []\n",
    "        for e in self.target_label_embeddings:\n",
    "            input = torch.tensor([0], device=x.device)\n",
    "            temp = e(input)\n",
    "            temp = temp.repeat(x.size(0), 1)\n",
    "            tmep = temp.unsqueeze(1)\n",
    "            target_label_embeddings_.append(temp)\n",
    "\n",
    "        class_embeddings = torch.stack(target_label_embeddings_, dim=1)\n",
    "\n",
    "        context = torch.stack(embeddings, dim=1)\n",
    "\n",
    "        return class_embeddings, context\n",
    "    \n",
    "class PeriodicActivation(nn.Module):\n",
    "    def __init__(self, sigma, embed_size, n_cont, num_target_labels,trainable: bool, initialization: str, linear_on:bool):\n",
    "        super(PeriodicActivation, self).__init__()\n",
    "\n",
    "        self.n = embed_size\n",
    "        self.sigma = sigma\n",
    "        self.trainable = trainable\n",
    "        self.initialization = initialization\n",
    "        self.linear_on = linear_on\n",
    "        self.n_cont = n_cont\n",
    "\n",
    "        if self.initialization == 'log-linear':\n",
    "            coefficients = self.sigma ** (torch.arange(self.n//2) / self.n)\n",
    "            coefficients = coefficients[None]\n",
    "        else:\n",
    "            assert self.initialization == 'normal'\n",
    "            coefficients = torch.normal(0.0, self.sigma, (1, self.n//2))\n",
    "\n",
    "        if self.trainable:\n",
    "            self.coefficients = nn.Parameter(coefficients)\n",
    "        else:\n",
    "            self.register_buffer('coefficients', coefficients)\n",
    "\n",
    "        if self.linear_on:\n",
    "            self.cont_embeddings = nn.ModuleList([nn.Linear(in_features=embed_size, out_features=embed_size) for _ in range(n_cont)])\n",
    "\n",
    "        \n",
    "        # Classifcation Embeddings for each target label\n",
    "        self.target_label_embeddings = nn.ModuleList([nn.Embedding(1, embed_size) for _ in range(num_target_labels)])\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(2) #(batch_size, n_features) -> (batch_size, n_features, 1)\n",
    "\n",
    "        temp = []\n",
    "        for i in range(self.n_cont):\n",
    "            input = x[:,i,:]\n",
    "            out = torch.cat([torch.cos(self.coefficients * input), torch.sin(self.coefficients * input)], dim=-1)\n",
    "            temp.append(out)\n",
    "        \n",
    "        embeddings = []\n",
    "        if self.linear_on:\n",
    "            x = torch.stack(temp, dim=1)\n",
    "            for i, e in enumerate(self.cont_embeddings):\n",
    "                goin_in = x[:,i,:]\n",
    "                goin_out = e(goin_in)\n",
    "                embeddings.append(goin_out)\n",
    "        else:\n",
    "            embeddings = temp\n",
    "\n",
    "        target_label_embeddings_ = []\n",
    "        for e in self.target_label_embeddings:\n",
    "            input = torch.tensor([0], device=x.device)\n",
    "            temp = e(input)\n",
    "            temp = temp.repeat(x.size(0), 1)\n",
    "            tmep = temp.unsqueeze(1)\n",
    "            target_label_embeddings_.append(temp)\n",
    "\n",
    "        class_embeddings = torch.stack(target_label_embeddings_, dim=1)\n",
    "\n",
    "        context = torch.stack(embeddings, dim=1)\n",
    "\n",
    "        return class_embeddings, context\n",
    "\n",
    "\n",
    "class classificationHead(nn.Module):\n",
    "    def __init__(self, embed_size, dropout, mlp_scale_classification, num_target_classes):\n",
    "        super(classificationHead, self).__init__()\n",
    "        \n",
    "        #flattening the embeddings out so each sample in batch is represented with a 460 dimensional vector\n",
    "        self.input = embed_size\n",
    "        self.lin1 = nn.Linear(self.input, mlp_scale_classification*self.input)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.lin2 = nn.Linear(mlp_scale_classification*self.input, mlp_scale_classification*self.input)\n",
    "        self.lin3 = nn.Linear(mlp_scale_classification*self.input, self.input)\n",
    "        self.lin4 = nn.Linear(self.input, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.initialize_weights()\n",
    "\n",
    "    def initialize_weights(self): #he_initialization.\n",
    "        torch.nn.init.kaiming_normal_(self.lin1.weight, nonlinearity='relu')\n",
    "        torch.nn.init.zeros_(self.lin1.bias)\n",
    "\n",
    "        torch.nn.init.kaiming_normal_(self.lin3.weight, nonlinearity='relu')\n",
    "        torch.nn.init.zeros_(self.lin3.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x= torch.reshape(x, (-1, self.input))\n",
    "\n",
    "        x = self.lin1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.lin2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.lin3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.lin4(x)\n",
    "  \n",
    "        return x\n",
    "\n",
    "class CATTransformer(nn.Module):\n",
    "    def __init__(self, \n",
    "                 rff_on = True,\n",
    "                 sigma=4,\n",
    "                 embed_size=20,\n",
    "                 input_size=1,\n",
    "                 n_cont = 0,\n",
    "                 num_layers=1,\n",
    "                 heads=1,\n",
    "                 forward_expansion=4, # Determines how wide the MLP is in the encoder. Its a scaling factor. \n",
    "                 decoder_dropout=0,\n",
    "                 classification_dropout = 0,\n",
    "                 pre_norm_on = False,\n",
    "                 mlp_scale_classification = 4,\n",
    "                 targets_classes : list=  [3,8],\n",
    "                embedding_scheme =\"rff_unique\",\n",
    "                trainable=True,\n",
    "                linear_on=True\n",
    "                 ):\n",
    "        super(CATTransformer, self).__init__()\n",
    "\n",
    "        assert(embedding_scheme in ['rff_unique', 'rff', 'log-linear_periodic', 'normal_periodic']), \"wrong embedding_scheme\"\n",
    "\n",
    "        if embedding_scheme == 'rff_unique':\n",
    "            self.embeddings = EmbeddingsRFFforIndividualFeatures(rff_on=rff_on, sigma=sigma, embed_size=embed_size, input_size=input_size, \n",
    "                                     n_cont=n_cont, num_target_labels=len(targets_classes), linear_on=linear_on)\n",
    "        elif embedding_scheme == 'log-linear_periodic':\n",
    "            self.embeddings = PeriodicActivation(sigma=sigma, embed_size=embed_size, n_cont=n_cont, \n",
    "                                                 num_target_labels=len(target_classes),trainable=trainable,initialization='log-linear', linear_on=linear_on)\n",
    "        elif embedding_scheme == 'normal_periodic':\n",
    "            self.embeddings = PeriodicActivation(sigma=sigma, embed_size=embed_size, n_cont=n_cont, \n",
    "                                                 num_target_labels=len(target_classes),trainable=trainable,initialization='normal',linear_on=linear_on)\n",
    "        else:\n",
    "            self.embeddings = EmbeddingsRFFforAllFeatures(rff_on=rff_on, sigma=sigma, embed_size=embed_size, input_size=input_size, \n",
    "                                     n_cont=n_cont, num_target_labels=len(targets_classes),linear_on=linear_on)\n",
    "            \n",
    "        self.decoder = Decoder(embed_size=embed_size, num_layers=num_layers, heads=heads, forward_expansion=forward_expansion, \n",
    "                               decoder_dropout=decoder_dropout, pre_norm_on=pre_norm_on)\n",
    "        self.classifying_heads = nn.ModuleList([classificationHead(embed_size=embed_size, dropout=classification_dropout, \n",
    "                                                                   mlp_scale_classification=mlp_scale_classification, \n",
    "                                                                   num_target_classes=x) for x in targets_classes])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        class_embed, context = self.embeddings(x)\n",
    "\n",
    "        x = self.decoder(class_embed, context)\n",
    "        \n",
    "        probability_dist_raw = []\n",
    "        for i, e in enumerate(self.classifying_heads):\n",
    "            input = x[:, i,:]\n",
    "            output = e(input)\n",
    "            probability_dist_raw.append(output)\n",
    "        \n",
    "        return probability_dist_raw\n",
    "    \n",
    "    # Training and Testing Loops\n",
    "def train(dataloader, model, loss_function, optimizer, device_in_use):\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "  \n",
    "    total_loss = 0\n",
    "    total_r2_score = 0\n",
    "    root_mean_squared_error_total = 0\n",
    "\n",
    "    for (features,labels_task1) in dataloader:\n",
    "        features,labels_task1 = features.to(device_in_use),labels_task1.to(device_in_use)\n",
    "\n",
    "        task_predictions = model(features) #contains a list of the tensor outputs for each task\n",
    "\n",
    "        loss = loss_function(task_predictions[0].squeeze(1), labels_task1)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Calculate R^2 score for the regression task\n",
    "        r2 = r2_score_manual(labels_task1, task_predictions[0].squeeze(1))\n",
    "        total_r2_score += r2\n",
    "\n",
    "        # Calculate RMSE score for the regression task\n",
    "        rmse_value = rmse(labels_task1, task_predictions[0].squeeze(1))\n",
    "        root_mean_squared_error_total+=rmse_value\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    avg_r2_score = total_r2_score / len(dataloader)\n",
    "    avg_rmse_score = root_mean_squared_error_total / len(dataloader)\n",
    "\n",
    "    return avg_loss, avg_r2_score, avg_rmse_score\n",
    "\n",
    "def test(dataloader, model, loss_function, device_in_use):\n",
    "  model.eval()\n",
    "  total_loss = 0\n",
    "  \n",
    "  total_loss = 0\n",
    "  total_r2_score = 0\n",
    "  root_mean_squared_error_total = 0\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for (features,labels_task1) in dataloader:\n",
    "        features,labels_task1 = features.to(device_in_use),labels_task1.to(device_in_use)\n",
    "\n",
    "        #compute prediction error\n",
    "        task_predictions = model(features) #contains a list of the tensor outputs for each task\n",
    "\n",
    "        loss = loss_function(task_predictions[0].squeeze(1), labels_task1)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Calculate R^2 score for the regression task\n",
    "        r2 = r2_score_manual(labels_task1, task_predictions[0].squeeze(1))\n",
    "        total_r2_score += r2\n",
    "        \n",
    "        # Calculate RMSE score for the regression task\n",
    "        rmse_value = rmse(labels_task1, task_predictions[0].squeeze(1))\n",
    "        root_mean_squared_error_total+=rmse_value\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    avg_r2_score = total_r2_score / len(dataloader)\n",
    "    avg_rmse_score = root_mean_squared_error_total / len(dataloader)\n",
    "\n",
    "    return avg_loss, avg_r2_score, avg_rmse_score\n",
    "\n",
    "def format_metric(value): # Used to format the metrics output\n",
    "    return f\"{value:.4f}\"\n",
    "\n",
    "def r2_score_manual(y_true, y_pred):\n",
    "    # Calculate the mean of true labels\n",
    "    y_mean = torch.mean(y_true)\n",
    "\n",
    "    # Calculate the total sum of squares\n",
    "    total_ss = torch.sum((y_true - y_mean)**2)\n",
    "\n",
    "    # Calculate the residual sum of squares\n",
    "    residual_ss = torch.sum((y_true - y_pred)**2)\n",
    "\n",
    "    # Calculate R-squared\n",
    "    r2 = 1 - (residual_ss / total_ss)\n",
    "\n",
    "    return r2.item()  # Convert to a Python float\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    # Calculate the squared differences\n",
    "    squared_diff = (y_true - y_pred)**2\n",
    "\n",
    "    # Calculate the mean of the squared differences\n",
    "    mean_squared_diff = torch.mean(squared_diff)\n",
    "\n",
    "    # Calculate the square root to obtain RMSE\n",
    "    rmse = torch.sqrt(mean_squared_diff)\n",
    "\n",
    "    return rmse.item()  # Convert to a Python float"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normal RFF to each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [ 1/500]       | Train: Loss 4.47777, R2 -0.22844, RMSE 2.05485                    | Test: Loss 3.52993, R2 0.03679, RMSE 1.87258\n",
      "Epoch [ 2/500]       | Train: Loss 2.77145, R2 0.23472, RMSE 1.65277                     | Test: Loss 1.95990, R2 0.46385, RMSE 1.38976\n",
      "Epoch [ 3/500]       | Train: Loss 1.82077, R2 0.49892, RMSE 1.34005                     | Test: Loss 1.43184, R2 0.61386, RMSE 1.17471\n",
      "Epoch [ 4/500]       | Train: Loss 1.42149, R2 0.60675, RMSE 1.18544                     | Test: Loss 1.22608, R2 0.67632, RMSE 1.10340\n",
      "Epoch [ 5/500]       | Train: Loss 1.20249, R2 0.66617, RMSE 1.09001                     | Test: Loss 1.05030, R2 0.70260, RMSE 1.02028\n",
      "Epoch [ 6/500]       | Train: Loss 1.04894, R2 0.70732, RMSE 1.01889                     | Test: Loss 0.97709, R2 0.72902, RMSE 0.98338\n",
      "Epoch [ 7/500]       | Train: Loss 0.99439, R2 0.72333, RMSE 0.99227                     | Test: Loss 0.92223, R2 0.73570, RMSE 0.95275\n",
      "Epoch [ 8/500]       | Train: Loss 0.95170, R2 0.73409, RMSE 0.97136                     | Test: Loss 0.98106, R2 0.72312, RMSE 0.98679\n",
      "Epoch [ 9/500]       | Train: Loss 0.91496, R2 0.74629, RMSE 0.95237                     | Test: Loss 0.89262, R2 0.75331, RMSE 0.93918\n",
      "Epoch [10/500]       | Train: Loss 0.89753, R2 0.74856, RMSE 0.94268                     | Test: Loss 0.83967, R2 0.76275, RMSE 0.90510\n",
      "Epoch [11/500]       | Train: Loss 0.86901, R2 0.75734, RMSE 0.92901                     | Test: Loss 0.92707, R2 0.74213, RMSE 0.95881\n",
      "Epoch [12/500]       | Train: Loss 0.85833, R2 0.76127, RMSE 0.92161                     | Test: Loss 0.85706, R2 0.75955, RMSE 0.91753\n",
      "Epoch [13/500]       | Train: Loss 0.84040, R2 0.76439, RMSE 0.91277                     | Test: Loss 0.92172, R2 0.73956, RMSE 0.95020\n",
      "Epoch [14/500]       | Train: Loss 0.85682, R2 0.75745, RMSE 0.92198                     | Test: Loss 0.85280, R2 0.74068, RMSE 0.91786\n",
      "Epoch [15/500]       | Train: Loss 0.81143, R2 0.76983, RMSE 0.89777                     | Test: Loss 0.82984, R2 0.77785, RMSE 0.90375\n",
      "Epoch [16/500]       | Train: Loss 0.80832, R2 0.77396, RMSE 0.89439                     | Test: Loss 0.83549, R2 0.75992, RMSE 0.90939\n",
      "Epoch [17/500]       | Train: Loss 0.82276, R2 0.76826, RMSE 0.90422                     | Test: Loss 0.96035, R2 0.73087, RMSE 0.96428\n",
      "Epoch [18/500]       | Train: Loss 0.79260, R2 0.77716, RMSE 0.88713                     | Test: Loss 0.83060, R2 0.77104, RMSE 0.90369\n",
      "Epoch [19/500]       | Train: Loss 0.79835, R2 0.77591, RMSE 0.89063                     | Test: Loss 0.85844, R2 0.75827, RMSE 0.91732\n",
      "Epoch [20/500]       | Train: Loss 0.80416, R2 0.77270, RMSE 0.89378                     | Test: Loss 0.87794, R2 0.76378, RMSE 0.92914\n",
      "Epoch [21/500]       | Train: Loss 0.77989, R2 0.78333, RMSE 0.88065                     | Test: Loss 0.81472, R2 0.77598, RMSE 0.89288\n",
      "Epoch [22/500]       | Train: Loss 0.75954, R2 0.78773, RMSE 0.86865                     | Test: Loss 1.12144, R2 0.74283, RMSE 0.98748\n",
      "Epoch [23/500]       | Train: Loss 0.75929, R2 0.78770, RMSE 0.86830                     | Test: Loss 0.81910, R2 0.77558, RMSE 0.89519\n",
      "Epoch [24/500]       | Train: Loss 0.75397, R2 0.78787, RMSE 0.86484                     | Test: Loss 0.79383, R2 0.78040, RMSE 0.88352\n",
      "Epoch [25/500]       | Train: Loss 0.75425, R2 0.78862, RMSE 0.86470                     | Test: Loss 0.82857, R2 0.77302, RMSE 0.90668\n",
      "Epoch [26/500]       | Train: Loss 0.73703, R2 0.79322, RMSE 0.85580                     | Test: Loss 0.79239, R2 0.78040, RMSE 0.88411\n",
      "Epoch [27/500]       | Train: Loss 0.72805, R2 0.79572, RMSE 0.84989                     | Test: Loss 0.80303, R2 0.78187, RMSE 0.89209\n",
      "Epoch [28/500]       | Train: Loss 0.73333, R2 0.79351, RMSE 0.85273                     | Test: Loss 0.83289, R2 0.76763, RMSE 0.90288\n",
      "Epoch [29/500]       | Train: Loss 0.71203, R2 0.80078, RMSE 0.84059                     | Test: Loss 0.80193, R2 0.77493, RMSE 0.89071\n",
      "Epoch [30/500]       | Train: Loss 0.70975, R2 0.80061, RMSE 0.83862                     | Test: Loss 0.81864, R2 0.74298, RMSE 0.89841\n",
      "Epoch [31/500]       | Train: Loss 0.70650, R2 0.80071, RMSE 0.83728                     | Test: Loss 0.85076, R2 0.76710, RMSE 0.91620\n",
      "Epoch [32/500]       | Train: Loss 0.71170, R2 0.79978, RMSE 0.84083                     | Test: Loss 0.79516, R2 0.77896, RMSE 0.88532\n",
      "Epoch [33/500]       | Train: Loss 0.71585, R2 0.80009, RMSE 0.84308                     | Test: Loss 0.76373, R2 0.79332, RMSE 0.86392\n",
      "Epoch [34/500]       | Train: Loss 0.68079, R2 0.80874, RMSE 0.82271                     | Test: Loss 0.91854, R2 0.75598, RMSE 0.94273\n",
      "Epoch [35/500]       | Train: Loss 0.70893, R2 0.80166, RMSE 0.83898                     | Test: Loss 0.81085, R2 0.78019, RMSE 0.89700\n",
      "Epoch [36/500]       | Train: Loss 0.69425, R2 0.80645, RMSE 0.82944                     | Test: Loss 0.84788, R2 0.77039, RMSE 0.91581\n",
      "Epoch [37/500]       | Train: Loss 0.67736, R2 0.80874, RMSE 0.82030                     | Test: Loss 0.76602, R2 0.79047, RMSE 0.86231\n",
      "Epoch [38/500]       | Train: Loss 0.67923, R2 0.81046, RMSE 0.81966                     | Test: Loss 0.80363, R2 0.78240, RMSE 0.89166\n",
      "Epoch [39/500]       | Train: Loss 0.67430, R2 0.80953, RMSE 0.81824                     | Test: Loss 0.77878, R2 0.78377, RMSE 0.86659\n",
      "Epoch [40/500]       | Train: Loss 0.68422, R2 0.80745, RMSE 0.82366                     | Test: Loss 0.77640, R2 0.79087, RMSE 0.87553\n",
      "Epoch [41/500]       | Train: Loss 0.65159, R2 0.81666, RMSE 0.80444                     | Test: Loss 0.77738, R2 0.78643, RMSE 0.86859\n",
      "Epoch [42/500]       | Train: Loss 0.66140, R2 0.81359, RMSE 0.81051                     | Test: Loss 0.77784, R2 0.79118, RMSE 0.87293\n",
      "Epoch [43/500]       | Train: Loss 0.65518, R2 0.81598, RMSE 0.80632                     | Test: Loss 0.79635, R2 0.78896, RMSE 0.88910\n",
      "Epoch [44/500]       | Train: Loss 0.65561, R2 0.81378, RMSE 0.80665                     | Test: Loss 0.74840, R2 0.79518, RMSE 0.85737\n",
      "Epoch [45/500]       | Train: Loss 0.64762, R2 0.81714, RMSE 0.80176                     | Test: Loss 0.78765, R2 0.78175, RMSE 0.87125\n",
      "Epoch [46/500]       | Train: Loss 0.65713, R2 0.81473, RMSE 0.80779                     | Test: Loss 0.77628, R2 0.78307, RMSE 0.87261\n",
      "Epoch [47/500]       | Train: Loss 0.62769, R2 0.82300, RMSE 0.78995                     | Test: Loss 0.77842, R2 0.78725, RMSE 0.87647\n",
      "Epoch [48/500]       | Train: Loss 0.64015, R2 0.81949, RMSE 0.79728                     | Test: Loss 0.86682, R2 0.75900, RMSE 0.92395\n",
      "Epoch [49/500]       | Train: Loss 0.63171, R2 0.82296, RMSE 0.79318                     | Test: Loss 0.79716, R2 0.79115, RMSE 0.88621\n",
      "Epoch [50/500]       | Train: Loss 0.62073, R2 0.82621, RMSE 0.78583                     | Test: Loss 0.74925, R2 0.78565, RMSE 0.85628\n",
      "Epoch [51/500]       | Train: Loss 0.62809, R2 0.82367, RMSE 0.78970                     | Test: Loss 0.81677, R2 0.77883, RMSE 0.89692\n",
      "Epoch [52/500]       | Train: Loss 0.60945, R2 0.82724, RMSE 0.77839                     | Test: Loss 0.81315, R2 0.73642, RMSE 0.89599\n",
      "Epoch [53/500]       | Train: Loss 0.60470, R2 0.83041, RMSE 0.77476                     | Test: Loss 0.89561, R2 0.73290, RMSE 0.91667\n",
      "Epoch [54/500]       | Train: Loss 0.60680, R2 0.83014, RMSE 0.77539                     | Test: Loss 0.73785, R2 0.79435, RMSE 0.84553\n",
      "Epoch [55/500]       | Train: Loss 0.59551, R2 0.83195, RMSE 0.76885                     | Test: Loss 0.77853, R2 0.78384, RMSE 0.87603\n",
      "Epoch [56/500]       | Train: Loss 0.59393, R2 0.83125, RMSE 0.76832                     | Test: Loss 0.77566, R2 0.79364, RMSE 0.87612\n",
      "Epoch [57/500]       | Train: Loss 0.59357, R2 0.83311, RMSE 0.76728                     | Test: Loss 0.78260, R2 0.78548, RMSE 0.87195\n",
      "Epoch [58/500]       | Train: Loss 0.61815, R2 0.82615, RMSE 0.78336                     | Test: Loss 0.83830, R2 0.78351, RMSE 0.90746\n",
      "Epoch [59/500]       | Train: Loss 0.59740, R2 0.83199, RMSE 0.77072                     | Test: Loss 0.83903, R2 0.75917, RMSE 0.90951\n",
      "Epoch [60/500]       | Train: Loss 0.60654, R2 0.82873, RMSE 0.77689                     | Test: Loss 0.74783, R2 0.79821, RMSE 0.85575\n",
      "Epoch [61/500]       | Train: Loss 0.58678, R2 0.83498, RMSE 0.76393                     | Test: Loss 0.80150, R2 0.79082, RMSE 0.88992\n",
      "Epoch [62/500]       | Train: Loss 0.58003, R2 0.83603, RMSE 0.75891                     | Test: Loss 0.77879, R2 0.79184, RMSE 0.87179\n",
      "Epoch [63/500]       | Train: Loss 0.57121, R2 0.83936, RMSE 0.75373                     | Test: Loss 0.80173, R2 0.78385, RMSE 0.88847\n",
      "Epoch [64/500]       | Train: Loss 0.56890, R2 0.84004, RMSE 0.75259                     | Test: Loss 0.80020, R2 0.78404, RMSE 0.88631\n",
      "Epoch [65/500]       | Train: Loss 0.55528, R2 0.84385, RMSE 0.74332                     | Test: Loss 0.77548, R2 0.78877, RMSE 0.87315\n",
      "Epoch [66/500]       | Train: Loss 0.56603, R2 0.83943, RMSE 0.74993                     | Test: Loss 0.77405, R2 0.79476, RMSE 0.87310\n",
      "Epoch [67/500]       | Train: Loss 0.56291, R2 0.84165, RMSE 0.74820                     | Test: Loss 0.88311, R2 0.78175, RMSE 0.91648\n",
      "Epoch [68/500]       | Train: Loss 0.55235, R2 0.84398, RMSE 0.74054                     | Test: Loss 0.76691, R2 0.78600, RMSE 0.86338\n",
      "Epoch [69/500]       | Train: Loss 0.54495, R2 0.84513, RMSE 0.73640                     | Test: Loss 0.73887, R2 0.79784, RMSE 0.84474\n",
      "Epoch [70/500]       | Train: Loss 0.53221, R2 0.84945, RMSE 0.72772                     | Test: Loss 0.84479, R2 0.76991, RMSE 0.90382\n",
      "Epoch [71/500]       | Train: Loss 0.53582, R2 0.85013, RMSE 0.72927                     | Test: Loss 0.78010, R2 0.77278, RMSE 0.87266\n",
      "Epoch [72/500]       | Train: Loss 0.53479, R2 0.84911, RMSE 0.72922                     | Test: Loss 0.76868, R2 0.78356, RMSE 0.86485\n",
      "Epoch [73/500]       | Train: Loss 0.53839, R2 0.84950, RMSE 0.73199                     | Test: Loss 0.79488, R2 0.78097, RMSE 0.87590\n",
      "Epoch [74/500]       | Train: Loss 0.52029, R2 0.85326, RMSE 0.71930                     | Test: Loss 1.03518, R2 0.71329, RMSE 0.96569\n",
      "Epoch [75/500]       | Train: Loss 0.52800, R2 0.85190, RMSE 0.72372                     | Test: Loss 0.78930, R2 0.77783, RMSE 0.88393\n",
      "Epoch [76/500]       | Train: Loss 0.52612, R2 0.85221, RMSE 0.72285                     | Test: Loss 0.77511, R2 0.78103, RMSE 0.86955\n",
      "Epoch [77/500]       | Train: Loss 0.51910, R2 0.85452, RMSE 0.71832                     | Test: Loss 0.88252, R2 0.77104, RMSE 0.92808\n",
      "Epoch [78/500]       | Train: Loss 0.53310, R2 0.85072, RMSE 0.72777                     | Test: Loss 0.77117, R2 0.78745, RMSE 0.86580\n",
      "Epoch [79/500]       | Train: Loss 0.53281, R2 0.84883, RMSE 0.72823                     | Test: Loss 0.83944, R2 0.75703, RMSE 0.91291\n",
      "Epoch [80/500]       | Train: Loss 0.52231, R2 0.85244, RMSE 0.72014                     | Test: Loss 0.78928, R2 0.78425, RMSE 0.88225\n",
      "Epoch [81/500]       | Train: Loss 0.50448, R2 0.85654, RMSE 0.70817                     | Test: Loss 0.84525, R2 0.76625, RMSE 0.91163\n",
      "Epoch [82/500]       | Train: Loss 0.53305, R2 0.84917, RMSE 0.72676                     | Test: Loss 0.76755, R2 0.78904, RMSE 0.85868\n",
      "Epoch [83/500]       | Train: Loss 0.50290, R2 0.85737, RMSE 0.70659                     | Test: Loss 0.78595, R2 0.77596, RMSE 0.87522\n",
      "Epoch [84/500]       | Train: Loss 0.49898, R2 0.85981, RMSE 0.70432                     | Test: Loss 0.75903, R2 0.78432, RMSE 0.86210\n",
      "Epoch [85/500]       | Train: Loss 0.49190, R2 0.86179, RMSE 0.69957                     | Test: Loss 0.79501, R2 0.78062, RMSE 0.88544\n",
      "Epoch [86/500]       | Train: Loss 0.48392, R2 0.86452, RMSE 0.69340                     | Test: Loss 0.84244, R2 0.77740, RMSE 0.90629\n",
      "Epoch [87/500]       | Train: Loss 0.49183, R2 0.86151, RMSE 0.69957                     | Test: Loss 0.76243, R2 0.77935, RMSE 0.86639\n",
      "Epoch [88/500]       | Train: Loss 0.49982, R2 0.85936, RMSE 0.70507                     | Test: Loss 0.88645, R2 0.75102, RMSE 0.93066\n",
      "Epoch [89/500]       | Train: Loss 0.48203, R2 0.86458, RMSE 0.69299                     | Test: Loss 0.80402, R2 0.76113, RMSE 0.89134\n",
      "Epoch [90/500]       | Train: Loss 0.48058, R2 0.86408, RMSE 0.69160                     | Test: Loss 0.88055, R2 0.75947, RMSE 0.92763\n",
      "Epoch [91/500]       | Train: Loss 0.47231, R2 0.86760, RMSE 0.68550                     | Test: Loss 0.80753, R2 0.78521, RMSE 0.89062\n",
      "Epoch [92/500]       | Train: Loss 0.46861, R2 0.86786, RMSE 0.68280                     | Test: Loss 0.81016, R2 0.77161, RMSE 0.88953\n",
      "Epoch [93/500]       | Train: Loss 0.46845, R2 0.86738, RMSE 0.68261                     | Test: Loss 0.79892, R2 0.78232, RMSE 0.88955\n",
      "Epoch [94/500]       | Train: Loss 0.46865, R2 0.86636, RMSE 0.68296                     | Test: Loss 0.92747, R2 0.75593, RMSE 0.94346\n",
      "Epoch [95/500]       | Train: Loss 0.45859, R2 0.86886, RMSE 0.67521                     | Test: Loss 0.77907, R2 0.78266, RMSE 0.86735\n",
      "Epoch [96/500]       | Train: Loss 0.47084, R2 0.86692, RMSE 0.68403                     | Test: Loss 0.79421, R2 0.78483, RMSE 0.88467\n",
      "Epoch [97/500]       | Train: Loss 0.44910, R2 0.87216, RMSE 0.66858                     | Test: Loss 0.79984, R2 0.78225, RMSE 0.87575\n",
      "Epoch [98/500]       | Train: Loss 0.44109, R2 0.87522, RMSE 0.66263                     | Test: Loss 0.80540, R2 0.78207, RMSE 0.89053\n",
      "Epoch [99/500]       | Train: Loss 0.44073, R2 0.87447, RMSE 0.66251                     | Test: Loss 0.80251, R2 0.78214, RMSE 0.88771\n",
      "Epoch [100/500]      | Train: Loss 0.44153, R2 0.87524, RMSE 0.66320                     | Test: Loss 0.78323, R2 0.78943, RMSE 0.87113\n",
      "Epoch [101/500]      | Train: Loss 0.43262, R2 0.87780, RMSE 0.65585                     | Test: Loss 0.85491, R2 0.76073, RMSE 0.91340\n",
      "Epoch [102/500]      | Train: Loss 0.43548, R2 0.87651, RMSE 0.65856                     | Test: Loss 0.81395, R2 0.77533, RMSE 0.89597\n",
      "Epoch [103/500]      | Train: Loss 0.42808, R2 0.87948, RMSE 0.65308                     | Test: Loss 0.81500, R2 0.77682, RMSE 0.89501\n",
      "Epoch [104/500]      | Train: Loss 0.43910, R2 0.87686, RMSE 0.66075                     | Test: Loss 0.83455, R2 0.76158, RMSE 0.90295\n",
      "Epoch [105/500]      | Train: Loss 0.43529, R2 0.87739, RMSE 0.65878                     | Test: Loss 0.88906, R2 0.76519, RMSE 0.92232\n",
      "Epoch [106/500]      | Train: Loss 0.41681, R2 0.88153, RMSE 0.64423                     | Test: Loss 0.82899, R2 0.76739, RMSE 0.90483\n",
      "Epoch [107/500]      | Train: Loss 0.41136, R2 0.88487, RMSE 0.64017                     | Test: Loss 0.79789, R2 0.77733, RMSE 0.88381\n",
      "Epoch [108/500]      | Train: Loss 0.41521, R2 0.88311, RMSE 0.64344                     | Test: Loss 1.05661, R2 0.74035, RMSE 0.96836\n",
      "Epoch [109/500]      | Train: Loss 0.41470, R2 0.88300, RMSE 0.64272                     | Test: Loss 0.79755, R2 0.77940, RMSE 0.88714\n",
      "Epoch [110/500]      | Train: Loss 0.42671, R2 0.87926, RMSE 0.65198                     | Test: Loss 0.82977, R2 0.77863, RMSE 0.90487\n",
      "Epoch [111/500]      | Train: Loss 0.40196, R2 0.88520, RMSE 0.63283                     | Test: Loss 0.80812, R2 0.76985, RMSE 0.89115\n",
      "Epoch [112/500]      | Train: Loss 0.39330, R2 0.88859, RMSE 0.62617                     | Test: Loss 0.80864, R2 0.77528, RMSE 0.88679\n",
      "Epoch [113/500]      | Train: Loss 0.40174, R2 0.88660, RMSE 0.63296                     | Test: Loss 0.77298, R2 0.77863, RMSE 0.86418\n",
      "Epoch [114/500]      | Train: Loss 0.39849, R2 0.88742, RMSE 0.63003                     | Test: Loss 1.02602, R2 0.75414, RMSE 0.96995\n",
      "Epoch [115/500]      | Train: Loss 0.39670, R2 0.88800, RMSE 0.62878                     | Test: Loss 0.79859, R2 0.77845, RMSE 0.87399\n",
      "Epoch [116/500]      | Train: Loss 0.40598, R2 0.88501, RMSE 0.63572                     | Test: Loss 0.78896, R2 0.78520, RMSE 0.86865\n",
      "Epoch [117/500]      | Train: Loss 0.38923, R2 0.88967, RMSE 0.62284                     | Test: Loss 0.90676, R2 0.75851, RMSE 0.94178\n",
      "Epoch [118/500]      | Train: Loss 0.38888, R2 0.88988, RMSE 0.62259                     | Test: Loss 0.81261, R2 0.77048, RMSE 0.89294\n",
      "Epoch [119/500]      | Train: Loss 0.38016, R2 0.89314, RMSE 0.61564                     | Test: Loss 0.79681, R2 0.76643, RMSE 0.88127\n",
      "Epoch [120/500]      | Train: Loss 0.38321, R2 0.89204, RMSE 0.61764                     | Test: Loss 0.79943, R2 0.77509, RMSE 0.88495\n",
      "Epoch [121/500]      | Train: Loss 0.38075, R2 0.89216, RMSE 0.61573                     | Test: Loss 0.82056, R2 0.77308, RMSE 0.89823\n",
      "Epoch [122/500]      | Train: Loss 0.38433, R2 0.88992, RMSE 0.61918                     | Test: Loss 0.90508, R2 0.76973, RMSE 0.94018\n",
      "Epoch [123/500]      | Train: Loss 0.36871, R2 0.89440, RMSE 0.60636                     | Test: Loss 0.84267, R2 0.77380, RMSE 0.91070\n",
      "Epoch [124/500]      | Train: Loss 0.36685, R2 0.89657, RMSE 0.60467                     | Test: Loss 0.84245, R2 0.76324, RMSE 0.91097\n",
      "Epoch [125/500]      | Train: Loss 0.35760, R2 0.89913, RMSE 0.59730                     | Test: Loss 0.80121, R2 0.78131, RMSE 0.88485\n",
      "Epoch [126/500]      | Train: Loss 0.35652, R2 0.89766, RMSE 0.59624                     | Test: Loss 0.88341, R2 0.74641, RMSE 0.93343\n",
      "Epoch [127/500]      | Train: Loss 0.35910, R2 0.89913, RMSE 0.59812                     | Test: Loss 0.84310, R2 0.75220, RMSE 0.91043\n",
      "Epoch [128/500]      | Train: Loss 0.35547, R2 0.89886, RMSE 0.59492                     | Test: Loss 0.88447, R2 0.73734, RMSE 0.93652\n",
      "Epoch [129/500]      | Train: Loss 0.33888, R2 0.90417, RMSE 0.58119                     | Test: Loss 0.81514, R2 0.77134, RMSE 0.89540\n",
      "Epoch [130/500]      | Train: Loss 0.35852, R2 0.89885, RMSE 0.59776                     | Test: Loss 0.84003, R2 0.77707, RMSE 0.91092\n",
      "Epoch [131/500]      | Train: Loss 0.34958, R2 0.90106, RMSE 0.58989                     | Test: Loss 0.84858, R2 0.76768, RMSE 0.91339\n",
      "Epoch [132/500]      | Train: Loss 0.34305, R2 0.90277, RMSE 0.58481                     | Test: Loss 0.90239, R2 0.76363, RMSE 0.94032\n",
      "Epoch [133/500]      | Train: Loss 0.34223, R2 0.90295, RMSE 0.58394                     | Test: Loss 0.82548, R2 0.76568, RMSE 0.90257\n",
      "Epoch [134/500]      | Train: Loss 0.33965, R2 0.90439, RMSE 0.58159                     | Test: Loss 0.88734, R2 0.76401, RMSE 0.93078\n",
      "Epoch [135/500]      | Train: Loss 0.34147, R2 0.90387, RMSE 0.58355                     | Test: Loss 0.83392, R2 0.75983, RMSE 0.90549\n",
      "Epoch [136/500]      | Train: Loss 0.32640, R2 0.90846, RMSE 0.57028                     | Test: Loss 0.81617, R2 0.78070, RMSE 0.88846\n",
      "Epoch [137/500]      | Train: Loss 0.32812, R2 0.90719, RMSE 0.57202                     | Test: Loss 0.89955, R2 0.75850, RMSE 0.93506\n",
      "Epoch [138/500]      | Train: Loss 0.32800, R2 0.90670, RMSE 0.57169                     | Test: Loss 0.83999, R2 0.77284, RMSE 0.90335\n",
      "Epoch [139/500]      | Train: Loss 0.31567, R2 0.91029, RMSE 0.56080                     | Test: Loss 0.85182, R2 0.75691, RMSE 0.91004\n",
      "Epoch [140/500]      | Train: Loss 0.31744, R2 0.91000, RMSE 0.56218                     | Test: Loss 0.86492, R2 0.75124, RMSE 0.92125\n",
      "Epoch [141/500]      | Train: Loss 0.34175, R2 0.90284, RMSE 0.58287                     | Test: Loss 0.80744, R2 0.78230, RMSE 0.88607\n",
      "Epoch [142/500]      | Train: Loss 0.31491, R2 0.91058, RMSE 0.56010                     | Test: Loss 0.81111, R2 0.77451, RMSE 0.89088\n",
      "Epoch [143/500]      | Train: Loss 0.31532, R2 0.91047, RMSE 0.56060                     | Test: Loss 0.86552, R2 0.76651, RMSE 0.92191\n",
      "Epoch [144/500]      | Train: Loss 0.30986, R2 0.91189, RMSE 0.55596                     | Test: Loss 0.83975, R2 0.77489, RMSE 0.91259\n",
      "Epoch [145/500]      | Train: Loss 0.29920, R2 0.91487, RMSE 0.54640                     | Test: Loss 0.86608, R2 0.76385, RMSE 0.92413\n",
      "Epoch [146/500]      | Train: Loss 0.30046, R2 0.91467, RMSE 0.54715                     | Test: Loss 0.93719, R2 0.74560, RMSE 0.95645\n",
      "Epoch [147/500]      | Train: Loss 0.30237, R2 0.91453, RMSE 0.54896                     | Test: Loss 0.83585, R2 0.74501, RMSE 0.90831\n",
      "Epoch [148/500]      | Train: Loss 0.29489, R2 0.91640, RMSE 0.54227                     | Test: Loss 0.83151, R2 0.76969, RMSE 0.90611\n",
      "Epoch [149/500]      | Train: Loss 0.29869, R2 0.91609, RMSE 0.54577                     | Test: Loss 0.83323, R2 0.77420, RMSE 0.90774\n",
      "Epoch [150/500]      | Train: Loss 0.29139, R2 0.91658, RMSE 0.53893                     | Test: Loss 0.83803, R2 0.76478, RMSE 0.90649\n",
      "Epoch [151/500]      | Train: Loss 0.29583, R2 0.91577, RMSE 0.54333                     | Test: Loss 0.82572, R2 0.77684, RMSE 0.89706\n",
      "Epoch [152/500]      | Train: Loss 0.29552, R2 0.91588, RMSE 0.54296                     | Test: Loss 0.82509, R2 0.78163, RMSE 0.90337\n",
      "Epoch [153/500]      | Train: Loss 0.28942, R2 0.91751, RMSE 0.53732                     | Test: Loss 0.84022, R2 0.75664, RMSE 0.91035\n",
      "Epoch [154/500]      | Train: Loss 0.28695, R2 0.91919, RMSE 0.53484                     | Test: Loss 0.88295, R2 0.75737, RMSE 0.93216\n",
      "Epoch [155/500]      | Train: Loss 0.28975, R2 0.91873, RMSE 0.53709                     | Test: Loss 0.84543, R2 0.76662, RMSE 0.91233\n",
      "Epoch [156/500]      | Train: Loss 0.27743, R2 0.92119, RMSE 0.52596                     | Test: Loss 0.86170, R2 0.76957, RMSE 0.92034\n",
      "Epoch [157/500]      | Train: Loss 0.26494, R2 0.92485, RMSE 0.51398                     | Test: Loss 0.93634, R2 0.70321, RMSE 0.95539\n",
      "Epoch [158/500]      | Train: Loss 0.27299, R2 0.92230, RMSE 0.52179                     | Test: Loss 0.87181, R2 0.76677, RMSE 0.92011\n",
      "Epoch [159/500]      | Train: Loss 0.26569, R2 0.92516, RMSE 0.51474                     | Test: Loss 0.83714, R2 0.76880, RMSE 0.91028\n",
      "Epoch [160/500]      | Train: Loss 0.26669, R2 0.92472, RMSE 0.51565                     | Test: Loss 0.87085, R2 0.76048, RMSE 0.92670\n",
      "Epoch [161/500]      | Train: Loss 0.26356, R2 0.92569, RMSE 0.51294                     | Test: Loss 0.85932, R2 0.73137, RMSE 0.92357\n",
      "Epoch [162/500]      | Train: Loss 0.25780, R2 0.92625, RMSE 0.50689                     | Test: Loss 0.86094, R2 0.77477, RMSE 0.92268\n",
      "Epoch [163/500]      | Train: Loss 0.26142, R2 0.92609, RMSE 0.51068                     | Test: Loss 0.82279, R2 0.77021, RMSE 0.89758\n",
      "Epoch [164/500]      | Train: Loss 0.25877, R2 0.92663, RMSE 0.50798                     | Test: Loss 0.85618, R2 0.75404, RMSE 0.91912\n",
      "Epoch [165/500]      | Train: Loss 0.26750, R2 0.92473, RMSE 0.51591                     | Test: Loss 0.83702, R2 0.75797, RMSE 0.91020\n",
      "Epoch [166/500]      | Train: Loss 0.25276, R2 0.92840, RMSE 0.50198                     | Test: Loss 0.85554, R2 0.76570, RMSE 0.91917\n",
      "Epoch [167/500]      | Train: Loss 0.24737, R2 0.92895, RMSE 0.49675                     | Test: Loss 0.86048, R2 0.76468, RMSE 0.91678\n",
      "Epoch [168/500]      | Train: Loss 0.24523, R2 0.93053, RMSE 0.49435                     | Test: Loss 0.86637, R2 0.76692, RMSE 0.92174\n",
      "Epoch [169/500]      | Train: Loss 0.23545, R2 0.93298, RMSE 0.48450                     | Test: Loss 0.87162, R2 0.76178, RMSE 0.92650\n",
      "Epoch [170/500]      | Train: Loss 0.24666, R2 0.92987, RMSE 0.49557                     | Test: Loss 0.87535, R2 0.76787, RMSE 0.93323\n",
      "Epoch [171/500]      | Train: Loss 0.24676, R2 0.93026, RMSE 0.49599                     | Test: Loss 0.82925, R2 0.76433, RMSE 0.90037\n",
      "Epoch [172/500]      | Train: Loss 0.23453, R2 0.93365, RMSE 0.48365                     | Test: Loss 0.86573, R2 0.75734, RMSE 0.92186\n",
      "Epoch [173/500]      | Train: Loss 0.23207, R2 0.93423, RMSE 0.48085                     | Test: Loss 0.82602, R2 0.76701, RMSE 0.90041\n",
      "Epoch [174/500]      | Train: Loss 0.22904, R2 0.93528, RMSE 0.47787                     | Test: Loss 0.89024, R2 0.76653, RMSE 0.93060\n",
      "Epoch [175/500]      | Train: Loss 0.23237, R2 0.93430, RMSE 0.48144                     | Test: Loss 0.86972, R2 0.76791, RMSE 0.92812\n",
      "Epoch [176/500]      | Train: Loss 0.23308, R2 0.93398, RMSE 0.48192                     | Test: Loss 0.87001, R2 0.76432, RMSE 0.92722\n",
      "Epoch [177/500]      | Train: Loss 0.22554, R2 0.93604, RMSE 0.47434                     | Test: Loss 0.87862, R2 0.74507, RMSE 0.92914\n",
      "Epoch [178/500]      | Train: Loss 0.22226, R2 0.93753, RMSE 0.47043                     | Test: Loss 0.84871, R2 0.75544, RMSE 0.91356\n",
      "Epoch [179/500]      | Train: Loss 0.21476, R2 0.93889, RMSE 0.46285                     | Test: Loss 0.84624, R2 0.75398, RMSE 0.91366\n",
      "Epoch [180/500]      | Train: Loss 0.22236, R2 0.93810, RMSE 0.47084                     | Test: Loss 0.86649, R2 0.75094, RMSE 0.92109\n",
      "Epoch [181/500]      | Train: Loss 0.21538, R2 0.93883, RMSE 0.46327                     | Test: Loss 0.87904, R2 0.75985, RMSE 0.92991\n",
      "Epoch [182/500]      | Train: Loss 0.22784, R2 0.93587, RMSE 0.47653                     | Test: Loss 0.88181, R2 0.75618, RMSE 0.92816\n",
      "Epoch [183/500]      | Train: Loss 0.21553, R2 0.93840, RMSE 0.46329                     | Test: Loss 0.88128, R2 0.75296, RMSE 0.93192\n",
      "Epoch [184/500]      | Train: Loss 0.21162, R2 0.93995, RMSE 0.45910                     | Test: Loss 0.85864, R2 0.76410, RMSE 0.91069\n",
      "Epoch [185/500]      | Train: Loss 0.20081, R2 0.94353, RMSE 0.44741                     | Test: Loss 0.90497, R2 0.73338, RMSE 0.94467\n",
      "Epoch [186/500]      | Train: Loss 0.20553, R2 0.94157, RMSE 0.45248                     | Test: Loss 0.89141, R2 0.75946, RMSE 0.93922\n",
      "Epoch [187/500]      | Train: Loss 0.19984, R2 0.94393, RMSE 0.44650                     | Test: Loss 0.90983, R2 0.76885, RMSE 0.94383\n",
      "Epoch [188/500]      | Train: Loss 0.20221, R2 0.94269, RMSE 0.44895                     | Test: Loss 0.87240, R2 0.75214, RMSE 0.92808\n",
      "Epoch [189/500]      | Train: Loss 0.20438, R2 0.94191, RMSE 0.45138                     | Test: Loss 0.82086, R2 0.77448, RMSE 0.89260\n",
      "Epoch [190/500]      | Train: Loss 0.19377, R2 0.94519, RMSE 0.43946                     | Test: Loss 0.88261, R2 0.74519, RMSE 0.93334\n",
      "Epoch [191/500]      | Train: Loss 0.19019, R2 0.94656, RMSE 0.43528                     | Test: Loss 0.87437, R2 0.75413, RMSE 0.92779\n",
      "Epoch [192/500]      | Train: Loss 0.18975, R2 0.94581, RMSE 0.43489                     | Test: Loss 0.87004, R2 0.76645, RMSE 0.92736\n",
      "Epoch [193/500]      | Train: Loss 0.18980, R2 0.94626, RMSE 0.43507                     | Test: Loss 0.87983, R2 0.75129, RMSE 0.93073\n",
      "Epoch [194/500]      | Train: Loss 0.18799, R2 0.94707, RMSE 0.43291                     | Test: Loss 0.84460, R2 0.76072, RMSE 0.90427\n",
      "Epoch [195/500]      | Train: Loss 0.17957, R2 0.94928, RMSE 0.42303                     | Test: Loss 0.91041, R2 0.75741, RMSE 0.94716\n",
      "Epoch [196/500]      | Train: Loss 0.18932, R2 0.94623, RMSE 0.43421                     | Test: Loss 0.85031, R2 0.76418, RMSE 0.90862\n",
      "Epoch [197/500]      | Train: Loss 0.19627, R2 0.94475, RMSE 0.44201                     | Test: Loss 0.89666, R2 0.71927, RMSE 0.94393\n",
      "Epoch [198/500]      | Train: Loss 0.18066, R2 0.94869, RMSE 0.42404                     | Test: Loss 0.98971, R2 0.72205, RMSE 0.97730\n",
      "Epoch [199/500]      | Train: Loss 0.17982, R2 0.94897, RMSE 0.42356                     | Test: Loss 1.04015, R2 0.69794, RMSE 0.98940\n",
      "Epoch [200/500]      | Train: Loss 0.18036, R2 0.94870, RMSE 0.42383                     | Test: Loss 0.85543, R2 0.76424, RMSE 0.91582\n",
      "Epoch [201/500]      | Train: Loss 0.18426, R2 0.94804, RMSE 0.42854                     | Test: Loss 0.88666, R2 0.76119, RMSE 0.92979\n",
      "Epoch [202/500]      | Train: Loss 0.17261, R2 0.95124, RMSE 0.41472                     | Test: Loss 0.92208, R2 0.73001, RMSE 0.95489\n",
      "Epoch [203/500]      | Train: Loss 0.17088, R2 0.95133, RMSE 0.41271                     | Test: Loss 0.91813, R2 0.73506, RMSE 0.95382\n",
      "Epoch [204/500]      | Train: Loss 0.16804, R2 0.95275, RMSE 0.40926                     | Test: Loss 0.86624, R2 0.76257, RMSE 0.92425\n",
      "Epoch [205/500]      | Train: Loss 0.16421, R2 0.95301, RMSE 0.40477                     | Test: Loss 0.93972, R2 0.73044, RMSE 0.96332\n",
      "Epoch [206/500]      | Train: Loss 0.16249, R2 0.95437, RMSE 0.40241                     | Test: Loss 0.84718, R2 0.76620, RMSE 0.91074\n",
      "Epoch [207/500]      | Train: Loss 0.16429, R2 0.95330, RMSE 0.40463                     | Test: Loss 0.87872, R2 0.75328, RMSE 0.93307\n",
      "Epoch [208/500]      | Train: Loss 0.16912, R2 0.95195, RMSE 0.41047                     | Test: Loss 0.91731, R2 0.74531, RMSE 0.95580\n",
      "Epoch [209/500]      | Train: Loss 0.16265, R2 0.95399, RMSE 0.40246                     | Test: Loss 0.87039, R2 0.76068, RMSE 0.91956\n",
      "Epoch [210/500]      | Train: Loss 0.16579, R2 0.95279, RMSE 0.40643                     | Test: Loss 0.93596, R2 0.71772, RMSE 0.95814\n",
      "Epoch [211/500]      | Train: Loss 0.16085, R2 0.95504, RMSE 0.40025                     | Test: Loss 0.92689, R2 0.75696, RMSE 0.95431\n",
      "Epoch [212/500]      | Train: Loss 0.15828, R2 0.95531, RMSE 0.39735                     | Test: Loss 0.90504, R2 0.74173, RMSE 0.94282\n",
      "Epoch [213/500]      | Train: Loss 0.15177, R2 0.95663, RMSE 0.38883                     | Test: Loss 0.86792, R2 0.76068, RMSE 0.92552\n",
      "Epoch [214/500]      | Train: Loss 0.15787, R2 0.95554, RMSE 0.39668                     | Test: Loss 0.91149, R2 0.75029, RMSE 0.95045\n",
      "Epoch [215/500]      | Train: Loss 0.15155, R2 0.95725, RMSE 0.38870                     | Test: Loss 0.88102, R2 0.76032, RMSE 0.92941\n",
      "Epoch [216/500]      | Train: Loss 0.15274, R2 0.95646, RMSE 0.39031                     | Test: Loss 0.92693, R2 0.75264, RMSE 0.95622\n",
      "Epoch [217/500]      | Train: Loss 0.15655, R2 0.95537, RMSE 0.39508                     | Test: Loss 0.89737, R2 0.75561, RMSE 0.94023\n",
      "Epoch [218/500]      | Train: Loss 0.14994, R2 0.95751, RMSE 0.38651                     | Test: Loss 0.88874, R2 0.75392, RMSE 0.93513\n",
      "Epoch [219/500]      | Train: Loss 0.14581, R2 0.95869, RMSE 0.38124                     | Test: Loss 0.89863, R2 0.76282, RMSE 0.94151\n",
      "Epoch [220/500]      | Train: Loss 0.14133, R2 0.95995, RMSE 0.37477                     | Test: Loss 0.95141, R2 0.64666, RMSE 0.96862\n",
      "Epoch [221/500]      | Train: Loss 0.14284, R2 0.95988, RMSE 0.37712                     | Test: Loss 0.90512, R2 0.75334, RMSE 0.94860\n",
      "Epoch [222/500]      | Train: Loss 0.14364, R2 0.95963, RMSE 0.37825                     | Test: Loss 0.88164, R2 0.75784, RMSE 0.93278\n",
      "Epoch [223/500]      | Train: Loss 0.14609, R2 0.95876, RMSE 0.38134                     | Test: Loss 0.88717, R2 0.74782, RMSE 0.93165\n",
      "Epoch [224/500]      | Train: Loss 0.14211, R2 0.95974, RMSE 0.37632                     | Test: Loss 0.87979, R2 0.76195, RMSE 0.93345\n",
      "Epoch [225/500]      | Train: Loss 0.14076, R2 0.96024, RMSE 0.37434                     | Test: Loss 0.91654, R2 0.75025, RMSE 0.95159\n",
      "Epoch [226/500]      | Train: Loss 0.14066, R2 0.96019, RMSE 0.37427                     | Test: Loss 1.01670, R2 0.73896, RMSE 0.98969\n",
      "Epoch [227/500]      | Train: Loss 0.13918, R2 0.96061, RMSE 0.37246                     | Test: Loss 0.86291, R2 0.75860, RMSE 0.91353\n",
      "Epoch [228/500]      | Train: Loss 0.13460, R2 0.96207, RMSE 0.36631                     | Test: Loss 0.90919, R2 0.76002, RMSE 0.94382\n",
      "Epoch [229/500]      | Train: Loss 0.13384, R2 0.96232, RMSE 0.36513                     | Test: Loss 0.96208, R2 0.75148, RMSE 0.96705\n",
      "Epoch [230/500]      | Train: Loss 0.13353, R2 0.96183, RMSE 0.36469                     | Test: Loss 0.90374, R2 0.73241, RMSE 0.94368\n",
      "Epoch [231/500]      | Train: Loss 0.13563, R2 0.96187, RMSE 0.36751                     | Test: Loss 0.90613, R2 0.74451, RMSE 0.94868\n",
      "Epoch [232/500]      | Train: Loss 0.13007, R2 0.96317, RMSE 0.36008                     | Test: Loss 0.90438, R2 0.75584, RMSE 0.94427\n",
      "Epoch [233/500]      | Train: Loss 0.13903, R2 0.96060, RMSE 0.37200                     | Test: Loss 0.98291, R2 0.73833, RMSE 0.97474\n",
      "Epoch [234/500]      | Train: Loss 0.12802, R2 0.96370, RMSE 0.35704                     | Test: Loss 0.89441, R2 0.75599, RMSE 0.93945\n",
      "Epoch [235/500]      | Train: Loss 0.12889, R2 0.96356, RMSE 0.35851                     | Test: Loss 0.91171, R2 0.74181, RMSE 0.94809\n",
      "Epoch [236/500]      | Train: Loss 0.12421, R2 0.96505, RMSE 0.35196                     | Test: Loss 0.97646, R2 0.72680, RMSE 0.97921\n",
      "Epoch [237/500]      | Train: Loss 0.12411, R2 0.96490, RMSE 0.35158                     | Test: Loss 0.93933, R2 0.75514, RMSE 0.96137\n",
      "Epoch [238/500]      | Train: Loss 0.11912, R2 0.96638, RMSE 0.34440                     | Test: Loss 0.90330, R2 0.74206, RMSE 0.93949\n",
      "Epoch [239/500]      | Train: Loss 0.12204, R2 0.96512, RMSE 0.34868                     | Test: Loss 0.86949, R2 0.75848, RMSE 0.92372\n",
      "Epoch [240/500]      | Train: Loss 0.12241, R2 0.96559, RMSE 0.34917                     | Test: Loss 0.89610, R2 0.73859, RMSE 0.93820\n",
      "Epoch [241/500]      | Train: Loss 0.12489, R2 0.96458, RMSE 0.35254                     | Test: Loss 0.92347, R2 0.75559, RMSE 0.95288\n",
      "Epoch [242/500]      | Train: Loss 0.12358, R2 0.96498, RMSE 0.35098                     | Test: Loss 0.92456, R2 0.73925, RMSE 0.95451\n",
      "Epoch [243/500]      | Train: Loss 0.11900, R2 0.96641, RMSE 0.34423                     | Test: Loss 0.88947, R2 0.75279, RMSE 0.93795\n",
      "Epoch [244/500]      | Train: Loss 0.11538, R2 0.96748, RMSE 0.33894                     | Test: Loss 0.95579, R2 0.74052, RMSE 0.97277\n",
      "Epoch [245/500]      | Train: Loss 0.12197, R2 0.96562, RMSE 0.34844                     | Test: Loss 0.88408, R2 0.75774, RMSE 0.92817\n",
      "Epoch [246/500]      | Train: Loss 0.11994, R2 0.96618, RMSE 0.34564                     | Test: Loss 0.87649, R2 0.74980, RMSE 0.92583\n",
      "Epoch [247/500]      | Train: Loss 0.11144, R2 0.96866, RMSE 0.33327                     | Test: Loss 0.94985, R2 0.75034, RMSE 0.96570\n",
      "Epoch [248/500]      | Train: Loss 0.11242, R2 0.96826, RMSE 0.33478                     | Test: Loss 0.90390, R2 0.74840, RMSE 0.94332\n",
      "Epoch [249/500]      | Train: Loss 0.11436, R2 0.96746, RMSE 0.33736                     | Test: Loss 0.91496, R2 0.75740, RMSE 0.95298\n",
      "Epoch [250/500]      | Train: Loss 0.11521, R2 0.96701, RMSE 0.33898                     | Test: Loss 0.90978, R2 0.74552, RMSE 0.94366\n",
      "Epoch [251/500]      | Train: Loss 0.11369, R2 0.96785, RMSE 0.33642                     | Test: Loss 0.87197, R2 0.76028, RMSE 0.92796\n",
      "Epoch [252/500]      | Train: Loss 0.11201, R2 0.96868, RMSE 0.33401                     | Test: Loss 0.99144, R2 0.70398, RMSE 0.98052\n",
      "Epoch [253/500]      | Train: Loss 0.10759, R2 0.96943, RMSE 0.32747                     | Test: Loss 0.95742, R2 0.75639, RMSE 0.96473\n",
      "Epoch [254/500]      | Train: Loss 0.11420, R2 0.96789, RMSE 0.33730                     | Test: Loss 0.88478, R2 0.75874, RMSE 0.93337\n",
      "Epoch [255/500]      | Train: Loss 0.10943, R2 0.96920, RMSE 0.33002                     | Test: Loss 0.88244, R2 0.75496, RMSE 0.93087\n",
      "Epoch [256/500]      | Train: Loss 0.10897, R2 0.96915, RMSE 0.32936                     | Test: Loss 0.93621, R2 0.74465, RMSE 0.95789\n",
      "Epoch [257/500]      | Train: Loss 0.10767, R2 0.96957, RMSE 0.32753                     | Test: Loss 0.87483, R2 0.76892, RMSE 0.92517\n",
      "Epoch [258/500]      | Train: Loss 0.10707, R2 0.96958, RMSE 0.32672                     | Test: Loss 1.23620, R2 0.71754, RMSE 1.04358\n",
      "Epoch [259/500]      | Train: Loss 0.10737, R2 0.96971, RMSE 0.32708                     | Test: Loss 1.21369, R2 0.73059, RMSE 1.03969\n",
      "Epoch [260/500]      | Train: Loss 0.11370, R2 0.96785, RMSE 0.33625                     | Test: Loss 0.96370, R2 0.73563, RMSE 0.97634\n",
      "Epoch [261/500]      | Train: Loss 0.11683, R2 0.96718, RMSE 0.34093                     | Test: Loss 0.88412, R2 0.75212, RMSE 0.92671\n",
      "Epoch [262/500]      | Train: Loss 0.10534, R2 0.97015, RMSE 0.32381                     | Test: Loss 0.88951, R2 0.75369, RMSE 0.93474\n",
      "Epoch [263/500]      | Train: Loss 0.11305, R2 0.96841, RMSE 0.33525                     | Test: Loss 0.91804, R2 0.72207, RMSE 0.95075\n",
      "Epoch [264/500]      | Train: Loss 0.10497, R2 0.97031, RMSE 0.32333                     | Test: Loss 0.90725, R2 0.74389, RMSE 0.94529\n",
      "Epoch [265/500]      | Train: Loss 0.10206, R2 0.97129, RMSE 0.31890                     | Test: Loss 0.88480, R2 0.76380, RMSE 0.92764\n",
      "Epoch [266/500]      | Train: Loss 0.10320, R2 0.97109, RMSE 0.32050                     | Test: Loss 0.95197, R2 0.74474, RMSE 0.96987\n",
      "Epoch [267/500]      | Train: Loss 0.10027, R2 0.97173, RMSE 0.31605                     | Test: Loss 0.87296, R2 0.75968, RMSE 0.92883\n",
      "Epoch [268/500]      | Train: Loss 0.10189, R2 0.97136, RMSE 0.31847                     | Test: Loss 0.88184, R2 0.76088, RMSE 0.93009\n",
      "Epoch [269/500]      | Train: Loss 0.10352, R2 0.97068, RMSE 0.32088                     | Test: Loss 0.91129, R2 0.74973, RMSE 0.94720\n",
      "Epoch [270/500]      | Train: Loss 0.09935, R2 0.97172, RMSE 0.31468                     | Test: Loss 0.95988, R2 0.75072, RMSE 0.96829\n",
      "Epoch [271/500]      | Train: Loss 0.10106, R2 0.97132, RMSE 0.31725                     | Test: Loss 0.91267, R2 0.74515, RMSE 0.95100\n",
      "Epoch [272/500]      | Train: Loss 0.10073, R2 0.97145, RMSE 0.31681                     | Test: Loss 0.91767, R2 0.74685, RMSE 0.95314\n",
      "Epoch [273/500]      | Train: Loss 0.09739, R2 0.97270, RMSE 0.31153                     | Test: Loss 0.90929, R2 0.75449, RMSE 0.94799\n",
      "Epoch [274/500]      | Train: Loss 0.09834, R2 0.97233, RMSE 0.31271                     | Test: Loss 0.86905, R2 0.75014, RMSE 0.92037\n",
      "Epoch [275/500]      | Train: Loss 0.09592, R2 0.97302, RMSE 0.30899                     | Test: Loss 0.88783, R2 0.73754, RMSE 0.93428\n",
      "Epoch [276/500]      | Train: Loss 0.09788, R2 0.97259, RMSE 0.31208                     | Test: Loss 0.87944, R2 0.74948, RMSE 0.93125\n",
      "Epoch [277/500]      | Train: Loss 0.09675, R2 0.97277, RMSE 0.31052                     | Test: Loss 0.92753, R2 0.75811, RMSE 0.95584\n",
      "Epoch [278/500]      | Train: Loss 0.09765, R2 0.97251, RMSE 0.31182                     | Test: Loss 0.93671, R2 0.74913, RMSE 0.96132\n",
      "Epoch [279/500]      | Train: Loss 0.09537, R2 0.97309, RMSE 0.30828                     | Test: Loss 0.93019, R2 0.73929, RMSE 0.95708\n",
      "Epoch [280/500]      | Train: Loss 0.09169, R2 0.97417, RMSE 0.30245                     | Test: Loss 0.87927, R2 0.75437, RMSE 0.92831\n",
      "Epoch [281/500]      | Train: Loss 0.09460, R2 0.97343, RMSE 0.30693                     | Test: Loss 0.96942, R2 0.73645, RMSE 0.96924\n",
      "Epoch [282/500]      | Train: Loss 0.09688, R2 0.97302, RMSE 0.31075                     | Test: Loss 0.88161, R2 0.75199, RMSE 0.92881\n",
      "Epoch [283/500]      | Train: Loss 0.09268, R2 0.97390, RMSE 0.30380                     | Test: Loss 0.87913, R2 0.75440, RMSE 0.92565\n",
      "Epoch [284/500]      | Train: Loss 0.09633, R2 0.97251, RMSE 0.30956                     | Test: Loss 0.87711, R2 0.75998, RMSE 0.92935\n",
      "Epoch [285/500]      | Train: Loss 0.09555, R2 0.97311, RMSE 0.30842                     | Test: Loss 1.01302, R2 0.75133, RMSE 0.98778\n",
      "Epoch [286/500]      | Train: Loss 0.09127, R2 0.97430, RMSE 0.30169                     | Test: Loss 0.91169, R2 0.75189, RMSE 0.94223\n",
      "Epoch [287/500]      | Train: Loss 0.09480, R2 0.97319, RMSE 0.30720                     | Test: Loss 1.00308, R2 0.72881, RMSE 0.98168\n",
      "Epoch [288/500]      | Train: Loss 0.09051, R2 0.97433, RMSE 0.30030                     | Test: Loss 0.94101, R2 0.72719, RMSE 0.96072\n",
      "Epoch [289/500]      | Train: Loss 0.09145, R2 0.97430, RMSE 0.30175                     | Test: Loss 0.91212, R2 0.73287, RMSE 0.95171\n",
      "Epoch [290/500]      | Train: Loss 0.09496, R2 0.97341, RMSE 0.30760                     | Test: Loss 0.91326, R2 0.73729, RMSE 0.95013\n",
      "Epoch [291/500]      | Train: Loss 0.08914, R2 0.97474, RMSE 0.29780                     | Test: Loss 0.92349, R2 0.75781, RMSE 0.95752\n",
      "Epoch [292/500]      | Train: Loss 0.08881, R2 0.97504, RMSE 0.29759                     | Test: Loss 0.92431, R2 0.74641, RMSE 0.95458\n",
      "Epoch [293/500]      | Train: Loss 0.09013, R2 0.97447, RMSE 0.29974                     | Test: Loss 0.90300, R2 0.75035, RMSE 0.94483\n",
      "Epoch [294/500]      | Train: Loss 0.09494, R2 0.97314, RMSE 0.30735                     | Test: Loss 0.89904, R2 0.75231, RMSE 0.93988\n",
      "Epoch [295/500]      | Train: Loss 0.09677, R2 0.97290, RMSE 0.31030                     | Test: Loss 0.90144, R2 0.74948, RMSE 0.94146\n",
      "Epoch [296/500]      | Train: Loss 0.09881, R2 0.97235, RMSE 0.31320                     | Test: Loss 0.91397, R2 0.75934, RMSE 0.94927\n",
      "Epoch [297/500]      | Train: Loss 0.09047, R2 0.97433, RMSE 0.30019                     | Test: Loss 0.93809, R2 0.74952, RMSE 0.96171\n",
      "Epoch [298/500]      | Train: Loss 0.08869, R2 0.97519, RMSE 0.29726                     | Test: Loss 0.89181, R2 0.75144, RMSE 0.93629\n",
      "Epoch [299/500]      | Train: Loss 0.08750, R2 0.97538, RMSE 0.29498                     | Test: Loss 0.88216, R2 0.75646, RMSE 0.93156\n",
      "Epoch [300/500]      | Train: Loss 0.08696, R2 0.97528, RMSE 0.29424                     | Test: Loss 0.87261, R2 0.74906, RMSE 0.92407\n",
      "Epoch [301/500]      | Train: Loss 0.08952, R2 0.97456, RMSE 0.29844                     | Test: Loss 0.89192, R2 0.75618, RMSE 0.93457\n",
      "Epoch [302/500]      | Train: Loss 0.08456, R2 0.97609, RMSE 0.29030                     | Test: Loss 0.89851, R2 0.74934, RMSE 0.93910\n",
      "Epoch [303/500]      | Train: Loss 0.08636, R2 0.97572, RMSE 0.29309                     | Test: Loss 0.85558, R2 0.76776, RMSE 0.90919\n",
      "Epoch [304/500]      | Train: Loss 0.08776, R2 0.97544, RMSE 0.29565                     | Test: Loss 0.98170, R2 0.73230, RMSE 0.97408\n",
      "Epoch [305/500]      | Train: Loss 0.08846, R2 0.97519, RMSE 0.29653                     | Test: Loss 0.89808, R2 0.73342, RMSE 0.93730\n",
      "Epoch [306/500]      | Train: Loss 0.08959, R2 0.97471, RMSE 0.29864                     | Test: Loss 1.19723, R2 0.72303, RMSE 1.03486\n",
      "Epoch [307/500]      | Train: Loss 0.08950, R2 0.97476, RMSE 0.29859                     | Test: Loss 0.86639, R2 0.75399, RMSE 0.92049\n",
      "Epoch [308/500]      | Train: Loss 0.08742, R2 0.97521, RMSE 0.29495                     | Test: Loss 0.87871, R2 0.75692, RMSE 0.92767\n",
      "Epoch [309/500]      | Train: Loss 0.08471, R2 0.97617, RMSE 0.29027                     | Test: Loss 0.92328, R2 0.73617, RMSE 0.95484\n",
      "Epoch [310/500]      | Train: Loss 0.08675, R2 0.97548, RMSE 0.29398                     | Test: Loss 0.89129, R2 0.74736, RMSE 0.93609\n",
      "Epoch [311/500]      | Train: Loss 0.08564, R2 0.97588, RMSE 0.29153                     | Test: Loss 0.90393, R2 0.76037, RMSE 0.94142\n",
      "Epoch [312/500]      | Train: Loss 0.08654, R2 0.97575, RMSE 0.29332                     | Test: Loss 1.02095, R2 0.70415, RMSE 0.99199\n",
      "Epoch [313/500]      | Train: Loss 0.08675, R2 0.97564, RMSE 0.29399                     | Test: Loss 0.93344, R2 0.73839, RMSE 0.96008\n",
      "Epoch [314/500]      | Train: Loss 0.08647, R2 0.97563, RMSE 0.29336                     | Test: Loss 0.91951, R2 0.72541, RMSE 0.95456\n",
      "Epoch [315/500]      | Train: Loss 0.08625, R2 0.97559, RMSE 0.29294                     | Test: Loss 0.86676, R2 0.76653, RMSE 0.91957\n",
      "Epoch [316/500]      | Train: Loss 0.08623, R2 0.97526, RMSE 0.29295                     | Test: Loss 0.91956, R2 0.75789, RMSE 0.95619\n",
      "Epoch [317/500]      | Train: Loss 0.08364, R2 0.97647, RMSE 0.28868                     | Test: Loss 0.88895, R2 0.75829, RMSE 0.92787\n",
      "Epoch [318/500]      | Train: Loss 0.08054, R2 0.97740, RMSE 0.28313                     | Test: Loss 0.91443, R2 0.75122, RMSE 0.95115\n",
      "Epoch [319/500]      | Train: Loss 0.08214, R2 0.97703, RMSE 0.28600                     | Test: Loss 0.91046, R2 0.71743, RMSE 0.94925\n",
      "Epoch [320/500]      | Train: Loss 0.08118, R2 0.97700, RMSE 0.28404                     | Test: Loss 0.90215, R2 0.75757, RMSE 0.93756\n",
      "Epoch [321/500]      | Train: Loss 0.08177, R2 0.97687, RMSE 0.28536                     | Test: Loss 0.89133, R2 0.75976, RMSE 0.93433\n",
      "Epoch [322/500]      | Train: Loss 0.08400, R2 0.97616, RMSE 0.28894                     | Test: Loss 0.91303, R2 0.74872, RMSE 0.95032\n",
      "Epoch [323/500]      | Train: Loss 0.08618, R2 0.97588, RMSE 0.29277                     | Test: Loss 0.89613, R2 0.75552, RMSE 0.93532\n",
      "Epoch [324/500]      | Train: Loss 0.08283, R2 0.97651, RMSE 0.28716                     | Test: Loss 0.90148, R2 0.73955, RMSE 0.94044\n",
      "Epoch [325/500]      | Train: Loss 0.07867, R2 0.97765, RMSE 0.27995                     | Test: Loss 0.89389, R2 0.75258, RMSE 0.94203\n",
      "Epoch [326/500]      | Train: Loss 0.08065, R2 0.97715, RMSE 0.28343                     | Test: Loss 0.88126, R2 0.75951, RMSE 0.93179\n",
      "Epoch [327/500]      | Train: Loss 0.08489, R2 0.97593, RMSE 0.29053                     | Test: Loss 0.85844, R2 0.75234, RMSE 0.91706\n",
      "Epoch [328/500]      | Train: Loss 0.08340, R2 0.97652, RMSE 0.28819                     | Test: Loss 0.90972, R2 0.75375, RMSE 0.94591\n",
      "Epoch [329/500]      | Train: Loss 0.08178, R2 0.97701, RMSE 0.28524                     | Test: Loss 0.87501, R2 0.74791, RMSE 0.92617\n",
      "Epoch [330/500]      | Train: Loss 0.08155, R2 0.97687, RMSE 0.28498                     | Test: Loss 0.93547, R2 0.73955, RMSE 0.95828\n",
      "Epoch [331/500]      | Train: Loss 0.08889, R2 0.97505, RMSE 0.29700                     | Test: Loss 0.85561, R2 0.76805, RMSE 0.91237\n",
      "Epoch [332/500]      | Train: Loss 0.07898, R2 0.97750, RMSE 0.28032                     | Test: Loss 0.87666, R2 0.76363, RMSE 0.93020\n",
      "Epoch [333/500]      | Train: Loss 0.07345, R2 0.97914, RMSE 0.27045                     | Test: Loss 0.91184, R2 0.75766, RMSE 0.95023\n",
      "Epoch [334/500]      | Train: Loss 0.08006, R2 0.97717, RMSE 0.28222                     | Test: Loss 0.94993, R2 0.74927, RMSE 0.96154\n",
      "Epoch [335/500]      | Train: Loss 0.07880, R2 0.97784, RMSE 0.28021                     | Test: Loss 0.88032, R2 0.75277, RMSE 0.93188\n",
      "Epoch [336/500]      | Train: Loss 0.07922, R2 0.97762, RMSE 0.28094                     | Test: Loss 0.87005, R2 0.75811, RMSE 0.92523\n",
      "Epoch [337/500]      | Train: Loss 0.07987, R2 0.97757, RMSE 0.28169                     | Test: Loss 0.90472, R2 0.75247, RMSE 0.94211\n",
      "Epoch [338/500]      | Train: Loss 0.07858, R2 0.97786, RMSE 0.27969                     | Test: Loss 0.87068, R2 0.76673, RMSE 0.92476\n",
      "Epoch [339/500]      | Train: Loss 0.07784, R2 0.97809, RMSE 0.27855                     | Test: Loss 0.87744, R2 0.74728, RMSE 0.93008\n",
      "Epoch [340/500]      | Train: Loss 0.07985, R2 0.97746, RMSE 0.28210                     | Test: Loss 0.88908, R2 0.75481, RMSE 0.92877\n",
      "Epoch [341/500]      | Train: Loss 0.07837, R2 0.97778, RMSE 0.27910                     | Test: Loss 0.87951, R2 0.75019, RMSE 0.92727\n",
      "Epoch [342/500]      | Train: Loss 0.07946, R2 0.97768, RMSE 0.28102                     | Test: Loss 0.88948, R2 0.73944, RMSE 0.93455\n",
      "Epoch [343/500]      | Train: Loss 0.08085, R2 0.97718, RMSE 0.28362                     | Test: Loss 0.87894, R2 0.75308, RMSE 0.93126\n",
      "Epoch [344/500]      | Train: Loss 0.07681, R2 0.97831, RMSE 0.27669                     | Test: Loss 0.89163, R2 0.71781, RMSE 0.93926\n",
      "Epoch [345/500]      | Train: Loss 0.07562, R2 0.97868, RMSE 0.27447                     | Test: Loss 0.88230, R2 0.75780, RMSE 0.92891\n",
      "Epoch [346/500]      | Train: Loss 0.07416, R2 0.97897, RMSE 0.27162                     | Test: Loss 0.93522, R2 0.74764, RMSE 0.96301\n",
      "Epoch [347/500]      | Train: Loss 0.07622, R2 0.97872, RMSE 0.27541                     | Test: Loss 0.91491, R2 0.75375, RMSE 0.94943\n",
      "Epoch [348/500]      | Train: Loss 0.07893, R2 0.97772, RMSE 0.28029                     | Test: Loss 0.87867, R2 0.75803, RMSE 0.92820\n",
      "Epoch [349/500]      | Train: Loss 0.07612, R2 0.97850, RMSE 0.27531                     | Test: Loss 0.88633, R2 0.74697, RMSE 0.93378\n",
      "Epoch [350/500]      | Train: Loss 0.07780, R2 0.97792, RMSE 0.27819                     | Test: Loss 0.88458, R2 0.74303, RMSE 0.93211\n",
      "Epoch [351/500]      | Train: Loss 0.07343, R2 0.97922, RMSE 0.27015                     | Test: Loss 0.91079, R2 0.75755, RMSE 0.94554\n",
      "Epoch [352/500]      | Train: Loss 0.07579, R2 0.97854, RMSE 0.27464                     | Test: Loss 0.86561, R2 0.76701, RMSE 0.91783\n",
      "Epoch [353/500]      | Train: Loss 0.07733, R2 0.97834, RMSE 0.27735                     | Test: Loss 0.88983, R2 0.75867, RMSE 0.93832\n",
      "Epoch [354/500]      | Train: Loss 0.07560, R2 0.97877, RMSE 0.27429                     | Test: Loss 0.89507, R2 0.74912, RMSE 0.93691\n",
      "Epoch [355/500]      | Train: Loss 0.08363, R2 0.97655, RMSE 0.28825                     | Test: Loss 0.90268, R2 0.73265, RMSE 0.94064\n",
      "Epoch [356/500]      | Train: Loss 0.07643, R2 0.97855, RMSE 0.27555                     | Test: Loss 0.87981, R2 0.75103, RMSE 0.93271\n",
      "Epoch [357/500]      | Train: Loss 0.07443, R2 0.97913, RMSE 0.27221                     | Test: Loss 0.88506, R2 0.75031, RMSE 0.93527\n",
      "Epoch [358/500]      | Train: Loss 0.07229, R2 0.97945, RMSE 0.26812                     | Test: Loss 0.92140, R2 0.73655, RMSE 0.94979\n",
      "Epoch [359/500]      | Train: Loss 0.07580, R2 0.97881, RMSE 0.27463                     | Test: Loss 0.89516, R2 0.73260, RMSE 0.93799\n",
      "Epoch [360/500]      | Train: Loss 0.07392, R2 0.97923, RMSE 0.27117                     | Test: Loss 0.91674, R2 0.75287, RMSE 0.94853\n",
      "Epoch [361/500]      | Train: Loss 0.07754, R2 0.97828, RMSE 0.27742                     | Test: Loss 0.90628, R2 0.75358, RMSE 0.94467\n",
      "Epoch [362/500]      | Train: Loss 0.07786, R2 0.97811, RMSE 0.27822                     | Test: Loss 0.85468, R2 0.76184, RMSE 0.90827\n",
      "Epoch [363/500]      | Train: Loss 0.07514, R2 0.97871, RMSE 0.27326                     | Test: Loss 0.89385, R2 0.75319, RMSE 0.93819\n",
      "Epoch [364/500]      | Train: Loss 0.07498, R2 0.97873, RMSE 0.27321                     | Test: Loss 0.88667, R2 0.75379, RMSE 0.93541\n",
      "Epoch [365/500]      | Train: Loss 0.06894, R2 0.98040, RMSE 0.26198                     | Test: Loss 0.91647, R2 0.72717, RMSE 0.95261\n",
      "Epoch [366/500]      | Train: Loss 0.07357, R2 0.97915, RMSE 0.27069                     | Test: Loss 0.89615, R2 0.75757, RMSE 0.93898\n",
      "Epoch [367/500]      | Train: Loss 0.07255, R2 0.97954, RMSE 0.26874                     | Test: Loss 0.87128, R2 0.74018, RMSE 0.92244\n",
      "Epoch [368/500]      | Train: Loss 0.07136, R2 0.98006, RMSE 0.26636                     | Test: Loss 0.87195, R2 0.74339, RMSE 0.92354\n",
      "Epoch [369/500]      | Train: Loss 0.07128, R2 0.97979, RMSE 0.26624                     | Test: Loss 0.91245, R2 0.74899, RMSE 0.95029\n",
      "Epoch [370/500]      | Train: Loss 0.07302, R2 0.97944, RMSE 0.26971                     | Test: Loss 0.87698, R2 0.75410, RMSE 0.93169\n",
      "Epoch [371/500]      | Train: Loss 0.07070, R2 0.98019, RMSE 0.26540                     | Test: Loss 0.87250, R2 0.75958, RMSE 0.92383\n",
      "Epoch [372/500]      | Train: Loss 0.07323, R2 0.97939, RMSE 0.26990                     | Test: Loss 0.87506, R2 0.74944, RMSE 0.92641\n",
      "Epoch [373/500]      | Train: Loss 0.07488, R2 0.97905, RMSE 0.27252                     | Test: Loss 0.86193, R2 0.76634, RMSE 0.91615\n",
      "Epoch [374/500]      | Train: Loss 0.07581, R2 0.97858, RMSE 0.27476                     | Test: Loss 0.86294, R2 0.75901, RMSE 0.92035\n",
      "Epoch [375/500]      | Train: Loss 0.07210, R2 0.97964, RMSE 0.26792                     | Test: Loss 0.88041, R2 0.74666, RMSE 0.93054\n",
      "Epoch [376/500]      | Train: Loss 0.07066, R2 0.97998, RMSE 0.26518                     | Test: Loss 0.86064, R2 0.75655, RMSE 0.91379\n",
      "Epoch [377/500]      | Train: Loss 0.07069, R2 0.98015, RMSE 0.26509                     | Test: Loss 0.87092, R2 0.75917, RMSE 0.92739\n",
      "Epoch [378/500]      | Train: Loss 0.06985, R2 0.98018, RMSE 0.26368                     | Test: Loss 0.91358, R2 0.73389, RMSE 0.95088\n",
      "Epoch [379/500]      | Train: Loss 0.07447, R2 0.97906, RMSE 0.27213                     | Test: Loss 0.97797, R2 0.74361, RMSE 0.97103\n",
      "Epoch [380/500]      | Train: Loss 0.07387, R2 0.97927, RMSE 0.27113                     | Test: Loss 0.87861, R2 0.75391, RMSE 0.93209\n",
      "Epoch [381/500]      | Train: Loss 0.07006, R2 0.98010, RMSE 0.26382                     | Test: Loss 0.88928, R2 0.75254, RMSE 0.93870\n",
      "Epoch [382/500]      | Train: Loss 0.07560, R2 0.97860, RMSE 0.27381                     | Test: Loss 0.91556, R2 0.75533, RMSE 0.94999\n",
      "Epoch [383/500]      | Train: Loss 0.07170, R2 0.97982, RMSE 0.26692                     | Test: Loss 0.88569, R2 0.76214, RMSE 0.92957\n",
      "Epoch [384/500]      | Train: Loss 0.07107, R2 0.97994, RMSE 0.26585                     | Test: Loss 0.91288, R2 0.74570, RMSE 0.94590\n",
      "Epoch [385/500]      | Train: Loss 0.06879, R2 0.98079, RMSE 0.26160                     | Test: Loss 0.88584, R2 0.75105, RMSE 0.93410\n",
      "Epoch [386/500]      | Train: Loss 0.07126, R2 0.97996, RMSE 0.26602                     | Test: Loss 0.86794, R2 0.76023, RMSE 0.92624\n",
      "Epoch [387/500]      | Train: Loss 0.06848, R2 0.98074, RMSE 0.26106                     | Test: Loss 0.96068, R2 0.74692, RMSE 0.96537\n",
      "Epoch [388/500]      | Train: Loss 0.06883, R2 0.98062, RMSE 0.26153                     | Test: Loss 0.91209, R2 0.75457, RMSE 0.95036\n",
      "Epoch [389/500]      | Train: Loss 0.07412, R2 0.97913, RMSE 0.27145                     | Test: Loss 0.87600, R2 0.75602, RMSE 0.92779\n",
      "Epoch [390/500]      | Train: Loss 0.07019, R2 0.98025, RMSE 0.26443                     | Test: Loss 0.86084, R2 0.75870, RMSE 0.91818\n",
      "Epoch [391/500]      | Train: Loss 0.07070, R2 0.98005, RMSE 0.26504                     | Test: Loss 0.90195, R2 0.76397, RMSE 0.94380\n",
      "Epoch [392/500]      | Train: Loss 0.07062, R2 0.98010, RMSE 0.26507                     | Test: Loss 0.85436, R2 0.76041, RMSE 0.91126\n",
      "Epoch [393/500]      | Train: Loss 0.07002, R2 0.98023, RMSE 0.26386                     | Test: Loss 0.92232, R2 0.74723, RMSE 0.95608\n",
      "Epoch [394/500]      | Train: Loss 0.07170, R2 0.97985, RMSE 0.26713                     | Test: Loss 0.86697, R2 0.76061, RMSE 0.92167\n",
      "Epoch [395/500]      | Train: Loss 0.07259, R2 0.97953, RMSE 0.26851                     | Test: Loss 0.84923, R2 0.76978, RMSE 0.91298\n",
      "Epoch [396/500]      | Train: Loss 0.06875, R2 0.98051, RMSE 0.26138                     | Test: Loss 0.85838, R2 0.76766, RMSE 0.90951\n",
      "Epoch [397/500]      | Train: Loss 0.07450, R2 0.97910, RMSE 0.27181                     | Test: Loss 0.86376, R2 0.76049, RMSE 0.91967\n",
      "Epoch [398/500]      | Train: Loss 0.07072, R2 0.98006, RMSE 0.26509                     | Test: Loss 0.92733, R2 0.75084, RMSE 0.95380\n",
      "Epoch [399/500]      | Train: Loss 0.06574, R2 0.98139, RMSE 0.25582                     | Test: Loss 0.94900, R2 0.72585, RMSE 0.96297\n",
      "Epoch [400/500]      | Train: Loss 0.06769, R2 0.98082, RMSE 0.25965                     | Test: Loss 0.88077, R2 0.76390, RMSE 0.93010\n",
      "Epoch [401/500]      | Train: Loss 0.06831, R2 0.98071, RMSE 0.26100                     | Test: Loss 0.93597, R2 0.72549, RMSE 0.95871\n",
      "Epoch [402/500]      | Train: Loss 0.06768, R2 0.98085, RMSE 0.25944                     | Test: Loss 0.84810, R2 0.76684, RMSE 0.90688\n",
      "Epoch [403/500]      | Train: Loss 0.06818, R2 0.98101, RMSE 0.26015                     | Test: Loss 0.86553, R2 0.76545, RMSE 0.91768\n",
      "Epoch [404/500]      | Train: Loss 0.06694, R2 0.98108, RMSE 0.25830                     | Test: Loss 0.89834, R2 0.76278, RMSE 0.93776\n",
      "Epoch [405/500]      | Train: Loss 0.06854, R2 0.98060, RMSE 0.26118                     | Test: Loss 0.90866, R2 0.75394, RMSE 0.94101\n",
      "Epoch [406/500]      | Train: Loss 0.06758, R2 0.98107, RMSE 0.25932                     | Test: Loss 0.89669, R2 0.76557, RMSE 0.93708\n",
      "Epoch [407/500]      | Train: Loss 0.06773, R2 0.98089, RMSE 0.25957                     | Test: Loss 0.84654, R2 0.76285, RMSE 0.90863\n",
      "Epoch [408/500]      | Train: Loss 0.06671, R2 0.98119, RMSE 0.25767                     | Test: Loss 0.85518, R2 0.76311, RMSE 0.91744\n",
      "Epoch [409/500]      | Train: Loss 0.06607, R2 0.98147, RMSE 0.25650                     | Test: Loss 0.95283, R2 0.75022, RMSE 0.96526\n",
      "Epoch [410/500]      | Train: Loss 0.06729, R2 0.98095, RMSE 0.25905                     | Test: Loss 0.88645, R2 0.75173, RMSE 0.93923\n",
      "Epoch [411/500]      | Train: Loss 0.06727, R2 0.98101, RMSE 0.25872                     | Test: Loss 0.86841, R2 0.76056, RMSE 0.92179\n",
      "Epoch [412/500]      | Train: Loss 0.06922, R2 0.98044, RMSE 0.26248                     | Test: Loss 0.87322, R2 0.74773, RMSE 0.92784\n",
      "Epoch [413/500]      | Train: Loss 0.06553, R2 0.98163, RMSE 0.25533                     | Test: Loss 0.88265, R2 0.76276, RMSE 0.93273\n",
      "Epoch [414/500]      | Train: Loss 0.06636, R2 0.98102, RMSE 0.25688                     | Test: Loss 0.85780, R2 0.77039, RMSE 0.91639\n",
      "Epoch [415/500]      | Train: Loss 0.06835, R2 0.98087, RMSE 0.26037                     | Test: Loss 0.84260, R2 0.76643, RMSE 0.90287\n",
      "Epoch [416/500]      | Train: Loss 0.06276, R2 0.98234, RMSE 0.24973                     | Test: Loss 0.86324, R2 0.76861, RMSE 0.91934\n",
      "Epoch [417/500]      | Train: Loss 0.06437, R2 0.98197, RMSE 0.25318                     | Test: Loss 0.85184, R2 0.76619, RMSE 0.91574\n",
      "Epoch [418/500]      | Train: Loss 0.06445, R2 0.98185, RMSE 0.25327                     | Test: Loss 0.85296, R2 0.76633, RMSE 0.91330\n",
      "Epoch [419/500]      | Train: Loss 0.06601, R2 0.98162, RMSE 0.25616                     | Test: Loss 0.85934, R2 0.75510, RMSE 0.91751\n",
      "Epoch [420/500]      | Train: Loss 0.06809, R2 0.98072, RMSE 0.26021                     | Test: Loss 0.93185, R2 0.74334, RMSE 0.95426\n",
      "Epoch [421/500]      | Train: Loss 0.06522, R2 0.98141, RMSE 0.25485                     | Test: Loss 0.88240, R2 0.72678, RMSE 0.93272\n",
      "Epoch [422/500]      | Train: Loss 0.06583, R2 0.98149, RMSE 0.25537                     | Test: Loss 0.97278, R2 0.75297, RMSE 0.97324\n",
      "Epoch [423/500]      | Train: Loss 0.06769, R2 0.98099, RMSE 0.25956                     | Test: Loss 0.86061, R2 0.75925, RMSE 0.91770\n",
      "Epoch [424/500]      | Train: Loss 0.06419, R2 0.98192, RMSE 0.25243                     | Test: Loss 0.85919, R2 0.75491, RMSE 0.92018\n",
      "Epoch [425/500]      | Train: Loss 0.06548, R2 0.98151, RMSE 0.25529                     | Test: Loss 0.88982, R2 0.75743, RMSE 0.93644\n",
      "Epoch [426/500]      | Train: Loss 0.06442, R2 0.98166, RMSE 0.25320                     | Test: Loss 0.84723, R2 0.77092, RMSE 0.90699\n",
      "Epoch [427/500]      | Train: Loss 0.06737, R2 0.98115, RMSE 0.25854                     | Test: Loss 0.84690, R2 0.76933, RMSE 0.90576\n",
      "Epoch [428/500]      | Train: Loss 0.06673, R2 0.98130, RMSE 0.25750                     | Test: Loss 0.84818, R2 0.76719, RMSE 0.91151\n",
      "Epoch [429/500]      | Train: Loss 0.06604, R2 0.98154, RMSE 0.25622                     | Test: Loss 0.85094, R2 0.76579, RMSE 0.91442\n",
      "Epoch [430/500]      | Train: Loss 0.06459, R2 0.98172, RMSE 0.25349                     | Test: Loss 0.90569, R2 0.75529, RMSE 0.94243\n",
      "Epoch [431/500]      | Train: Loss 0.06579, R2 0.98147, RMSE 0.25570                     | Test: Loss 0.87307, R2 0.76061, RMSE 0.92900\n",
      "Epoch [432/500]      | Train: Loss 0.06590, R2 0.98162, RMSE 0.25585                     | Test: Loss 0.99937, R2 0.74922, RMSE 0.97471\n",
      "Epoch [433/500]      | Train: Loss 0.06530, R2 0.98174, RMSE 0.25476                     | Test: Loss 0.96164, R2 0.72164, RMSE 0.96842\n",
      "Epoch [434/500]      | Train: Loss 0.06492, R2 0.98155, RMSE 0.25356                     | Test: Loss 0.85635, R2 0.75636, RMSE 0.92004\n",
      "Epoch [435/500]      | Train: Loss 0.06372, R2 0.98217, RMSE 0.25176                     | Test: Loss 0.88283, R2 0.76614, RMSE 0.93319\n",
      "Epoch [436/500]      | Train: Loss 0.06567, R2 0.98158, RMSE 0.25556                     | Test: Loss 0.95218, R2 0.75617, RMSE 0.96379\n",
      "Epoch [437/500]      | Train: Loss 0.06477, R2 0.98185, RMSE 0.25375                     | Test: Loss 0.84556, R2 0.77142, RMSE 0.90378\n",
      "Epoch [438/500]      | Train: Loss 0.06331, R2 0.98220, RMSE 0.25123                     | Test: Loss 0.88219, R2 0.74651, RMSE 0.93377\n",
      "Epoch [439/500]      | Train: Loss 0.06384, R2 0.98185, RMSE 0.25199                     | Test: Loss 0.89607, R2 0.75527, RMSE 0.94002\n",
      "Epoch [440/500]      | Train: Loss 0.06447, R2 0.98202, RMSE 0.25317                     | Test: Loss 0.88376, R2 0.76592, RMSE 0.93240\n",
      "Epoch [441/500]      | Train: Loss 0.06804, R2 0.98087, RMSE 0.26007                     | Test: Loss 0.85192, R2 0.75468, RMSE 0.91535\n",
      "Epoch [442/500]      | Train: Loss 0.06821, R2 0.98088, RMSE 0.26004                     | Test: Loss 0.97781, R2 0.73856, RMSE 0.97154\n",
      "Epoch [443/500]      | Train: Loss 0.06493, R2 0.98174, RMSE 0.25431                     | Test: Loss 0.85111, R2 0.76980, RMSE 0.91365\n",
      "Epoch [444/500]      | Train: Loss 0.06181, R2 0.98255, RMSE 0.24799                     | Test: Loss 0.89334, R2 0.75872, RMSE 0.94125\n",
      "Epoch [445/500]      | Train: Loss 0.06669, R2 0.98118, RMSE 0.25756                     | Test: Loss 0.88400, R2 0.72836, RMSE 0.93400\n",
      "Epoch [446/500]      | Train: Loss 0.05953, R2 0.98323, RMSE 0.24342                     | Test: Loss 0.84207, R2 0.73784, RMSE 0.90212\n",
      "Epoch [447/500]      | Train: Loss 0.06370, R2 0.98216, RMSE 0.25188                     | Test: Loss 0.93046, R2 0.76017, RMSE 0.95855\n",
      "Epoch [448/500]      | Train: Loss 0.06286, R2 0.98226, RMSE 0.25010                     | Test: Loss 0.85907, R2 0.76277, RMSE 0.91337\n",
      "Epoch [449/500]      | Train: Loss 0.06265, R2 0.98230, RMSE 0.24957                     | Test: Loss 0.84193, R2 0.76939, RMSE 0.90643\n",
      "Epoch [450/500]      | Train: Loss 0.06153, R2 0.98269, RMSE 0.24746                     | Test: Loss 0.88690, R2 0.71296, RMSE 0.93289\n",
      "Epoch [451/500]      | Train: Loss 0.05983, R2 0.98318, RMSE 0.24377                     | Test: Loss 0.87904, R2 0.75260, RMSE 0.93046\n",
      "Epoch [452/500]      | Train: Loss 0.06193, R2 0.98251, RMSE 0.24835                     | Test: Loss 0.86223, R2 0.76819, RMSE 0.91635\n",
      "Epoch [453/500]      | Train: Loss 0.06278, R2 0.98236, RMSE 0.24981                     | Test: Loss 0.87352, R2 0.76587, RMSE 0.92691\n",
      "Epoch [454/500]      | Train: Loss 0.06152, R2 0.98260, RMSE 0.24752                     | Test: Loss 0.86244, R2 0.74407, RMSE 0.91972\n",
      "Epoch [455/500]      | Train: Loss 0.06583, R2 0.98137, RMSE 0.25578                     | Test: Loss 0.84150, R2 0.76360, RMSE 0.90827\n",
      "Epoch [456/500]      | Train: Loss 0.05976, R2 0.98321, RMSE 0.24355                     | Test: Loss 0.86551, R2 0.75943, RMSE 0.92475\n",
      "Epoch [457/500]      | Train: Loss 0.05843, R2 0.98357, RMSE 0.24111                     | Test: Loss 0.87366, R2 0.76756, RMSE 0.93089\n",
      "Epoch [458/500]      | Train: Loss 0.05977, R2 0.98306, RMSE 0.24360                     | Test: Loss 0.86393, R2 0.75923, RMSE 0.92313\n",
      "Epoch [459/500]      | Train: Loss 0.06306, R2 0.98228, RMSE 0.25031                     | Test: Loss 0.90145, R2 0.76071, RMSE 0.93992\n",
      "Epoch [460/500]      | Train: Loss 0.06142, R2 0.98262, RMSE 0.24726                     | Test: Loss 0.92640, R2 0.75754, RMSE 0.95351\n",
      "Epoch [461/500]      | Train: Loss 0.06080, R2 0.98296, RMSE 0.24570                     | Test: Loss 0.88656, R2 0.75703, RMSE 0.93704\n",
      "Epoch [462/500]      | Train: Loss 0.06177, R2 0.98267, RMSE 0.24773                     | Test: Loss 0.89986, R2 0.75967, RMSE 0.93801\n",
      "Epoch [463/500]      | Train: Loss 0.06040, R2 0.98299, RMSE 0.24522                     | Test: Loss 0.84735, R2 0.75990, RMSE 0.91327\n",
      "Epoch [464/500]      | Train: Loss 0.05800, R2 0.98369, RMSE 0.24038                     | Test: Loss 0.85025, R2 0.76206, RMSE 0.91826\n",
      "Epoch [465/500]      | Train: Loss 0.06178, R2 0.98240, RMSE 0.24797                     | Test: Loss 0.85460, R2 0.76018, RMSE 0.91722\n",
      "Epoch [466/500]      | Train: Loss 0.06046, R2 0.98272, RMSE 0.24532                     | Test: Loss 0.84680, R2 0.77167, RMSE 0.91052\n",
      "Epoch [467/500]      | Train: Loss 0.05939, R2 0.98338, RMSE 0.24314                     | Test: Loss 0.84708, R2 0.75256, RMSE 0.91030\n",
      "Epoch [468/500]      | Train: Loss 0.06181, R2 0.98254, RMSE 0.24798                     | Test: Loss 0.83621, R2 0.77346, RMSE 0.90225\n",
      "Epoch [469/500]      | Train: Loss 0.06307, R2 0.98228, RMSE 0.25055                     | Test: Loss 0.84347, R2 0.76246, RMSE 0.90656\n",
      "Epoch [470/500]      | Train: Loss 0.06370, R2 0.98200, RMSE 0.25150                     | Test: Loss 0.86979, R2 0.76496, RMSE 0.92727\n",
      "Epoch [471/500]      | Train: Loss 0.06351, R2 0.98227, RMSE 0.25097                     | Test: Loss 0.85504, R2 0.76598, RMSE 0.91158\n",
      "Epoch [472/500]      | Train: Loss 0.05935, R2 0.98330, RMSE 0.24302                     | Test: Loss 0.85832, R2 0.75634, RMSE 0.92065\n",
      "Epoch [473/500]      | Train: Loss 0.06119, R2 0.98299, RMSE 0.24670                     | Test: Loss 0.87230, R2 0.76551, RMSE 0.92734\n",
      "Epoch [474/500]      | Train: Loss 0.06199, R2 0.98260, RMSE 0.24861                     | Test: Loss 0.85560, R2 0.77013, RMSE 0.91494\n",
      "Epoch [475/500]      | Train: Loss 0.06264, R2 0.98241, RMSE 0.24929                     | Test: Loss 0.87773, R2 0.76068, RMSE 0.92625\n",
      "Epoch [476/500]      | Train: Loss 0.06127, R2 0.98285, RMSE 0.24670                     | Test: Loss 0.84655, R2 0.77334, RMSE 0.91212\n",
      "Epoch [477/500]      | Train: Loss 0.06207, R2 0.98250, RMSE 0.24848                     | Test: Loss 0.85433, R2 0.77267, RMSE 0.90822\n",
      "Epoch [478/500]      | Train: Loss 0.05999, R2 0.98309, RMSE 0.24429                     | Test: Loss 0.83944, R2 0.76241, RMSE 0.90721\n",
      "Epoch [479/500]      | Train: Loss 0.05687, R2 0.98402, RMSE 0.23780                     | Test: Loss 0.85361, R2 0.75924, RMSE 0.90968\n",
      "Epoch [480/500]      | Train: Loss 0.05713, R2 0.98392, RMSE 0.23837                     | Test: Loss 0.84145, R2 0.77134, RMSE 0.90790\n",
      "Epoch [481/500]      | Train: Loss 0.06108, R2 0.98266, RMSE 0.24657                     | Test: Loss 0.86640, R2 0.76893, RMSE 0.92492\n",
      "Epoch [482/500]      | Train: Loss 0.06066, R2 0.98304, RMSE 0.24540                     | Test: Loss 1.04775, R2 0.74109, RMSE 0.98982\n",
      "Epoch [483/500]      | Train: Loss 0.06075, R2 0.98290, RMSE 0.24563                     | Test: Loss 0.83618, R2 0.76491, RMSE 0.90420\n",
      "Epoch [484/500]      | Train: Loss 0.05871, R2 0.98355, RMSE 0.24165                     | Test: Loss 0.88425, R2 0.76089, RMSE 0.92946\n",
      "Epoch [485/500]      | Train: Loss 0.06078, R2 0.98273, RMSE 0.24563                     | Test: Loss 0.85203, R2 0.76992, RMSE 0.90809\n",
      "Epoch [486/500]      | Train: Loss 0.05989, R2 0.98309, RMSE 0.24404                     | Test: Loss 0.84387, R2 0.77451, RMSE 0.90530\n",
      "Epoch [487/500]      | Train: Loss 0.05880, R2 0.98356, RMSE 0.24195                     | Test: Loss 0.90167, R2 0.76561, RMSE 0.94304\n",
      "Epoch [488/500]      | Train: Loss 0.05874, R2 0.98325, RMSE 0.24174                     | Test: Loss 0.87679, R2 0.76094, RMSE 0.93069\n",
      "Epoch [489/500]      | Train: Loss 0.06129, R2 0.98276, RMSE 0.24687                     | Test: Loss 0.91904, R2 0.74051, RMSE 0.95109\n",
      "Epoch [490/500]      | Train: Loss 0.05730, R2 0.98390, RMSE 0.23869                     | Test: Loss 0.87193, R2 0.74782, RMSE 0.92857\n",
      "Epoch [491/500]      | Train: Loss 0.05778, R2 0.98373, RMSE 0.23981                     | Test: Loss 0.87517, R2 0.75605, RMSE 0.92871\n",
      "Epoch [492/500]      | Train: Loss 0.05679, R2 0.98411, RMSE 0.23769                     | Test: Loss 0.83807, R2 0.76184, RMSE 0.90708\n",
      "Epoch [493/500]      | Train: Loss 0.05901, R2 0.98322, RMSE 0.24243                     | Test: Loss 0.93875, R2 0.72914, RMSE 0.95587\n",
      "Epoch [494/500]      | Train: Loss 0.06004, R2 0.98322, RMSE 0.24418                     | Test: Loss 0.85637, R2 0.77322, RMSE 0.91894\n",
      "Epoch [495/500]      | Train: Loss 0.05833, R2 0.98360, RMSE 0.24091                     | Test: Loss 0.87694, R2 0.75757, RMSE 0.93332\n",
      "Epoch [496/500]      | Train: Loss 0.05966, R2 0.98297, RMSE 0.24325                     | Test: Loss 0.90931, R2 0.76581, RMSE 0.94538\n",
      "Epoch [497/500]      | Train: Loss 0.05652, R2 0.98401, RMSE 0.23730                     | Test: Loss 0.86831, R2 0.76568, RMSE 0.92169\n",
      "Epoch [498/500]      | Train: Loss 0.05897, R2 0.98329, RMSE 0.24209                     | Test: Loss 0.82811, R2 0.76860, RMSE 0.89626\n",
      "Epoch [499/500]      | Train: Loss 0.05732, R2 0.98395, RMSE 0.23891                     | Test: Loss 0.85484, R2 0.75509, RMSE 0.91713\n",
      "Epoch [500/500]      | Train: Loss 0.05916, R2 0.98335, RMSE 0.24258                     | Test: Loss 0.90477, R2 0.76599, RMSE 0.94222\n",
      "Best rmse 0.8447356820106506\n",
      "100 epochs of training and evaluation took, 246.59375\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkoAAAHUCAYAAADFpwc3AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABwAUlEQVR4nO3dd3iTVf8G8PtJmibdmw5aStl7DwsiIAjIUERefRUR3Aqo/NwbEBS36KviBifgAERlCMhSRJC9ZZRSoKXQ0t0mTXJ+f5wmTZrRUtqmJffnunK1ffIkORlN7nzPeBQhhAAREREROVB5ugFERERE9RWDEhEREZELDEpERERELjAoEREREbnAoERERETkAoMSERERkQsMSkREREQuMCgRERERucCgREREROQCgxIRAEVRqnRav379Jd3O9OnToShKtS67fv36GmlDfTdx4kQ0bdrU5fnz58+v0nPl7jouxubNmzF9+nTk5ORUaX/Lc3z+/Pkauf3a9vPPP2PUqFGIjo6Gr68vwsPDMWjQIHzzzTcoLS31dPOIPM7H0w0gqg/++usvu79nzpyJdevW4ffff7fb3q5du0u6nbvvvhvDhg2r1mW7deuGv/7665Lb0NCNGDHC4flKTk7G2LFj8eijj1q3abXaGrm9zZs3Y8aMGZg4cSJCQ0Nr5DrrAyEE7rzzTsyfPx/Dhw/HW2+9hYSEBOTm5mLdunWYNGkSzp8/j4cfftjTTSXyKAYlIgBXXHGF3d9RUVFQqVQO2ysqKiqCv79/lW8nPj4e8fHx1WpjcHBwpe3xBlFRUYiKinLYHh0dzcfnIrz++uuYP38+ZsyYgRdeeMHuvFGjRuGJJ57A0aNHa+S2Lvb/hKg+YdcbURUNGDAAHTp0wMaNG9GnTx/4+/vjzjvvBAAsWrQIQ4YMQWxsLPz8/NC2bVs89dRTKCwstLsOZ11vTZs2xciRI7Fy5Up069YNfn5+aNOmDT7//HO7/Zx1vU2cOBGBgYE4evQohg8fjsDAQCQkJODRRx+FXq+3u/ypU6cwduxYBAUFITQ0FOPGjcO2bdugKArmz5/v9r6fO3cOkyZNQrt27RAYGIhGjRrh6quvxqZNm+z2O3HiBBRFwRtvvIG33noLSUlJCAwMRHJyMrZs2eJwvfPnz0fr1q2h1WrRtm1bfPnll27bcTGOHDmCW2+9FY0aNbJe//vvv2+3j9lsxqxZs9C6dWv4+fkhNDQUnTp1wjvvvANAPl+PP/44ACApKanGumABYNmyZUhOToa/vz+CgoJwzTXXOFTKzp07h3vvvRcJCQnQarWIiopC3759sWbNGus+O3fuxMiRI633My4uDiNGjMCpU6dc3nZpaSleffVVtGnTBs8//7zTfWJiYnDllVcCcN3ta3m+bV8/ltfk3r17MWTIEAQFBWHQoEGYOnUqAgICkJeX53BbN998M6Kjo+26+hYtWoTk5GQEBAQgMDAQQ4cOxc6dO13eJ6LawooS0UVIT0/HbbfdhieeeAIvv/wyVCr5XePIkSMYPny49cPg0KFDePXVV7F161aH7jtndu/ejUcffRRPPfUUoqOj8emnn+Kuu+5CixYtcNVVV7m9bGlpKa677jrcddddePTRR7Fx40bMnDkTISEh1kpBYWEhBg4ciOzsbLz66qto0aIFVq5ciZtvvrlK9zs7OxsAMG3aNMTExKCgoABLlizBgAEDsHbtWgwYMMBu//fffx9t2rTBnDlzAADPP/88hg8fjpSUFISEhACQIemOO+7A9ddfjzfffBO5ubmYPn069Hq99XGtrgMHDqBPnz5o0qQJ3nzzTcTExGDVqlV46KGHcP78eUybNg0A8Nprr2H69Ol47rnncNVVV6G0tBSHDh2yjke6++67kZ2djf/9739YvHgxYmNjAVx6F+y3336LcePGYciQIViwYAH0ej1ee+016+NpCSjjx4/Hjh078NJLL6FVq1bIycnBjh07kJWVBUA+r9dccw2SkpLw/vvvIzo6GhkZGVi3bh3y8/Nd3v4///yD7Oxs3HPPPdUeM+eOwWDAddddh/vuuw9PPfUUjEYjYmJi8M477+C7777D3Xffbd03JycHP/30EyZPngyNRgMAePnll/Hcc8/hjjvuwHPPPQeDwYDXX38d/fr1w9atW72++5nqmCAiBxMmTBABAQF22/r37y8AiLVr17q9rNlsFqWlpWLDhg0CgNi9e7f1vGnTpomK/3aJiYlCp9OJ1NRU67bi4mIRHh4u7rvvPuu2devWCQBi3bp1du0EIL777ju76xw+fLho3bq19e/3339fABArVqyw2+++++4TAMS8efPc3qeKjEajKC0tFYMGDRI33HCDdXtKSooAIDp27CiMRqN1+9atWwUAsWDBAiGEECaTScTFxYlu3boJs9ls3e/EiRNCo9GIxMTEi2oPADF58mTr30OHDhXx8fEiNzfXbr8pU6YInU4nsrOzhRBCjBw5UnTp0sXtdb/++usCgEhJSalSWyzP8blz55yeb7nvHTt2FCaTybo9Pz9fNGrUSPTp08e6LTAwUEydOtXlbf3zzz8CgFi6dGmV2maxcOFCAUB8+OGHVdrf2WtPiPLn2/b1Y3lNfv755w7X061bN7v7J4QQH3zwgQAg9u7dK4QQ4uTJk8LHx0c8+OCDdvvl5+eLmJgYcdNNN1WpzUQ1hV1vRBchLCwMV199tcP248eP49Zbb0VMTAzUajU0Gg369+8PADh48GCl19ulSxc0adLE+rdOp0OrVq2Qmppa6WUVRcGoUaPstnXq1Mnushs2bEBQUJDDQPJbbrml0uu3+PDDD9GtWzfodDr4+PhAo9Fg7dq1Tu/fiBEjoFar7doDwNqmw4cP48yZM7j11lvtKhqJiYno06dPldvkTElJCdauXYsbbrgB/v7+MBqN1tPw4cNRUlJi7Qbs1asXdu/ejUmTJmHVqlVOu4VqmuW+jx8/3q5yFhgYiBtvvBFbtmxBUVGRtX3z58/HrFmzsGXLFodZaC1atEBYWBiefPJJfPjhhzhw4ECtt7+qbrzxRodtd9xxBzZv3ozDhw9bt82bNw89e/ZEhw4dAACrVq2C0WjE7bffbvfc6XQ69O/f/7Kf9Un1D4MS0UWwdL3YKigoQL9+/fD3339j1qxZWL9+PbZt24bFixcDAIqLiyu93oiICIdtWq22Spf19/eHTqdzuGxJSYn176ysLERHRztc1tk2Z9566y088MAD6N27N3788Uds2bIF27Ztw7Bhw5y2seL9scxAs+xr6TqKiYlxuKyzbRcjKysLRqMR//vf/6DRaOxOw4cPBwDr1P2nn34ab7zxBrZs2YJrr70WERERGDRoEP75559LakNl7QOcv5bi4uJgNptx4cIFAHKczoQJE/Dpp58iOTkZ4eHhuP3225GRkQEACAkJwYYNG9ClSxc888wzaN++PeLi4jBt2jS3U/stoTwlJaWm7x4A+ZoMDg522D5u3DhotVrrmKYDBw5g27ZtuOOOO6z7nD17FgDQs2dPh+dv0aJFDWbZBbp8cIwS0UVwNp7j999/x5kzZ7B+/XprFQlAldfdqQsRERHYunWrw3bLB25lvv76awwYMABz58612+5uHExl7XF1+1VtkythYWFQq9UYP348Jk+e7HSfpKQkAICPjw8eeeQRPPLII8jJycGaNWvwzDPPYOjQoUhLS6uVmVqW+56enu5w3pkzZ6BSqRAWFgYAiIyMxJw5czBnzhycPHkSy5Ytw1NPPYXMzEysXLkSANCxY0csXLgQQgjs2bMH8+fPx4svvgg/Pz889dRTTtvQo0cPhIeH46effsLs2bMrHadkCeIVJwi4Ci2uri8sLAzXX389vvzyS8yaNQvz5s2DTqezq2xGRkYCAH744QckJia6bRdRXWBFiegSWT4UKq7b89FHH3miOU71798f+fn5WLFihd32hQsXVunyiqI43L89e/Y4zNKqqtatWyM2NhYLFiyAEMK6PTU1FZs3b67WdVr4+/tj4MCB2LlzJzp16oQePXo4nJxV8EJDQzF27FhMnjwZ2dnZOHHiBADHatilat26NRo3boxvv/3W7r4XFhbixx9/tM6Eq6hJkyaYMmUKrrnmGuzYscPhfEVR0LlzZ7z99tsIDQ11uo+FRqPBk08+iUOHDmHmzJlO98nMzMSff/4JANbFO/fs2WO3z7Jlyyq9vxXdcccdOHPmDJYvX46vv/4aN9xwg936VEOHDoWPjw+OHTvm9Lnr0aPHRd8m0aVgRYnoEvXp0wdhYWG4//77MW3aNGg0GnzzzTfYvXu3p5tmNWHCBLz99tu47bbbMGvWLLRo0QIrVqzAqlWrAKDSWWYjR47EzJkzMW3aNPTv3x+HDx/Giy++iKSkJBiNxotuj0qlwsyZM3H33XfjhhtuwD333IOcnBxMnz79krveAOCdd97BlVdeiX79+uGBBx5A06ZNkZ+fj6NHj+Lnn3+2zkQcNWoUOnTogB49eiAqKgqpqamYM2cOEhMT0bJlSwCyYmO5zgkTJkCj0aB169YICgpy24aff/7Z6T5jx47Fa6+9hnHjxmHkyJG47777oNfr8frrryMnJwevvPIKACA3NxcDBw7ErbfeijZt2iAoKAjbtm3DypUrMWbMGADAL7/8gg8++ACjR49Gs2bNIITA4sWLkZOTg2uuucZt+x5//HEcPHgQ06ZNw9atW3HrrbdaF5zcuHEjPv74Y8yYMQN9+/ZFTEwMBg8ejNmzZyMsLAyJiYlYu3attXv5YgwZMgTx8fGYNGkSMjIy7LrdABnKXnzxRTz77LM4fvw4hg0bhrCwMJw9exZbt25FQEAAZsyYcdG3S1Rtnh1LTlQ/uZr11r59e6f7b968WSQnJwt/f38RFRUl7r77brFjxw6HGUGuZr2NGDHC4Tr79+8v+vfvb/3b1ay3iu10dTsnT54UY8aMEYGBgSIoKEjceOONYvny5QKA+Omnn1w9FEIIIfR6vXjsscdE48aNhU6nE926dRNLly4VEyZMsJuhZpkF9frrrztcBwAxbdo0u22ffvqpaNmypfD19RWtWrUSn3/+ucN1VgUqzHqztOXOO+8UjRs3FhqNRkRFRYk+ffqIWbNmWfd58803RZ8+fURkZKTw9fUVTZo0EXfddZc4ceKE3XU9/fTTIi4uTqhUKqezv2xZHntXJ4ulS5eK3r17C51OJwICAsSgQYPEn3/+aT2/pKRE3H///aJTp04iODhY+Pn5idatW4tp06aJwsJCIYQQhw4dErfccoto3ry58PPzEyEhIaJXr15i/vz5VX7sfvrpJzFixAgRFRUlfHx8RFhYmBg4cKD48MMPhV6vt+6Xnp4uxo4dK8LDw0VISIi47bbbrLPuKs56c/aatPXMM88IACIhIcFu5p+tpUuXioEDB4rg4GCh1WpFYmKiGDt2rFizZk2V7xtRTVCEsKn9EpFXsaxXc/LkyWqvGE5EdDlj1xuRl3jvvfcAAG3atEFpaSl+//13vPvuu7jtttsYkoiIXGBQIvIS/v7+ePvtt3HixAno9Xo0adIETz75JJ577jlPN42IqN5i1xsRERGRC1wegIiIiMgFBiUiIiIiFxiUiIiIiFxo0IO5zWYzzpw5g6CgoEqX4CciIiKyEEIgPz8fcXFxbhfdbdBB6cyZM0hISPB0M4iIiKiBSktLc7tESoMOSpbDA6SlpTk9UjURERGRM3l5eUhISKj0cEQNOihZutuCg4MZlIiIiOiiVTZ0h4O5iYiIiFxgUCIiIiJygUGJiIiIyIUGPUaJiIiopgghYDQaYTKZPN0UqgFqtRo+Pj6XvHwQgxIREXk9g8GA9PR0FBUVebopVIP8/f0RGxsLX1/fal8HgxIREXk1s9mMlJQUqNVqxMXFwdfXl4sYN3BCCBgMBpw7dw4pKSlo2bKl20Ul3WFQIiIir2YwGGA2m5GQkAB/f39PN4dqiJ+fHzQaDVJTU2EwGKDT6ap1PRzMTUREBFS74kD1V008p3xVEBEREbnAoERERETkAoMSERERWQ0YMABTp071dDPqDQ7mJiIiaoAqm5k3YcIEzJ8//6Kvd/HixdBoNNVslTRx4kTk5ORg6dKll3Q99QGDEhERUQOUnp5u/X3RokV44YUXcPjwYes2Pz8/u/1LS0urFIDCw8NrrpGXAXa9uXH751sx9O2NOJSR5+mmEBFRHRJCoMhg9MhJCFGlNsbExFhPISEhUBTF+ndJSQlCQ0Px3XffYcCAAdDpdPj666+RlZWFW265BfHx8fD390fHjh2xYMECu+ut2PXWtGlTvPzyy7jzzjsRFBSEJk2a4OOPP76kx3fDhg3o1asXtFotYmNj8dRTT8FoNFrP/+GHH9CxY0f4+fkhIiICgwcPRmFhIQBg/fr16NWrFwICAhAaGoq+ffsiNTX1ktrjDitKbhw/V4BTF4pRbOBy9kRE3qS41IR2L6zyyG0feHEo/H1r5uP5ySefxJtvvol58+ZBq9WipKQE3bt3x5NPPong4GD8+uuvGD9+PJo1a4bevXu7vJ4333wTM2fOxDPPPIMffvgBDzzwAK666iq0adPmott0+vRpDB8+HBMnTsSXX36JQ4cO4Z577oFOp8P06dORnp6OW265Ba+99hpuuOEG5OfnY9OmTdZDzIwePRr33HMPFixYAIPBgK1bt9bqAqEMSm6oyh54c9XCPRERUb0ydepUjBkzxm7bY489Zv39wQcfxMqVK/H999+7DUrDhw/HpEmTAMjw9fbbb2P9+vXVCkoffPABEhIS8N5770FRFLRp0wZnzpzBk08+iRdeeAHp6ekwGo0YM2YMEhMTAQAdO3YEAGRnZyM3NxcjR45E8+bNAQBt27a96DZcDAYlNywBtaplUCIiujz4adQ48OJQj912TenRo4fd3yaTCa+88goWLVqE06dPQ6/XQ6/XIyAgwO31dOrUyfq7pYsvMzOzWm06ePAgkpOT7apAffv2RUFBAU6dOoXOnTtj0KBB6NixI4YOHYohQ4Zg7NixCAsLQ3h4OCZOnIihQ4fimmuuweDBg3HTTTchNja2Wm2pCo5RcsNSUWJMIiLyLoqiwN/XxyOnmuxGqhiA3nzzTbz99tt44okn8Pvvv2PXrl0YOnQoDAaD2+upOAhcURSYzeZqtUkI4XAfLQUJRVGgVquxevVqrFixAu3atcP//vc/tG7dGikpKQCAefPm4a+//kKfPn2waNEitGrVClu2bKlWW6qCQckNy/NoZt8bERFdBjZt2oTrr78et912Gzp37oxmzZrhyJEjddqGdu3aYfPmzXa9NZs3b0ZQUBAaN24MQAamvn37YsaMGdi5cyd8fX2xZMkS6/5du3bF008/jc2bN6NDhw749ttva6297Hpzg2OUiIjoctKiRQv8+OOP2Lx5M8LCwvDWW28hIyOjVsb55ObmYteuXXbbwsPDMWnSJMyZMwcPPvggpkyZgsOHD2PatGl45JFHoFKp8Pfff2Pt2rUYMmQIGjVqhL///hvnzp1D27ZtkZKSgo8//hjXXXcd4uLicPjwYfz777+4/fbba7z9FgxKbqg4RomIiC4jzz//PFJSUjB06FD4+/vj3nvvxejRo5Gbm1vjt7V+/Xp07drVbptlEczly5fj8ccfR+fOnREeHo677roLzz33HAAgODgYGzduxJw5c5CXl4fExES8+eabuPbaa3H27FkcOnQIX3zxBbKyshAbG4spU6bgvvvuq/H2WyiiAaeAvLw8hISEIDc3F8HBwTV+/cPmbMShjHx8fVdvXNkyssavn4iIPK+kpAQpKSlISkqCTqfzdHOoBrl7bquaIThGqQrMDTdLEhER0SVgUHKDs96IiIi8G4OSG6qyR4cVJSIiIu/EoOSGtaLEoEREROSVGJTcsCyIVc01tYiIiKiBY1Byw7I8ALveiIiIvBODkhuWBda54CQREZF3YlByQ2U9Fg2TEhERkTdiUHKDhzAhIiLybgxKbigco0REROTVGJTcYEWJiIjqK0VR3J4mTpxY7etu2rQp5syZU2P7NWQ8KK4blgUnuY4SERHVN+np6dbfFy1ahBdeeAGHDx+2bvPz8/NEsy47rCi5ocCy4KSHG0JERHVLCMBQ6JlTFT90YmJirKeQkBAoimK3bePGjejevTt0Oh2aNWuGGTNmwGg0Wi8/ffp0NGnSBFqtFnFxcXjooYcAAAMGDEBqair+7//+z1qdqq65c+eiefPm8PX1RevWrfHVV1/Zne+qDQDwwQcfoGXLltDpdIiOjsbYsWOr3Y5LwYqSGxyjRETkpUqLgJfjPHPbz5wBfAMu6SpWrVqF2267De+++y769euHY8eO4d577wUATJs2DT/88APefvttLFy4EO3bt0dGRgZ2794NAFi8eDE6d+6Me++9F/fcc0+127BkyRI8/PDDmDNnDgYPHoxffvkFd9xxB+Lj4zFw4EC3bfjnn3/w0EMP4auvvkKfPn2QnZ2NTZs2XdJjUl0MSm5wjBIRETVEL730Ep566ilMmDABANCsWTPMnDkTTzzxBKZNm4aTJ08iJiYGgwcPhkajQZMmTdCrVy8AQHh4ONRqNYKCghATE1PtNrzxxhuYOHEiJk2aBAB45JFHsGXLFrzxxhsYOHCg2zacPHkSAQEBGDlyJIKCgpCYmIiuXbte4qNSPQxKbnBlbiIiL6Xxl5UdT932Jdq+fTu2bduGl156ybrNZDKhpKQERUVF+M9//oM5c+agWbNmGDZsGIYPH45Ro0bBx6fmYsHBgwetVSyLvn374p133gEAt2245pprkJiYaD1v2LBhuOGGG+Dvf+mPzcXiGCU3eFBcIiIvpSiy+8sTp0sYE2RhNpsxY8YM7Nq1y3rau3cvjhw5Ap1Oh4SEBBw+fBjvv/8+/Pz8MGnSJFx11VUoLS2tgQevXMXxTUII6zZ3bQgKCsKOHTuwYMECxMbG4oUXXkDnzp2Rk5NTo+2rCgYlNxR2vRERUQPUrVs3HD58GC1atHA4qcqmdPv5+eG6667Du+++i/Xr1+Ovv/7C3r17AQC+vr4wmUyX1Ia2bdvijz/+sNu2efNmtG3b1vq3uzb4+Phg8ODBeO2117Bnzx6cOHECv//++yW1qTrY9eaGJQizoERERA3JCy+8gJEjRyIhIQH/+c9/oFKpsGfPHuzduxezZs3C/PnzYTKZ0Lt3b/j7++Orr76Cn58fEhMTAcj1kTZu3Ij//ve/0Gq1iIyMdHlbp0+fxq5du+y2NWnSBI8//jhuuukmdOvWDYMGDcLPP/+MxYsXY82aNQDgtg2//PILjh8/jquuugphYWFYvnw5zGYzWrduXWuPmSusKLnBMUpERNQQDR06FL/88gtWr16Nnj174oorrsBbb71lDUKhoaH45JNP0LdvX3Tq1Alr167Fzz//jIiICADAiy++iBMnTqB58+aIiopye1tvvPEGunbtandatmwZRo8ejXfeeQevv/462rdvj48++gjz5s3DgAEDKm1DaGgoFi9ejKuvvhpt27bFhx9+iAULFqB9+/a1+rg5o4gGPAAnLy8PISEhyM3NRXBwcI1f/wNfb8eKfRmYeX17jE9uWuPXT0REnldSUoKUlBQkJSVBp9N5ujlUg9w9t1XNEKwoucHlAYiIiLwbg5IbXHCSiIjIuzEouVG+PICHG0JEREQewaDkBitKRERE3o1ByQ1WlIiIvEcDnttELtTEc1pvgtLs2bOhKAqmTp3q6aZYsaJERHT502g0AICioiIPt4RqmuU5tTzH1VEvFpzctm0bPv74Y3Tq1MnTTbHDWW9ERJc/tVqN0NBQZGZmAgD8/f0dDr1BDYsQAkVFRcjMzERoaCjUanW1r8vjQamgoADjxo3DJ598glmzZnm6OXa44CQRkXeIiYkBAGtYostDaGio9bmtLo8HpcmTJ2PEiBEYPHhwpUFJr9dDr9db/87Ly6vVtingNwoiIm+gKApiY2PRqFGjGj8wLHmGRqO5pEqShUeD0sKFC7Fjxw5s27atSvvPnj0bM2bMqOVWlSs7biDM7HsjIvIKarW6Rj5c6fLhscHcaWlpePjhh/H1119Xecn4p59+Grm5udZTWlparbZR4RglIiIir+axitL27duRmZmJ7t27W7eZTCZs3LgR7733HvR6vUOq12q10Gq1ddZGjlEiIiLybh4LSoMGDcLevXvttt1xxx1o06YNnnzyyXpR+ixfR4lBiYiIyBt5LCgFBQWhQ4cOdtsCAgIQERHhsN1TuDwAERGRd6s3C07WZwJMSkRERN7I48sD2Fq/fr2nm2CHFSUiIiLvxoqSGxzMTURE5N0YlNxQqXhQXCIiIm/GoOSG9aC47HsjIiLySgxKbliXB/BwO4iIiMgzGJTcsBzpjWOUiIiIvBODkhvlC056uCFERETkEQxKbnDWGxERkXdjUHKj/KC4DEpERETeiEHJDS44SURE5N0YlNywdL2xoEREROSdGJTcUKxBiUmJiIjIGzEoucExSkRERN6NQckNjlEiIiLybgxKbnB5ACIiIu/GoOQGF5wkIiLybgxKbnAwNxERkXdjUHJD4RglIiIir8ag5AbHKBEREXk3BiU3OEaJiIjIuzEoucGKEhERkXdjUHJDYUWJiIjIqzEouaHiytxERERejUHJDcXa9ebZdhAREZFnMCi5oeI6SkRERF6NQckNHhSXiIjIuzEoucGD4hIREXk3BiU3rF1vnm0GEREReQiDkhvlC04yKhEREXkjBiU3FC44SURE5NUYlNzo9feDWOr7HGL1qZ5uChEREXkAg5IbIbmH0EV1HL7mYk83hYiIiDyAQckNociHR4HZwy0hIiIiT2BQcqcsKEEwKBEREXkjBiW3LOsDMCgRERF5IwYlNwQrSkRERF6NQckda1Di8gBERETeiEHJHctgblaUiIiIvBKDkjvseiMiIvJqDEpuCMvS3FwegIiIyCsxKLnFMUpERETejEHJHY5RIiIi8moMSu5wjBIREZFXY1Byh0GJiIjIqzEoucOuNyIiIq/GoORO2aw3BRzMTURE5I0YlNxh1xsREZFXY1Byh0GJiIjIqzEoucOgRERE5NUYlNyxDObmGCUiIiKvxKDkDme9EREReTUGJXfY9UZEROTVGJTcsQQlHhSXiIjIKzEouWPteuMYJSIiIm/EoOSOimOUiIiIvBmDkhsKV+YmIiLyagxK7ijqsl9YUSIiIvJGDEruWCpK7HojIiLySgxK7nAwNxERkVdjUHJD4TpKREREXo1ByR3LrDeOUSIiIvJKDEru8FhvREREXo1ByQ12vREREXk3BiV3ypYHULHrjYiIyCsxKLlhWXASnPVGRETklRiU3OFgbiIiIq/GoOQO11EiIiLyagxKbijWMUoCgmGJiIjI6zAouaGo5BgllWKGmTmJiIjI63g0KM2dOxedOnVCcHAwgoODkZycjBUrVniySfbKKkoKBMysKBEREXkdjwal+Ph4vPLKK/jnn3/wzz//4Oqrr8b111+P/fv3e7JZ5crGKKkYlIiIiLySjydvfNSoUXZ/v/TSS5g7dy62bNmC9u3be6hV5SzLA6hg5goBREREXsijQcmWyWTC999/j8LCQiQnJzvdR6/XQ6/XW//Oy8ur1TYpqvKKEoMSERGR9/H4YO69e/ciMDAQWq0W999/P5YsWYJ27do53Xf27NkICQmxnhISEmq1bYrNsd7Y9UZEROR9PB6UWrdujV27dmHLli144IEHMGHCBBw4cMDpvk8//TRyc3Otp7S0tNptnIpjlIiIiLyZx7vefH190aJFCwBAjx49sG3bNrzzzjv46KOPHPbVarXQarV11jb7ilKd3SwRERHVEx6vKFUkhLAbh+RJimI7RolJiYiIyNt4tKL0zDPP4Nprr0VCQgLy8/OxcOFCrF+/HitXrvRks6zKV+bmrDciIiJv5NGgdPbsWYwfPx7p6ekICQlBp06dsHLlSlxzzTWebFa5spW5OZibiIjIO3k0KH322WeevPlK2R7rjWOUiIiIvE+9G6NUr1jHKJk5RomIiMgLMSi5Y3cIEw+3hYiIiOocg5I7CscoEREReTMGJXdslwfwcFOIiIio7jEouWMzRsnMvjciIiKvw6DkjiUoKTwoLhERkTdiUHKHB8UlIiLyagxK7th2vTEoEREReR0GJXe4PAAREZFXY1ByxyYogfPeiIiIvA6DkjvWdZTMrCgRERF5IQYld2wqSiYmJSIiIq/DoOQOgxIREZFXY1Byx24wN4MSERGRt2FQcse6jpKZFSUiIiIvxKDkDrveiIiIvBqDkjsMSkRERF6NQckdm5W5TRyjRERE5HUYlNyxPdab2cNtISIiojrHoORO2YKTKggYmZSIiIi8DoOSO1wegIiIyKsxKLljCUqKGSYWlIiIiLwOg5I7NmOUTOx6IyIi8joMSu7YLQ/g4bYQERFRnWNQcofLAxAREXk1BiV3bAdzc8FJIiIir8Og5I7NGCUjgxIREZHXYVByx6brjRUlIiIi78Og5I7NgpMco0REROR9GJTc4UFxiYiIvBqDkjt26ygxKBEREXkbBiV3bJcHYFAiIiLyOgxK7vBYb0RERF6NQckddr0RERF5NQYld2y63riOEhERkfdhUHKHK3MTERF5NQYld2yXB+AYJSIiIq/DoORO2YKTisIxSkRERN6IQckdLg9ARETk1RiU3GHXGxERkVerVlBKS0vDqVOnrH9v3boVU6dOxccff1xjDasXOJibiIjIq1UrKN16661Yt24dACAjIwPXXHMNtm7dimeeeQYvvvhijTbQo6zrKHF5ACIiIm9UraC0b98+9OrVCwDw3XffoUOHDti8eTO+/fZbzJ8/vybb51msKBEREXm1agWl0tJSaLVaAMCaNWtw3XXXAQDatGmD9PT0mmudp3GMEhERkVerVlBq3749PvzwQ2zatAmrV6/GsGHDAABnzpxBREREjTbQo+xmvXm4LURERFTnqhWUXn31VXz00UcYMGAAbrnlFnTu3BkAsGzZMmuX3GXBso4SBExmJiUiIiJv41OdCw0YMADnz59HXl4ewsLCrNvvvfde+Pv711jjPM626405iYiIyOtUq6JUXFwMvV5vDUmpqamYM2cODh8+jEaNGtVoAz3KdjA3xygRERF5nWoFpeuvvx5ffvklACAnJwe9e/fGm2++idGjR2Pu3Lk12kCPshmjxOUBiIiIvE+1gtKOHTvQr18/AMAPP/yA6OhopKam4ssvv8S7775bow30KOs6SlwegIiIyBtVKygVFRUhKCgIAPDbb79hzJgxUKlUuOKKK5CamlqjDfQouzFKDEpERETeplpBqUWLFli6dCnS0tKwatUqDBkyBACQmZmJ4ODgGm2gR9kuD8AxSkRERF6nWkHphRdewGOPPYamTZuiV69eSE5OBiCrS127dq3RBnpUWVBSK6woEREReaNqLQ8wduxYXHnllUhPT7euoQQAgwYNwg033FBjjfM4pTxHmrg+ABERkdepVlACgJiYGMTExODUqVNQFAWNGze+vBabBKwLTgKAECYPNoSIiIg8oVpdb2azGS+++CJCQkKQmJiIJk2aIDQ0FDNnzoT5clrB2qaiJC6n+0VERERVUq2K0rPPPovPPvsMr7zyCvr27QshBP78809Mnz4dJSUleOmll2q6nZ5h2/VmZkWJiIjI21QrKH3xxRf49NNPcd1111m3de7cGY0bN8akSZMuy6AEVpSIiIi8TrW63rKzs9GmTRuH7W3atEF2dvYlN6resAlKZsGgRERE5G2qFZQ6d+6M9957z2H7e++9h06dOl1yo+oN24qSiV1vRERE3qZaXW+vvfYaRowYgTVr1iA5ORmKomDz5s1IS0vD8uXLa7qNnmM3RokVJSIiIm9TrYpS//798e+//+KGG25ATk4OsrOzMWbMGOzfvx/z5s2r6TZ6Dme9ERERebVqr6MUFxfnMGh79+7d+OKLL/D5559fcsPqBdugxHWUiIiIvE61Kkpew2bByctqfSgiIiKqEgYldxQFAmVhibPeiIiIvA6DUmXKut/Y9UZEROR9LmqM0pgxY9yen5OTcyltqadkRUmYhYfbQURERHXtooJSSEhIpefffvvtl9Sg+kYoKigCMPMQJkRERF7nooJSTU/9nz17NhYvXoxDhw7Bz88Pffr0wauvvorWrVvX6O1cEsVSUeIYJSIiIm/j0TFKGzZswOTJk7FlyxasXr0aRqMRQ4YMQWFhoSebZc+yRAAHcxMREXmdaq+jVBNWrlxp9/e8efPQqFEjbN++HVdddZWHWlWBZTA3K0pERERex6NBqaLc3FwAQHh4uNPz9Xo99Hq99e+8vLzab5R11huDEhERkbepN8sDCCHwyCOP4Morr0SHDh2c7jN79myEhIRYTwkJCbXfMFaUiIiIvFa9CUpTpkzBnj17sGDBApf7PP3008jNzbWe0tLSar9hrCgRERF5rXrR9fbggw9i2bJl2LhxI+Lj413up9VqodVq67BlYEWJiIjIi3k0KAkh8OCDD2LJkiVYv349kpKSPNkc5yzHe+PK3ERERF7Ho0Fp8uTJ+Pbbb/HTTz8hKCgIGRkZAOTClX5+fp5smpVQ+wIAfGCC2SygUimVXIKIiIguFx4dozR37lzk5uZiwIABiI2NtZ4WLVrkyWbZKwtKvjDCJHgYEyIiIm/i8a63+k5RawAAGhhhMgto1B5uEBEREdWZejPrrd4qqyhpFBmUiIiIyHswKFXGtqLUACpgREREVHMYlCqh2IxRMrOiRERE5FUYlCpjrSiZYGRQIiIi8ioMSpWwVJQ0rCgRERF5HQalytgO5uYYJSIiIq/CoFQZm643znojIiLyLgxKlbHpejOaGJSIiIi8CYNSZWyCUqmJB8YlIiLyJgxKlSnrevOFEXojgxIREZE3YVCqjM2Ck3qjycONISIiorrEoFQZm1lv+lJWlIiIiLwJg1JlbCtKHKNERETkVRiUKmMdzG1iRYmIiMjLMChVxmbWG8coEREReRcGpcrYDeZmRYmIiMibMChVxnYwN4MSERGRV2FQqkxZUPKFEfpSdr0RERF5Ewalyqh8AMjB3AbOeiMiIvIqDEqVsR3MzVlvREREXoVBqTJ2s94YlIiIiLwJg1JlLMd6U7g8ABERkbdhUKpMWUXJByZWlIiIiLwMg1JlbLreDAxKREREXoVBqTKWrjeOUSIiIvI6DEqVsZv1xjFKRERE3oRBqTKc9UZEROS1GJQqoy5bcJKz3oiIiLwOg1JlWFEiIiLyWgxKlbE51htnvREREXkXBqXKlM1603AdJSIiIq/DoFQZ64KTHKNERETkbRiUKmPpelNM0BsYlIiIiLwJg1JlyrreAMBkNHiwIURERFTXGJQqU1ZRAgCYSj3XDiIiIqpzDEqVUZVXlMxGPYQQHmwMERER1SUGpcqo1BBQAAA+wgijmUGJiIjIWzAoVUZRuOgkERGRl2JQqgofLQBApxh4YFwiIiIvwqBUBYo2GAAQiGIUcYkAIiIir8GgVBU6GZSClCIUGowebgwRERHVFQalqiirKAWhGAUlDEpERETegkGpKmwqSvkMSkRERF6DQakqyipKwShEvp5BiYiIyFswKFWFLgQAEKyw642IiMibMChVhaXrDUUo0PMwJkRERN6CQakqtDZBiRUlIiIir8GgVBXWwdzFyGNQIiIi8hoMSlWhlWOUZNcbgxIREZG3YFCqCpvlAdj1RkRE5D0YlKrCdowSK0pEREReg0GpKqzLAxQhv4Sz3oiIiLwFg1JV2CwPwKBEdJnIzwCE8HQriKieY1CqirKuN1/FhNKSIg83hogu2aHlwJutgRVPerolRFTPMShVhW8ghCIfKsWQ7+HGENElO/yr/Ln1IyDrmGfbQkT1GoNSVahUEGVLBGhK82Ays1xP1KAFNCr/ffs8z7WjOkqL2WVIVIcYlKpI8QsFAISigOOUiBo6Y0n574VZnmvHxcpJA16KBX6409MtIfIaDEpVpPiFAQBClQJcKGJQImrQSovLf29I3enb5wEQwP7Fnm4JkddgUKqqsqAUgkJkFxo83BgiuiRGffnvhkLPteNiscuNqM4xKFWVpetNYVAiavCMNhUlfYHn2nHRGJSI6hqDUlVZKkpKAS4wKBE1bKwoEVEVMShVlWWMEgqQXXQJQcnAdZiIPM5ujFIDrSgxNBHVCQalqrJWlAqrX1Ha+DrwcixwbF0NNoyILprtrLeGFJRsw5HtfSCiWsOgVFW6UABA6KUM5v59lvz5y//VTJuIqHrsglID6nqzrSjZVsWIqNYwKFWVzfIAHMxN1MCV2gQlYwlgMnquLRfDZLM0CStKRHWCQamqrMsDXOIYJQCcuULkYRVDRkPpfrOtfrGiRFQnGJSqqibGKBFR/eAQlBpI95ttOGJQIqoTDEpVVbaOUggKcaHwUkveyiU3x+utfAZ4ryegb0CrKlP9UTFkNJSKUqnNrFkGJaqvSvKAc4c93Yoaw6BUVWUVJbUioC7JQbHB5OEGeZCpFDizCzCbPdeGLe8D5/8Fdi/0XBsuV6ZSYOXTwOGVnm5J7bFdRwlomEHJyKBE9dSHfYH3e8nPicuAR4PSxo0bMWrUKMTFxUFRFCxdutSTzXHPRwsREg8AaKacwd7TuZdwZQ18jNKvjwAf9wf+eMvTLbEf3AoAe76/vD/g68Jf7wFbPgAW3OzpltQOIcpDhl+4/NlQVue263qrUNkuzOLaSlQ/5JyUPw8v92w7aohHg1JhYSE6d+6M9957z5PNqDIlqi0AoJXqNHacvODh1njQji/lz/WveOb2XX0Y5KUDi++WH/CerHY1dP+uqpvb0RcA39xU/nqqKyabMYYBkfJnbY1RKskFNr8H5J2pmeuzXbDWtrp04Cfg9WbA5v/VzO0Q1QTl8ui08ui9uPbaazFr1iyMGTPGk82ouqjWAICWyinsSL3IoMRvejWnYreJRXF2+e+lXAG92jIP1M3t/D0XOLIKWPZg3dyehW1Vxr+Wg9KqZ4DfngXeagt8NwHIPHRp12fX9WZTUVrygPy5+vlLu36qHzL2AqmbPd2K6rGt8itqz7WjBjWouKfX65GXl2d3qlNRbQCUBaWTFyAuJvxwzZOa4+pDzWyzFg4HeVdPcY6sglhU7N6pSRdSK2lLDVRtjXrg+zuAbZ/ZbLPcJ8U69hCGWnq9HPyl/PcDS4Fv/1P96zKbKgzmtvndx7f610v1ixDAh1cC864Fzu73dGsuXsmlDEupnxpUUJo9ezZCQkKsp4SEhLptQCPZ9dZBdQIfGZ5G7rd3Vf2ytiXzy6W6pHho9p7twFvbAa22AYpBqXoy9tj/XZJTe7dl+0Ffsat017fAq02B7V8A548Aa6ZX7w3435XA/sVyXJ3lNWEJSho/QBsof6/JipLRpmvPR2t/nmXsxsUqyQPmdATy08u32YZYH7/y381eOtHkj7eBVc/W7/dXQxFwfIPj2EpbepsCwP4lrvcryQV+vAf497eaa19NsP2C01AmSVSiQQWlp59+Grm5udZTWlpa3TYgqjUABaFKIbqrjiD0yI+up+iaSoGja8sHidp+KJi4DtMlsf1Qc/U7g1L1XDhh/3dxTu3dlu3zVZQF7PwayD0l/15a1pX080PA2hflh+CmNy/+NkpsPnQs1R1LwPDRAb4B8vfKBnPnZwDn/q389s4fAV5NlB/YgGNQAtx/SLpybC2Qd9p+m+2XBJVNF0dNjYdqSEryZJj+6z0g65jzfVI3y+fRk359FPjyOmDjG673Kcgs//3oGtf7bXwD2PvdpVUpa4Pte8Zl8j7coIKSVqtFcHCw3aluGxBkHadk5ar7YNObwNdjgKX3y7/tZqtw/MwlcRmUbD7saqsr5XJX8fVcU2X0HV/JIGTLtjqydjrw02TgqxscL5u+S/7cv8R5tWDJ/cCng2UAOroGOPZ7+XkFNh+M+36QP402QUkXIn+3Hd8GyC85Pz8sX19CAJ8PBd7vCWSnuL+fG1+X/99/lU1QUfk47nOuGuOUnFWJLO8pQth/uF6opI1Vvk0zsH0+kHmwZq6vNtl2UVUMlABwcovsyvqwX9WuL+8MsHuR+0khQgArnry4AL/7W/lz42uu97ENc2d2AkXZwNqZwN8f2++Xfbzqt1uXbKvQ+joeHlNLGlRQqhfie9r9WZB+xPl+f74rfx78Wf60DUeGyyUo1YOuN9vfbasCdf1NxmwGtn4iB2FWJv8ssG9x/ZyZV7GiZPumd+EE8OPdQPru8m0FmcDeH9wvfph3Blg2RQYh22pHjk1F2BKizv/r+LhYuqtyTgKnt1doXx6wewFwahtw6Bfg6xtl2CoqCz62Hzqnd8jr3rNI/q3RAeHN5O9ZR+2v9+sxMiRs+UB+IFkeF2fTnU1G2f5j6+wnGphNzsdZuVtb5t/fgG2fOm4vPOe4zfKYF18ATDa3eykfoKd3lF9+3w8yLC57qGqXLTwPpGz0TNeX7f+ds+7No2vlz8LMyqukJiPwv+7AknuBvz8sv2xF5/+V5699sWrvN7ava7WTSqNFwVn7vw/+DGx6A1jxuOtjEto+5p7uerR9fEsqCUpmE7BmRv3rPqzAo0GpoKAAu3btwq5duwAAKSkp2LVrF06erGY/fl1I6GX357GDO+Qbyb4f5Tcv6wujwovV9oPEXFq18rvZ5PkXfUU13Z7iHDnY9mKmpNfHrrfdC4Dlj8lBmJX55kbghzvkrK/6JqdCRak4B0j9qzwk7f1ehhGLVc8CP94FvN/bvqqx5ztg9Qvyjf3klvLtxzfIn4ZCxyqORdoW59sB+X8GyFV/N74uv9Fb2L6GUjbKn7ZBqTgb+H2mDD+ArChFtJS/ny8LSkLY/2+ePwKc2GTTtq327TEZgW/GllXDRssB2xYXTjgPSgd+kiHxz3eBLXPLby/3lOxG+fVR4NQ/9pep+OEJyPeUgz87jis7slp+KF/suKusY8AnA4F3u8q/9y2WPzP2VG3c009TgC9GAX9/VLXbyz5eHmgvVYZNeM91MiTDNkie/Kv8d7MJSNlkP65s55flX2xXPS1D8/H1jtdpG/orq7odXgG8GFb+t7vB9xWfa9vbtq3C2rJUfo+uAV5vYT+JoDLbPgP++qDq+1emsoqSEOWfIweWyvX46lv3YQVO6sJ1559//sHAgQOtfz/yyCMAgAkTJmD+/PkealUlGne3+7PV4Q8BUQTs+EJuaD4IGL/YMVBU7G4rLQbUGte3YzTID92gGGDCshpoeA2p6W7D9a/Iwbb7FwPTq9jNU5WuN32BrNz56ABVHXwfOL6u6vtavv3u+BJInlw77amoIFNWTRL7OD//9A45XsdSOQlvJj/IdnwBpP5pv2/hOeDQr3Lqu2X/nFQ5RqTPQ8CZHeVjjCJa2HeLpGwAutxiX02q6J/PHbfpQuUb8P6lwOAZMpxUrBzYDnz9fgJwbILjFGvbRVJ9dEBkWVDKTQP+mCPP7zKufB9jCXDij/K/DywF/tcDuP59oElvGaJcPfentjnZqABHVwNzOpRvKskDBjwJbLDpjjm+DojvUf53vpOgtOML4J/P7LepfGRl7YPeMuT1vl9+8CZdBXS9TQbVMzuBvg/bf1if+7e8WwiQz+uxteWPQXYKENnC+f0EZNj7d4X8feWTQKebAP9w4OwBOf4sqUKX1/kjwNw+ckB9VBsgrhtwzYuyTel7gJ8mAVc9AbS7TgaSXd8Cve+Twx+csa0oXTghK2G6UCA4DghLsu9SPvEH0Ppa+fuf7wBrZwC97gWGvy637f3B8fpTNgLNBsiqmaEQCEu0f/1l7HX4Em3nx3sqbHBTja84jsp2nFLeaSC0bBKTbeWmIFMeZuu7iXLYwaJxVXs/PbNTTnQAgOYDrROWLom7MUqlxbL7UxcC3L3G/nkpLZFLk0Q0l+cbCgEogK//pbfpEnk0KA0YMODiptjXB43aAVc/D/3epdCe2ws/USE4HFtbFpIqlEIdglIRoHMzxurcIeD8YXkyFJYPOq2O0mL5jxSWWP3rsLDt3hI10HVUscujKuy63lwEpQsngDfbyGBwa9lhTs4ekN8Or/w/+abryuGV8kN35Fvlj/uJP4Gi80C7651fRl8hpFlmU1VkW1msi0H9f74jT0VZ8u+Jy4Gmfe33yUuXlQRbsZ1lUKoYkiwW3uq4bde3wK5vHLfZ3udj62QVJvUPuLT3e8dtPe8Ctn4K5J+R3/Kdda+YK1RpLV9eACAw2vGbuj4P8I8oD2FrpsntW2y+XZ894Fj5yjoiq2r/t6+82tTxJrkmlO2YrrS/7S/XuIcMD0cqdDOsf1lW0WzHVh3fAFz1uKx4/LtKXrfD/a3QDdNisBwasH627BYCyquW+xfLLyWWMVvrZgE975a3ofaVK+3bvkdt/p/963Pv93LgcL9HZeCqqGKX857vgM7/BeYmy7+nbAfUPsD6V4HgWLnNZJCntL/lyVgCjHgTWP64vL7vxgOPH5cV57QtMlyNcVKt0ufL58l624sc9wmxmSF9dC3Qdyrw23PAnrL3hq0fy6BkNjvvPjfqZWVn0Xj5ON34mf1r8Ow+x8vYqfA5V5Ijg4FGZ7996STH/yHbqkzuKfl/GRBl/3rOOw1Etbr4sZm2g8oXjpNhte3Ii7uOimyrqLZdb1s+lK+jrLLhKpkH7ZfN+es9WfFtex1w81dy318fA3rcUR5iPcSjQalBUhTgqsegje8pZy84U5BpHyKKLziO4aisMmO7f27ZP0F1zR8JnP4HeOAvILpd1S+XdQwIibefuWMbRsyl8p+9OFt+c6vM/qVyJeQmfcqrPBXf7N05vgE4uEx+6Dlrj21o2v0toM+V33KFkM/b77Nk6XrFEzIomU0ykIY3k10qTfvJdlkO3VGcDbQcAvS4C5g/XG67/08gxqYaYGGZrWX5vVEb+/PNZjnI1raN+Wfl9outeBVfABbfB3QcK7+5V2QqlRUSXajs/rK1Z5EMQYZCIKjscTyz034fXQgQ2qTq7QmKAxp3k5WMimy7OQD5Qf1hX+cDmiNauA7Ove6VgW73t+VjeJoNcN4lAkB+Y7f5cGo2sPxD0SLrqHxdRLYCTlXoUrPuU/amHhQH3PSl7OLZ8Jr8kFpwC3CyrGLVrL/8ADtt02V2siwoBTeWQa/zrYAwyfAY2EiGq29vkmHLEpI63iQDSdrf8nn55RFZoauKxt1lRXv9bMfzNAH2A9sB+TjuWwxcMcnx/cjyGCtq2eYNZavw/zRZVh3+/kj+H+akAlDsuxwBWVWyXSX8g97yp7v/9+3z5MnW2hnlXbF7FgKj5zr+vxzf4BiSK7Ltjjt3EJg3zPG1ZtTLwOGsuyjzoHzeLOctuQ9oatPNnlEWlArPA76B9gHIbHb+pXLX17LtI9+W74uGQvuQFNPJsVt187uy4hYUK780WHw1GmhfYSLE+aPyeel8CxDS2PH2CzJlZdgi+5isxD6R4v5LfGVsu95yT8qKcmC0fE3YStloX1n+fab8eXCZ/LKZulk+r7rQ6relhjAoVZe7EuWpbfbfxvb9KMev2KrsyN+WCgAA5J2qflASovzNe/+SqgellI1yvEHLIcA4m2/4FUupP94lPyBv/0l+cLlyIVX+EwLynzyxLzD2M/kmbGEyym+drjgLplUZr1RwVnZh2q2/ZJAVB9vnZcRbQLvR5X8f+U2ebL/1pO9yDEomo6z8WeSmOQalP+fIN/2kq8q3lRYC6TsBjb/719PZ/fIDLaEX0GqoPBDwkbIqQ8Ze2ZXQ/wnZnbB9vhw35Gr9ksPL5YDoc4fkfc3Ya992QHaD2L45xXR0P0jd1x/o+B/HoBTeXIZDYQYGPisrF2um2YekHneWd7VdMxOI6yK7xISQoWTl08DQl+Tz1+HG8u4hHx0w5lPZPV0xAIxfAiT0llUvS5BqNsAxKFnYBqWBzwLrXnLcp/tEIKGnPOWclFU6S0gCZMg+sto+KGWWdTlGtZGVGIv+T5T/fut38v0hJxVokgw0v1p+kGfsBT4e4NiOtqPk66Vi1aTlUKDfY477A8CwV2WV4NubZRdai0Gy8pV1VAaDdbOcXw6QVaGKFY5Prna9f8+7y0NWns2XB1cBKb6nvM+b/+f8y6NtVRCQVaaI5rIi9OujMmSWLQSMNiOdh3VbTfvJyzgL5Ol7ymfMxXYpn20JlHdDBsbI7sGck/ZdYqe2yuB8eIUMUBN+Ll9nLvek8/v2a9lrIiBKVq8rfmGJ7+EYlCwTKWxDkkXFNZcW3ir/t3+fKQOm2Qh0ull+8U3ZVBZMhPwSUJAh/0/NRvkabu7kOc5Jk1+IzSb5+aD2kRWpoS8DsZ3K96s4WH5uH2DwdMfrqxicbB1fV95t7mq4QB1iUKquwEZlfd+OU3HNx9bZj5KvGJKAiwtKuU6mu7qy70dg9XTgP/OB+O72gyUtU6GrYktZyb5iN0HFD2DLG9P6V9wHJduBj/npckbNDR/ZD5wtzpaPqzOupqm7GsBtG1QvpMoPWts363MHHZ+XDa/Kb/8V2U5rT9sqv+3rQoGrnwcW3+N4e866hdbOkD8tg4wtLB86rUcAHcbIb4WWx+TATzJE2A5uTuovx/lYbC6bXbniCfma+vlhx9tu0ge4+Wvgf13l+CLLDCrLdPmKmg2w/0bZZZycmeLqaPXhzYGW19hva9QemPiLHG9hKJQBw1AoP3CyU2QbWg6xrw427Wv/Gm1+NTDZpvuqWX95ENvibPktOTAKuG+j/Ja/f6n8wNf4yw9f3wBZwrcEpbaj5Adk+m75hq/PBRKukOf1fUg+f5ZxPE2vlP9HR9fK/2+1VgY6i9bDZVCyiO8lu7UHvSA/PBt3Lw8hABDd3vnjBsiQ2W28/bbblsjB/pZuPUvXIACMeFtW6SxBqdPNsrskMNr5ArC3/yRfM4oC3LdJPlZ+ofK84hzg00Guq3iRrWUgqRiUnFH5yEDd56GySsUv8ssQUH4/uo6X48s+7i+rN0NfBjqVDeI98Wd5d2zLIcDIOfJLWMWKpOX9xvbxt7yeu46XXWBFF2TXzfl/ZaXml/8r7w7qdrv94HxbR34r//+M7SRfM5Yqh0WXW+R7ke04uoBGcjadZUbkiU3y9ZO+S1b4bCss8T3lRATbqpWli7biRIH4Xs7H64UkOB+wXpHtFyDLmME938mKsmX5CgDofa+s5n5/h6zgnPzbMSj9u0pWPxN6y3FltpXcZVPk/6GFswkMa6ZX3l5bWz+W91Hl437sVx1hULoU8T2dBqWcPcsRXtllXc1I2f6F/MC3/VDf9Y18861KNeiHsjf0xXcDD+2U5VQLVx90zlQcO3YhVXbDuVyYr+xNWp8vTxW74pxNb84+LkvV1n3O2welfYvleIWbvrSftWLLVRXJVk6qHHhruxBd2lYgON7+W68u1PmMK9sKiO033KjWjl0OgH03nLO/Lbdl+wZ6+Fd5+vEuWbqHUj7eQFHLBQVNBvuQBMhvg/lnZDj74Q65refd8kP7lbLusya9gYAI+W3a9ht386vtx8VYNOsvB3dbdLpZDnq2ne0GAGM/lyFyxBsymPSdKscV3LmyvOvO3+Y/wTdAftMGyl9flm/lPn6VB3m1Brj6WVlRs1RogqLlqf/jsmoizOUDfrtPlAE9roscMza6bOyRUS+DRssh8u+o1sCNn5TfTmIfeTrxhwxLfR60vx/xPWVYAoAxH5evih3RXHZbWCZp5J6SYSm2i/v7VVFgFDB+qfywUGvkgUUtA279I2TwUvvKyt/wNxy7SUa9IwPzrd/Zf3lRqcpDEiB/HzxDDvwF5AD1gszyUH/1c7KSfecqYPU0oOVgORsvsW9517Wvvwxcttd742fy/1UbJLudPugt//8HPiNfhw/vlvfJNthZxi0BwH+/lfd7+Ouy69g3ABj4nAwxx9c5X6Ih4Qr5en5gc/lroFl/eV50B+CrMUCb4bIq+feHjstMAPZrG8X3kgG278PAzMjy7V3GybBtG2DGL5aDtdU+QME5WZ358S55nm33Y8f/ADd+6jgO6ew+YN3L8osaIJ+zttfJyp9Fh7HlX2yunCorOa5mwNlS1LLbzfLl7cQm+6Co8pGV5fAk4NpX5etswyvy/zo8SY7JbDVMDlsAHMfdAfLxmBEm231ml+vZrABwxWQZzHYtKO/OtXX1c/K2rIG1y6WNz60hDEqXwjL7AMDf6IgckxZD1f8g3FCFlXFtK0pF2fJbiqLIlYgBWWGwOPkX8NFVwAvnUWWWhfFsvy3aVpe2zJVvOE2S5RtYRbZ96nu+k5WT5ClAXFfnt3dys3yzOHdQDqx8aKf8lm02AV9e7/xb3LlD9lPKiyrcP8sH/5fXyRkwzhgKyscguQpKF1JlwLPtonFW5Tt/GPijCqsvW/z9of3fCVfIoLV/ifwwO/a7rFA4O17TuB+AzwY7v15L1c4/Qs5a6joeWPmUYyib8o8MMCueLG9LREtg6GzZPXDbj/INr0/ZayqxT3lQ6jBWdn0eWyfHN9iK6SQD2J9z5JgZ/3A5UPjpU/IAtvuXyGOkdbhRniyumSFPVWH5kGwxWH4wxnau2uV63i1PzlTsvlSpgUFODhLro5WVhco0vdJ+HIrt9d6ywPllbGeShSbYvUdcFLUPkDxJ/m4olMtPRLWWYSeiOfD4MRkGnFWRuk2QH+juZtVatBkhJzeoNPK1qi+Qs76i28uKCgA0uQK4y83yHbYhCZCPgeVxUKmAe36XVVJL2LRdRdyi32NyEkXv+8rbHdNRdqNaNGojP2SzjsnXcWRrWd3I2AP0vKfsNp1Mu49sCUzdU/5YjV8iF2/UBsmJAcNfl+9xaX/Lylz/J4EuZZMV1BoZ+nNOyqpiZMuyLzJl4nvKIDa57AtWQSbwXg/HCnhY0/L3sH6PyqBYWiy/+BSeKw9JgAyvcV3k+1qHsXK/KyaVB6W21wGhiXLmp2+gfL9o3N15+EvsA4z5RN6OoiqvgLcaJoNRcY4MRJZ9LfJOydOJTY7vlREt5O39u6r8y54w23/pqlj5tuhwo3wsBj4tv8RbJm5Ed5CvxT4PA3+8U/4l0fJlxsMYlC5Fl3FyVdbEK5Hb6zPc/9U2/KQ8j06qKqyMu+lNOdvlv98C3/5XfkhbpqwC5VNtLcyl8p/P3bduu4X6yr6x2wWlsu684gvygxeQyb3nPfJbrC3bbizLP9df78mSuCt7vyv//eAy+U38zC7Xpe70XbILxKLQRRA0G113EwkzMKcTcO961+Nyck64X4TPL8ymXCzkB4RlodCKfHRy5uOZHfbjdmI6yg+2tC2yymj5Rmn7zbHlUDkma/B0uX9ESzlY+IaP5Jt2xZlkY+eVfyuO7+kYlCyLJfZ/Ur6hZx2T418sH1ItBsuTRZMr7NsLyO6mLuMAbbDs+vMLkx9kQdHA1ArjkrRBsrskoBHQZ4rzx+diKYp8gyTXfAPkVGpb7gbbKkrVQpJlX9vxI9pAYNJml7tXi6sp/bYatQGecVJ5dSaiuaz0WFj+R9yxDZS6EFmBBGQwUxSg/Rg5M7DtdY7d/zd8JCuslpmywbHAoGnyPaP/k/bXHdgIuGURsPAWWYntdJP8shjXtfw5iWgOXF/W9ZW6WQ6MF2YZyJoPlCHJ0uaxZcs/CCEnNIQmyttoeY38ohQYLafUN2onxxYtuEVWFPPT5WreyVNkezv/V17H/qUywAyaJgNLmM39bNRWfsnKOy1D+altsqJq6UIe+JwMebaD6TMPysML7fkO1s8clUZWhj4rC0qDp8vqrMlgP5ap5z0yKIU3Ax6wmV0b2bJ8AoNtl7cHKaLBzc8vl5eXh5CQEOTm5tb94Uwsck/LNxddCM7kFOPzbxfgobPP4qBogqWmvnjc7xeE3/hWeXm7IhfjnJy6Z52cXeSuLW/bdM/1e9R+ef0Wg2WV4ega+26Uu9bIMSS25nRyXHwQkN92LNOo3bnqCdlN8sfbVe+fHv4G0Oseuf6RMAGz4+3P14XKf2Bn5d9G7csHz1bUtB/QfrQcPBnfE7j5G+DNssHxPjrgubPAdJsA+nyWnNZ8/l/5BmcZZDl2nux2+HdleeUPkON/mvaT327XvSxDlsYfOGsTNBL7yvEith9gF1Lldbe7Xg4YfylGbu//pOyXtw05+WflbDGzUb5BN7/a/tt2VZiMwMwI+ft/v2VAIaotliq3J2/XVOoYmM0mGVg0fo6Xdaa0WE4Qie7guBaWLZNRVogiW8kvrNogObzh7H7ZBe7qsTj1jwx+trNsj6yW1bI+DwFDZjq/XA2paoZgRelS2Uy7jAv1Q2LXq9Hpp0+t2xYUDMKwf2IwSBmK/4hVslxsO8juYo7L9MlA+U3qmhfl32aTLHcm9JLfkipeV8VjEFm63iqu+rvgZtm9k5Mqqzrdbnd9YE3bw1e4Y1nHJcVFNcmZfYvldFVnC/gpKmDYK/KbtLM1fFyFJMC+X77d9eXT4oHyGW39n5Tl7xs/k90eE36Rj2dcV2D+CFmRaj5QVlxsFx31DQJaXVs+W8+2+8lUKkNVUbYMahXftMISy9e20vjJEJtzEuh+h+MbS1A08FjZVPX0XfKb5cVS+wCjP5SVsFbDLv7yRFQ1nghJFW/XWVVRpQZUVQxJgHxfuuKByvdT+5QfB9VSQUzoVflAbNtFVS1aXgM8dlQOPagnWFGqYSWlJnz2Rwp2nryANQfLx9+oYUKf8ALMmzQUPm80c3LJsnL5mI+B7yeWb+79gKwAWdZzAYBJf8tS9Yqn5IJyzQbIgdZqX+ezJJKnyG6z0ETZV//l9W7Wn6mEb1DVFzUb8Sbw2/OOU2Ob9pPfIiyHo3CnUTsZkELiZcnaVGo/uBKQs65sB63bspTUAVninfS37Jra9JYctHr188BVj8nQWZBpP6jUwmSUpXFLl5YQcpZZ3hm5GJpt5YeIiBqEqmYIBqVa9OfR8/i/RbtwRbMIbDpyDheKSvH62E74j/I7sPMrORU97W85Gyeuqxy0GRIPzL2yvNtm8jY5XqniwoFV7bLzj5CzVt4rS+62s62qsu6IK9EdqrAarRP3/C5n6yiKDD17vpN94ZaF5rpNkN1vlmOmdbkNGP2+/XWc2SnHDES0kI9Btwmyi88yW8cSDKM7AHf9JrvD1BrZx2+ZjSeEvJ7oDu6Pu0RERJclBqV65qMNxzB7hZxm/uzwthjZORYRAVr4qhXHMu3m94DfnpW/P35crj1jGVBdKUWuC2NZZ6RpPzm9/rUk+9063iRLnLbX26QP0O8R2T/sTkQLOV7qgyvKB/oBcjZSn4eAdzrZ7287I+OxI46DJTP2AZ8Pld1/w2bLxyNjn1wjaMBT5YOW3ck+Xn4wz+cy5dRsvzD7ad1ERERlGJTqGb3RhGvnbMLx8+VT2AN81fjq7t7o1iTMfufC8/II0CEJwMO75BTOD3rLqcqj3pWVHEUF+EcC7/cqX9367t/lwoqB0eWDdhPLVom1PXL1fRvldOxT/8gF5wDgtsXyMA8qlVxWP+uI7GZbcq883zdIVnY2vi7HSDW/Wh7H59RWOcvhwE9y4J1vALD8CbkKsmWK7LgfgW/KBo+/cMH5ITvMJufThi/GkdXycbFdf4SIiMgJBqV66GB6Hr78KxV7T+dg32m5MmuTcH9Mv64dArUa9EqyqX7kpcvuooCy8ThCyFPFkPHb87Ly0myAnFVl8fEA2bV0/QdA13Hls7pajwBuKTsMRGEW8HpZtebpU47TeIUAZoSW/12Vo1Hb2vOdnKHV6145CNxH53hoDyIiIg9gUKrnMvNLMPq9P3Emt/w4Yl/c2Qv9W0W5uZQTRr1cqbjddbKryaIoW07PbDVUdmV9PkwuXHnnb3KlZosjq+UgcFdrkVgWm7SdbUdERNTAMSg1ACnnCzHl2x3Yf0ZWlxLC/XBX3yRk5uvx064zSIoMwJ1XNkV4gBZdEkIv7cYKs+TK15YpnBfjQqo8kC0HPRMR0WWCQamBMJsFzhXoMeLdTThfYHC6j69ahQX3XoHuiWFOzyciIqKLU9UM4WRULdUllUpBdLAOSyf3xaPXtMLA1lEY0i4avW3GKxlMZjzw9XaknC9EkcGIr7ak4mimq4PTEhERUU1hRame2vjvOdz++VaX57eLDcbyh90sKU9EREQusaLUwPVtEYmbesTjvquaYflD/RCotT/azIH0PAx/ZxO+/OsEdp68gPySUvx1LAsmc4PNvURERPUOK0oNRJHBCKNZYOfJHLz3+xFsO3HB6X6TBzbHY0NaQ/HUsYaIiIgaAFaULjP+vj4I1mnQv1VUWRACNGoFSZEB0KjLQ9H7646hzfMr8fbqf2E0mWFmhYmIiKjaWFFqoC4UGhDqr4GiKMgq0GPl/gw8u8Tx2GtBOh/Mm9gTPZraH8ojM68EpWaBxqEXcSRpIiKiywQrSpe5sABfa/daRKAW43on4tUbO8JPo0bXJqHw9ZFPbX6JEQ8t2Illu89ge+oFCCGQV1KK4e9uwjVvbcDag2dxwuawKkRERFSOFaXLjNksoFIp2J2WgyU7T2P+5hN259/UIx6lJoElO8sPZuvvq8aqqVchIdy/jltLRETkGVxwkgAARzPzMWfNEWxNyUZmvt7lfr2SwjHn5i6IY1ccERF5AQYlcrDmwFms2JeB7EI9EiMCcF2XOBzOyMe0ZfthMJoRpPXBXf2SMLZ7PPx9feDvq4ZOo/Z0s4mIiGocgxJV2b7TuXj+p33YeTLHbrvWR4U7+ibh8aGtoVZxuQEiIrp8cDA3VVmHxiH4/r5kzBzdAYkR5eOU9EYzPtxwDOM+3YKTWUUebCEREZFnsKJEdrIK9Jj5ywF0jA9FmL8GzyzZi5JSMwCgW5NQqFUKUrOKMHN0B3RPDEOonwYp5wuxdNdpRAVq8d9eTbD/TC5Ss4pwQ9fGXPiSiIjqJXa9UY1Iyy7CnfO34UgVD8LbLjYYB9LzAABf3NkL/VtF1WbziIiIqqWqGcLH5TlEABLC/fHNPb3x2R8piA/zh0al4KVfDyJfb7TbL0jnA7NZWEMSAHz1VyqDEhERNWisKNFFO5tXgqU7T2N4x1jsO52LXWk5uL9/cxxIz8O4T/+227dviwhMG9UeJaUmzF5+CI3D/PDajZ2g4uBwIiLyIHa9kUd89kcKdqflwNdHhR+2nwIA+KgUmIWA5bBz13aIwaNDWqFFoyAPtpSIiLwZgxJ53MmsIsz89QBWHzgLAOicEIrdaTkAZHj6T48ENIsMgI9aQdcmYdhw+BxGdY5Fs6hAD7aaiIi8AYMS1Rubj52HAgVXNAvHt1tP4qddZ7A1Jdvpvs2iArBq6lXQqLlyBRER1R4GJaq3hBDYeOQ81hw4i/ySUvx7tsBuEPjEPk3Rv3UUtD4q9GwaDo1aBSEECg0mBGo5/4CIiC4dgxI1GAajGasPnEXK+QK88du/dueF+muQFBmAPadyYTIL9EoKx/9u6YroYB0AoNRkhskseKgVIiK6KAxK1CD9sucMPv8jBXqjGSezihyWIQCAK1tE4p3/dsGutBy8tPwgcopKsfLhfmhUFp6IiIgqw6BEDd6ZnGKsPZSJI2fzcSg9H8M6xOCVFYdgMJkd9r37yiQ8dW0blBjN8NeosWp/BuLD/NExPsQDLSciovqOQYkuSz/tOo3py/bjQlEpIgO1OF+gtztfpQB+GjUKDSb4+qjw4W3d0L9VIx7Ul4iI7DAo0WVLbzQhv8SIyEAthBC458vtWHPwrMv940J06JoYhmCdBhm5xbilVxMMaR9Thy0mIqL6hkGJvIbJLLBw20kYjGYMbhuN4+cLEeqnwUvLDzpdhkCjVjB1cCvc1jsRIf4aD7SYiIg8jUGJCEB2oQHz/0yBSQiUlJrxxeYTMJYtER4VpEW/FpE4V6BHtyZh6NIkFL/uScfd/ZLQJoavJyKiyxmDEpETBXojPtl4HD9sP4XTOcVO91EpwG1XJKJNTDAiA30RpNMguXmE3T57TuXAT6NGy2gehoWIqCFiUCJyI7+kFMt2n0FWgQEKgDdX/+t2/9uuaIIrW0ShbWwQDEYzhr2zCf4aNTY9ORCh/r5102giIqoxDEpEF+H7f9LwyabjeOM/nXEuX4+1hzKRll2Ev45lWbvqAECtUmCy+Tu5WQReG9sJCeH+nmg2ERFVE4MSUQ0wmwWeXboXfx/PRoHeiMx8vcM+igL8t2cT3HdVM5wr0KN1TBCCdRwkTkRUnzEoEdUwo8mM11cdxvHzhejfKgrbUy9g3+lcHMkscNg3IsAXiRH+8PNVQ61SYWTHWHRvGoacolJ0TQiFius6ERF5FIMSUR1ZfzgTj/+wB3nFpQjQ+iC70OB2/7Hd4/HajZ0YloiIPIhBiagOmcvGLZmEwJ9Hz+N8gQH7TueiSbg/igxGLN55GsfPFdpdpnGoH0Z1jkNCuB+Sm0UgKTIAQgDrDmei1CQwrAMXxSQiqi0MSkT1jBACP+9Jx9SFO2F28l/nq1bZHcfuxevbY/wViVAUVp6IiGoagxJRPbVyXzoW7ziNYD8NCvVG5BSV4u+ULKfhqUm4P7okhCIzvwSnLhRjeMdYPDmsDY9dR0R0iRiUiBqQtOwipOeWIMRPgzB/DT77IwXzN5+A3mh22LddbDC6NAlFRm4J8ktKMbR9DG67IhE6jRqAXFRze+oF9GkeAY1aVdd3hYioQWBQImrgcotKsfbQWcz85QAaBekwoE0UPv8jBaUmx39ZnUaFyEAtigwm62DyAa2jMHtMR8SG+NV104mI6j0GJaLLhMFoho9KgUqlID23GFuOZ2Hf6Tw0DvWDwSSPX5eeW+L0smqVgl5Nw9GzaRjaxYUgI7cYof6+GNU5jt13ROTVGJSIvIQQAr8fysSPO04hwNcHapWCzgmhWLj1JHafynV6mb4tInBd5ziE+GkQpNOgTUwQDmXk4921R9ChcQieG9GWg8iJ6LLGoETk5YQQ+GTTcazYlwG1ouBcgR5JkQHYfDTLbnadMwNbR8Ff64M9p3LkAPKhbbjuExFdVhiUiMipg+l5+GH7KRw7V4D8EiPO5skZde40DvVD+7hgXNkyEs2jAnEwPQ8HzuShbWww/LVqXN2mEcdCEVGDwqBERFViMgss3Xka4QG+GNA6ChuPnMeRs/kwC4EjZwvw0+4zMDiZfVdRs6gARAfpYBICg9o0Qmp2ERQAd/RNQotGgbV/R4iILgKDEhHViLySUhw4k4edJ3Pwx9FzSM8pQWyoDkFaDdJzi6E3mnEoI9/tdXRtEoqWjQKRll2MUzlFiAnWoXN8KMIDfRER4IueTcPRLEqGqZJSEwDgnxMX0KNpmHXZAyKimsSgRER15kKhAd9vT8PuU7nQ+ahxociARkFanM4pxp9HzztdTLMiX7UKvj4qFOiN1m1NI/wxolMs+raIREKYPxLC/WEyC2Tml+Bcvh5Nwv0R6u9bi/eMiC5XDEpEVC9k5pVg+d505BYbkRDuh5gQHY5lFiDtQjGyCgw4k1OMrSeyYapCmgrS+SC/pDxIqRSgU3wo8opLcb5Aj+TmEZiQ3BQXikpxNq8Efr5qNAn3R7vYYOw/k4eYEC1aNAqyXr5Qb8Sve9NxLLMA/+mRgLySUnSJD+XAdSIvwKBERA1God6InOJSlJSa4KNS8HdKNppFBuC3A2dx+kIx/knNxrl8vbUypVYpCPP3xfkC/UXflo9KgZ+vGv6+ahTpTci3qWABsptwUJtGUKkUNA71w5mcEqgUoHlUIEqMJsQE6xAf5o8ArRqqsiUUzELAv2xphuq6UGiAj1pBkE5T7esgoqpjUCKiy0pWgR5HMgvQLCoAoX6+8PVRIS27CCv3ZUBAoE1MMD7ZdBwnsgoRHaRDdLAORQYjjp4rQFq261l9jUP9cC5fX+mSCZUJ0vmge2IYLhQacCgjH9HBOuQUGeDro0LLRkEoMhjRNjYY+XojTl8oxsDWjRAe6IttKdn44+h5ZBcaoFEruKNvEtrHBSMqUItzBXoE6zRoFhWA0znFyCkqRaHeiOJSE2JD/NA6OggFeiN0GhWMZoFigwkqRYFKBfhp1EiMCHAIb7lFpSg1mxEZqAUgB/MbzWZofdQQQuCvY1mIDfVDUmTAJT0eRPUdgxIREeR6UmdySxAR4AuzEMgtLkWRwYRigwkms0C7uGAUl5qw+WgWWjQKwK97MpCaVQi90YyzeSVoHOaH7EIDMvP0CPHT4Oi5AuQUGao07srTgnQ+aBYZgOJSE3QaNc7l662ruLePC0Z8mB+2HM9GbnEpgnQ+CNT6ID23BFofFW5PTkR+iREms4BZACVGE4LL9jmXr0egzgcXikpRbDChd1I4kiIDoFGrcCA9D3klpYgK1EKrUSMpIgBpF4pwLl+PlfsyoNWocPeVzdAyOhAZuSUwCYGUc4XILS5FYoQ/ooK06NYkDFkFssIW7KeBRq0gr9iIMH8NTmQV4WR2IRQo6JkUjkCtDwDgbF4JTucUIz7MD+fzDTALgXaxwXbdqEIInLpQjIhAX/j7+li3m8wCRQYjNGoVTl0oRlKkY8Ckyw+DEhFRLRBCQAggt7gUigLrgYePnSvAzpM5CAvwRbPIAKRmFSE+zA+lJjN2nLyAAK0PUrOKEKzzgaIo2HsqFyYhkBjuj2s7xqBtbDDWHszEukOZOJVTjOxCA3Qa+cFdpDchPszP+gGv9VHhZHYRjmQWIFjnA6NJQOOjgp9GVoWMZoH8Ell5ckZRgIb7zm8vWOcDXx8VsgoNDvdJpQCNgmRlMTzAFypFwfHzhfBVqxCgVcNkFtCoVbhQIfg2CtKiX8so5JeUIr/ECK1GPrYlpSYUGkwI1PogQOuDQK0aeqMZWQUGZBcaEBeqQ4DWB+cLDGgXG4zYEB2KDCakXShCRm4JEsL8UGQwodBgRMtGQRBCQG80I+1CEXzVKug0amh95KQGjVqegnQ+KDKYoDeaym7TB8UGE3acvID8EiNaNgpEi+ggZOQWo3lUINQqBTqNGuEBvgjz94VOo0JGbgnySkpxvsCA/BIjRnWKRYi/BvvP5CG7wIDIIC3UigKdRoUmEf5Iyy5GqUlWHbMLDTiRVYhigwkCAh3iQqBWKTibp0dJqQnBfhqYzAKxITrEhcrX+8nsIvioFPiW3RetWm39PSOvBH5l7QPk/1NesRH+WjWMJgGdRv4/KYqCIoMRpSaBEL/a6Y5mUCIiugxYgpmzAeZms3A58Nxokss2pGYVIdjPB/pSM8ICNGgZHQSD0Yx1hzKRU1SKDo1D0DomCNmFepwvMCApMgC/7knHkp2n0TjUD+3igqFRq6D1USG/xIi8klJEBPqi2GBCmL8vjGYzdqTmID2vBMUGI9rEBCNAq8aFwlKczS/B+QI9mkUGQqNWIVCrhlqlwuGzeTiaWYBwf1/ofNWID/NH41AdjmUW4kRWITLz9VApcixaxYNA+2nUSIoMQF5JqcNCqTHBOmTkyYqZj0qBsSGU/S4jGrV8LTo7cLctRQEiAnyh9VEjr7jUbpxgiJ8GRQYjdBo18kuMmDywOR4f2qZW2sugREREDY7RZMbhs/lIjAhAgK8aJaVmlJTKakp2oVx2QqVSIIRATlEpsgr10BvNCA/wRWyI7Cb1USvw06hxodCAE1lF0GlUyC0uRanJjM7xocgvMcJgMkOlKDAYzYgM9IVapeBkdhFaxwThjyPnsf9MHiIDfRHsp4HBKNugVlkqPEYU6E0o1MvuOst+/2bkQ6VSoPVRYfOxLGjUCgJ8fdA4zA+RgVqculBkXc7i2LkC+PuqYRZAi6hAmMuqSyWlJuiNZpSazDAYzcgrMULro0Kg1gcFeiMK9Ub4qFVoExOEmGAddqXlYO/pXJSazDh1oRhNwv3ho1ZwodCArEIDig0mxIToEFJW+dEbzUg5XwhAhpKEcD9kFchqXIHeiAK90Vpxu1BUiiCtD5KiAhCskwFGLgGiQnSIDn4aNXKLS6FS5AG7LQEpSOsDKPKA3gaT+ZKql2O6NcZbN3W51JeVUw0mKH3wwQd4/fXXkZ6ejvbt22POnDno169flS7LoERERHRxCvRGGIxmhPlr7A5+LYTAuQI9grQa+PmqYTSZoVYpdvuYzMLp+C2TWSAjrwTFBhOaRwVYL2PpCtYbZfAL0KqRXWjAhcJS6I2yGzM+zB+FBiPUioITWYUI8/eF3mhGoyAtwgJqb520qmYIH5fn1IFFixZh6tSp+OCDD9C3b1989NFHuPbaa3HgwAE0adLEk00jIiK6LAVqfQCt43ZFUdAoSGf926ds/J0tV4Pc1WXLaTi7To1akWP5ym4zNsTP4diQfr5yBf7aDEbV5dGKUu/evdGtWzfMnTvXuq1t27YYPXo0Zs+eXenlWVEiIiKi6qhqhnCMi3XEYDBg+/btGDJkiN32IUOGYPPmzU4vo9frkZeXZ3ciIiIiqi0eC0rnz5+HyWRCdHS03fbo6GhkZGQ4vczs2bMREhJiPSUkJNRFU4mIiMhLeSwoWdgOEgPkwK+K2yyefvpp5ObmWk9paWl10UQiIiLyUh4bzB0ZGQm1Wu1QPcrMzHSoMllotVpotU5GoBERERHVAo9VlHx9fdG9e3esXr3abvvq1avRp08fD7WKiIiIqJxHlwd45JFHMH78ePTo0QPJycn4+OOPcfLkSdx///2ebBYRERERAA8HpZtvvhlZWVl48cUXkZ6ejg4dOmD58uVITEz0ZLOIiIiIANSDlbkvBddRIiIiouqo9+soEREREdV3DEpERERELjAoEREREbnAoERERETkAoMSERERkQseXR7gUlkm7PHguERERHQxLNmhssn/DToo5efnAwAPjktERETVkp+fj5CQEJfnN+h1lMxmM86cOYOgoCCXB9Ktrry8PCQkJCAtLY1rNHkAH3/P43PgWXz8PYuPv+fV9nMghEB+fj7i4uKgUrkeidSgK0oqlQrx8fG1ehvBwcH8J/EgPv6ex+fAs/j4exYff8+rzefAXSXJgoO5iYiIiFxgUCIiIiJygUHJBa1Wi2nTpkGr1Xq6KV6Jj7/n8TnwLD7+nsXH3/Pqy3PQoAdzExEREdUmVpSIiIiIXGBQIiIiInKBQYmIiIjIBQYlIiIiIhcYlFz44IMPkJSUBJ1Oh+7du2PTpk2ebtJlYePGjRg1ahTi4uKgKAqWLl1qd74QAtOnT0dcXBz8/PwwYMAA7N+/324fvV6PBx98EJGRkQgICMB1112HU6dO1eG9aJhmz56Nnj17IigoCI0aNcLo0aNx+PBhu334+NeuuXPnolOnTtYF9JKTk7FixQrr+Xz869bs2bOhKAqmTp1q3cbnoPZMnz4diqLYnWJiYqzn19vHXpCDhQsXCo1GIz755BNx4MAB8fDDD4uAgACRmprq6aY1eMuXLxfPPvus+PHHHwUAsWTJErvzX3nlFREUFCR+/PFHsXfvXnHzzTeL2NhYkZeXZ93n/vvvF40bNxarV68WO3bsEAMHDhSdO3cWRqOxju9NwzJ06FAxb948sW/fPrFr1y4xYsQI0aRJE1FQUGDdh49/7Vq2bJn49ddfxeHDh8Xhw4fFM888IzQajdi3b58Qgo9/Xdq6dato2rSp6NSpk3j44Yet2/kc1J5p06aJ9u3bi/T0dOspMzPTen59fewZlJzo1auXuP/+++22tWnTRjz11FMeatHlqWJQMpvNIiYmRrzyyivWbSUlJSIkJER8+OGHQgghcnJyhEajEQsXLrTuc/r0aaFSqcTKlSvrrO2Xg8zMTAFAbNiwQQjBx99TwsLCxKeffsrHvw7l5+eLli1bitWrV4v+/ftbgxKfg9o1bdo00blzZ6fn1efHnl1vFRgMBmzfvh1Dhgyx2z5kyBBs3rzZQ63yDikpKcjIyLB77LVaLfr372997Ldv347S0lK7feLi4tChQwc+PxcpNzcXABAeHg6Aj39dM5lMWLhwIQoLC5GcnMzHvw5NnjwZI0aMwODBg+228zmofUeOHEFcXBySkpLw3//+F8ePHwdQvx/7Bn1Q3Npw/vx5mEwmREdH222Pjo5GRkaGh1rlHSyPr7PHPjU11bqPr68vwsLCHPbh81N1Qgg88sgjuPLKK9GhQwcAfPzryt69e5GcnIySkhIEBgZiyZIlaNeunfWNno9/7Vq4cCF27NiBbdu2OZzH/4Ha1bt3b3z55Zdo1aoVzp49i1mzZqFPnz7Yv39/vX7sGZRcUBTF7m8hhMM2qh3Veez5/FycKVOmYM+ePfjjjz8czuPjX7tat26NXbt2IScnBz/++CMmTJiADRs2WM/n41970tLS8PDDD+O3336DTqdzuR+fg9px7bXXWn/v2LEjkpOT0bx5c3zxxRe44oorANTPx55dbxVERkZCrVY7pNPMzEyHpEs1yzL7wd1jHxMTA4PBgAsXLrjch9x78MEHsWzZMqxbtw7x8fHW7Xz864avry9atGiBHj16YPbs2ejcuTPeeecdPv51YPv27cjMzET37t3h4+MDHx8fbNiwAe+++y58fHysjyGfg7oREBCAjh074siRI/X69c+gVIGvry+6d++O1atX221fvXo1+vTp46FWeYekpCTExMTYPfYGgwEbNmywPvbdu3eHRqOx2yc9PR379u3j81MJIQSmTJmCxYsX4/fff0dSUpLd+Xz8PUMIAb1ez8e/DgwaNAh79+7Frl27rKcePXpg3Lhx2LVrF5o1a8bnoA7p9XocPHgQsbGx9fv1X2vDxBswy/IAn332mThw4ICYOnWqCAgIECdOnPB00xq8/Px8sXPnTrFz504BQLz11lti586d1qUXXnnlFRESEiIWL14s9u7dK2655Ran00Pj4+PFmjVrxI4dO8TVV1/NqblV8MADD4iQkBCxfv16u+m5RUVF1n34+Neup59+WmzcuFGkpKSIPXv2iGeeeUaoVCrx22+/CSH4+HuC7aw3Ifgc1KZHH31UrF+/Xhw/flxs2bJFjBw5UgQFBVk/W+vrY8+g5ML7778vEhMTha+vr+jWrZt1CjVdmnXr1gkADqcJEyYIIeQU0WnTpomYmBih1WrFVVddJfbu3Wt3HcXFxWLKlCkiPDxc+Pn5iZEjR4qTJ0964N40LM4edwBi3rx51n34+NeuO++80/q+EhUVJQYNGmQNSULw8feEikGJz0HtsayLpNFoRFxcnBgzZozYv3+/9fz6+tgrQghRe/UqIiIiooaLY5SIiIiIXGBQIiIiInKBQYmIiIjIBQYlIiIiIhcYlIiIiIhcYFAiIiIicoFBiYiIiMgFBiUiIiIiFxiUiIhsKIqCpUuXeroZRFRPMCgRUb0xceJEKIricBo2bJinm0ZEXsrH0w0gIrI1bNgwzJs3z26bVqv1UGuIyNuxokRE9YpWq0VMTIzdKSwsDIDsFps7dy6uvfZa+Pn5ISkpCd9//73d5ffu3Yurr74afn5+iIiIwL333ouCggK7fT7//HO0b98eWq0WsbGxmDJlit3558+fxw033AB/f3+0bNkSy5Ytq907TUT1FoMSETUozz//PG688Ubs3r0bt912G2655RYcPHgQAFBUVIRhw4YhLCwM27Ztw/fff481a9bYBaG5c+di8uTJuPfee7F3714sW7YMLVq0sLuNGTNm4KabbsKePXswfPhwjBs3DtnZ2XV6P4monhBERPXEhAkThFqtFgEBAXanF198UQghBABx//33212md+/e4oEHHhBCCPHxxx+LsLAwUVBQYD3/119/FSqVSmRkZAghhIiLixPPPvusyzYAEM8995z174KCAqEoilixYkWN3U8iajg4RomI6pWBAwdi7ty5dtvCw8OtvycnJ9udl5ycjF27dgEADh48iM6dOyMgIMB6ft++fWE2m3H48GEoioIzZ85g0KBBbtvQqVMn6+8BAQEICgpCZmZmde8SETVgDEpEVK8EBAQ4dIVVRlEUAIAQwvq7s338/PyqdH0ajcbhsmaz+aLaRESXB45RIqIGZcuWLQ5/t2nTBgDQrl077Nq1C4WFhdbz//zzT6hUKrRq1QpBQUFo2rQp1q5dW6dtJqKGixUlIqpX9Ho9MjIy7Lb5+PggMjISAPD999+jR48euPLKK/HNN99g69at+OyzzwAA48aNw7Rp0zBhwgRMnz4d586dw4MPPojx48cjOjoaADB9+nTcf//9aNSoEa699lrk5+fjzz//xIMPPli3d5SIGgQGJSKqV1auXInY2Fi7ba1bt8ahQ4cAyBlpCxcuxKRJkxATE4NvvvkG7dq1AwD4+/tj1apVePjhh9GzZ0/4+/vjxhtvxFtvvWW9rgkTJqCkpARvv/02HnvsMURGRmLs2LF1dweJqEFRhBDC040gIqoKRVGwZMkSjB492tNNISIvwTFKRERERC4wKBERERG5wDFKRNRgcKQAEdU1VpSIiIiIXGBQIiIiInKBQYmIiIjIBQYlIiIiIhcYlIiIiIhcYFAiIiIicoFBiYiIiMgFBiUiIiIiF/4fwJ0CDJAS1BoAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class LossFunction(nn.Module):\n",
    "    def __init__(self, num_tasks):\n",
    "        super(LossFunction, self).__init__()\n",
    "        self.num_tasks = num_tasks\n",
    "\n",
    "        self.loss_fns = [nn.MSELoss() for x in range(num_tasks)] \n",
    "\n",
    "    def forward(self, predictions, labels_task1):\n",
    "\n",
    "        #task 1\n",
    "        target = labels_task1\n",
    "        prediction = predictions\n",
    "        loss_fn = self.loss_fns[0]\n",
    "\n",
    "        task_loss = loss_fn(prediction, target)\n",
    "        \n",
    "        return task_loss\n",
    "    \n",
    "model = CATTransformer(rff_on=True,\n",
    "                       sigma=2,\n",
    "                       embed_size=160,\n",
    "                       heads=5,\n",
    "                       forward_expansion=8,\n",
    "                       pre_norm_on=False,\n",
    "                       mlp_scale_classification=8,\n",
    "                       decoder_dropout=0,\n",
    "                       classification_dropout=0.1,\n",
    "                       targets_classes=target_classes,\n",
    "                       n_cont=len(cont_columns),\n",
    "                       embedding_scheme='rff_unique',\n",
    "                       linear_on=True).to(device_in_use)\n",
    "\n",
    "def format_metric(value): # Used to format the metrics output\n",
    "    return f\"{value:.5f}\"\n",
    "\n",
    "loss_functions = LossFunction(1)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr = 0.0001) # Maybe try messing around with optimizers. try other torch optimizers with different configurations.\n",
    "epochs = 500 #Set the number of epochs\n",
    "\n",
    "train_losses = []\n",
    "train_rmse = [] \n",
    "test_losses = []\n",
    "test_rmse = []  \n",
    "\n",
    "\n",
    "#Time will be recorded for all 100 epochs - This means the results will not be comparable to Xgboost but that is ok, we will only compare between transformer models who will also train for 100 epochs\n",
    "start_time = time.process_time()\n",
    "\n",
    "for t in range(epochs):\n",
    "    train_loss, r2_train, rmse_train = train(train_dataloader, model, loss_functions, optimizer, device_in_use=device_in_use)\n",
    "    test_loss, r2_test, rmse_test = test(test_dataloader, model, loss_functions, device_in_use=device_in_use)\n",
    "    train_losses.append(train_loss)\n",
    "    train_rmse.append(rmse_train)\n",
    "    # train_accuracies_2.append(train_accuracy_2)\n",
    "    # train_recalls.append(train_recall) \n",
    "    # train_f1_scores.append(train_f1)\n",
    "    test_losses.append(test_loss)\n",
    "    test_rmse.append(rmse_test)\n",
    "    # test_accuracies_2.append(test_accuracy_2)\n",
    "    # test_recalls.append(test_recall)\n",
    "    # test_f1_scores.append(test_f1)\n",
    "    # Formatting for easier reading\n",
    "    epoch_str = f\"Epoch [{t+1:2}/{epochs}]\"\n",
    "    train_metrics = f\"Train: Loss {format_metric(train_loss)}, R2 {format_metric(r2_train)}, RMSE {format_metric(rmse_train)}\"\n",
    "    test_metrics = f\"Test: Loss {format_metric(test_loss)}, R2 {format_metric(r2_test)}, RMSE {format_metric(rmse_test)}\"\n",
    "    print(f\"{epoch_str:20} | {train_metrics:65} | {test_metrics}\")\n",
    "\n",
    "total_time = time.process_time() - start_time\n",
    "\n",
    "# Plotting the loss curves\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, epochs+1), train_losses, label='Train Loss')\n",
    "plt.plot(range(1, epochs+1), [l for l in test_losses], label='Test Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Test Loss Curve')\n",
    "plt.legend()\n",
    "\n",
    "best_index = test_rmse.index(min(test_rmse))\n",
    "print(f\"Best rmse {test_rmse[best_index]}\")\n",
    "print(f\"100 epochs of training and evaluation took, {total_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mixed Normal RFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [ 1/500]       | Train: Loss 4.48823, R2 -0.24883, RMSE 2.06654                    | Test: Loss 3.51427, R2 0.01138, RMSE 1.86075\n",
      "Epoch [ 2/500]       | Train: Loss 3.21265, R2 0.11007, RMSE 1.78270                     | Test: Loss 2.39937, R2 0.32660, RMSE 1.54162\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\smbm2\\projects\\CAT-Transformer\\rff_experiments\\california.ipynb Cell 8\u001b[0m line \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/rff_experiments/california.ipynb#X11sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mprocess_time()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/rff_experiments/california.ipynb#X11sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/rff_experiments/california.ipynb#X11sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m     train_loss, r2_train, rmse_train \u001b[39m=\u001b[39m train(train_dataloader, model, loss_functions, optimizer, device_in_use\u001b[39m=\u001b[39;49mdevice_in_use)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/rff_experiments/california.ipynb#X11sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m     test_loss, r2_test, rmse_test \u001b[39m=\u001b[39m test(test_dataloader, model, loss_functions, device_in_use\u001b[39m=\u001b[39mdevice_in_use)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/rff_experiments/california.ipynb#X11sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m     train_losses\u001b[39m.\u001b[39mappend(train_loss)\n",
      "\u001b[1;32mc:\\Users\\smbm2\\projects\\CAT-Transformer\\rff_experiments\\california.ipynb Cell 8\u001b[0m line \u001b[0;36m4\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/rff_experiments/california.ipynb#X11sZmlsZQ%3D%3D?line=398'>399</a>\u001b[0m \u001b[39mfor\u001b[39;00m (features,labels_task1) \u001b[39min\u001b[39;00m dataloader:\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/rff_experiments/california.ipynb#X11sZmlsZQ%3D%3D?line=399'>400</a>\u001b[0m     features,labels_task1 \u001b[39m=\u001b[39m features\u001b[39m.\u001b[39mto(device_in_use),labels_task1\u001b[39m.\u001b[39mto(device_in_use)\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/rff_experiments/california.ipynb#X11sZmlsZQ%3D%3D?line=401'>402</a>\u001b[0m     task_predictions \u001b[39m=\u001b[39m model(features) \u001b[39m#contains a list of the tensor outputs for each task\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/rff_experiments/california.ipynb#X11sZmlsZQ%3D%3D?line=403'>404</a>\u001b[0m     loss \u001b[39m=\u001b[39m loss_function(task_predictions[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39msqueeze(\u001b[39m1\u001b[39m), labels_task1)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/rff_experiments/california.ipynb#X11sZmlsZQ%3D%3D?line=404'>405</a>\u001b[0m     total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\smbm2\\AppData\\Local\\miniconda3\\envs\\ml-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mc:\\Users\\smbm2\\projects\\CAT-Transformer\\rff_experiments\\california.ipynb Cell 8\u001b[0m line \u001b[0;36m3\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/rff_experiments/california.ipynb#X11sZmlsZQ%3D%3D?line=375'>376</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/rff_experiments/california.ipynb#X11sZmlsZQ%3D%3D?line=376'>377</a>\u001b[0m     class_embed, context \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membeddings(x)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/rff_experiments/california.ipynb#X11sZmlsZQ%3D%3D?line=378'>379</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder(class_embed, context)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/rff_experiments/california.ipynb#X11sZmlsZQ%3D%3D?line=380'>381</a>\u001b[0m     probability_dist_raw \u001b[39m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\smbm2\\AppData\\Local\\miniconda3\\envs\\ml-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mc:\\Users\\smbm2\\projects\\CAT-Transformer\\rff_experiments\\california.ipynb Cell 8\u001b[0m line \u001b[0;36m2\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/rff_experiments/california.ipynb#X11sZmlsZQ%3D%3D?line=209'>210</a>\u001b[0m     \u001b[39mfor\u001b[39;00m i, e \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcont_embeddings):\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/rff_experiments/california.ipynb#X11sZmlsZQ%3D%3D?line=210'>211</a>\u001b[0m         goin_in \u001b[39m=\u001b[39m x[:,i,:]\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/rff_experiments/california.ipynb#X11sZmlsZQ%3D%3D?line=211'>212</a>\u001b[0m         goin_out \u001b[39m=\u001b[39m e(goin_in)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/rff_experiments/california.ipynb#X11sZmlsZQ%3D%3D?line=212'>213</a>\u001b[0m         embeddings\u001b[39m.\u001b[39mappend(goin_out)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/rff_experiments/california.ipynb#X11sZmlsZQ%3D%3D?line=213'>214</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\smbm2\\AppData\\Local\\miniconda3\\envs\\ml-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\smbm2\\AppData\\Local\\miniconda3\\envs\\ml-env\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class LossFunction(nn.Module):\n",
    "    def __init__(self, num_tasks):\n",
    "        super(LossFunction, self).__init__()\n",
    "        self.num_tasks = num_tasks\n",
    "\n",
    "        self.loss_fns = [nn.MSELoss() for x in range(num_tasks)] \n",
    "\n",
    "    def forward(self, predictions, labels_task1):\n",
    "\n",
    "        #task 1\n",
    "        target = labels_task1\n",
    "        prediction = predictions\n",
    "        loss_fn = self.loss_fns[0]\n",
    "\n",
    "        task_loss = loss_fn(prediction, target)\n",
    "        \n",
    "        return task_loss\n",
    "    \n",
    "model = CATTransformer(rff_on=True,\n",
    "                       sigma=2,\n",
    "                       embed_size=160,\n",
    "                       heads=5,\n",
    "                       forward_expansion=8,\n",
    "                       pre_norm_on=False,\n",
    "                       mlp_scale_classification=8,\n",
    "                       decoder_dropout=0,\n",
    "                       classification_dropout=0.1,\n",
    "                       targets_classes=target_classes,\n",
    "                       n_cont=len(cont_columns),\n",
    "                       embedding_scheme='rff',\n",
    "                       linear_on=True).to(device_in_use)\n",
    "\n",
    "def format_metric(value): # Used to format the metrics output\n",
    "    return f\"{value:.5f}\"\n",
    "\n",
    "loss_functions = LossFunction(1)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr = 0.0001) # Maybe try messing around with optimizers. try other torch optimizers with different configurations.\n",
    "epochs = 500 #Set the number of epochs\n",
    "\n",
    "train_losses = []\n",
    "train_rmse = [] \n",
    "test_losses = []\n",
    "test_rmse = []  \n",
    "\n",
    "\n",
    "#Time will be recorded for all 100 epochs - This means the results will not be comparable to Xgboost but that is ok, we will only compare between transformer models who will also train for 100 epochs\n",
    "start_time = time.process_time()\n",
    "\n",
    "for t in range(epochs):\n",
    "    train_loss, r2_train, rmse_train = train(train_dataloader, model, loss_functions, optimizer, device_in_use=device_in_use)\n",
    "    test_loss, r2_test, rmse_test = test(test_dataloader, model, loss_functions, device_in_use=device_in_use)\n",
    "    train_losses.append(train_loss)\n",
    "    train_rmse.append(rmse_train)\n",
    "    # train_accuracies_2.append(train_accuracy_2)\n",
    "    # train_recalls.append(train_recall) \n",
    "    # train_f1_scores.append(train_f1)\n",
    "    test_losses.append(test_loss)\n",
    "    test_rmse.append(rmse_test)\n",
    "    # test_accuracies_2.append(test_accuracy_2)\n",
    "    # test_recalls.append(test_recall)\n",
    "    # test_f1_scores.append(test_f1)\n",
    "    # Formatting for easier reading\n",
    "    epoch_str = f\"Epoch [{t+1:2}/{epochs}]\"\n",
    "    train_metrics = f\"Train: Loss {format_metric(train_loss)}, R2 {format_metric(r2_train)}, RMSE {format_metric(rmse_train)}\"\n",
    "    test_metrics = f\"Test: Loss {format_metric(test_loss)}, R2 {format_metric(r2_test)}, RMSE {format_metric(rmse_test)}\"\n",
    "    print(f\"{epoch_str:20} | {train_metrics:65} | {test_metrics}\")\n",
    "\n",
    "total_time = time.process_time() - start_time\n",
    "\n",
    "# Plotting the loss curves\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, epochs+1), train_losses, label='Train Loss')\n",
    "plt.plot(range(1, epochs+1), [l for l in test_losses], label='Test Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Test Loss Curve')\n",
    "plt.legend()\n",
    "\n",
    "best_index = test_rmse.index(min(test_rmse))\n",
    "print(f\"Best rmse {test_rmse[best_index]}\")\n",
    "print(f\"100 epochs of training and evaluation took, {total_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainable Normal RFF to each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [ 1/500]       | Train: Loss 2.84888, R2 0.19175, RMSE 1.61176                     | Test: Loss 1.66902, R2 0.56019, RMSE 1.27863\n",
      "Epoch [ 2/500]       | Train: Loss 1.09614, R2 0.69477, RMSE 1.04166                     | Test: Loss 0.87904, R2 0.76184, RMSE 0.92573\n",
      "Epoch [ 3/500]       | Train: Loss 1.01985, R2 0.71425, RMSE 1.00616                     | Test: Loss 0.99535, R2 0.73564, RMSE 0.98835\n",
      "Epoch [ 4/500]       | Train: Loss 0.99578, R2 0.72085, RMSE 0.99310                     | Test: Loss 0.92373, R2 0.75728, RMSE 0.95491\n",
      "Epoch [ 5/500]       | Train: Loss 0.96804, R2 0.73098, RMSE 0.98024                     | Test: Loss 0.84753, R2 0.77362, RMSE 0.91305\n",
      "Epoch [ 6/500]       | Train: Loss 0.93174, R2 0.73888, RMSE 0.96261                     | Test: Loss 0.90191, R2 0.75774, RMSE 0.94088\n",
      "Epoch [ 7/500]       | Train: Loss 0.91588, R2 0.74294, RMSE 0.95264                     | Test: Loss 0.83965, R2 0.77477, RMSE 0.91098\n",
      "Epoch [ 8/500]       | Train: Loss 0.95350, R2 0.73403, RMSE 0.97093                     | Test: Loss 1.10736, R2 0.69538, RMSE 1.04394\n",
      "Epoch [ 9/500]       | Train: Loss 0.93654, R2 0.73732, RMSE 0.96383                     | Test: Loss 0.82768, R2 0.76634, RMSE 0.90121\n",
      "Epoch [10/500]       | Train: Loss 0.91435, R2 0.74297, RMSE 0.95215                     | Test: Loss 0.82296, R2 0.77385, RMSE 0.90389\n",
      "Epoch [11/500]       | Train: Loss 0.88314, R2 0.75331, RMSE 0.93606                     | Test: Loss 0.99918, R2 0.73897, RMSE 0.99283\n",
      "Epoch [12/500]       | Train: Loss 0.92317, R2 0.74136, RMSE 0.95681                     | Test: Loss 0.93788, R2 0.74082, RMSE 0.95538\n",
      "Epoch [13/500]       | Train: Loss 0.91362, R2 0.74424, RMSE 0.95089                     | Test: Loss 0.89986, R2 0.75274, RMSE 0.94564\n",
      "Epoch [14/500]       | Train: Loss 0.86331, R2 0.75957, RMSE 0.92476                     | Test: Loss 0.80797, R2 0.78005, RMSE 0.89100\n",
      "Epoch [15/500]       | Train: Loss 0.85239, R2 0.75956, RMSE 0.92057                     | Test: Loss 0.79043, R2 0.78425, RMSE 0.88323\n",
      "Epoch [16/500]       | Train: Loss 0.83682, R2 0.76493, RMSE 0.91127                     | Test: Loss 0.88180, R2 0.75275, RMSE 0.93307\n",
      "Epoch [17/500]       | Train: Loss 0.85136, R2 0.76210, RMSE 0.91895                     | Test: Loss 0.84064, R2 0.76643, RMSE 0.91165\n",
      "Epoch [18/500]       | Train: Loss 0.83311, R2 0.76467, RMSE 0.90977                     | Test: Loss 0.86789, R2 0.77681, RMSE 0.91922\n",
      "Epoch [19/500]       | Train: Loss 0.82652, R2 0.76922, RMSE 0.90624                     | Test: Loss 0.84048, R2 0.76899, RMSE 0.91222\n",
      "Epoch [20/500]       | Train: Loss 0.85722, R2 0.76006, RMSE 0.92250                     | Test: Loss 0.84298, R2 0.77728, RMSE 0.91205\n",
      "Epoch [21/500]       | Train: Loss 0.83490, R2 0.76405, RMSE 0.90923                     | Test: Loss 0.90953, R2 0.74232, RMSE 0.94503\n",
      "Epoch [22/500]       | Train: Loss 0.88081, R2 0.75265, RMSE 0.93416                     | Test: Loss 0.77247, R2 0.78010, RMSE 0.87052\n",
      "Epoch [23/500]       | Train: Loss 0.84674, R2 0.76106, RMSE 0.91684                     | Test: Loss 0.79215, R2 0.77194, RMSE 0.87742\n",
      "Epoch [24/500]       | Train: Loss 0.82927, R2 0.76815, RMSE 0.90747                     | Test: Loss 0.96452, R2 0.70777, RMSE 0.95727\n",
      "Epoch [25/500]       | Train: Loss 0.80375, R2 0.77480, RMSE 0.89369                     | Test: Loss 0.78307, R2 0.78759, RMSE 0.87528\n",
      "Epoch [26/500]       | Train: Loss 0.81498, R2 0.76913, RMSE 0.90056                     | Test: Loss 0.92739, R2 0.75853, RMSE 0.95315\n",
      "Epoch [27/500]       | Train: Loss 0.80833, R2 0.77458, RMSE 0.89525                     | Test: Loss 0.86669, R2 0.76669, RMSE 0.92604\n",
      "Epoch [28/500]       | Train: Loss 0.80991, R2 0.76934, RMSE 0.89700                     | Test: Loss 0.89096, R2 0.75122, RMSE 0.93856\n",
      "Epoch [29/500]       | Train: Loss 0.82078, R2 0.77038, RMSE 0.90187                     | Test: Loss 0.87553, R2 0.74666, RMSE 0.92814\n",
      "Epoch [30/500]       | Train: Loss 0.78239, R2 0.78003, RMSE 0.88195                     | Test: Loss 0.84862, R2 0.76636, RMSE 0.91363\n",
      "Epoch [31/500]       | Train: Loss 0.82146, R2 0.77026, RMSE 0.90283                     | Test: Loss 0.76769, R2 0.79372, RMSE 0.86805\n",
      "Epoch [32/500]       | Train: Loss 0.78148, R2 0.78180, RMSE 0.88014                     | Test: Loss 0.77487, R2 0.76657, RMSE 0.87683\n",
      "Epoch [33/500]       | Train: Loss 0.75418, R2 0.78935, RMSE 0.86489                     | Test: Loss 0.88021, R2 0.74386, RMSE 0.93130\n",
      "Epoch [34/500]       | Train: Loss 0.76885, R2 0.78439, RMSE 0.87414                     | Test: Loss 0.82356, R2 0.77035, RMSE 0.90001\n",
      "Epoch [35/500]       | Train: Loss 0.74969, R2 0.78877, RMSE 0.86342                     | Test: Loss 0.78300, R2 0.78529, RMSE 0.87326\n",
      "Epoch [36/500]       | Train: Loss 0.74332, R2 0.78990, RMSE 0.85963                     | Test: Loss 0.79800, R2 0.79367, RMSE 0.88758\n",
      "Epoch [37/500]       | Train: Loss 0.75632, R2 0.78887, RMSE 0.86615                     | Test: Loss 1.00024, R2 0.71421, RMSE 0.99603\n",
      "Epoch [38/500]       | Train: Loss 0.74454, R2 0.79079, RMSE 0.85989                     | Test: Loss 0.92534, R2 0.76427, RMSE 0.95312\n",
      "Epoch [39/500]       | Train: Loss 0.73824, R2 0.79346, RMSE 0.85646                     | Test: Loss 0.77885, R2 0.79208, RMSE 0.87208\n",
      "Epoch [40/500]       | Train: Loss 0.73816, R2 0.79257, RMSE 0.85640                     | Test: Loss 0.88924, R2 0.75479, RMSE 0.93726\n",
      "Epoch [41/500]       | Train: Loss 0.75188, R2 0.78878, RMSE 0.86392                     | Test: Loss 0.77617, R2 0.78636, RMSE 0.87541\n",
      "Epoch [42/500]       | Train: Loss 0.74265, R2 0.79155, RMSE 0.85882                     | Test: Loss 0.90764, R2 0.74258, RMSE 0.94444\n",
      "Epoch [43/500]       | Train: Loss 0.72181, R2 0.79784, RMSE 0.84664                     | Test: Loss 0.84186, R2 0.76691, RMSE 0.91077\n",
      "Epoch [44/500]       | Train: Loss 0.67996, R2 0.80886, RMSE 0.82247                     | Test: Loss 0.78654, R2 0.78155, RMSE 0.87930\n",
      "Epoch [45/500]       | Train: Loss 0.71620, R2 0.79845, RMSE 0.84354                     | Test: Loss 0.90454, R2 0.75039, RMSE 0.94048\n",
      "Epoch [46/500]       | Train: Loss 0.69835, R2 0.80529, RMSE 0.83297                     | Test: Loss 0.88899, R2 0.76195, RMSE 0.93671\n",
      "Epoch [47/500]       | Train: Loss 0.67625, R2 0.81011, RMSE 0.81955                     | Test: Loss 0.81362, R2 0.76945, RMSE 0.89423\n",
      "Epoch [48/500]       | Train: Loss 0.67936, R2 0.80952, RMSE 0.82108                     | Test: Loss 0.81430, R2 0.79111, RMSE 0.89098\n",
      "Epoch [49/500]       | Train: Loss 0.66433, R2 0.81416, RMSE 0.81180                     | Test: Loss 0.91991, R2 0.73274, RMSE 0.95487\n",
      "Epoch [50/500]       | Train: Loss 0.68916, R2 0.80540, RMSE 0.82681                     | Test: Loss 0.80560, R2 0.77737, RMSE 0.88785\n",
      "Epoch [51/500]       | Train: Loss 0.69459, R2 0.80567, RMSE 0.82973                     | Test: Loss 0.82681, R2 0.77337, RMSE 0.90276\n",
      "Epoch [52/500]       | Train: Loss 0.65274, R2 0.81783, RMSE 0.80483                     | Test: Loss 0.91606, R2 0.75176, RMSE 0.94831\n",
      "Epoch [53/500]       | Train: Loss 0.66723, R2 0.81195, RMSE 0.81400                     | Test: Loss 0.75290, R2 0.77993, RMSE 0.86341\n",
      "Epoch [54/500]       | Train: Loss 0.64253, R2 0.81845, RMSE 0.79902                     | Test: Loss 0.83266, R2 0.77295, RMSE 0.90449\n",
      "Epoch [55/500]       | Train: Loss 0.65623, R2 0.81724, RMSE 0.80642                     | Test: Loss 0.84234, R2 0.77297, RMSE 0.91115\n",
      "Epoch [56/500]       | Train: Loss 0.64264, R2 0.81966, RMSE 0.79784                     | Test: Loss 0.78970, R2 0.78872, RMSE 0.87879\n",
      "Epoch [57/500]       | Train: Loss 0.61977, R2 0.82282, RMSE 0.78481                     | Test: Loss 0.85517, R2 0.77212, RMSE 0.91548\n",
      "Epoch [58/500]       | Train: Loss 0.60917, R2 0.82739, RMSE 0.77851                     | Test: Loss 0.80203, R2 0.78380, RMSE 0.87690\n",
      "Epoch [59/500]       | Train: Loss 0.61452, R2 0.82543, RMSE 0.78157                     | Test: Loss 0.96128, R2 0.73797, RMSE 0.97117\n",
      "Epoch [60/500]       | Train: Loss 0.62179, R2 0.82448, RMSE 0.78547                     | Test: Loss 0.80472, R2 0.77602, RMSE 0.88686\n",
      "Epoch [61/500]       | Train: Loss 0.61964, R2 0.82590, RMSE 0.78404                     | Test: Loss 0.93000, R2 0.74103, RMSE 0.93739\n",
      "Epoch [62/500]       | Train: Loss 0.59591, R2 0.83270, RMSE 0.76887                     | Test: Loss 0.81507, R2 0.77083, RMSE 0.89516\n",
      "Epoch [63/500]       | Train: Loss 0.60320, R2 0.82940, RMSE 0.77451                     | Test: Loss 0.75117, R2 0.79682, RMSE 0.85770\n",
      "Epoch [64/500]       | Train: Loss 0.58744, R2 0.83507, RMSE 0.76377                     | Test: Loss 0.77236, R2 0.79038, RMSE 0.86288\n",
      "Epoch [65/500]       | Train: Loss 0.59532, R2 0.83040, RMSE 0.76864                     | Test: Loss 0.80574, R2 0.78122, RMSE 0.88533\n",
      "Epoch [66/500]       | Train: Loss 0.57734, R2 0.83870, RMSE 0.75733                     | Test: Loss 0.78680, R2 0.77227, RMSE 0.88217\n",
      "Epoch [67/500]       | Train: Loss 0.60654, R2 0.82932, RMSE 0.77662                     | Test: Loss 0.76815, R2 0.78573, RMSE 0.86324\n",
      "Epoch [68/500]       | Train: Loss 0.55257, R2 0.84370, RMSE 0.74096                     | Test: Loss 0.88152, R2 0.75513, RMSE 0.92933\n",
      "Epoch [69/500]       | Train: Loss 0.56468, R2 0.84012, RMSE 0.74965                     | Test: Loss 0.75873, R2 0.78713, RMSE 0.86003\n",
      "Epoch [70/500]       | Train: Loss 0.56289, R2 0.84130, RMSE 0.74863                     | Test: Loss 0.75686, R2 0.79649, RMSE 0.85791\n",
      "Epoch [71/500]       | Train: Loss 0.54550, R2 0.84510, RMSE 0.73598                     | Test: Loss 0.79433, R2 0.75866, RMSE 0.88860\n",
      "Epoch [72/500]       | Train: Loss 0.53018, R2 0.85011, RMSE 0.72614                     | Test: Loss 0.76348, R2 0.79559, RMSE 0.86761\n",
      "Epoch [73/500]       | Train: Loss 0.53728, R2 0.84702, RMSE 0.73188                     | Test: Loss 0.80391, R2 0.77548, RMSE 0.89094\n",
      "Epoch [74/500]       | Train: Loss 0.54293, R2 0.84748, RMSE 0.73498                     | Test: Loss 0.77462, R2 0.78776, RMSE 0.87263\n",
      "Epoch [75/500]       | Train: Loss 0.51472, R2 0.85448, RMSE 0.71542                     | Test: Loss 0.85403, R2 0.77788, RMSE 0.90821\n",
      "Epoch [76/500]       | Train: Loss 0.52106, R2 0.85164, RMSE 0.71955                     | Test: Loss 0.88564, R2 0.77490, RMSE 0.92720\n",
      "Epoch [77/500]       | Train: Loss 0.51565, R2 0.85321, RMSE 0.71604                     | Test: Loss 0.85895, R2 0.77061, RMSE 0.91933\n",
      "Epoch [78/500]       | Train: Loss 0.51713, R2 0.85412, RMSE 0.71718                     | Test: Loss 0.82245, R2 0.78585, RMSE 0.90028\n",
      "Epoch [79/500]       | Train: Loss 0.51538, R2 0.85446, RMSE 0.71604                     | Test: Loss 0.78051, R2 0.77613, RMSE 0.87630\n",
      "Epoch [80/500]       | Train: Loss 0.51190, R2 0.85460, RMSE 0.71375                     | Test: Loss 0.79029, R2 0.78410, RMSE 0.88122\n",
      "Epoch [81/500]       | Train: Loss 0.50176, R2 0.85836, RMSE 0.70717                     | Test: Loss 0.76854, R2 0.79319, RMSE 0.86252\n",
      "Epoch [82/500]       | Train: Loss 0.51548, R2 0.85550, RMSE 0.71589                     | Test: Loss 0.75896, R2 0.79146, RMSE 0.85361\n",
      "Epoch [83/500]       | Train: Loss 0.47974, R2 0.86506, RMSE 0.69149                     | Test: Loss 0.76394, R2 0.79223, RMSE 0.86462\n",
      "Epoch [84/500]       | Train: Loss 0.50936, R2 0.85639, RMSE 0.71201                     | Test: Loss 0.83432, R2 0.77764, RMSE 0.90431\n",
      "Epoch [85/500]       | Train: Loss 0.51399, R2 0.85632, RMSE 0.71513                     | Test: Loss 0.80932, R2 0.77575, RMSE 0.88661\n",
      "Epoch [86/500]       | Train: Loss 0.49125, R2 0.86134, RMSE 0.69877                     | Test: Loss 0.81943, R2 0.77480, RMSE 0.89752\n",
      "Epoch [87/500]       | Train: Loss 0.49845, R2 0.86053, RMSE 0.70349                     | Test: Loss 0.82146, R2 0.76133, RMSE 0.89744\n",
      "Epoch [88/500]       | Train: Loss 0.47252, R2 0.86711, RMSE 0.68561                     | Test: Loss 0.85070, R2 0.77613, RMSE 0.91474\n",
      "Epoch [89/500]       | Train: Loss 0.46448, R2 0.86898, RMSE 0.68002                     | Test: Loss 0.79123, R2 0.78796, RMSE 0.87737\n",
      "Epoch [90/500]       | Train: Loss 0.47252, R2 0.86655, RMSE 0.68569                     | Test: Loss 0.82209, R2 0.76596, RMSE 0.89034\n",
      "Epoch [91/500]       | Train: Loss 0.47025, R2 0.86751, RMSE 0.68394                     | Test: Loss 0.91913, R2 0.76995, RMSE 0.93879\n",
      "Epoch [92/500]       | Train: Loss 0.47827, R2 0.86439, RMSE 0.69019                     | Test: Loss 0.80062, R2 0.78446, RMSE 0.88445\n",
      "Epoch [93/500]       | Train: Loss 0.45045, R2 0.87218, RMSE 0.66953                     | Test: Loss 0.86329, R2 0.77668, RMSE 0.91591\n",
      "Epoch [94/500]       | Train: Loss 0.43913, R2 0.87580, RMSE 0.66109                     | Test: Loss 0.78559, R2 0.78337, RMSE 0.88194\n",
      "Epoch [95/500]       | Train: Loss 0.44671, R2 0.87253, RMSE 0.66710                     | Test: Loss 0.78690, R2 0.78765, RMSE 0.87930\n",
      "Epoch [96/500]       | Train: Loss 0.44893, R2 0.87362, RMSE 0.66889                     | Test: Loss 0.81245, R2 0.77899, RMSE 0.89387\n",
      "Epoch [97/500]       | Train: Loss 0.45527, R2 0.87150, RMSE 0.67313                     | Test: Loss 0.83902, R2 0.76415, RMSE 0.90310\n",
      "Epoch [98/500]       | Train: Loss 0.44751, R2 0.87465, RMSE 0.66720                     | Test: Loss 0.77997, R2 0.78450, RMSE 0.87421\n",
      "Epoch [99/500]       | Train: Loss 0.42240, R2 0.88077, RMSE 0.64846                     | Test: Loss 0.82536, R2 0.76122, RMSE 0.89864\n",
      "Epoch [100/500]      | Train: Loss 0.42874, R2 0.87963, RMSE 0.65333                     | Test: Loss 0.82293, R2 0.77232, RMSE 0.89614\n",
      "Epoch [101/500]      | Train: Loss 0.45586, R2 0.87056, RMSE 0.67336                     | Test: Loss 0.83176, R2 0.77721, RMSE 0.90249\n",
      "Epoch [102/500]      | Train: Loss 0.42952, R2 0.87915, RMSE 0.65365                     | Test: Loss 0.78878, R2 0.77975, RMSE 0.87588\n",
      "Epoch [103/500]      | Train: Loss 0.42429, R2 0.87985, RMSE 0.65009                     | Test: Loss 0.79217, R2 0.78976, RMSE 0.87816\n",
      "Epoch [104/500]      | Train: Loss 0.40407, R2 0.88538, RMSE 0.63437                     | Test: Loss 0.79626, R2 0.77749, RMSE 0.88465\n",
      "Epoch [105/500]      | Train: Loss 0.42288, R2 0.88044, RMSE 0.64889                     | Test: Loss 0.80735, R2 0.77662, RMSE 0.88564\n",
      "Epoch [106/500]      | Train: Loss 0.41863, R2 0.88167, RMSE 0.64557                     | Test: Loss 0.83711, R2 0.77279, RMSE 0.90183\n",
      "Epoch [107/500]      | Train: Loss 0.41105, R2 0.88406, RMSE 0.63963                     | Test: Loss 0.82808, R2 0.76900, RMSE 0.90189\n",
      "Epoch [108/500]      | Train: Loss 0.40469, R2 0.88671, RMSE 0.63396                     | Test: Loss 0.80640, R2 0.77553, RMSE 0.88835\n",
      "Epoch [109/500]      | Train: Loss 0.40807, R2 0.88449, RMSE 0.63724                     | Test: Loss 1.02908, R2 0.76896, RMSE 0.95155\n",
      "Epoch [110/500]      | Train: Loss 0.39105, R2 0.88953, RMSE 0.62405                     | Test: Loss 0.79936, R2 0.77941, RMSE 0.88215\n",
      "Epoch [111/500]      | Train: Loss 0.38054, R2 0.89079, RMSE 0.61557                     | Test: Loss 0.82134, R2 0.77171, RMSE 0.90212\n",
      "Epoch [112/500]      | Train: Loss 0.40516, R2 0.88584, RMSE 0.63495                     | Test: Loss 0.79675, R2 0.78311, RMSE 0.87600\n",
      "Epoch [113/500]      | Train: Loss 0.38148, R2 0.89173, RMSE 0.61644                     | Test: Loss 0.80483, R2 0.77011, RMSE 0.88934\n",
      "Epoch [114/500]      | Train: Loss 0.37873, R2 0.89305, RMSE 0.61406                     | Test: Loss 0.78622, R2 0.77400, RMSE 0.87285\n",
      "Epoch [115/500]      | Train: Loss 0.40566, R2 0.88559, RMSE 0.63497                     | Test: Loss 0.80983, R2 0.78351, RMSE 0.89311\n",
      "Epoch [116/500]      | Train: Loss 0.37058, R2 0.89587, RMSE 0.60782                     | Test: Loss 0.82132, R2 0.76685, RMSE 0.89563\n",
      "Epoch [117/500]      | Train: Loss 0.39131, R2 0.88896, RMSE 0.62333                     | Test: Loss 0.85732, R2 0.76407, RMSE 0.91937\n",
      "Epoch [118/500]      | Train: Loss 0.37497, R2 0.89392, RMSE 0.61135                     | Test: Loss 0.79635, R2 0.77994, RMSE 0.88634\n",
      "Epoch [119/500]      | Train: Loss 0.37561, R2 0.89387, RMSE 0.61137                     | Test: Loss 0.79937, R2 0.78226, RMSE 0.87719\n",
      "Epoch [120/500]      | Train: Loss 0.35133, R2 0.90136, RMSE 0.59175                     | Test: Loss 0.79399, R2 0.77975, RMSE 0.88006\n",
      "Epoch [121/500]      | Train: Loss 0.34598, R2 0.90292, RMSE 0.58713                     | Test: Loss 0.86035, R2 0.76468, RMSE 0.92372\n",
      "Epoch [122/500]      | Train: Loss 0.34544, R2 0.90250, RMSE 0.58644                     | Test: Loss 0.79920, R2 0.77729, RMSE 0.87965\n",
      "Epoch [123/500]      | Train: Loss 0.33942, R2 0.90349, RMSE 0.58160                     | Test: Loss 0.78697, R2 0.78495, RMSE 0.87069\n",
      "Epoch [124/500]      | Train: Loss 0.33889, R2 0.90472, RMSE 0.58098                     | Test: Loss 0.81043, R2 0.77443, RMSE 0.88576\n",
      "Epoch [125/500]      | Train: Loss 0.33022, R2 0.90614, RMSE 0.57342                     | Test: Loss 0.88677, R2 0.74875, RMSE 0.93149\n",
      "Epoch [126/500]      | Train: Loss 0.32929, R2 0.90663, RMSE 0.57260                     | Test: Loss 0.88979, R2 0.75584, RMSE 0.92734\n",
      "Epoch [127/500]      | Train: Loss 0.33888, R2 0.90385, RMSE 0.58105                     | Test: Loss 0.78600, R2 0.76905, RMSE 0.87634\n",
      "Epoch [128/500]      | Train: Loss 0.32194, R2 0.90918, RMSE 0.56645                     | Test: Loss 0.81905, R2 0.78044, RMSE 0.89717\n",
      "Epoch [129/500]      | Train: Loss 0.32837, R2 0.90811, RMSE 0.57195                     | Test: Loss 0.87326, R2 0.76207, RMSE 0.92573\n",
      "Epoch [130/500]      | Train: Loss 0.32745, R2 0.90735, RMSE 0.57096                     | Test: Loss 0.85325, R2 0.75851, RMSE 0.91237\n",
      "Epoch [131/500]      | Train: Loss 0.33118, R2 0.90567, RMSE 0.57398                     | Test: Loss 0.88456, R2 0.75849, RMSE 0.93123\n",
      "Epoch [132/500]      | Train: Loss 0.35851, R2 0.89929, RMSE 0.59755                     | Test: Loss 0.80927, R2 0.78309, RMSE 0.89198\n",
      "Epoch [133/500]      | Train: Loss 0.32858, R2 0.90692, RMSE 0.57172                     | Test: Loss 0.80504, R2 0.76882, RMSE 0.89244\n",
      "Epoch [134/500]      | Train: Loss 0.35044, R2 0.90097, RMSE 0.58996                     | Test: Loss 0.84662, R2 0.76609, RMSE 0.90356\n",
      "Epoch [135/500]      | Train: Loss 0.33010, R2 0.90762, RMSE 0.57353                     | Test: Loss 0.83543, R2 0.76885, RMSE 0.90531\n",
      "Epoch [136/500]      | Train: Loss 0.32415, R2 0.90819, RMSE 0.56708                     | Test: Loss 0.85200, R2 0.76269, RMSE 0.91202\n",
      "Epoch [137/500]      | Train: Loss 0.35445, R2 0.89917, RMSE 0.59372                     | Test: Loss 0.79327, R2 0.77664, RMSE 0.88434\n",
      "Epoch [138/500]      | Train: Loss 0.38018, R2 0.89278, RMSE 0.61390                     | Test: Loss 0.87702, R2 0.73955, RMSE 0.92997\n",
      "Epoch [139/500]      | Train: Loss 0.32217, R2 0.90838, RMSE 0.56627                     | Test: Loss 0.82016, R2 0.77221, RMSE 0.89739\n",
      "Epoch [140/500]      | Train: Loss 0.29768, R2 0.91503, RMSE 0.54453                     | Test: Loss 0.80808, R2 0.78057, RMSE 0.88382\n",
      "Epoch [141/500]      | Train: Loss 0.29142, R2 0.91793, RMSE 0.53830                     | Test: Loss 0.78781, R2 0.78061, RMSE 0.87961\n",
      "Epoch [142/500]      | Train: Loss 0.30918, R2 0.91273, RMSE 0.55514                     | Test: Loss 0.80064, R2 0.77345, RMSE 0.87877\n",
      "Epoch [143/500]      | Train: Loss 0.28500, R2 0.91898, RMSE 0.53269                     | Test: Loss 0.83111, R2 0.77638, RMSE 0.90289\n",
      "Epoch [144/500]      | Train: Loss 0.29235, R2 0.91724, RMSE 0.53924                     | Test: Loss 0.83549, R2 0.77662, RMSE 0.90884\n",
      "Epoch [145/500]      | Train: Loss 0.28345, R2 0.92032, RMSE 0.53125                     | Test: Loss 0.86073, R2 0.76171, RMSE 0.91839\n",
      "Epoch [146/500]      | Train: Loss 0.28311, R2 0.91965, RMSE 0.53112                     | Test: Loss 0.85532, R2 0.75298, RMSE 0.91003\n",
      "Epoch [147/500]      | Train: Loss 0.27439, R2 0.92170, RMSE 0.52257                     | Test: Loss 0.92332, R2 0.72972, RMSE 0.95431\n",
      "Epoch [148/500]      | Train: Loss 0.29183, R2 0.91753, RMSE 0.53911                     | Test: Loss 0.86816, R2 0.73983, RMSE 0.92312\n",
      "Epoch [149/500]      | Train: Loss 0.28068, R2 0.92067, RMSE 0.52798                     | Test: Loss 0.85868, R2 0.77064, RMSE 0.92311\n",
      "Epoch [150/500]      | Train: Loss 0.27910, R2 0.92140, RMSE 0.52751                     | Test: Loss 1.12940, R2 0.74406, RMSE 1.00030\n",
      "Epoch [151/500]      | Train: Loss 0.26875, R2 0.92378, RMSE 0.51710                     | Test: Loss 0.87615, R2 0.75477, RMSE 0.92741\n",
      "Epoch [152/500]      | Train: Loss 0.28623, R2 0.91977, RMSE 0.53399                     | Test: Loss 0.84840, R2 0.74552, RMSE 0.91486\n",
      "Epoch [153/500]      | Train: Loss 0.27387, R2 0.92236, RMSE 0.52223                     | Test: Loss 0.91219, R2 0.75014, RMSE 0.94768\n",
      "Epoch [154/500]      | Train: Loss 0.27483, R2 0.92198, RMSE 0.52315                     | Test: Loss 0.92135, R2 0.74375, RMSE 0.95387\n",
      "Epoch [155/500]      | Train: Loss 0.25529, R2 0.92807, RMSE 0.50446                     | Test: Loss 0.86606, R2 0.76908, RMSE 0.92164\n",
      "Epoch [156/500]      | Train: Loss 0.25865, R2 0.92709, RMSE 0.50750                     | Test: Loss 0.85115, R2 0.75786, RMSE 0.91429\n",
      "Epoch [157/500]      | Train: Loss 0.24348, R2 0.93087, RMSE 0.49270                     | Test: Loss 1.01243, R2 0.72584, RMSE 0.97712\n",
      "Epoch [158/500]      | Train: Loss 0.25721, R2 0.92749, RMSE 0.50614                     | Test: Loss 0.91533, R2 0.74750, RMSE 0.95171\n",
      "Epoch [159/500]      | Train: Loss 0.27355, R2 0.92276, RMSE 0.52215                     | Test: Loss 0.91595, R2 0.73426, RMSE 0.94760\n",
      "Epoch [160/500]      | Train: Loss 0.25876, R2 0.92727, RMSE 0.50761                     | Test: Loss 0.85403, R2 0.76833, RMSE 0.91822\n",
      "Epoch [161/500]      | Train: Loss 0.25020, R2 0.92980, RMSE 0.49912                     | Test: Loss 0.86132, R2 0.72950, RMSE 0.91737\n",
      "Epoch [162/500]      | Train: Loss 0.24610, R2 0.93038, RMSE 0.49524                     | Test: Loss 0.82677, R2 0.76502, RMSE 0.90231\n",
      "Epoch [163/500]      | Train: Loss 0.23313, R2 0.93311, RMSE 0.48163                     | Test: Loss 1.17758, R2 0.74335, RMSE 1.00803\n",
      "Epoch [164/500]      | Train: Loss 0.24667, R2 0.93018, RMSE 0.49574                     | Test: Loss 0.87779, R2 0.75900, RMSE 0.92799\n",
      "Epoch [165/500]      | Train: Loss 0.25280, R2 0.92914, RMSE 0.50159                     | Test: Loss 0.88234, R2 0.76965, RMSE 0.93100\n",
      "Epoch [166/500]      | Train: Loss 0.25634, R2 0.92729, RMSE 0.50522                     | Test: Loss 0.83091, R2 0.76804, RMSE 0.90275\n",
      "Epoch [167/500]      | Train: Loss 0.25368, R2 0.92872, RMSE 0.50230                     | Test: Loss 0.91589, R2 0.75682, RMSE 0.94646\n",
      "Epoch [168/500]      | Train: Loss 0.24945, R2 0.92988, RMSE 0.49828                     | Test: Loss 0.82914, R2 0.76723, RMSE 0.89327\n",
      "Epoch [169/500]      | Train: Loss 0.22752, R2 0.93593, RMSE 0.47550                     | Test: Loss 0.85643, R2 0.76944, RMSE 0.91757\n",
      "Epoch [170/500]      | Train: Loss 0.22009, R2 0.93827, RMSE 0.46801                     | Test: Loss 0.81113, R2 0.76705, RMSE 0.89260\n",
      "Epoch [171/500]      | Train: Loss 0.24084, R2 0.93220, RMSE 0.48969                     | Test: Loss 0.85021, R2 0.75953, RMSE 0.91336\n",
      "Epoch [172/500]      | Train: Loss 0.22811, R2 0.93529, RMSE 0.47643                     | Test: Loss 0.83986, R2 0.76489, RMSE 0.90448\n",
      "Epoch [173/500]      | Train: Loss 0.23533, R2 0.93353, RMSE 0.48376                     | Test: Loss 1.01131, R2 0.70549, RMSE 1.00014\n",
      "Epoch [174/500]      | Train: Loss 0.23372, R2 0.93456, RMSE 0.48170                     | Test: Loss 1.13628, R2 0.72937, RMSE 1.00337\n",
      "Epoch [175/500]      | Train: Loss 0.21602, R2 0.93919, RMSE 0.46380                     | Test: Loss 0.86408, R2 0.75609, RMSE 0.92303\n",
      "Epoch [176/500]      | Train: Loss 0.20875, R2 0.94110, RMSE 0.45603                     | Test: Loss 0.91784, R2 0.74235, RMSE 0.94952\n",
      "Epoch [177/500]      | Train: Loss 0.22161, R2 0.93778, RMSE 0.46962                     | Test: Loss 0.86894, R2 0.76262, RMSE 0.91730\n",
      "Epoch [178/500]      | Train: Loss 0.21079, R2 0.94034, RMSE 0.45767                     | Test: Loss 0.88686, R2 0.74004, RMSE 0.93189\n",
      "Epoch [179/500]      | Train: Loss 0.20881, R2 0.94081, RMSE 0.45589                     | Test: Loss 0.90457, R2 0.76453, RMSE 0.93907\n",
      "Epoch [180/500]      | Train: Loss 0.21361, R2 0.93971, RMSE 0.46104                     | Test: Loss 0.88575, R2 0.74942, RMSE 0.93205\n",
      "Epoch [181/500]      | Train: Loss 0.21187, R2 0.93920, RMSE 0.45911                     | Test: Loss 0.92160, R2 0.75145, RMSE 0.94658\n",
      "Epoch [182/500]      | Train: Loss 0.21483, R2 0.93877, RMSE 0.46226                     | Test: Loss 0.87807, R2 0.76514, RMSE 0.93229\n",
      "Epoch [183/500]      | Train: Loss 0.19510, R2 0.94456, RMSE 0.44061                     | Test: Loss 0.86008, R2 0.75774, RMSE 0.91635\n",
      "Epoch [184/500]      | Train: Loss 0.19286, R2 0.94568, RMSE 0.43828                     | Test: Loss 0.86787, R2 0.76141, RMSE 0.92321\n",
      "Epoch [185/500]      | Train: Loss 0.19695, R2 0.94456, RMSE 0.44277                     | Test: Loss 0.82161, R2 0.77364, RMSE 0.89032\n",
      "Epoch [186/500]      | Train: Loss 0.20965, R2 0.94076, RMSE 0.45664                     | Test: Loss 0.87043, R2 0.74612, RMSE 0.92831\n",
      "Epoch [187/500]      | Train: Loss 0.21315, R2 0.93995, RMSE 0.46027                     | Test: Loss 0.87126, R2 0.76766, RMSE 0.92774\n",
      "Epoch [188/500]      | Train: Loss 0.19934, R2 0.94361, RMSE 0.44531                     | Test: Loss 0.86186, R2 0.75905, RMSE 0.92153\n",
      "Epoch [189/500]      | Train: Loss 0.19638, R2 0.94456, RMSE 0.44158                     | Test: Loss 0.87768, R2 0.77017, RMSE 0.92907\n",
      "Epoch [190/500]      | Train: Loss 0.19391, R2 0.94491, RMSE 0.43908                     | Test: Loss 0.95534, R2 0.72317, RMSE 0.96874\n",
      "Epoch [191/500]      | Train: Loss 0.19329, R2 0.94510, RMSE 0.43857                     | Test: Loss 0.88070, R2 0.76851, RMSE 0.92819\n",
      "Epoch [192/500]      | Train: Loss 0.18981, R2 0.94608, RMSE 0.43434                     | Test: Loss 0.89498, R2 0.76333, RMSE 0.93932\n",
      "Epoch [193/500]      | Train: Loss 0.17742, R2 0.95006, RMSE 0.42039                     | Test: Loss 0.84301, R2 0.75983, RMSE 0.91193\n",
      "Epoch [194/500]      | Train: Loss 0.18359, R2 0.94815, RMSE 0.42725                     | Test: Loss 0.85774, R2 0.76422, RMSE 0.91409\n",
      "Epoch [195/500]      | Train: Loss 0.19462, R2 0.94510, RMSE 0.44002                     | Test: Loss 0.87402, R2 0.75272, RMSE 0.92624\n",
      "Epoch [196/500]      | Train: Loss 0.18571, R2 0.94782, RMSE 0.42961                     | Test: Loss 0.87954, R2 0.75344, RMSE 0.93164\n",
      "Epoch [197/500]      | Train: Loss 0.18553, R2 0.94800, RMSE 0.42924                     | Test: Loss 0.84565, R2 0.76051, RMSE 0.90999\n",
      "Epoch [198/500]      | Train: Loss 0.22671, R2 0.93559, RMSE 0.47422                     | Test: Loss 0.86366, R2 0.75162, RMSE 0.92526\n",
      "Epoch [199/500]      | Train: Loss 0.19404, R2 0.94495, RMSE 0.43948                     | Test: Loss 0.83356, R2 0.75998, RMSE 0.90534\n",
      "Epoch [200/500]      | Train: Loss 0.18037, R2 0.94961, RMSE 0.42333                     | Test: Loss 1.00574, R2 0.73313, RMSE 0.98470\n",
      "Epoch [201/500]      | Train: Loss 0.18465, R2 0.94821, RMSE 0.42792                     | Test: Loss 0.88906, R2 0.76193, RMSE 0.93396\n",
      "Epoch [202/500]      | Train: Loss 0.18757, R2 0.94762, RMSE 0.43140                     | Test: Loss 0.92418, R2 0.74464, RMSE 0.95626\n",
      "Epoch [203/500]      | Train: Loss 0.17686, R2 0.95031, RMSE 0.41956                     | Test: Loss 0.96726, R2 0.73908, RMSE 0.96241\n",
      "Epoch [204/500]      | Train: Loss 0.18593, R2 0.94751, RMSE 0.42955                     | Test: Loss 0.88620, R2 0.76322, RMSE 0.93493\n",
      "Epoch [205/500]      | Train: Loss 0.16768, R2 0.95215, RMSE 0.40836                     | Test: Loss 0.85874, R2 0.77029, RMSE 0.92168\n",
      "Epoch [206/500]      | Train: Loss 0.16054, R2 0.95475, RMSE 0.39963                     | Test: Loss 0.88361, R2 0.76554, RMSE 0.93736\n",
      "Epoch [207/500]      | Train: Loss 0.16421, R2 0.95325, RMSE 0.40430                     | Test: Loss 0.84767, R2 0.77280, RMSE 0.90587\n",
      "Epoch [208/500]      | Train: Loss 0.16122, R2 0.95427, RMSE 0.40022                     | Test: Loss 0.85622, R2 0.75857, RMSE 0.91472\n",
      "Epoch [209/500]      | Train: Loss 0.16479, R2 0.95337, RMSE 0.40496                     | Test: Loss 0.85990, R2 0.76234, RMSE 0.91541\n",
      "Epoch [210/500]      | Train: Loss 0.15774, R2 0.95490, RMSE 0.39607                     | Test: Loss 0.84170, R2 0.76583, RMSE 0.90344\n",
      "Epoch [211/500]      | Train: Loss 0.16577, R2 0.95296, RMSE 0.40578                     | Test: Loss 0.87507, R2 0.75347, RMSE 0.92911\n",
      "Epoch [212/500]      | Train: Loss 0.15648, R2 0.95587, RMSE 0.39461                     | Test: Loss 0.87531, R2 0.76398, RMSE 0.92366\n",
      "Epoch [213/500]      | Train: Loss 0.15203, R2 0.95699, RMSE 0.38930                     | Test: Loss 0.90069, R2 0.75192, RMSE 0.93853\n",
      "Epoch [214/500]      | Train: Loss 0.16331, R2 0.95422, RMSE 0.40312                     | Test: Loss 0.87407, R2 0.76057, RMSE 0.92719\n",
      "Epoch [215/500]      | Train: Loss 0.15257, R2 0.95713, RMSE 0.38932                     | Test: Loss 0.91478, R2 0.74675, RMSE 0.94942\n",
      "Epoch [216/500]      | Train: Loss 0.16496, R2 0.95392, RMSE 0.40436                     | Test: Loss 0.88537, R2 0.74396, RMSE 0.93355\n",
      "Epoch [217/500]      | Train: Loss 0.15119, R2 0.95726, RMSE 0.38780                     | Test: Loss 0.90380, R2 0.75424, RMSE 0.94211\n",
      "Epoch [218/500]      | Train: Loss 0.14295, R2 0.95997, RMSE 0.37718                     | Test: Loss 0.86130, R2 0.75692, RMSE 0.91332\n",
      "Epoch [219/500]      | Train: Loss 0.14694, R2 0.95862, RMSE 0.38246                     | Test: Loss 0.86051, R2 0.75686, RMSE 0.91492\n",
      "Epoch [220/500]      | Train: Loss 0.14596, R2 0.95915, RMSE 0.38090                     | Test: Loss 0.84905, R2 0.75023, RMSE 0.90967\n",
      "Epoch [221/500]      | Train: Loss 0.14703, R2 0.95821, RMSE 0.38223                     | Test: Loss 1.21773, R2 0.71424, RMSE 1.04427\n",
      "Epoch [222/500]      | Train: Loss 0.14691, R2 0.95831, RMSE 0.38204                     | Test: Loss 0.89821, R2 0.75573, RMSE 0.93958\n",
      "Epoch [223/500]      | Train: Loss 0.14141, R2 0.96014, RMSE 0.37494                     | Test: Loss 1.00066, R2 0.69880, RMSE 0.98614\n",
      "Epoch [224/500]      | Train: Loss 0.14233, R2 0.96013, RMSE 0.37632                     | Test: Loss 0.84020, R2 0.77230, RMSE 0.90596\n",
      "Epoch [225/500]      | Train: Loss 0.14395, R2 0.95951, RMSE 0.37822                     | Test: Loss 1.00615, R2 0.74463, RMSE 0.98655\n",
      "Epoch [226/500]      | Train: Loss 0.14299, R2 0.95947, RMSE 0.37735                     | Test: Loss 0.93131, R2 0.70876, RMSE 0.95854\n",
      "Epoch [227/500]      | Train: Loss 0.14697, R2 0.95880, RMSE 0.38193                     | Test: Loss 0.95687, R2 0.73906, RMSE 0.97137\n",
      "Epoch [228/500]      | Train: Loss 0.14269, R2 0.96011, RMSE 0.37585                     | Test: Loss 0.92311, R2 0.75781, RMSE 0.94520\n",
      "Epoch [229/500]      | Train: Loss 0.13827, R2 0.96127, RMSE 0.37095                     | Test: Loss 0.88660, R2 0.76284, RMSE 0.93448\n",
      "Epoch [230/500]      | Train: Loss 0.14328, R2 0.95949, RMSE 0.37749                     | Test: Loss 0.86085, R2 0.76241, RMSE 0.92053\n",
      "Epoch [231/500]      | Train: Loss 0.14715, R2 0.95834, RMSE 0.38218                     | Test: Loss 0.85449, R2 0.75805, RMSE 0.91687\n",
      "Epoch [232/500]      | Train: Loss 0.13740, R2 0.96106, RMSE 0.36996                     | Test: Loss 0.88928, R2 0.76026, RMSE 0.92732\n",
      "Epoch [233/500]      | Train: Loss 0.13613, R2 0.96128, RMSE 0.36786                     | Test: Loss 0.88172, R2 0.75672, RMSE 0.92649\n",
      "Epoch [234/500]      | Train: Loss 0.13559, R2 0.96189, RMSE 0.36699                     | Test: Loss 0.95873, R2 0.72582, RMSE 0.97335\n",
      "Epoch [235/500]      | Train: Loss 0.12984, R2 0.96330, RMSE 0.35938                     | Test: Loss 0.91205, R2 0.72980, RMSE 0.95008\n",
      "Epoch [236/500]      | Train: Loss 0.12864, R2 0.96377, RMSE 0.35775                     | Test: Loss 0.83477, R2 0.76882, RMSE 0.90125\n",
      "Epoch [237/500]      | Train: Loss 0.14419, R2 0.95938, RMSE 0.37854                     | Test: Loss 0.89027, R2 0.75087, RMSE 0.93573\n",
      "Epoch [238/500]      | Train: Loss 0.13033, R2 0.96336, RMSE 0.36011                     | Test: Loss 0.90588, R2 0.74086, RMSE 0.94362\n",
      "Epoch [239/500]      | Train: Loss 0.13049, R2 0.96303, RMSE 0.36002                     | Test: Loss 0.88904, R2 0.75219, RMSE 0.93371\n",
      "Epoch [240/500]      | Train: Loss 0.12950, R2 0.96349, RMSE 0.35894                     | Test: Loss 0.87839, R2 0.74948, RMSE 0.92776\n",
      "Epoch [241/500]      | Train: Loss 0.13090, R2 0.96286, RMSE 0.36073                     | Test: Loss 0.89072, R2 0.75920, RMSE 0.93996\n",
      "Epoch [242/500]      | Train: Loss 0.12541, R2 0.96493, RMSE 0.35261                     | Test: Loss 0.87003, R2 0.75785, RMSE 0.92457\n",
      "Epoch [243/500]      | Train: Loss 0.13562, R2 0.96163, RMSE 0.36652                     | Test: Loss 0.89456, R2 0.76281, RMSE 0.93817\n",
      "Epoch [244/500]      | Train: Loss 0.13220, R2 0.96245, RMSE 0.36176                     | Test: Loss 0.84612, R2 0.76644, RMSE 0.90459\n",
      "Epoch [245/500]      | Train: Loss 0.12179, R2 0.96524, RMSE 0.34778                     | Test: Loss 0.87879, R2 0.74295, RMSE 0.92823\n",
      "Epoch [246/500]      | Train: Loss 0.13803, R2 0.96099, RMSE 0.37008                     | Test: Loss 0.90940, R2 0.73602, RMSE 0.94608\n",
      "Epoch [247/500]      | Train: Loss 0.12526, R2 0.96459, RMSE 0.35229                     | Test: Loss 0.85473, R2 0.76461, RMSE 0.91290\n",
      "Epoch [248/500]      | Train: Loss 0.13664, R2 0.96148, RMSE 0.36797                     | Test: Loss 0.85261, R2 0.75425, RMSE 0.91225\n",
      "Epoch [249/500]      | Train: Loss 0.13229, R2 0.96234, RMSE 0.36216                     | Test: Loss 1.07471, R2 0.70064, RMSE 1.00355\n",
      "Epoch [250/500]      | Train: Loss 0.12517, R2 0.96462, RMSE 0.35270                     | Test: Loss 1.03501, R2 0.71313, RMSE 0.99185\n",
      "Epoch [251/500]      | Train: Loss 0.11693, R2 0.96698, RMSE 0.34097                     | Test: Loss 0.90419, R2 0.75643, RMSE 0.94284\n",
      "Epoch [252/500]      | Train: Loss 0.12440, R2 0.96520, RMSE 0.35104                     | Test: Loss 0.88331, R2 0.75555, RMSE 0.93333\n",
      "Epoch [253/500]      | Train: Loss 0.14045, R2 0.96028, RMSE 0.37361                     | Test: Loss 1.03346, R2 0.66374, RMSE 0.98669\n",
      "Epoch [254/500]      | Train: Loss 0.12090, R2 0.96601, RMSE 0.34637                     | Test: Loss 1.17244, R2 0.71992, RMSE 1.01710\n",
      "Epoch [255/500]      | Train: Loss 0.11722, R2 0.96677, RMSE 0.34128                     | Test: Loss 0.91800, R2 0.73664, RMSE 0.95130\n",
      "Epoch [256/500]      | Train: Loss 0.11456, R2 0.96726, RMSE 0.33763                     | Test: Loss 0.92241, R2 0.75280, RMSE 0.95136\n",
      "Epoch [257/500]      | Train: Loss 0.12303, R2 0.96543, RMSE 0.34964                     | Test: Loss 0.91591, R2 0.74084, RMSE 0.94867\n",
      "Epoch [258/500]      | Train: Loss 0.12125, R2 0.96595, RMSE 0.34706                     | Test: Loss 0.91538, R2 0.75554, RMSE 0.95072\n",
      "Epoch [259/500]      | Train: Loss 0.11431, R2 0.96772, RMSE 0.33717                     | Test: Loss 0.90857, R2 0.74418, RMSE 0.94680\n",
      "Epoch [260/500]      | Train: Loss 0.11332, R2 0.96852, RMSE 0.33508                     | Test: Loss 0.86233, R2 0.76302, RMSE 0.92128\n",
      "Epoch [261/500]      | Train: Loss 0.12390, R2 0.96504, RMSE 0.35118                     | Test: Loss 0.91280, R2 0.74315, RMSE 0.94953\n",
      "Epoch [262/500]      | Train: Loss 0.11633, R2 0.96738, RMSE 0.33904                     | Test: Loss 0.88836, R2 0.75788, RMSE 0.93611\n",
      "Epoch [263/500]      | Train: Loss 0.12394, R2 0.96525, RMSE 0.35087                     | Test: Loss 0.92488, R2 0.71288, RMSE 0.95791\n",
      "Epoch [264/500]      | Train: Loss 0.13295, R2 0.96256, RMSE 0.36301                     | Test: Loss 0.88144, R2 0.75718, RMSE 0.92066\n",
      "Epoch [265/500]      | Train: Loss 0.13377, R2 0.96249, RMSE 0.36412                     | Test: Loss 0.96752, R2 0.72256, RMSE 0.97715\n",
      "Epoch [266/500]      | Train: Loss 0.11220, R2 0.96824, RMSE 0.33385                     | Test: Loss 0.90451, R2 0.74585, RMSE 0.94354\n",
      "Epoch [267/500]      | Train: Loss 0.09942, R2 0.97197, RMSE 0.31446                     | Test: Loss 0.88000, R2 0.75942, RMSE 0.93133\n",
      "Epoch [268/500]      | Train: Loss 0.10660, R2 0.97018, RMSE 0.32546                     | Test: Loss 0.92855, R2 0.73328, RMSE 0.95869\n",
      "Epoch [269/500]      | Train: Loss 0.12404, R2 0.96482, RMSE 0.35040                     | Test: Loss 0.85126, R2 0.76925, RMSE 0.91186\n",
      "Epoch [270/500]      | Train: Loss 0.12321, R2 0.96543, RMSE 0.34941                     | Test: Loss 0.89421, R2 0.74017, RMSE 0.93894\n",
      "Epoch [271/500]      | Train: Loss 0.12061, R2 0.96636, RMSE 0.34585                     | Test: Loss 0.87511, R2 0.76154, RMSE 0.93182\n",
      "Epoch [272/500]      | Train: Loss 0.11372, R2 0.96820, RMSE 0.33574                     | Test: Loss 0.85038, R2 0.77048, RMSE 0.91036\n",
      "Epoch [273/500]      | Train: Loss 0.10833, R2 0.96931, RMSE 0.32800                     | Test: Loss 0.86370, R2 0.75663, RMSE 0.91787\n",
      "Epoch [274/500]      | Train: Loss 0.10996, R2 0.96910, RMSE 0.33053                     | Test: Loss 0.92664, R2 0.75559, RMSE 0.95530\n",
      "Epoch [275/500]      | Train: Loss 0.11021, R2 0.96883, RMSE 0.33062                     | Test: Loss 0.89244, R2 0.75687, RMSE 0.94037\n",
      "Epoch [276/500]      | Train: Loss 0.10674, R2 0.97001, RMSE 0.32557                     | Test: Loss 0.88274, R2 0.75055, RMSE 0.92830\n",
      "Epoch [277/500]      | Train: Loss 0.11843, R2 0.96669, RMSE 0.34266                     | Test: Loss 0.87886, R2 0.75930, RMSE 0.92947\n",
      "Epoch [278/500]      | Train: Loss 0.12231, R2 0.96544, RMSE 0.34816                     | Test: Loss 0.88582, R2 0.75695, RMSE 0.93142\n",
      "Epoch [279/500]      | Train: Loss 0.10447, R2 0.97069, RMSE 0.32255                     | Test: Loss 0.90109, R2 0.75093, RMSE 0.93123\n",
      "Epoch [280/500]      | Train: Loss 0.10841, R2 0.96929, RMSE 0.32776                     | Test: Loss 0.88695, R2 0.75057, RMSE 0.92557\n",
      "Epoch [281/500]      | Train: Loss 0.10444, R2 0.97083, RMSE 0.32177                     | Test: Loss 0.88194, R2 0.74860, RMSE 0.92947\n",
      "Epoch [282/500]      | Train: Loss 0.10014, R2 0.97185, RMSE 0.31540                     | Test: Loss 0.88220, R2 0.74663, RMSE 0.92848\n",
      "Epoch [283/500]      | Train: Loss 0.10157, R2 0.97163, RMSE 0.31737                     | Test: Loss 0.87182, R2 0.75101, RMSE 0.92516\n",
      "Epoch [284/500]      | Train: Loss 0.09975, R2 0.97216, RMSE 0.31451                     | Test: Loss 0.87726, R2 0.76021, RMSE 0.92664\n",
      "Epoch [285/500]      | Train: Loss 0.11245, R2 0.96819, RMSE 0.33391                     | Test: Loss 0.86592, R2 0.76816, RMSE 0.91891\n",
      "Epoch [286/500]      | Train: Loss 0.11251, R2 0.96859, RMSE 0.33357                     | Test: Loss 0.96460, R2 0.74537, RMSE 0.97314\n",
      "Epoch [287/500]      | Train: Loss 0.12072, R2 0.96603, RMSE 0.34595                     | Test: Loss 0.94044, R2 0.72974, RMSE 0.95638\n",
      "Epoch [288/500]      | Train: Loss 0.12213, R2 0.96561, RMSE 0.34671                     | Test: Loss 0.90312, R2 0.76160, RMSE 0.94128\n",
      "Epoch [289/500]      | Train: Loss 0.11095, R2 0.96868, RMSE 0.33204                     | Test: Loss 0.85101, R2 0.77016, RMSE 0.91260\n",
      "Epoch [290/500]      | Train: Loss 0.10251, R2 0.97114, RMSE 0.31929                     | Test: Loss 0.93969, R2 0.74284, RMSE 0.96031\n",
      "Epoch [291/500]      | Train: Loss 0.09244, R2 0.97395, RMSE 0.30302                     | Test: Loss 0.87582, R2 0.74713, RMSE 0.92115\n",
      "Epoch [292/500]      | Train: Loss 0.09221, R2 0.97410, RMSE 0.30271                     | Test: Loss 0.94926, R2 0.75376, RMSE 0.96018\n",
      "Epoch [293/500]      | Train: Loss 0.08977, R2 0.97432, RMSE 0.29873                     | Test: Loss 0.85864, R2 0.76401, RMSE 0.91304\n",
      "Epoch [294/500]      | Train: Loss 0.09819, R2 0.97237, RMSE 0.31210                     | Test: Loss 0.92796, R2 0.74786, RMSE 0.95886\n",
      "Epoch [295/500]      | Train: Loss 0.10416, R2 0.97075, RMSE 0.32121                     | Test: Loss 0.87637, R2 0.75802, RMSE 0.92201\n",
      "Epoch [296/500]      | Train: Loss 0.10136, R2 0.97162, RMSE 0.31704                     | Test: Loss 0.90739, R2 0.74965, RMSE 0.94591\n",
      "Epoch [297/500]      | Train: Loss 0.09600, R2 0.97286, RMSE 0.30899                     | Test: Loss 0.93506, R2 0.73856, RMSE 0.95457\n",
      "Epoch [298/500]      | Train: Loss 0.09060, R2 0.97423, RMSE 0.30019                     | Test: Loss 0.87361, R2 0.74467, RMSE 0.92341\n",
      "Epoch [299/500]      | Train: Loss 0.08783, R2 0.97521, RMSE 0.29516                     | Test: Loss 0.86700, R2 0.75904, RMSE 0.92854\n",
      "Epoch [300/500]      | Train: Loss 0.09185, R2 0.97400, RMSE 0.30209                     | Test: Loss 0.93222, R2 0.74680, RMSE 0.95665\n",
      "Epoch [301/500]      | Train: Loss 0.09397, R2 0.97376, RMSE 0.30550                     | Test: Loss 0.88111, R2 0.75753, RMSE 0.92726\n",
      "Epoch [302/500]      | Train: Loss 0.09905, R2 0.97212, RMSE 0.31288                     | Test: Loss 0.89461, R2 0.76226, RMSE 0.93915\n",
      "Epoch [303/500]      | Train: Loss 0.08989, R2 0.97480, RMSE 0.29907                     | Test: Loss 0.86717, R2 0.75271, RMSE 0.92804\n",
      "Epoch [304/500]      | Train: Loss 0.09482, R2 0.97332, RMSE 0.30638                     | Test: Loss 0.87655, R2 0.75253, RMSE 0.93019\n",
      "Epoch [305/500]      | Train: Loss 0.09371, R2 0.97346, RMSE 0.30476                     | Test: Loss 0.85816, R2 0.76712, RMSE 0.92006\n",
      "Epoch [306/500]      | Train: Loss 0.10142, R2 0.97173, RMSE 0.31726                     | Test: Loss 0.87295, R2 0.76246, RMSE 0.93042\n",
      "Epoch [307/500]      | Train: Loss 0.09212, R2 0.97402, RMSE 0.30272                     | Test: Loss 0.83718, R2 0.76700, RMSE 0.90089\n",
      "Epoch [308/500]      | Train: Loss 0.08741, R2 0.97539, RMSE 0.29488                     | Test: Loss 0.84710, R2 0.76895, RMSE 0.91118\n",
      "Epoch [309/500]      | Train: Loss 0.09245, R2 0.97384, RMSE 0.30310                     | Test: Loss 0.86814, R2 0.76388, RMSE 0.92333\n",
      "Epoch [310/500]      | Train: Loss 0.09422, R2 0.97355, RMSE 0.30517                     | Test: Loss 0.88035, R2 0.75352, RMSE 0.93064\n",
      "Epoch [311/500]      | Train: Loss 0.09210, R2 0.97421, RMSE 0.30254                     | Test: Loss 0.88562, R2 0.75816, RMSE 0.93126\n",
      "Epoch [312/500]      | Train: Loss 0.08717, R2 0.97547, RMSE 0.29444                     | Test: Loss 0.86332, R2 0.75652, RMSE 0.92490\n",
      "Epoch [313/500]      | Train: Loss 0.08886, R2 0.97481, RMSE 0.29683                     | Test: Loss 0.86827, R2 0.76208, RMSE 0.92482\n",
      "Epoch [314/500]      | Train: Loss 0.10002, R2 0.97184, RMSE 0.31421                     | Test: Loss 0.89814, R2 0.74900, RMSE 0.93729\n",
      "Epoch [315/500]      | Train: Loss 0.09998, R2 0.97227, RMSE 0.31300                     | Test: Loss 0.89118, R2 0.75238, RMSE 0.93108\n",
      "Epoch [316/500]      | Train: Loss 0.11171, R2 0.96867, RMSE 0.33214                     | Test: Loss 0.93143, R2 0.74354, RMSE 0.95697\n",
      "Epoch [317/500]      | Train: Loss 0.10113, R2 0.97103, RMSE 0.31577                     | Test: Loss 1.15041, R2 0.70942, RMSE 1.03064\n",
      "Epoch [318/500]      | Train: Loss 0.13035, R2 0.96304, RMSE 0.35787                     | Test: Loss 0.92603, R2 0.75520, RMSE 0.95009\n",
      "Epoch [319/500]      | Train: Loss 0.09618, R2 0.97284, RMSE 0.30922                     | Test: Loss 0.85206, R2 0.75713, RMSE 0.91125\n",
      "Epoch [320/500]      | Train: Loss 0.09037, R2 0.97462, RMSE 0.29903                     | Test: Loss 0.92396, R2 0.74575, RMSE 0.94941\n",
      "Epoch [321/500]      | Train: Loss 0.09165, R2 0.97422, RMSE 0.30154                     | Test: Loss 0.88005, R2 0.74465, RMSE 0.92473\n",
      "Epoch [322/500]      | Train: Loss 0.08254, R2 0.97700, RMSE 0.28571                     | Test: Loss 0.85164, R2 0.76244, RMSE 0.91018\n",
      "Epoch [323/500]      | Train: Loss 0.08769, R2 0.97524, RMSE 0.29466                     | Test: Loss 0.90573, R2 0.75985, RMSE 0.94429\n",
      "Epoch [324/500]      | Train: Loss 0.07810, R2 0.97798, RMSE 0.27869                     | Test: Loss 0.86914, R2 0.76755, RMSE 0.91888\n",
      "Epoch [325/500]      | Train: Loss 0.07952, R2 0.97768, RMSE 0.28083                     | Test: Loss 0.88530, R2 0.76444, RMSE 0.93690\n",
      "Epoch [326/500]      | Train: Loss 0.07935, R2 0.97771, RMSE 0.28089                     | Test: Loss 0.84050, R2 0.77190, RMSE 0.90657\n",
      "Epoch [327/500]      | Train: Loss 0.08557, R2 0.97592, RMSE 0.29151                     | Test: Loss 0.88434, R2 0.74719, RMSE 0.93222\n",
      "Epoch [328/500]      | Train: Loss 0.07969, R2 0.97781, RMSE 0.28128                     | Test: Loss 0.84875, R2 0.75545, RMSE 0.91392\n",
      "Epoch [329/500]      | Train: Loss 0.08273, R2 0.97679, RMSE 0.28644                     | Test: Loss 0.94419, R2 0.74093, RMSE 0.96146\n",
      "Epoch [330/500]      | Train: Loss 0.08559, R2 0.97594, RMSE 0.29121                     | Test: Loss 0.92025, R2 0.75406, RMSE 0.95465\n",
      "Epoch [331/500]      | Train: Loss 0.08479, R2 0.97600, RMSE 0.29032                     | Test: Loss 0.89215, R2 0.75313, RMSE 0.92856\n",
      "Epoch [332/500]      | Train: Loss 0.08446, R2 0.97640, RMSE 0.28925                     | Test: Loss 0.87087, R2 0.75185, RMSE 0.92519\n",
      "Epoch [333/500]      | Train: Loss 0.07851, R2 0.97794, RMSE 0.27948                     | Test: Loss 0.86380, R2 0.76006, RMSE 0.92373\n",
      "Epoch [334/500]      | Train: Loss 0.09060, R2 0.97452, RMSE 0.29938                     | Test: Loss 0.91948, R2 0.76600, RMSE 0.94871\n",
      "Epoch [335/500]      | Train: Loss 0.08928, R2 0.97486, RMSE 0.29699                     | Test: Loss 0.87429, R2 0.76253, RMSE 0.92879\n",
      "Epoch [336/500]      | Train: Loss 0.08087, R2 0.97719, RMSE 0.28307                     | Test: Loss 0.86242, R2 0.76832, RMSE 0.92324\n",
      "Epoch [337/500]      | Train: Loss 0.07988, R2 0.97741, RMSE 0.28170                     | Test: Loss 0.89655, R2 0.73981, RMSE 0.94245\n",
      "Epoch [338/500]      | Train: Loss 0.08795, R2 0.97557, RMSE 0.29466                     | Test: Loss 0.85130, R2 0.76316, RMSE 0.91498\n",
      "Epoch [339/500]      | Train: Loss 0.08030, R2 0.97729, RMSE 0.28230                     | Test: Loss 0.90590, R2 0.75872, RMSE 0.94134\n",
      "Epoch [340/500]      | Train: Loss 0.08564, R2 0.97599, RMSE 0.29111                     | Test: Loss 0.88786, R2 0.75798, RMSE 0.92402\n",
      "Epoch [341/500]      | Train: Loss 0.11388, R2 0.96832, RMSE 0.33404                     | Test: Loss 1.01489, R2 0.73276, RMSE 1.00249\n",
      "Epoch [342/500]      | Train: Loss 0.10552, R2 0.97018, RMSE 0.32264                     | Test: Loss 0.84030, R2 0.76849, RMSE 0.90895\n",
      "Epoch [343/500]      | Train: Loss 0.09066, R2 0.97444, RMSE 0.30006                     | Test: Loss 0.83423, R2 0.77094, RMSE 0.90426\n",
      "Epoch [344/500]      | Train: Loss 0.08686, R2 0.97554, RMSE 0.29328                     | Test: Loss 0.84822, R2 0.76236, RMSE 0.91284\n",
      "Epoch [345/500]      | Train: Loss 0.07804, R2 0.97818, RMSE 0.27824                     | Test: Loss 0.88108, R2 0.75541, RMSE 0.93079\n",
      "Epoch [346/500]      | Train: Loss 0.08156, R2 0.97701, RMSE 0.28430                     | Test: Loss 1.05122, R2 0.70855, RMSE 0.99627\n",
      "Epoch [347/500]      | Train: Loss 0.08936, R2 0.97496, RMSE 0.29702                     | Test: Loss 1.21280, R2 0.73208, RMSE 1.03454\n",
      "Epoch [348/500]      | Train: Loss 0.08410, R2 0.97648, RMSE 0.28817                     | Test: Loss 0.88756, R2 0.75140, RMSE 0.93613\n",
      "Epoch [349/500]      | Train: Loss 0.09172, R2 0.97439, RMSE 0.30029                     | Test: Loss 0.89198, R2 0.75887, RMSE 0.93799\n",
      "Epoch [350/500]      | Train: Loss 0.08715, R2 0.97528, RMSE 0.29243                     | Test: Loss 1.06105, R2 0.69553, RMSE 1.00390\n",
      "Epoch [351/500]      | Train: Loss 0.08338, R2 0.97645, RMSE 0.28640                     | Test: Loss 0.89143, R2 0.76145, RMSE 0.93621\n",
      "Epoch [352/500]      | Train: Loss 0.08054, R2 0.97758, RMSE 0.28158                     | Test: Loss 0.88992, R2 0.76621, RMSE 0.93127\n",
      "Epoch [353/500]      | Train: Loss 0.08496, R2 0.97618, RMSE 0.28983                     | Test: Loss 0.85410, R2 0.76763, RMSE 0.91243\n",
      "Epoch [354/500]      | Train: Loss 0.07249, R2 0.97950, RMSE 0.26837                     | Test: Loss 0.98039, R2 0.73143, RMSE 0.97342\n",
      "Epoch [355/500]      | Train: Loss 0.07928, R2 0.97774, RMSE 0.27986                     | Test: Loss 0.88768, R2 0.75848, RMSE 0.93540\n",
      "Epoch [356/500]      | Train: Loss 0.07339, R2 0.97952, RMSE 0.26938                     | Test: Loss 0.94929, R2 0.75528, RMSE 0.95767\n",
      "Epoch [357/500]      | Train: Loss 0.07093, R2 0.98002, RMSE 0.26504                     | Test: Loss 0.85356, R2 0.76795, RMSE 0.91690\n",
      "Epoch [358/500]      | Train: Loss 0.07104, R2 0.98017, RMSE 0.26554                     | Test: Loss 0.89043, R2 0.75631, RMSE 0.93649\n",
      "Epoch [359/500]      | Train: Loss 0.08087, R2 0.97749, RMSE 0.28264                     | Test: Loss 0.87548, R2 0.76077, RMSE 0.92732\n",
      "Epoch [360/500]      | Train: Loss 0.08282, R2 0.97671, RMSE 0.28652                     | Test: Loss 0.86026, R2 0.76169, RMSE 0.91876\n",
      "Epoch [361/500]      | Train: Loss 0.09973, R2 0.97232, RMSE 0.31245                     | Test: Loss 0.91572, R2 0.73829, RMSE 0.94984\n",
      "Epoch [362/500]      | Train: Loss 0.08595, R2 0.97589, RMSE 0.29176                     | Test: Loss 0.95608, R2 0.73580, RMSE 0.96799\n",
      "Epoch [363/500]      | Train: Loss 0.07948, R2 0.97762, RMSE 0.28062                     | Test: Loss 0.84089, R2 0.76622, RMSE 0.90798\n",
      "Epoch [364/500]      | Train: Loss 0.07418, R2 0.97912, RMSE 0.27128                     | Test: Loss 0.87382, R2 0.75866, RMSE 0.92691\n",
      "Epoch [365/500]      | Train: Loss 0.07887, R2 0.97796, RMSE 0.27911                     | Test: Loss 0.86347, R2 0.76690, RMSE 0.91807\n",
      "Epoch [366/500]      | Train: Loss 0.09935, R2 0.97234, RMSE 0.31243                     | Test: Loss 0.89655, R2 0.75730, RMSE 0.94120\n",
      "Epoch [367/500]      | Train: Loss 0.08170, R2 0.97713, RMSE 0.28438                     | Test: Loss 1.24816, R2 0.72614, RMSE 1.02908\n",
      "Epoch [368/500]      | Train: Loss 0.07090, R2 0.97997, RMSE 0.26536                     | Test: Loss 0.87418, R2 0.76233, RMSE 0.92536\n",
      "Epoch [369/500]      | Train: Loss 0.07981, R2 0.97754, RMSE 0.28020                     | Test: Loss 0.97200, R2 0.72835, RMSE 0.97987\n",
      "Epoch [370/500]      | Train: Loss 0.07649, R2 0.97851, RMSE 0.27554                     | Test: Loss 0.88377, R2 0.75524, RMSE 0.93570\n",
      "Epoch [371/500]      | Train: Loss 0.07275, R2 0.97969, RMSE 0.26899                     | Test: Loss 0.88474, R2 0.75709, RMSE 0.93417\n",
      "Epoch [372/500]      | Train: Loss 0.07027, R2 0.98025, RMSE 0.26399                     | Test: Loss 0.89676, R2 0.74175, RMSE 0.93942\n",
      "Epoch [373/500]      | Train: Loss 0.06939, R2 0.98075, RMSE 0.26222                     | Test: Loss 0.91908, R2 0.74876, RMSE 0.95466\n",
      "Epoch [374/500]      | Train: Loss 0.06667, R2 0.98120, RMSE 0.25718                     | Test: Loss 1.01386, R2 0.71887, RMSE 0.98210\n",
      "Epoch [375/500]      | Train: Loss 0.07355, R2 0.97959, RMSE 0.27008                     | Test: Loss 0.90755, R2 0.76554, RMSE 0.94466\n",
      "Epoch [376/500]      | Train: Loss 0.07170, R2 0.97981, RMSE 0.26664                     | Test: Loss 0.85963, R2 0.76229, RMSE 0.91979\n",
      "Epoch [377/500]      | Train: Loss 0.07364, R2 0.97933, RMSE 0.27049                     | Test: Loss 0.88453, R2 0.75885, RMSE 0.93487\n",
      "Epoch [378/500]      | Train: Loss 0.07613, R2 0.97866, RMSE 0.27453                     | Test: Loss 0.95581, R2 0.73351, RMSE 0.96896\n",
      "Epoch [379/500]      | Train: Loss 0.08151, R2 0.97716, RMSE 0.28442                     | Test: Loss 0.84914, R2 0.75993, RMSE 0.91518\n",
      "Epoch [380/500]      | Train: Loss 0.07568, R2 0.97854, RMSE 0.27388                     | Test: Loss 0.87543, R2 0.76205, RMSE 0.92939\n",
      "Epoch [381/500]      | Train: Loss 0.07370, R2 0.97938, RMSE 0.26929                     | Test: Loss 0.88772, R2 0.75494, RMSE 0.93069\n",
      "Epoch [382/500]      | Train: Loss 0.07058, R2 0.98024, RMSE 0.26459                     | Test: Loss 0.85321, R2 0.76123, RMSE 0.91328\n",
      "Epoch [383/500]      | Train: Loss 0.06803, R2 0.98092, RMSE 0.25977                     | Test: Loss 0.85237, R2 0.75812, RMSE 0.91046\n",
      "Epoch [384/500]      | Train: Loss 0.07742, R2 0.97824, RMSE 0.27693                     | Test: Loss 0.92343, R2 0.72772, RMSE 0.95744\n",
      "Epoch [385/500]      | Train: Loss 0.08026, R2 0.97733, RMSE 0.28174                     | Test: Loss 0.86130, R2 0.76449, RMSE 0.92226\n",
      "Epoch [386/500]      | Train: Loss 0.07734, R2 0.97794, RMSE 0.27678                     | Test: Loss 0.90656, R2 0.76400, RMSE 0.94460\n",
      "Epoch [387/500]      | Train: Loss 0.06928, R2 0.98033, RMSE 0.26218                     | Test: Loss 0.90013, R2 0.75535, RMSE 0.94429\n",
      "Epoch [388/500]      | Train: Loss 0.07100, R2 0.98008, RMSE 0.26530                     | Test: Loss 0.86323, R2 0.76136, RMSE 0.92163\n",
      "Epoch [389/500]      | Train: Loss 0.07373, R2 0.97942, RMSE 0.27032                     | Test: Loss 0.88204, R2 0.75068, RMSE 0.93090\n",
      "Epoch [390/500]      | Train: Loss 0.08117, R2 0.97732, RMSE 0.28310                     | Test: Loss 0.97165, R2 0.69983, RMSE 0.96384\n",
      "Epoch [391/500]      | Train: Loss 0.07658, R2 0.97833, RMSE 0.27488                     | Test: Loss 0.87305, R2 0.75985, RMSE 0.92184\n",
      "Epoch [392/500]      | Train: Loss 0.07821, R2 0.97814, RMSE 0.27768                     | Test: Loss 0.98697, R2 0.72280, RMSE 0.97868\n",
      "Epoch [393/500]      | Train: Loss 0.07095, R2 0.98003, RMSE 0.26548                     | Test: Loss 0.89786, R2 0.75850, RMSE 0.94383\n",
      "Epoch [394/500]      | Train: Loss 0.07379, R2 0.97945, RMSE 0.27002                     | Test: Loss 0.90318, R2 0.74690, RMSE 0.94103\n",
      "Epoch [395/500]      | Train: Loss 0.06531, R2 0.98159, RMSE 0.25481                     | Test: Loss 1.03696, R2 0.72293, RMSE 0.99086\n",
      "Epoch [396/500]      | Train: Loss 0.07268, R2 0.97944, RMSE 0.26839                     | Test: Loss 0.88223, R2 0.74415, RMSE 0.92853\n",
      "Epoch [397/500]      | Train: Loss 0.06651, R2 0.98133, RMSE 0.25650                     | Test: Loss 0.90454, R2 0.75674, RMSE 0.94156\n",
      "Epoch [398/500]      | Train: Loss 0.07066, R2 0.98027, RMSE 0.26382                     | Test: Loss 0.92962, R2 0.75663, RMSE 0.95132\n",
      "Epoch [399/500]      | Train: Loss 0.07385, R2 0.97927, RMSE 0.26939                     | Test: Loss 0.94796, R2 0.74477, RMSE 0.96282\n",
      "Epoch [400/500]      | Train: Loss 0.06714, R2 0.98101, RMSE 0.25796                     | Test: Loss 0.84623, R2 0.76943, RMSE 0.90594\n",
      "Epoch [401/500]      | Train: Loss 0.07326, R2 0.97944, RMSE 0.26887                     | Test: Loss 0.88734, R2 0.75492, RMSE 0.93663\n",
      "Epoch [402/500]      | Train: Loss 0.08252, R2 0.97665, RMSE 0.28503                     | Test: Loss 1.01528, R2 0.70753, RMSE 0.99504\n",
      "Epoch [403/500]      | Train: Loss 0.07574, R2 0.97877, RMSE 0.27367                     | Test: Loss 0.93623, R2 0.74987, RMSE 0.96176\n",
      "Epoch [404/500]      | Train: Loss 0.07259, R2 0.97957, RMSE 0.26840                     | Test: Loss 0.89828, R2 0.75494, RMSE 0.94131\n",
      "Epoch [405/500]      | Train: Loss 0.07359, R2 0.97918, RMSE 0.27012                     | Test: Loss 0.90670, R2 0.74898, RMSE 0.94200\n",
      "Epoch [406/500]      | Train: Loss 0.07796, R2 0.97784, RMSE 0.27741                     | Test: Loss 0.90113, R2 0.74855, RMSE 0.94077\n",
      "Epoch [407/500]      | Train: Loss 0.06910, R2 0.98067, RMSE 0.26144                     | Test: Loss 1.08253, R2 0.73787, RMSE 0.99393\n",
      "Epoch [408/500]      | Train: Loss 0.06609, R2 0.98158, RMSE 0.25562                     | Test: Loss 0.88209, R2 0.74987, RMSE 0.93393\n",
      "Epoch [409/500]      | Train: Loss 0.06913, R2 0.98055, RMSE 0.26063                     | Test: Loss 0.85472, R2 0.76497, RMSE 0.91371\n",
      "Epoch [410/500]      | Train: Loss 0.10252, R2 0.97120, RMSE 0.31786                     | Test: Loss 0.85381, R2 0.75734, RMSE 0.91731\n",
      "Epoch [411/500]      | Train: Loss 0.08509, R2 0.97634, RMSE 0.28880                     | Test: Loss 0.92492, R2 0.74791, RMSE 0.95537\n",
      "Epoch [412/500]      | Train: Loss 0.10620, R2 0.96993, RMSE 0.32393                     | Test: Loss 0.91711, R2 0.73361, RMSE 0.95145\n",
      "Epoch [413/500]      | Train: Loss 0.10427, R2 0.97059, RMSE 0.32027                     | Test: Loss 0.88297, R2 0.76910, RMSE 0.92909\n",
      "Epoch [414/500]      | Train: Loss 0.09000, R2 0.97492, RMSE 0.29777                     | Test: Loss 0.89801, R2 0.75673, RMSE 0.93878\n",
      "Epoch [415/500]      | Train: Loss 0.08352, R2 0.97613, RMSE 0.28707                     | Test: Loss 0.97827, R2 0.71000, RMSE 0.97339\n",
      "Epoch [416/500]      | Train: Loss 0.06961, R2 0.98057, RMSE 0.26193                     | Test: Loss 0.89420, R2 0.75300, RMSE 0.93490\n",
      "Epoch [417/500]      | Train: Loss 0.06902, R2 0.98065, RMSE 0.26160                     | Test: Loss 0.88903, R2 0.75574, RMSE 0.93470\n",
      "Epoch [418/500]      | Train: Loss 0.06419, R2 0.98189, RMSE 0.25204                     | Test: Loss 0.86303, R2 0.75893, RMSE 0.92335\n",
      "Epoch [419/500]      | Train: Loss 0.06922, R2 0.98060, RMSE 0.26034                     | Test: Loss 0.88052, R2 0.75914, RMSE 0.92812\n",
      "Epoch [420/500]      | Train: Loss 0.06354, R2 0.98235, RMSE 0.25089                     | Test: Loss 0.96262, R2 0.74917, RMSE 0.96698\n",
      "Epoch [421/500]      | Train: Loss 0.05952, R2 0.98341, RMSE 0.24312                     | Test: Loss 0.91773, R2 0.75616, RMSE 0.95208\n",
      "Epoch [422/500]      | Train: Loss 0.06590, R2 0.98169, RMSE 0.25469                     | Test: Loss 0.91886, R2 0.75713, RMSE 0.95083\n",
      "Epoch [423/500]      | Train: Loss 0.06495, R2 0.98173, RMSE 0.25294                     | Test: Loss 0.85786, R2 0.77095, RMSE 0.91475\n",
      "Epoch [424/500]      | Train: Loss 0.06181, R2 0.98267, RMSE 0.24756                     | Test: Loss 0.89307, R2 0.75470, RMSE 0.93734\n",
      "Epoch [425/500]      | Train: Loss 0.06289, R2 0.98239, RMSE 0.24920                     | Test: Loss 0.84552, R2 0.76721, RMSE 0.91011\n",
      "Epoch [426/500]      | Train: Loss 0.05764, R2 0.98378, RMSE 0.23901                     | Test: Loss 0.82821, R2 0.77817, RMSE 0.89737\n",
      "Epoch [427/500]      | Train: Loss 0.05772, R2 0.98371, RMSE 0.23960                     | Test: Loss 0.85046, R2 0.75274, RMSE 0.91298\n",
      "Epoch [428/500]      | Train: Loss 0.05477, R2 0.98466, RMSE 0.23320                     | Test: Loss 0.92838, R2 0.76035, RMSE 0.95405\n",
      "Epoch [429/500]      | Train: Loss 0.05614, R2 0.98409, RMSE 0.23611                     | Test: Loss 0.89206, R2 0.76191, RMSE 0.93772\n",
      "Epoch [430/500]      | Train: Loss 0.06874, R2 0.98093, RMSE 0.25991                     | Test: Loss 0.87562, R2 0.76336, RMSE 0.93018\n",
      "Epoch [431/500]      | Train: Loss 0.06521, R2 0.98170, RMSE 0.25388                     | Test: Loss 0.87798, R2 0.75295, RMSE 0.93291\n",
      "Epoch [432/500]      | Train: Loss 0.06497, R2 0.98166, RMSE 0.25376                     | Test: Loss 0.89634, R2 0.75444, RMSE 0.94018\n",
      "Epoch [433/500]      | Train: Loss 0.06980, R2 0.98037, RMSE 0.26318                     | Test: Loss 0.85677, R2 0.76242, RMSE 0.91775\n",
      "Epoch [434/500]      | Train: Loss 0.06834, R2 0.98074, RMSE 0.26038                     | Test: Loss 0.85279, R2 0.76594, RMSE 0.90673\n",
      "Epoch [435/500]      | Train: Loss 0.06493, R2 0.98192, RMSE 0.25368                     | Test: Loss 0.97817, R2 0.70273, RMSE 0.97109\n",
      "Epoch [436/500]      | Train: Loss 0.06145, R2 0.98276, RMSE 0.24700                     | Test: Loss 0.95031, R2 0.73791, RMSE 0.96438\n",
      "Epoch [437/500]      | Train: Loss 0.06277, R2 0.98232, RMSE 0.24926                     | Test: Loss 0.90527, R2 0.76588, RMSE 0.93837\n",
      "Epoch [438/500]      | Train: Loss 0.06075, R2 0.98302, RMSE 0.24504                     | Test: Loss 0.86129, R2 0.76379, RMSE 0.92111\n",
      "Epoch [439/500]      | Train: Loss 0.07523, R2 0.97888, RMSE 0.27178                     | Test: Loss 0.95990, R2 0.73974, RMSE 0.97071\n",
      "Epoch [440/500]      | Train: Loss 0.09901, R2 0.97213, RMSE 0.31043                     | Test: Loss 0.87932, R2 0.76174, RMSE 0.92756\n",
      "Epoch [441/500]      | Train: Loss 0.07389, R2 0.97930, RMSE 0.26999                     | Test: Loss 0.85899, R2 0.76187, RMSE 0.91698\n",
      "Epoch [442/500]      | Train: Loss 0.06712, R2 0.98125, RMSE 0.25767                     | Test: Loss 0.87502, R2 0.75561, RMSE 0.92535\n",
      "Epoch [443/500]      | Train: Loss 0.06468, R2 0.98174, RMSE 0.25282                     | Test: Loss 0.93471, R2 0.74490, RMSE 0.96083\n",
      "Epoch [444/500]      | Train: Loss 0.06170, R2 0.98277, RMSE 0.24727                     | Test: Loss 0.86070, R2 0.77094, RMSE 0.91742\n",
      "Epoch [445/500]      | Train: Loss 0.06093, R2 0.98288, RMSE 0.24569                     | Test: Loss 0.86180, R2 0.76215, RMSE 0.92061\n",
      "Epoch [446/500]      | Train: Loss 0.05961, R2 0.98326, RMSE 0.24303                     | Test: Loss 0.94320, R2 0.72328, RMSE 0.96296\n",
      "Epoch [447/500]      | Train: Loss 0.07325, R2 0.97922, RMSE 0.26866                     | Test: Loss 0.98972, R2 0.73851, RMSE 0.98582\n",
      "Epoch [448/500]      | Train: Loss 0.06951, R2 0.98058, RMSE 0.26206                     | Test: Loss 0.92133, R2 0.74890, RMSE 0.94585\n",
      "Epoch [449/500]      | Train: Loss 0.06462, R2 0.98198, RMSE 0.25271                     | Test: Loss 0.84195, R2 0.76577, RMSE 0.90524\n",
      "Epoch [450/500]      | Train: Loss 0.06177, R2 0.98266, RMSE 0.24714                     | Test: Loss 0.87958, R2 0.75436, RMSE 0.93182\n",
      "Epoch [451/500]      | Train: Loss 0.06528, R2 0.98182, RMSE 0.25372                     | Test: Loss 0.91077, R2 0.73679, RMSE 0.95053\n",
      "Epoch [452/500]      | Train: Loss 0.05942, R2 0.98337, RMSE 0.24270                     | Test: Loss 0.85934, R2 0.76050, RMSE 0.92022\n",
      "Epoch [453/500]      | Train: Loss 0.06294, R2 0.98231, RMSE 0.24929                     | Test: Loss 0.87726, R2 0.77543, RMSE 0.93034\n",
      "Epoch [454/500]      | Train: Loss 0.06073, R2 0.98312, RMSE 0.24434                     | Test: Loss 0.86670, R2 0.75889, RMSE 0.92601\n",
      "Epoch [455/500]      | Train: Loss 0.06580, R2 0.98146, RMSE 0.25529                     | Test: Loss 0.90268, R2 0.75270, RMSE 0.94251\n",
      "Epoch [456/500]      | Train: Loss 0.06953, R2 0.98057, RMSE 0.26245                     | Test: Loss 0.92900, R2 0.75008, RMSE 0.95744\n",
      "Epoch [457/500]      | Train: Loss 0.06466, R2 0.98195, RMSE 0.25266                     | Test: Loss 0.90793, R2 0.71976, RMSE 0.94458\n",
      "Epoch [458/500]      | Train: Loss 0.06155, R2 0.98278, RMSE 0.24687                     | Test: Loss 0.89022, R2 0.74284, RMSE 0.93663\n",
      "Epoch [459/500]      | Train: Loss 0.06951, R2 0.98039, RMSE 0.26196                     | Test: Loss 0.85479, R2 0.76048, RMSE 0.91443\n",
      "Epoch [460/500]      | Train: Loss 0.06275, R2 0.98241, RMSE 0.24864                     | Test: Loss 0.92470, R2 0.72164, RMSE 0.95480\n",
      "Epoch [461/500]      | Train: Loss 0.06644, R2 0.98129, RMSE 0.25678                     | Test: Loss 0.95064, R2 0.71942, RMSE 0.96298\n",
      "Epoch [462/500]      | Train: Loss 0.06199, R2 0.98278, RMSE 0.24730                     | Test: Loss 0.88974, R2 0.75237, RMSE 0.93438\n",
      "Epoch [463/500]      | Train: Loss 0.06229, R2 0.98245, RMSE 0.24821                     | Test: Loss 1.16339, R2 0.72211, RMSE 1.01872\n",
      "Epoch [464/500]      | Train: Loss 0.05535, R2 0.98442, RMSE 0.23423                     | Test: Loss 0.90826, R2 0.75046, RMSE 0.94266\n",
      "Epoch [465/500]      | Train: Loss 0.05579, R2 0.98431, RMSE 0.23548                     | Test: Loss 0.86283, R2 0.76546, RMSE 0.92209\n",
      "Epoch [466/500]      | Train: Loss 0.05603, R2 0.98428, RMSE 0.23567                     | Test: Loss 0.89081, R2 0.74390, RMSE 0.93592\n",
      "Epoch [467/500]      | Train: Loss 0.06014, R2 0.98331, RMSE 0.24426                     | Test: Loss 0.86026, R2 0.76413, RMSE 0.92260\n",
      "Epoch [468/500]      | Train: Loss 0.05682, R2 0.98402, RMSE 0.23739                     | Test: Loss 0.83874, R2 0.76584, RMSE 0.90643\n",
      "Epoch [469/500]      | Train: Loss 0.06063, R2 0.98290, RMSE 0.24503                     | Test: Loss 0.87930, R2 0.76016, RMSE 0.92915\n",
      "Epoch [470/500]      | Train: Loss 0.05887, R2 0.98344, RMSE 0.24099                     | Test: Loss 0.85750, R2 0.75460, RMSE 0.91807\n",
      "Epoch [471/500]      | Train: Loss 0.05779, R2 0.98379, RMSE 0.23891                     | Test: Loss 0.92438, R2 0.75106, RMSE 0.95459\n",
      "Epoch [472/500]      | Train: Loss 0.05954, R2 0.98331, RMSE 0.24286                     | Test: Loss 0.86492, R2 0.75519, RMSE 0.92205\n",
      "Epoch [473/500]      | Train: Loss 0.06101, R2 0.98284, RMSE 0.24529                     | Test: Loss 0.94710, R2 0.74856, RMSE 0.96406\n",
      "Epoch [474/500]      | Train: Loss 0.06304, R2 0.98247, RMSE 0.24975                     | Test: Loss 0.85042, R2 0.75346, RMSE 0.91355\n",
      "Epoch [475/500]      | Train: Loss 0.05501, R2 0.98447, RMSE 0.23344                     | Test: Loss 0.95197, R2 0.73102, RMSE 0.96000\n",
      "Epoch [476/500]      | Train: Loss 0.05970, R2 0.98315, RMSE 0.24239                     | Test: Loss 0.87010, R2 0.74079, RMSE 0.92611\n",
      "Epoch [477/500]      | Train: Loss 0.05524, R2 0.98437, RMSE 0.23418                     | Test: Loss 0.85853, R2 0.76969, RMSE 0.91947\n",
      "Epoch [478/500]      | Train: Loss 0.05532, R2 0.98433, RMSE 0.23441                     | Test: Loss 0.83457, R2 0.77230, RMSE 0.90669\n",
      "Epoch [479/500]      | Train: Loss 0.06421, R2 0.98188, RMSE 0.25254                     | Test: Loss 0.83165, R2 0.76311, RMSE 0.90375\n",
      "Epoch [480/500]      | Train: Loss 0.06144, R2 0.98270, RMSE 0.24681                     | Test: Loss 0.83838, R2 0.76812, RMSE 0.90470\n",
      "Epoch [481/500]      | Train: Loss 0.06323, R2 0.98233, RMSE 0.24941                     | Test: Loss 0.91288, R2 0.74515, RMSE 0.94928\n",
      "Epoch [482/500]      | Train: Loss 0.06931, R2 0.98086, RMSE 0.26026                     | Test: Loss 0.90004, R2 0.74894, RMSE 0.93886\n",
      "Epoch [483/500]      | Train: Loss 0.07524, R2 0.97895, RMSE 0.27110                     | Test: Loss 0.85739, R2 0.75786, RMSE 0.91880\n",
      "Epoch [484/500]      | Train: Loss 0.07028, R2 0.98011, RMSE 0.26382                     | Test: Loss 0.87291, R2 0.76561, RMSE 0.93037\n",
      "Epoch [485/500]      | Train: Loss 0.06204, R2 0.98253, RMSE 0.24786                     | Test: Loss 0.84753, R2 0.77374, RMSE 0.90720\n",
      "Epoch [486/500]      | Train: Loss 0.05884, R2 0.98357, RMSE 0.24124                     | Test: Loss 0.84245, R2 0.76630, RMSE 0.90761\n",
      "Epoch [487/500]      | Train: Loss 0.05731, R2 0.98385, RMSE 0.23776                     | Test: Loss 0.86263, R2 0.75675, RMSE 0.92197\n",
      "Epoch [488/500]      | Train: Loss 0.06010, R2 0.98317, RMSE 0.24401                     | Test: Loss 0.84918, R2 0.75280, RMSE 0.91129\n",
      "Epoch [489/500]      | Train: Loss 0.06626, R2 0.98138, RMSE 0.25530                     | Test: Loss 0.89573, R2 0.75300, RMSE 0.93799\n",
      "Epoch [490/500]      | Train: Loss 0.06547, R2 0.98152, RMSE 0.25451                     | Test: Loss 0.93493, R2 0.72925, RMSE 0.96119\n",
      "Epoch [491/500]      | Train: Loss 0.06119, R2 0.98276, RMSE 0.24576                     | Test: Loss 0.87899, R2 0.75888, RMSE 0.93148\n",
      "Epoch [492/500]      | Train: Loss 0.05331, R2 0.98501, RMSE 0.22971                     | Test: Loss 0.86120, R2 0.74246, RMSE 0.92175\n",
      "Epoch [493/500]      | Train: Loss 0.06244, R2 0.98274, RMSE 0.24792                     | Test: Loss 0.85792, R2 0.76613, RMSE 0.91931\n",
      "Epoch [494/500]      | Train: Loss 0.06797, R2 0.98075, RMSE 0.25892                     | Test: Loss 0.84762, R2 0.77185, RMSE 0.90912\n",
      "Epoch [495/500]      | Train: Loss 0.05936, R2 0.98333, RMSE 0.24249                     | Test: Loss 0.90627, R2 0.75793, RMSE 0.94404\n",
      "Epoch [496/500]      | Train: Loss 0.05673, R2 0.98405, RMSE 0.23713                     | Test: Loss 0.87498, R2 0.76846, RMSE 0.92926\n",
      "Epoch [497/500]      | Train: Loss 0.05405, R2 0.98471, RMSE 0.23158                     | Test: Loss 0.86292, R2 0.75772, RMSE 0.92605\n",
      "Epoch [498/500]      | Train: Loss 0.05249, R2 0.98534, RMSE 0.22804                     | Test: Loss 0.91364, R2 0.74624, RMSE 0.94333\n",
      "Epoch [499/500]      | Train: Loss 0.05230, R2 0.98538, RMSE 0.22740                     | Test: Loss 0.86635, R2 0.76203, RMSE 0.92207\n",
      "Epoch [500/500]      | Train: Loss 0.05235, R2 0.98549, RMSE 0.22703                     | Test: Loss 0.88965, R2 0.75493, RMSE 0.93833\n",
      "Best rmse 0.8536079823970795\n",
      "500 epochs of training and evaluation took, 278.140625\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class LossFunction(nn.Module):\n",
    "    def __init__(self, num_tasks):\n",
    "        super(LossFunction, self).__init__()\n",
    "        self.num_tasks = num_tasks\n",
    "\n",
    "        self.loss_fns = [nn.MSELoss() for x in range(num_tasks)] \n",
    "\n",
    "    def forward(self, predictions, labels_task1):\n",
    "\n",
    "        #task 1\n",
    "        target = labels_task1\n",
    "        prediction = predictions\n",
    "        loss_fn = self.loss_fns[0]\n",
    "\n",
    "        task_loss = loss_fn(prediction, target)\n",
    "        \n",
    "        return task_loss\n",
    "    \n",
    "model = CATTransformer(rff_on=True,\n",
    "                       sigma=2,\n",
    "                       embed_size=160,\n",
    "                       heads=5,\n",
    "                       forward_expansion=8,\n",
    "                       pre_norm_on=False,\n",
    "                       mlp_scale_classification=8,\n",
    "                    \n",
    "                       decoder_dropout=0,\n",
    "                       classification_dropout=0.1,\n",
    "                       targets_classes=target_classes,\n",
    "                       n_cont=len(cont_columns),\n",
    "                     \n",
    "                       embedding_scheme='normal_periodic',\n",
    "                       trainable=True,\n",
    "                       linear_on=True).to(device_in_use)\n",
    "\n",
    "def format_metric(value): # Used to format the metrics output\n",
    "    return f\"{value:.5f}\"\n",
    "\n",
    "loss_functions = LossFunction(1)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr = 0.0001) # Maybe try messing around with optimizers. try other torch optimizers with different configurations.\n",
    "epochs = 500 #Set the number of epochs\n",
    "\n",
    "train_losses = []\n",
    "train_rmse = [] \n",
    "test_losses = []\n",
    "test_rmse = []  \n",
    "\n",
    "\n",
    "#Time will be recorded for all 100 epochs - This means the results will not be comparable to Xgboost but that is ok, we will only compare between transformer models who will also train for 100 epochs\n",
    "start_time = time.process_time()\n",
    "\n",
    "for t in range(epochs):\n",
    "    train_loss, r2_train, rmse_train = train(train_dataloader, model, loss_functions, optimizer, device_in_use=device_in_use)\n",
    "    test_loss, r2_test, rmse_test = test(test_dataloader, model, loss_functions, device_in_use=device_in_use)\n",
    "    train_losses.append(train_loss)\n",
    "    train_rmse.append(rmse_train)\n",
    "    # train_accuracies_2.append(train_accuracy_2)\n",
    "    # train_recalls.append(train_recall) \n",
    "    # train_f1_scores.append(train_f1)\n",
    "    test_losses.append(test_loss)\n",
    "    test_rmse.append(rmse_test)\n",
    "    # test_accuracies_2.append(test_accuracy_2)\n",
    "    # test_recalls.append(test_recall)\n",
    "    # test_f1_scores.append(test_f1)\n",
    "    # Formatting for easier reading\n",
    "    epoch_str = f\"Epoch [{t+1:2}/{epochs}]\"\n",
    "    train_metrics = f\"Train: Loss {format_metric(train_loss)}, R2 {format_metric(r2_train)}, RMSE {format_metric(rmse_train)}\"\n",
    "    test_metrics = f\"Test: Loss {format_metric(test_loss)}, R2 {format_metric(r2_test)}, RMSE {format_metric(rmse_test)}\"\n",
    "    print(f\"{epoch_str:20} | {train_metrics:65} | {test_metrics}\")\n",
    "\n",
    "total_time = time.process_time() - start_time\n",
    "\n",
    "# Plotting the loss curves\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, epochs+1), train_losses, label='Train Loss')\n",
    "plt.plot(range(1, epochs+1), [l for l in test_losses], label='Test Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Test Loss Curve')\n",
    "plt.legend()\n",
    "\n",
    "best_index = test_rmse.index(min(test_rmse))\n",
    "print(f\"Best rmse {test_rmse[best_index]}\")\n",
    "print(f\"500 epochs of training and evaluation took, {total_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainable Log-Linear RFF to each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [ 1/300]       | Train: Loss 4.45481, R2 -0.23046, RMSE 2.05961                    | Test: Loss 3.54626, R2 0.06343, RMSE 1.87444\n",
      "Epoch [ 2/300]       | Train: Loss 2.27213, R2 0.36853, RMSE 1.48636                     | Test: Loss 1.29914, R2 0.63680, RMSE 1.13769\n",
      "Epoch [ 3/300]       | Train: Loss 1.34155, R2 0.62537, RMSE 1.15278                     | Test: Loss 1.08298, R2 0.70167, RMSE 1.03299\n",
      "Epoch [ 4/300]       | Train: Loss 1.20717, R2 0.66150, RMSE 1.09344                     | Test: Loss 1.35381, R2 0.68534, RMSE 1.10305\n",
      "Epoch [ 5/300]       | Train: Loss 1.09766, R2 0.69409, RMSE 1.04276                     | Test: Loss 0.94554, R2 0.75128, RMSE 0.96576\n",
      "Epoch [ 6/300]       | Train: Loss 1.04482, R2 0.70870, RMSE 1.01738                     | Test: Loss 0.94165, R2 0.73667, RMSE 0.96603\n",
      "Epoch [ 7/300]       | Train: Loss 1.03948, R2 0.71137, RMSE 1.01391                     | Test: Loss 0.86261, R2 0.76750, RMSE 0.92150\n",
      "Epoch [ 8/300]       | Train: Loss 0.99608, R2 0.72084, RMSE 0.99385                     | Test: Loss 0.87854, R2 0.74703, RMSE 0.93138\n",
      "Epoch [ 9/300]       | Train: Loss 1.01000, R2 0.71620, RMSE 1.00123                     | Test: Loss 0.93895, R2 0.74310, RMSE 0.95863\n",
      "Epoch [10/300]       | Train: Loss 0.96892, R2 0.72851, RMSE 0.98053                     | Test: Loss 0.83113, R2 0.77783, RMSE 0.90719\n",
      "Epoch [11/300]       | Train: Loss 0.97549, R2 0.73039, RMSE 0.98116                     | Test: Loss 0.86190, R2 0.76977, RMSE 0.92191\n",
      "Epoch [12/300]       | Train: Loss 0.95923, R2 0.73343, RMSE 0.97553                     | Test: Loss 0.84766, R2 0.76060, RMSE 0.91648\n",
      "Epoch [13/300]       | Train: Loss 0.96858, R2 0.72926, RMSE 0.97992                     | Test: Loss 0.82712, R2 0.76222, RMSE 0.90358\n",
      "Epoch [14/300]       | Train: Loss 0.94597, R2 0.73597, RMSE 0.96759                     | Test: Loss 0.87533, R2 0.75695, RMSE 0.92888\n",
      "Epoch [15/300]       | Train: Loss 0.95750, R2 0.73075, RMSE 0.97403                     | Test: Loss 0.84268, R2 0.76172, RMSE 0.91343\n",
      "Epoch [16/300]       | Train: Loss 0.93992, R2 0.73681, RMSE 0.96437                     | Test: Loss 0.93767, R2 0.74474, RMSE 0.96229\n",
      "Epoch [17/300]       | Train: Loss 0.93366, R2 0.73910, RMSE 0.96014                     | Test: Loss 1.15785, R2 0.73860, RMSE 1.00478\n",
      "Epoch [18/300]       | Train: Loss 0.93106, R2 0.74158, RMSE 0.96038                     | Test: Loss 0.85625, R2 0.75847, RMSE 0.92002\n",
      "Epoch [19/300]       | Train: Loss 0.90626, R2 0.74836, RMSE 0.94684                     | Test: Loss 0.82711, R2 0.76644, RMSE 0.90400\n",
      "Epoch [20/300]       | Train: Loss 0.89903, R2 0.74709, RMSE 0.94397                     | Test: Loss 0.80722, R2 0.77225, RMSE 0.89074\n",
      "Epoch [21/300]       | Train: Loss 0.90388, R2 0.74857, RMSE 0.94616                     | Test: Loss 0.86781, R2 0.77198, RMSE 0.92558\n",
      "Epoch [22/300]       | Train: Loss 0.90800, R2 0.74566, RMSE 0.94915                     | Test: Loss 0.84416, R2 0.76726, RMSE 0.90802\n",
      "Epoch [23/300]       | Train: Loss 0.89331, R2 0.74966, RMSE 0.94068                     | Test: Loss 0.80616, R2 0.77907, RMSE 0.89360\n",
      "Epoch [24/300]       | Train: Loss 0.88620, R2 0.75206, RMSE 0.93620                     | Test: Loss 0.81306, R2 0.76697, RMSE 0.89522\n",
      "Epoch [25/300]       | Train: Loss 0.88236, R2 0.75198, RMSE 0.93498                     | Test: Loss 0.83628, R2 0.77094, RMSE 0.90860\n",
      "Epoch [26/300]       | Train: Loss 0.87131, R2 0.75563, RMSE 0.92928                     | Test: Loss 0.96135, R2 0.75372, RMSE 0.97425\n",
      "Epoch [27/300]       | Train: Loss 0.86896, R2 0.75658, RMSE 0.92805                     | Test: Loss 0.79353, R2 0.76791, RMSE 0.88425\n",
      "Epoch [28/300]       | Train: Loss 0.87301, R2 0.75579, RMSE 0.93045                     | Test: Loss 0.78793, R2 0.78087, RMSE 0.88191\n",
      "Epoch [29/300]       | Train: Loss 0.86871, R2 0.75569, RMSE 0.92695                     | Test: Loss 0.83550, R2 0.77211, RMSE 0.90768\n",
      "Epoch [30/300]       | Train: Loss 0.86250, R2 0.75808, RMSE 0.92457                     | Test: Loss 0.89509, R2 0.76894, RMSE 0.94014\n",
      "Epoch [31/300]       | Train: Loss 0.86161, R2 0.76221, RMSE 0.92237                     | Test: Loss 0.81169, R2 0.77303, RMSE 0.89679\n",
      "Epoch [32/300]       | Train: Loss 0.85840, R2 0.76129, RMSE 0.92289                     | Test: Loss 0.78112, R2 0.76839, RMSE 0.88006\n",
      "Epoch [33/300]       | Train: Loss 0.83579, R2 0.76694, RMSE 0.91106                     | Test: Loss 0.79799, R2 0.78498, RMSE 0.88570\n",
      "Epoch [34/300]       | Train: Loss 0.85508, R2 0.76178, RMSE 0.91960                     | Test: Loss 0.87671, R2 0.75061, RMSE 0.92749\n",
      "Epoch [35/300]       | Train: Loss 0.84892, R2 0.76401, RMSE 0.91733                     | Test: Loss 0.77695, R2 0.77584, RMSE 0.87535\n",
      "Epoch [36/300]       | Train: Loss 0.83480, R2 0.76646, RMSE 0.90819                     | Test: Loss 0.83398, R2 0.77811, RMSE 0.90399\n",
      "Epoch [37/300]       | Train: Loss 0.82881, R2 0.76826, RMSE 0.90525                     | Test: Loss 0.78458, R2 0.75738, RMSE 0.87863\n",
      "Epoch [38/300]       | Train: Loss 0.82216, R2 0.76940, RMSE 0.90353                     | Test: Loss 0.75381, R2 0.79729, RMSE 0.85597\n",
      "Epoch [39/300]       | Train: Loss 0.81046, R2 0.77252, RMSE 0.89699                     | Test: Loss 0.83073, R2 0.78317, RMSE 0.90485\n",
      "Epoch [40/300]       | Train: Loss 0.80865, R2 0.77428, RMSE 0.89594                     | Test: Loss 0.86238, R2 0.74497, RMSE 0.91614\n",
      "Epoch [41/300]       | Train: Loss 0.81221, R2 0.77166, RMSE 0.89621                     | Test: Loss 0.76784, R2 0.79319, RMSE 0.86658\n",
      "Epoch [42/300]       | Train: Loss 0.81727, R2 0.77311, RMSE 0.89890                     | Test: Loss 0.74740, R2 0.79117, RMSE 0.85927\n",
      "Epoch [43/300]       | Train: Loss 0.81681, R2 0.77336, RMSE 0.89973                     | Test: Loss 0.77006, R2 0.77297, RMSE 0.86795\n",
      "Epoch [44/300]       | Train: Loss 0.80768, R2 0.77371, RMSE 0.89454                     | Test: Loss 0.87735, R2 0.72605, RMSE 0.91741\n",
      "Epoch [45/300]       | Train: Loss 0.80242, R2 0.77338, RMSE 0.89233                     | Test: Loss 0.75331, R2 0.79589, RMSE 0.85710\n",
      "Epoch [46/300]       | Train: Loss 0.80411, R2 0.77513, RMSE 0.89246                     | Test: Loss 0.82593, R2 0.76578, RMSE 0.90212\n",
      "Epoch [47/300]       | Train: Loss 0.79613, R2 0.77655, RMSE 0.88841                     | Test: Loss 0.79034, R2 0.76617, RMSE 0.88467\n",
      "Epoch [48/300]       | Train: Loss 0.80531, R2 0.77401, RMSE 0.89439                     | Test: Loss 0.77669, R2 0.78466, RMSE 0.87555\n",
      "Epoch [49/300]       | Train: Loss 0.79191, R2 0.77836, RMSE 0.88616                     | Test: Loss 0.77720, R2 0.78203, RMSE 0.87551\n",
      "Epoch [50/300]       | Train: Loss 0.79412, R2 0.77573, RMSE 0.88702                     | Test: Loss 0.73548, R2 0.79092, RMSE 0.84951\n",
      "Epoch [51/300]       | Train: Loss 0.78325, R2 0.77927, RMSE 0.88210                     | Test: Loss 0.77084, R2 0.78827, RMSE 0.87048\n",
      "Epoch [52/300]       | Train: Loss 0.78514, R2 0.78128, RMSE 0.88292                     | Test: Loss 0.75928, R2 0.79744, RMSE 0.86418\n",
      "Epoch [53/300]       | Train: Loss 0.77384, R2 0.78350, RMSE 0.87622                     | Test: Loss 0.77603, R2 0.78368, RMSE 0.87223\n",
      "Epoch [54/300]       | Train: Loss 0.77606, R2 0.78293, RMSE 0.87763                     | Test: Loss 0.77286, R2 0.77425, RMSE 0.87158\n",
      "Epoch [55/300]       | Train: Loss 0.77505, R2 0.78486, RMSE 0.87638                     | Test: Loss 0.76986, R2 0.78726, RMSE 0.87137\n",
      "Epoch [56/300]       | Train: Loss 0.76378, R2 0.78705, RMSE 0.87004                     | Test: Loss 0.74702, R2 0.79413, RMSE 0.85519\n",
      "Epoch [57/300]       | Train: Loss 0.77402, R2 0.78481, RMSE 0.87555                     | Test: Loss 0.73816, R2 0.79024, RMSE 0.85038\n",
      "Epoch [58/300]       | Train: Loss 0.75914, R2 0.78789, RMSE 0.86800                     | Test: Loss 0.75117, R2 0.79583, RMSE 0.86054\n",
      "Epoch [59/300]       | Train: Loss 0.76952, R2 0.78258, RMSE 0.87437                     | Test: Loss 0.79333, R2 0.78640, RMSE 0.88488\n",
      "Epoch [60/300]       | Train: Loss 0.74887, R2 0.78971, RMSE 0.86166                     | Test: Loss 0.74938, R2 0.79661, RMSE 0.85790\n",
      "Epoch [61/300]       | Train: Loss 0.74798, R2 0.79156, RMSE 0.86049                     | Test: Loss 0.76740, R2 0.78961, RMSE 0.87249\n",
      "Epoch [62/300]       | Train: Loss 0.74972, R2 0.78874, RMSE 0.86240                     | Test: Loss 0.72317, R2 0.79435, RMSE 0.84121\n",
      "Epoch [63/300]       | Train: Loss 0.73617, R2 0.79217, RMSE 0.85488                     | Test: Loss 0.74798, R2 0.79297, RMSE 0.85279\n",
      "Epoch [64/300]       | Train: Loss 0.74190, R2 0.79017, RMSE 0.85777                     | Test: Loss 0.79495, R2 0.78709, RMSE 0.88711\n",
      "Epoch [65/300]       | Train: Loss 0.73012, R2 0.79577, RMSE 0.85165                     | Test: Loss 0.73616, R2 0.78910, RMSE 0.85204\n",
      "Epoch [66/300]       | Train: Loss 0.73397, R2 0.79542, RMSE 0.85391                     | Test: Loss 0.75642, R2 0.79172, RMSE 0.86486\n",
      "Epoch [67/300]       | Train: Loss 0.73717, R2 0.79306, RMSE 0.85528                     | Test: Loss 0.75373, R2 0.78757, RMSE 0.86036\n",
      "Epoch [68/300]       | Train: Loss 0.71277, R2 0.79964, RMSE 0.84112                     | Test: Loss 0.72277, R2 0.79817, RMSE 0.83787\n",
      "Epoch [69/300]       | Train: Loss 0.72453, R2 0.79827, RMSE 0.84783                     | Test: Loss 0.71409, R2 0.80112, RMSE 0.83639\n",
      "Epoch [70/300]       | Train: Loss 0.71102, R2 0.79976, RMSE 0.84068                     | Test: Loss 0.75092, R2 0.79508, RMSE 0.86260\n",
      "Epoch [71/300]       | Train: Loss 0.72500, R2 0.79571, RMSE 0.84792                     | Test: Loss 0.74389, R2 0.79315, RMSE 0.85245\n",
      "Epoch [72/300]       | Train: Loss 0.72040, R2 0.79923, RMSE 0.84533                     | Test: Loss 0.75932, R2 0.79570, RMSE 0.86679\n",
      "Epoch [73/300]       | Train: Loss 0.72139, R2 0.79888, RMSE 0.84265                     | Test: Loss 0.74676, R2 0.77792, RMSE 0.85740\n",
      "Epoch [74/300]       | Train: Loss 0.71123, R2 0.80211, RMSE 0.84006                     | Test: Loss 0.83782, R2 0.77054, RMSE 0.90915\n",
      "Epoch [75/300]       | Train: Loss 0.70606, R2 0.80186, RMSE 0.83703                     | Test: Loss 0.72257, R2 0.79684, RMSE 0.84719\n",
      "Epoch [76/300]       | Train: Loss 0.70002, R2 0.80428, RMSE 0.83279                     | Test: Loss 0.75535, R2 0.79942, RMSE 0.85845\n",
      "Epoch [77/300]       | Train: Loss 0.69959, R2 0.80212, RMSE 0.83396                     | Test: Loss 0.76626, R2 0.79662, RMSE 0.87096\n",
      "Epoch [78/300]       | Train: Loss 0.69134, R2 0.80541, RMSE 0.82792                     | Test: Loss 0.75949, R2 0.77701, RMSE 0.86757\n",
      "Epoch [79/300]       | Train: Loss 0.69222, R2 0.80628, RMSE 0.82905                     | Test: Loss 0.70411, R2 0.80880, RMSE 0.83368\n",
      "Epoch [80/300]       | Train: Loss 0.68868, R2 0.80783, RMSE 0.82636                     | Test: Loss 0.73014, R2 0.78757, RMSE 0.85197\n",
      "Epoch [81/300]       | Train: Loss 0.69090, R2 0.80640, RMSE 0.82768                     | Test: Loss 0.77288, R2 0.78373, RMSE 0.87328\n",
      "Epoch [82/300]       | Train: Loss 0.68044, R2 0.81023, RMSE 0.82078                     | Test: Loss 0.73335, R2 0.79439, RMSE 0.84863\n",
      "Epoch [83/300]       | Train: Loss 0.68219, R2 0.80811, RMSE 0.82283                     | Test: Loss 0.80488, R2 0.79065, RMSE 0.89146\n",
      "Epoch [84/300]       | Train: Loss 0.68838, R2 0.80776, RMSE 0.82673                     | Test: Loss 0.74926, R2 0.79812, RMSE 0.85894\n",
      "Epoch [85/300]       | Train: Loss 0.69449, R2 0.80442, RMSE 0.83081                     | Test: Loss 0.75483, R2 0.79328, RMSE 0.86194\n",
      "Epoch [86/300]       | Train: Loss 0.67567, R2 0.80999, RMSE 0.81900                     | Test: Loss 0.72264, R2 0.79960, RMSE 0.84244\n",
      "Epoch [87/300]       | Train: Loss 0.66718, R2 0.81296, RMSE 0.81336                     | Test: Loss 0.72177, R2 0.80143, RMSE 0.84428\n",
      "Epoch [88/300]       | Train: Loss 0.66598, R2 0.81447, RMSE 0.81311                     | Test: Loss 0.72393, R2 0.78603, RMSE 0.84180\n",
      "Epoch [89/300]       | Train: Loss 0.65754, R2 0.81670, RMSE 0.80696                     | Test: Loss 0.73635, R2 0.79793, RMSE 0.84613\n",
      "Epoch [90/300]       | Train: Loss 0.66703, R2 0.81352, RMSE 0.81354                     | Test: Loss 0.73060, R2 0.80008, RMSE 0.84845\n",
      "Epoch [91/300]       | Train: Loss 0.66043, R2 0.81366, RMSE 0.81010                     | Test: Loss 0.72853, R2 0.80476, RMSE 0.84620\n",
      "Epoch [92/300]       | Train: Loss 0.64918, R2 0.81759, RMSE 0.80201                     | Test: Loss 0.75815, R2 0.79648, RMSE 0.86764\n",
      "Epoch [93/300]       | Train: Loss 0.66201, R2 0.81352, RMSE 0.81152                     | Test: Loss 0.72677, R2 0.80108, RMSE 0.84686\n",
      "Epoch [94/300]       | Train: Loss 0.64691, R2 0.81854, RMSE 0.80131                     | Test: Loss 0.68192, R2 0.81533, RMSE 0.80692\n",
      "Epoch [95/300]       | Train: Loss 0.67689, R2 0.81155, RMSE 0.81836                     | Test: Loss 0.72938, R2 0.76631, RMSE 0.84549\n",
      "Epoch [96/300]       | Train: Loss 0.66038, R2 0.81448, RMSE 0.80898                     | Test: Loss 0.72319, R2 0.79663, RMSE 0.84275\n",
      "Epoch [97/300]       | Train: Loss 0.64322, R2 0.81990, RMSE 0.79942                     | Test: Loss 0.70978, R2 0.80188, RMSE 0.83060\n",
      "Epoch [98/300]       | Train: Loss 0.63392, R2 0.82217, RMSE 0.79339                     | Test: Loss 0.72228, R2 0.80308, RMSE 0.84339\n",
      "Epoch [99/300]       | Train: Loss 0.63106, R2 0.82233, RMSE 0.79148                     | Test: Loss 0.71271, R2 0.80245, RMSE 0.83387\n",
      "Epoch [100/300]      | Train: Loss 0.62263, R2 0.82304, RMSE 0.78661                     | Test: Loss 0.73713, R2 0.80117, RMSE 0.85381\n",
      "Epoch [101/300]      | Train: Loss 0.63080, R2 0.82370, RMSE 0.79150                     | Test: Loss 0.72849, R2 0.79674, RMSE 0.84951\n",
      "Epoch [102/300]      | Train: Loss 0.62449, R2 0.82619, RMSE 0.78692                     | Test: Loss 0.73692, R2 0.80227, RMSE 0.85275\n",
      "Epoch [103/300]      | Train: Loss 0.62838, R2 0.82476, RMSE 0.78971                     | Test: Loss 0.75927, R2 0.78283, RMSE 0.86005\n",
      "Epoch [104/300]      | Train: Loss 0.62326, R2 0.82594, RMSE 0.78679                     | Test: Loss 0.70659, R2 0.79976, RMSE 0.83066\n",
      "Epoch [105/300]      | Train: Loss 0.60883, R2 0.82828, RMSE 0.77821                     | Test: Loss 0.68704, R2 0.81051, RMSE 0.81090\n",
      "Epoch [106/300]      | Train: Loss 0.60405, R2 0.83014, RMSE 0.77557                     | Test: Loss 0.71906, R2 0.80730, RMSE 0.84211\n",
      "Epoch [107/300]      | Train: Loss 0.61979, R2 0.82448, RMSE 0.78520                     | Test: Loss 0.75162, R2 0.79899, RMSE 0.85617\n",
      "Epoch [108/300]      | Train: Loss 0.61788, R2 0.82693, RMSE 0.78293                     | Test: Loss 0.71681, R2 0.79252, RMSE 0.83797\n",
      "Epoch [109/300]      | Train: Loss 0.62062, R2 0.82539, RMSE 0.78433                     | Test: Loss 0.78959, R2 0.79334, RMSE 0.87943\n",
      "Epoch [110/300]      | Train: Loss 0.60937, R2 0.82842, RMSE 0.77809                     | Test: Loss 0.67777, R2 0.81273, RMSE 0.81083\n",
      "Epoch [111/300]      | Train: Loss 0.59920, R2 0.83369, RMSE 0.77198                     | Test: Loss 0.75920, R2 0.79056, RMSE 0.85743\n",
      "Epoch [112/300]      | Train: Loss 0.59624, R2 0.83279, RMSE 0.76959                     | Test: Loss 0.70316, R2 0.80612, RMSE 0.82880\n",
      "Epoch [113/300]      | Train: Loss 0.61366, R2 0.82830, RMSE 0.78025                     | Test: Loss 0.70137, R2 0.80350, RMSE 0.82472\n",
      "Epoch [114/300]      | Train: Loss 0.60704, R2 0.83032, RMSE 0.77738                     | Test: Loss 0.73349, R2 0.79531, RMSE 0.85347\n",
      "Epoch [115/300]      | Train: Loss 0.60091, R2 0.83062, RMSE 0.77334                     | Test: Loss 0.77923, R2 0.76987, RMSE 0.87463\n",
      "Epoch [116/300]      | Train: Loss 0.58112, R2 0.83614, RMSE 0.75975                     | Test: Loss 0.69125, R2 0.80235, RMSE 0.81911\n",
      "Epoch [117/300]      | Train: Loss 0.59387, R2 0.83488, RMSE 0.76825                     | Test: Loss 0.77010, R2 0.78646, RMSE 0.87285\n",
      "Epoch [118/300]      | Train: Loss 0.60372, R2 0.83020, RMSE 0.77508                     | Test: Loss 0.85975, R2 0.78148, RMSE 0.90738\n",
      "Epoch [119/300]      | Train: Loss 0.58300, R2 0.83691, RMSE 0.76012                     | Test: Loss 0.83940, R2 0.77143, RMSE 0.89304\n",
      "Epoch [120/300]      | Train: Loss 0.58217, R2 0.83674, RMSE 0.76018                     | Test: Loss 0.69864, R2 0.80247, RMSE 0.82810\n",
      "Epoch [121/300]      | Train: Loss 0.57247, R2 0.83840, RMSE 0.75430                     | Test: Loss 0.92408, R2 0.69772, RMSE 0.91368\n",
      "Epoch [122/300]      | Train: Loss 0.57500, R2 0.83721, RMSE 0.75562                     | Test: Loss 0.77411, R2 0.78183, RMSE 0.87509\n",
      "Epoch [123/300]      | Train: Loss 0.58570, R2 0.83736, RMSE 0.76276                     | Test: Loss 0.82287, R2 0.75033, RMSE 0.88975\n",
      "Epoch [124/300]      | Train: Loss 0.57158, R2 0.83956, RMSE 0.75455                     | Test: Loss 1.15852, R2 0.76826, RMSE 0.96102\n",
      "Epoch [125/300]      | Train: Loss 0.56443, R2 0.84115, RMSE 0.74939                     | Test: Loss 0.73607, R2 0.78986, RMSE 0.84826\n",
      "Epoch [126/300]      | Train: Loss 0.57006, R2 0.84085, RMSE 0.75221                     | Test: Loss 0.69593, R2 0.79776, RMSE 0.82322\n",
      "Epoch [127/300]      | Train: Loss 0.56424, R2 0.84204, RMSE 0.74829                     | Test: Loss 0.71893, R2 0.80406, RMSE 0.83941\n",
      "Epoch [128/300]      | Train: Loss 0.57728, R2 0.84005, RMSE 0.75592                     | Test: Loss 0.71687, R2 0.80726, RMSE 0.83839\n",
      "Epoch [129/300]      | Train: Loss 0.57880, R2 0.83716, RMSE 0.75826                     | Test: Loss 0.68122, R2 0.81389, RMSE 0.81493\n",
      "Epoch [130/300]      | Train: Loss 0.55347, R2 0.84380, RMSE 0.74123                     | Test: Loss 0.69994, R2 0.80233, RMSE 0.82770\n",
      "Epoch [131/300]      | Train: Loss 0.54678, R2 0.84762, RMSE 0.73751                     | Test: Loss 0.77319, R2 0.78940, RMSE 0.86874\n",
      "Epoch [132/300]      | Train: Loss 0.54830, R2 0.84596, RMSE 0.73845                     | Test: Loss 0.70082, R2 0.80574, RMSE 0.82968\n",
      "Epoch [133/300]      | Train: Loss 0.55315, R2 0.84382, RMSE 0.74183                     | Test: Loss 0.73393, R2 0.78910, RMSE 0.84972\n",
      "Epoch [134/300]      | Train: Loss 0.55207, R2 0.84431, RMSE 0.73996                     | Test: Loss 0.72587, R2 0.78910, RMSE 0.84499\n",
      "Epoch [135/300]      | Train: Loss 0.57197, R2 0.83914, RMSE 0.75372                     | Test: Loss 0.71679, R2 0.79749, RMSE 0.83839\n",
      "Epoch [136/300]      | Train: Loss 0.55354, R2 0.84393, RMSE 0.74180                     | Test: Loss 0.70992, R2 0.78834, RMSE 0.83678\n",
      "Epoch [137/300]      | Train: Loss 0.53180, R2 0.85036, RMSE 0.72721                     | Test: Loss 0.71989, R2 0.77431, RMSE 0.84267\n",
      "Epoch [138/300]      | Train: Loss 0.54130, R2 0.84856, RMSE 0.73364                     | Test: Loss 0.72809, R2 0.78193, RMSE 0.84946\n",
      "Epoch [139/300]      | Train: Loss 0.53740, R2 0.84815, RMSE 0.73087                     | Test: Loss 0.69260, R2 0.80945, RMSE 0.82528\n",
      "Epoch [140/300]      | Train: Loss 0.51613, R2 0.85503, RMSE 0.71649                     | Test: Loss 0.73035, R2 0.80587, RMSE 0.84796\n",
      "Epoch [141/300]      | Train: Loss 0.53167, R2 0.85029, RMSE 0.72733                     | Test: Loss 0.70358, R2 0.80494, RMSE 0.83100\n",
      "Epoch [142/300]      | Train: Loss 0.52421, R2 0.85224, RMSE 0.72208                     | Test: Loss 0.74353, R2 0.79012, RMSE 0.85438\n",
      "Epoch [143/300]      | Train: Loss 0.51914, R2 0.85325, RMSE 0.71860                     | Test: Loss 0.69736, R2 0.81064, RMSE 0.82254\n",
      "Epoch [144/300]      | Train: Loss 0.52788, R2 0.85119, RMSE 0.72475                     | Test: Loss 0.73749, R2 0.79607, RMSE 0.85130\n",
      "Epoch [145/300]      | Train: Loss 0.52351, R2 0.85370, RMSE 0.72118                     | Test: Loss 0.72872, R2 0.80438, RMSE 0.84787\n",
      "Epoch [146/300]      | Train: Loss 0.51101, R2 0.85694, RMSE 0.71269                     | Test: Loss 0.70125, R2 0.80605, RMSE 0.82767\n",
      "Epoch [147/300]      | Train: Loss 0.52672, R2 0.85116, RMSE 0.72378                     | Test: Loss 0.71132, R2 0.80179, RMSE 0.83761\n",
      "Epoch [148/300]      | Train: Loss 0.51054, R2 0.85592, RMSE 0.71238                     | Test: Loss 0.89687, R2 0.78069, RMSE 0.90721\n",
      "Epoch [149/300]      | Train: Loss 0.51679, R2 0.85470, RMSE 0.71703                     | Test: Loss 0.68200, R2 0.80779, RMSE 0.81497\n",
      "Epoch [150/300]      | Train: Loss 0.50834, R2 0.85794, RMSE 0.71025                     | Test: Loss 0.72743, R2 0.80815, RMSE 0.84592\n",
      "Epoch [151/300]      | Train: Loss 0.51135, R2 0.85644, RMSE 0.71316                     | Test: Loss 0.69686, R2 0.80349, RMSE 0.82685\n",
      "Epoch [152/300]      | Train: Loss 0.50752, R2 0.85819, RMSE 0.71040                     | Test: Loss 0.69315, R2 0.81410, RMSE 0.81793\n",
      "Epoch [153/300]      | Train: Loss 0.50345, R2 0.85829, RMSE 0.70686                     | Test: Loss 0.71843, R2 0.79951, RMSE 0.84075\n",
      "Epoch [154/300]      | Train: Loss 0.50323, R2 0.85906, RMSE 0.70692                     | Test: Loss 0.72968, R2 0.78815, RMSE 0.84267\n",
      "Epoch [155/300]      | Train: Loss 0.49894, R2 0.85930, RMSE 0.70456                     | Test: Loss 0.73422, R2 0.80032, RMSE 0.84810\n",
      "Epoch [156/300]      | Train: Loss 0.49530, R2 0.85974, RMSE 0.70183                     | Test: Loss 0.71259, R2 0.80127, RMSE 0.83639\n",
      "Epoch [157/300]      | Train: Loss 0.48797, R2 0.86212, RMSE 0.69674                     | Test: Loss 0.72138, R2 0.77669, RMSE 0.84264\n",
      "Epoch [158/300]      | Train: Loss 0.48823, R2 0.86202, RMSE 0.69673                     | Test: Loss 0.70596, R2 0.80985, RMSE 0.82920\n",
      "Epoch [159/300]      | Train: Loss 0.48834, R2 0.86264, RMSE 0.69661                     | Test: Loss 0.70077, R2 0.80982, RMSE 0.83232\n",
      "Epoch [160/300]      | Train: Loss 0.48325, R2 0.86488, RMSE 0.69338                     | Test: Loss 0.71502, R2 0.80076, RMSE 0.83602\n",
      "Epoch [161/300]      | Train: Loss 0.50905, R2 0.85614, RMSE 0.71077                     | Test: Loss 0.72994, R2 0.80034, RMSE 0.84732\n",
      "Epoch [162/300]      | Train: Loss 0.47663, R2 0.86516, RMSE 0.68837                     | Test: Loss 0.72444, R2 0.80896, RMSE 0.83911\n",
      "Epoch [163/300]      | Train: Loss 0.47531, R2 0.86554, RMSE 0.68769                     | Test: Loss 0.75811, R2 0.80315, RMSE 0.85772\n",
      "Epoch [164/300]      | Train: Loss 0.48102, R2 0.86536, RMSE 0.69199                     | Test: Loss 0.70449, R2 0.80690, RMSE 0.82980\n",
      "Epoch [165/300]      | Train: Loss 0.47371, R2 0.86727, RMSE 0.68613                     | Test: Loss 0.71031, R2 0.80085, RMSE 0.83223\n",
      "Epoch [166/300]      | Train: Loss 0.47392, R2 0.86623, RMSE 0.68667                     | Test: Loss 0.75819, R2 0.79825, RMSE 0.86173\n",
      "Epoch [167/300]      | Train: Loss 0.46791, R2 0.86896, RMSE 0.68269                     | Test: Loss 0.69002, R2 0.80734, RMSE 0.81856\n",
      "Epoch [168/300]      | Train: Loss 0.46505, R2 0.86972, RMSE 0.68034                     | Test: Loss 0.67682, R2 0.81573, RMSE 0.80961\n",
      "Epoch [169/300]      | Train: Loss 0.46653, R2 0.86786, RMSE 0.68126                     | Test: Loss 0.70490, R2 0.80120, RMSE 0.82861\n",
      "Epoch [170/300]      | Train: Loss 0.47938, R2 0.86450, RMSE 0.69013                     | Test: Loss 0.70784, R2 0.80488, RMSE 0.83146\n",
      "Epoch [171/300]      | Train: Loss 0.46921, R2 0.86773, RMSE 0.68274                     | Test: Loss 0.66453, R2 0.81650, RMSE 0.80139\n",
      "Epoch [172/300]      | Train: Loss 0.46465, R2 0.86876, RMSE 0.67958                     | Test: Loss 0.72459, R2 0.80410, RMSE 0.84459\n",
      "Epoch [173/300]      | Train: Loss 0.46837, R2 0.86855, RMSE 0.68250                     | Test: Loss 0.73234, R2 0.79459, RMSE 0.85056\n",
      "Epoch [174/300]      | Train: Loss 0.45617, R2 0.87083, RMSE 0.67340                     | Test: Loss 0.74919, R2 0.80085, RMSE 0.85782\n",
      "Epoch [175/300]      | Train: Loss 0.46600, R2 0.87120, RMSE 0.67927                     | Test: Loss 0.71958, R2 0.79427, RMSE 0.84483\n",
      "Epoch [176/300]      | Train: Loss 0.45354, R2 0.87267, RMSE 0.67172                     | Test: Loss 0.80647, R2 0.79601, RMSE 0.88350\n",
      "Epoch [177/300]      | Train: Loss 0.44635, R2 0.87341, RMSE 0.66710                     | Test: Loss 0.70364, R2 0.80197, RMSE 0.83032\n",
      "Epoch [178/300]      | Train: Loss 0.44717, R2 0.87485, RMSE 0.66691                     | Test: Loss 0.72758, R2 0.80443, RMSE 0.84770\n",
      "Epoch [179/300]      | Train: Loss 0.45717, R2 0.86911, RMSE 0.67418                     | Test: Loss 0.70329, R2 0.80209, RMSE 0.83260\n",
      "Epoch [180/300]      | Train: Loss 0.44924, R2 0.87376, RMSE 0.66820                     | Test: Loss 0.78312, R2 0.79775, RMSE 0.87425\n",
      "Epoch [181/300]      | Train: Loss 0.43941, R2 0.87658, RMSE 0.66130                     | Test: Loss 0.71156, R2 0.79863, RMSE 0.83899\n",
      "Epoch [182/300]      | Train: Loss 0.43685, R2 0.87658, RMSE 0.65971                     | Test: Loss 0.72667, R2 0.79782, RMSE 0.84816\n",
      "Epoch [183/300]      | Train: Loss 0.44573, R2 0.87384, RMSE 0.66610                     | Test: Loss 0.76596, R2 0.78298, RMSE 0.86612\n",
      "Epoch [184/300]      | Train: Loss 0.43637, R2 0.87691, RMSE 0.65835                     | Test: Loss 0.70038, R2 0.81063, RMSE 0.83151\n",
      "Epoch [185/300]      | Train: Loss 0.43619, R2 0.87767, RMSE 0.65863                     | Test: Loss 0.69868, R2 0.80862, RMSE 0.82390\n",
      "Epoch [186/300]      | Train: Loss 0.44153, R2 0.87565, RMSE 0.66250                     | Test: Loss 0.76299, R2 0.76906, RMSE 0.86666\n",
      "Epoch [187/300]      | Train: Loss 0.42342, R2 0.87963, RMSE 0.64892                     | Test: Loss 0.84866, R2 0.77779, RMSE 0.90067\n",
      "Epoch [188/300]      | Train: Loss 0.42358, R2 0.88103, RMSE 0.64908                     | Test: Loss 0.71742, R2 0.79913, RMSE 0.84132\n",
      "Epoch [189/300]      | Train: Loss 0.42833, R2 0.88022, RMSE 0.65277                     | Test: Loss 0.72691, R2 0.80067, RMSE 0.84257\n",
      "Epoch [190/300]      | Train: Loss 0.43028, R2 0.87714, RMSE 0.65389                     | Test: Loss 0.73128, R2 0.78786, RMSE 0.84962\n",
      "Epoch [191/300]      | Train: Loss 0.42908, R2 0.87747, RMSE 0.65361                     | Test: Loss 0.68695, R2 0.80777, RMSE 0.81935\n",
      "Epoch [192/300]      | Train: Loss 0.43229, R2 0.87787, RMSE 0.65602                     | Test: Loss 0.71725, R2 0.80126, RMSE 0.83385\n",
      "Epoch [193/300]      | Train: Loss 0.41140, R2 0.88416, RMSE 0.63971                     | Test: Loss 0.69790, R2 0.80053, RMSE 0.81962\n",
      "Epoch [194/300]      | Train: Loss 0.43047, R2 0.87779, RMSE 0.65390                     | Test: Loss 0.72228, R2 0.79436, RMSE 0.84572\n",
      "Epoch [195/300]      | Train: Loss 0.41570, R2 0.88297, RMSE 0.64315                     | Test: Loss 0.72845, R2 0.79495, RMSE 0.84372\n",
      "Epoch [196/300]      | Train: Loss 0.41927, R2 0.88214, RMSE 0.64606                     | Test: Loss 0.74480, R2 0.79991, RMSE 0.85614\n",
      "Epoch [197/300]      | Train: Loss 0.40493, R2 0.88420, RMSE 0.63498                     | Test: Loss 0.69009, R2 0.80313, RMSE 0.82308\n",
      "Epoch [198/300]      | Train: Loss 0.41597, R2 0.88208, RMSE 0.64343                     | Test: Loss 0.73159, R2 0.79978, RMSE 0.85074\n",
      "Epoch [199/300]      | Train: Loss 0.40489, R2 0.88670, RMSE 0.63497                     | Test: Loss 0.75163, R2 0.80119, RMSE 0.86062\n",
      "Epoch [200/300]      | Train: Loss 0.41835, R2 0.88217, RMSE 0.64516                     | Test: Loss 0.72230, R2 0.79846, RMSE 0.84007\n",
      "Epoch [201/300]      | Train: Loss 0.39735, R2 0.88797, RMSE 0.62881                     | Test: Loss 0.74582, R2 0.78173, RMSE 0.85499\n",
      "Epoch [202/300]      | Train: Loss 0.40312, R2 0.88723, RMSE 0.63321                     | Test: Loss 0.71868, R2 0.80230, RMSE 0.84030\n",
      "Epoch [203/300]      | Train: Loss 0.40091, R2 0.88665, RMSE 0.63197                     | Test: Loss 0.73224, R2 0.80242, RMSE 0.85001\n",
      "Epoch [204/300]      | Train: Loss 0.38984, R2 0.88943, RMSE 0.62279                     | Test: Loss 0.79595, R2 0.78425, RMSE 0.88329\n",
      "Epoch [205/300]      | Train: Loss 0.39611, R2 0.88833, RMSE 0.62844                     | Test: Loss 0.78508, R2 0.78797, RMSE 0.87711\n",
      "Epoch [206/300]      | Train: Loss 0.39120, R2 0.89024, RMSE 0.62399                     | Test: Loss 0.71140, R2 0.79395, RMSE 0.83227\n",
      "Epoch [207/300]      | Train: Loss 0.39880, R2 0.88733, RMSE 0.63014                     | Test: Loss 0.72644, R2 0.79392, RMSE 0.84687\n",
      "Epoch [208/300]      | Train: Loss 0.40407, R2 0.88567, RMSE 0.63426                     | Test: Loss 0.70789, R2 0.80749, RMSE 0.82717\n",
      "Epoch [209/300]      | Train: Loss 0.39527, R2 0.88929, RMSE 0.62725                     | Test: Loss 0.72205, R2 0.80511, RMSE 0.84459\n",
      "Epoch [210/300]      | Train: Loss 0.39040, R2 0.88982, RMSE 0.62375                     | Test: Loss 0.74354, R2 0.77693, RMSE 0.85678\n",
      "Epoch [211/300]      | Train: Loss 0.39333, R2 0.88932, RMSE 0.62601                     | Test: Loss 0.72267, R2 0.79524, RMSE 0.84136\n",
      "Epoch [212/300]      | Train: Loss 0.37804, R2 0.89365, RMSE 0.61377                     | Test: Loss 0.70902, R2 0.79309, RMSE 0.83557\n",
      "Epoch [213/300]      | Train: Loss 0.38509, R2 0.89111, RMSE 0.61945                     | Test: Loss 0.78606, R2 0.78685, RMSE 0.88192\n",
      "Epoch [214/300]      | Train: Loss 0.39889, R2 0.88649, RMSE 0.62983                     | Test: Loss 0.71588, R2 0.80853, RMSE 0.84043\n",
      "Epoch [215/300]      | Train: Loss 0.38353, R2 0.89144, RMSE 0.61798                     | Test: Loss 0.75885, R2 0.79300, RMSE 0.85687\n",
      "Epoch [216/300]      | Train: Loss 0.37946, R2 0.89275, RMSE 0.61477                     | Test: Loss 0.75354, R2 0.79332, RMSE 0.86207\n",
      "Epoch [217/300]      | Train: Loss 0.37162, R2 0.89496, RMSE 0.60791                     | Test: Loss 0.75556, R2 0.79446, RMSE 0.85926\n",
      "Epoch [218/300]      | Train: Loss 0.37570, R2 0.89217, RMSE 0.61168                     | Test: Loss 0.72175, R2 0.79674, RMSE 0.84494\n",
      "Epoch [219/300]      | Train: Loss 0.37138, R2 0.89525, RMSE 0.60801                     | Test: Loss 0.73198, R2 0.79826, RMSE 0.85010\n",
      "Epoch [220/300]      | Train: Loss 0.36419, R2 0.89722, RMSE 0.60252                     | Test: Loss 0.70904, R2 0.80328, RMSE 0.83378\n",
      "Epoch [221/300]      | Train: Loss 0.36356, R2 0.89735, RMSE 0.60163                     | Test: Loss 0.76834, R2 0.78208, RMSE 0.87070\n",
      "Epoch [222/300]      | Train: Loss 0.37122, R2 0.89565, RMSE 0.60776                     | Test: Loss 0.77640, R2 0.79447, RMSE 0.86864\n",
      "Epoch [223/300]      | Train: Loss 0.36523, R2 0.89611, RMSE 0.60310                     | Test: Loss 0.78046, R2 0.79868, RMSE 0.87392\n",
      "Epoch [224/300]      | Train: Loss 0.37037, R2 0.89563, RMSE 0.60751                     | Test: Loss 0.72386, R2 0.80561, RMSE 0.84527\n",
      "Epoch [225/300]      | Train: Loss 0.36381, R2 0.89745, RMSE 0.60211                     | Test: Loss 0.73469, R2 0.80371, RMSE 0.84984\n",
      "Epoch [226/300]      | Train: Loss 0.36511, R2 0.89783, RMSE 0.60331                     | Test: Loss 0.81494, R2 0.79532, RMSE 0.88597\n",
      "Epoch [227/300]      | Train: Loss 0.36329, R2 0.89748, RMSE 0.60180                     | Test: Loss 0.72421, R2 0.79466, RMSE 0.84227\n",
      "Epoch [228/300]      | Train: Loss 0.37018, R2 0.89520, RMSE 0.60763                     | Test: Loss 0.74664, R2 0.78405, RMSE 0.85613\n",
      "Epoch [229/300]      | Train: Loss 0.36017, R2 0.89766, RMSE 0.59863                     | Test: Loss 0.75196, R2 0.79595, RMSE 0.85889\n",
      "Epoch [230/300]      | Train: Loss 0.36842, R2 0.89625, RMSE 0.60605                     | Test: Loss 0.70215, R2 0.80816, RMSE 0.82398\n",
      "Epoch [231/300]      | Train: Loss 0.35594, R2 0.89936, RMSE 0.59539                     | Test: Loss 0.75773, R2 0.76686, RMSE 0.86384\n",
      "Epoch [232/300]      | Train: Loss 0.35949, R2 0.89888, RMSE 0.59784                     | Test: Loss 0.73302, R2 0.78768, RMSE 0.84587\n",
      "Epoch [233/300]      | Train: Loss 0.36152, R2 0.89817, RMSE 0.60063                     | Test: Loss 0.74124, R2 0.80095, RMSE 0.85749\n",
      "Epoch [234/300]      | Train: Loss 0.34296, R2 0.90360, RMSE 0.58399                     | Test: Loss 0.75359, R2 0.79926, RMSE 0.86030\n",
      "Epoch [235/300]      | Train: Loss 0.35169, R2 0.89965, RMSE 0.59147                     | Test: Loss 0.72388, R2 0.79416, RMSE 0.84308\n",
      "Epoch [236/300]      | Train: Loss 0.35153, R2 0.90054, RMSE 0.59188                     | Test: Loss 0.71667, R2 0.79967, RMSE 0.83676\n",
      "Epoch [237/300]      | Train: Loss 0.34348, R2 0.90286, RMSE 0.58473                     | Test: Loss 0.73964, R2 0.79347, RMSE 0.85094\n",
      "Epoch [238/300]      | Train: Loss 0.35700, R2 0.89931, RMSE 0.59610                     | Test: Loss 0.74242, R2 0.79099, RMSE 0.85678\n",
      "Epoch [239/300]      | Train: Loss 0.33839, R2 0.90372, RMSE 0.58062                     | Test: Loss 0.74436, R2 0.79435, RMSE 0.85431\n",
      "Epoch [240/300]      | Train: Loss 0.34578, R2 0.90109, RMSE 0.58673                     | Test: Loss 0.74084, R2 0.79129, RMSE 0.85373\n",
      "Epoch [241/300]      | Train: Loss 0.36016, R2 0.89888, RMSE 0.59871                     | Test: Loss 0.74816, R2 0.79668, RMSE 0.85325\n",
      "Epoch [242/300]      | Train: Loss 0.35684, R2 0.89843, RMSE 0.59587                     | Test: Loss 0.71352, R2 0.80209, RMSE 0.82395\n",
      "Epoch [243/300]      | Train: Loss 0.34520, R2 0.90285, RMSE 0.58645                     | Test: Loss 0.79151, R2 0.78597, RMSE 0.88059\n",
      "Epoch [244/300]      | Train: Loss 0.34700, R2 0.90211, RMSE 0.58777                     | Test: Loss 0.73641, R2 0.78999, RMSE 0.85252\n",
      "Epoch [245/300]      | Train: Loss 0.33878, R2 0.90366, RMSE 0.58086                     | Test: Loss 0.74511, R2 0.79481, RMSE 0.85490\n",
      "Epoch [246/300]      | Train: Loss 0.33538, R2 0.90562, RMSE 0.57811                     | Test: Loss 0.76429, R2 0.79015, RMSE 0.86768\n",
      "Epoch [247/300]      | Train: Loss 0.33013, R2 0.90621, RMSE 0.57352                     | Test: Loss 0.72221, R2 0.79330, RMSE 0.84235\n",
      "Epoch [248/300]      | Train: Loss 0.33777, R2 0.90443, RMSE 0.58009                     | Test: Loss 0.75711, R2 0.78447, RMSE 0.86264\n",
      "Epoch [249/300]      | Train: Loss 0.33601, R2 0.90616, RMSE 0.57831                     | Test: Loss 0.75123, R2 0.79009, RMSE 0.86373\n",
      "Epoch [250/300]      | Train: Loss 0.33769, R2 0.90492, RMSE 0.58001                     | Test: Loss 0.76365, R2 0.79633, RMSE 0.86940\n",
      "Epoch [251/300]      | Train: Loss 0.33264, R2 0.90552, RMSE 0.57565                     | Test: Loss 0.75585, R2 0.78937, RMSE 0.86253\n",
      "Epoch [252/300]      | Train: Loss 0.32650, R2 0.90615, RMSE 0.57048                     | Test: Loss 0.72136, R2 0.79914, RMSE 0.84203\n",
      "Epoch [253/300]      | Train: Loss 0.33151, R2 0.90615, RMSE 0.57472                     | Test: Loss 0.76034, R2 0.78921, RMSE 0.86368\n",
      "Epoch [254/300]      | Train: Loss 0.32502, R2 0.90773, RMSE 0.56901                     | Test: Loss 0.77033, R2 0.77927, RMSE 0.86914\n",
      "Epoch [255/300]      | Train: Loss 0.32988, R2 0.90717, RMSE 0.57307                     | Test: Loss 0.71892, R2 0.80106, RMSE 0.83745\n",
      "Epoch [256/300]      | Train: Loss 0.33391, R2 0.90692, RMSE 0.57661                     | Test: Loss 0.79964, R2 0.75194, RMSE 0.88985\n",
      "Epoch [257/300]      | Train: Loss 0.33172, R2 0.90521, RMSE 0.57480                     | Test: Loss 0.74806, R2 0.79238, RMSE 0.85957\n",
      "Epoch [258/300]      | Train: Loss 0.32240, R2 0.90897, RMSE 0.56694                     | Test: Loss 0.74327, R2 0.79348, RMSE 0.85526\n",
      "Epoch [259/300]      | Train: Loss 0.33494, R2 0.90576, RMSE 0.57805                     | Test: Loss 0.73648, R2 0.76076, RMSE 0.85094\n",
      "Epoch [260/300]      | Train: Loss 0.32694, R2 0.90747, RMSE 0.57091                     | Test: Loss 0.76449, R2 0.77073, RMSE 0.86772\n",
      "Epoch [261/300]      | Train: Loss 0.31845, R2 0.91019, RMSE 0.56326                     | Test: Loss 0.74054, R2 0.78919, RMSE 0.85366\n",
      "Epoch [262/300]      | Train: Loss 0.31664, R2 0.91023, RMSE 0.56171                     | Test: Loss 0.84540, R2 0.77545, RMSE 0.90465\n",
      "Epoch [263/300]      | Train: Loss 0.31488, R2 0.91020, RMSE 0.55982                     | Test: Loss 0.77398, R2 0.78194, RMSE 0.87453\n",
      "Epoch [264/300]      | Train: Loss 0.31098, R2 0.91193, RMSE 0.55655                     | Test: Loss 0.74799, R2 0.77377, RMSE 0.85631\n",
      "Epoch [265/300]      | Train: Loss 0.31568, R2 0.91073, RMSE 0.56068                     | Test: Loss 0.73447, R2 0.79689, RMSE 0.84329\n",
      "Epoch [266/300]      | Train: Loss 0.32082, R2 0.90963, RMSE 0.56554                     | Test: Loss 0.76673, R2 0.77781, RMSE 0.86800\n",
      "Epoch [267/300]      | Train: Loss 0.31239, R2 0.91258, RMSE 0.55743                     | Test: Loss 0.80719, R2 0.78809, RMSE 0.89011\n",
      "Epoch [268/300]      | Train: Loss 0.31389, R2 0.91225, RMSE 0.55902                     | Test: Loss 0.74936, R2 0.78470, RMSE 0.86152\n",
      "Epoch [269/300]      | Train: Loss 0.30784, R2 0.91324, RMSE 0.55360                     | Test: Loss 0.75367, R2 0.80411, RMSE 0.86036\n",
      "Epoch [270/300]      | Train: Loss 0.31395, R2 0.91093, RMSE 0.55922                     | Test: Loss 0.75623, R2 0.78853, RMSE 0.86502\n",
      "Epoch [271/300]      | Train: Loss 0.30908, R2 0.91203, RMSE 0.55494                     | Test: Loss 0.78499, R2 0.78008, RMSE 0.88027\n",
      "Epoch [272/300]      | Train: Loss 0.31040, R2 0.91274, RMSE 0.55616                     | Test: Loss 0.73665, R2 0.79470, RMSE 0.84384\n",
      "Epoch [273/300]      | Train: Loss 0.30534, R2 0.91300, RMSE 0.55151                     | Test: Loss 0.73166, R2 0.79630, RMSE 0.84990\n",
      "Epoch [274/300]      | Train: Loss 0.29819, R2 0.91672, RMSE 0.54505                     | Test: Loss 0.76520, R2 0.78176, RMSE 0.87025\n",
      "Epoch [275/300]      | Train: Loss 0.30562, R2 0.91387, RMSE 0.55174                     | Test: Loss 0.74986, R2 0.79513, RMSE 0.85433\n",
      "Epoch [276/300]      | Train: Loss 0.30926, R2 0.91190, RMSE 0.55513                     | Test: Loss 0.74272, R2 0.79746, RMSE 0.85386\n",
      "Epoch [277/300]      | Train: Loss 0.30224, R2 0.91472, RMSE 0.54867                     | Test: Loss 0.86055, R2 0.75946, RMSE 0.90748\n",
      "Epoch [278/300]      | Train: Loss 0.30188, R2 0.91498, RMSE 0.54810                     | Test: Loss 0.75619, R2 0.77519, RMSE 0.86692\n",
      "Epoch [279/300]      | Train: Loss 0.29275, R2 0.91721, RMSE 0.54018                     | Test: Loss 0.73072, R2 0.79935, RMSE 0.84223\n",
      "Epoch [280/300]      | Train: Loss 0.29774, R2 0.91533, RMSE 0.54471                     | Test: Loss 0.72959, R2 0.79367, RMSE 0.84434\n",
      "Epoch [281/300]      | Train: Loss 0.29623, R2 0.91536, RMSE 0.54337                     | Test: Loss 0.74803, R2 0.78888, RMSE 0.85578\n",
      "Epoch [282/300]      | Train: Loss 0.28946, R2 0.91725, RMSE 0.53702                     | Test: Loss 0.77310, R2 0.79173, RMSE 0.87265\n",
      "Epoch [283/300]      | Train: Loss 0.28930, R2 0.91846, RMSE 0.53682                     | Test: Loss 0.74110, R2 0.79153, RMSE 0.85299\n",
      "Epoch [284/300]      | Train: Loss 0.29219, R2 0.91780, RMSE 0.53937                     | Test: Loss 0.72713, R2 0.79717, RMSE 0.84536\n",
      "Epoch [285/300]      | Train: Loss 0.29570, R2 0.91658, RMSE 0.54276                     | Test: Loss 0.74467, R2 0.78935, RMSE 0.85202\n",
      "Epoch [286/300]      | Train: Loss 0.29385, R2 0.91686, RMSE 0.54112                     | Test: Loss 0.84610, R2 0.77072, RMSE 0.90372\n",
      "Epoch [287/300]      | Train: Loss 0.28973, R2 0.91795, RMSE 0.53725                     | Test: Loss 0.74184, R2 0.79412, RMSE 0.85390\n",
      "Epoch [288/300]      | Train: Loss 0.28522, R2 0.91876, RMSE 0.53296                     | Test: Loss 0.77274, R2 0.78697, RMSE 0.87467\n",
      "Epoch [289/300]      | Train: Loss 0.29581, R2 0.91654, RMSE 0.54305                     | Test: Loss 0.84816, R2 0.78287, RMSE 0.90483\n",
      "Epoch [290/300]      | Train: Loss 0.28295, R2 0.91976, RMSE 0.53050                     | Test: Loss 0.73471, R2 0.79789, RMSE 0.84856\n",
      "Epoch [291/300]      | Train: Loss 0.28519, R2 0.92006, RMSE 0.53245                     | Test: Loss 0.74766, R2 0.79535, RMSE 0.85569\n",
      "Epoch [292/300]      | Train: Loss 0.28202, R2 0.92006, RMSE 0.52995                     | Test: Loss 0.76974, R2 0.77714, RMSE 0.87163\n",
      "Epoch [293/300]      | Train: Loss 0.28387, R2 0.92009, RMSE 0.53218                     | Test: Loss 0.76120, R2 0.78155, RMSE 0.86776\n",
      "Epoch [294/300]      | Train: Loss 0.28629, R2 0.91955, RMSE 0.53386                     | Test: Loss 0.74313, R2 0.79925, RMSE 0.85581\n",
      "Epoch [295/300]      | Train: Loss 0.28265, R2 0.92027, RMSE 0.53068                     | Test: Loss 0.74523, R2 0.79215, RMSE 0.85652\n",
      "Epoch [296/300]      | Train: Loss 0.27472, R2 0.92169, RMSE 0.52352                     | Test: Loss 0.78165, R2 0.78840, RMSE 0.87629\n",
      "Epoch [297/300]      | Train: Loss 0.28132, R2 0.92070, RMSE 0.52942                     | Test: Loss 0.76306, R2 0.79360, RMSE 0.86906\n",
      "Epoch [298/300]      | Train: Loss 0.28124, R2 0.92049, RMSE 0.52918                     | Test: Loss 0.76085, R2 0.77831, RMSE 0.86886\n",
      "Epoch [299/300]      | Train: Loss 0.27966, R2 0.92139, RMSE 0.52760                     | Test: Loss 0.77461, R2 0.77848, RMSE 0.87003\n",
      "Epoch [300/300]      | Train: Loss 0.27869, R2 0.92080, RMSE 0.52702                     | Test: Loss 0.79781, R2 0.79235, RMSE 0.88154\n",
      "Best rmse 0.801388660302529\n",
      "100 epochs of training and evaluation took, 69.21875\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class LossFunction(nn.Module):\n",
    "    def __init__(self, num_tasks):\n",
    "        super(LossFunction, self).__init__()\n",
    "        self.num_tasks = num_tasks\n",
    "\n",
    "        self.loss_fns = [nn.MSELoss() for x in range(num_tasks)] \n",
    "\n",
    "    def forward(self, predictions, labels_task1):\n",
    "\n",
    "        #task 1\n",
    "        target = labels_task1\n",
    "        prediction = predictions\n",
    "        loss_fn = self.loss_fns[0]\n",
    "\n",
    "        task_loss = loss_fn(prediction, target)\n",
    "        \n",
    "        return task_loss\n",
    "    \n",
    "model = CATTransformer(rff_on=True,\n",
    "                       sigma=2,\n",
    "                       embed_size=160,\n",
    "                       heads=5,\n",
    "                       forward_expansion=8,\n",
    "                       pre_norm_on=False,\n",
    "                       mlp_scale_classification=8,\n",
    "                  \n",
    "                       decoder_dropout=0,\n",
    "                       classification_dropout=0.1,\n",
    "                       targets_classes=target_classes,\n",
    "                       n_cont=len(cont_columns),\n",
    "                     \n",
    "                       embedding_scheme='log-linear_periodic',\n",
    "                       trainable=True,\n",
    "                       linear_on=True).to(device_in_use)\n",
    "\n",
    "def format_metric(value): # Used to format the metrics output\n",
    "    return f\"{value:.5f}\"\n",
    "\n",
    "loss_functions = LossFunction(1)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr = 0.0001) # Maybe try messing around with optimizers. try other torch optimizers with different configurations.\n",
    "epochs = 500 #Set the number of epochs\n",
    "\n",
    "train_losses = []\n",
    "train_rmse = [] \n",
    "test_losses = []\n",
    "test_rmse = []  \n",
    "\n",
    "\n",
    "#Time will be recorded for all 100 epochs - This means the results will not be comparable to Xgboost but that is ok, we will only compare between transformer models who will also train for 100 epochs\n",
    "start_time = time.process_time()\n",
    "\n",
    "for t in range(epochs):\n",
    "    train_loss, r2_train, rmse_train = train(train_dataloader, model, loss_functions, optimizer, device_in_use=device_in_use)\n",
    "    test_loss, r2_test, rmse_test = test(test_dataloader, model, loss_functions, device_in_use=device_in_use)\n",
    "    train_losses.append(train_loss)\n",
    "    train_rmse.append(rmse_train)\n",
    "    # train_accuracies_2.append(train_accuracy_2)\n",
    "    # train_recalls.append(train_recall) \n",
    "    # train_f1_scores.append(train_f1)\n",
    "    test_losses.append(test_loss)\n",
    "    test_rmse.append(rmse_test)\n",
    "    # test_accuracies_2.append(test_accuracy_2)\n",
    "    # test_recalls.append(test_recall)\n",
    "    # test_f1_scores.append(test_f1)\n",
    "    # Formatting for easier reading\n",
    "    epoch_str = f\"Epoch [{t+1:2}/{epochs}]\"\n",
    "    train_metrics = f\"Train: Loss {format_metric(train_loss)}, R2 {format_metric(r2_train)}, RMSE {format_metric(rmse_train)}\"\n",
    "    test_metrics = f\"Test: Loss {format_metric(test_loss)}, R2 {format_metric(r2_test)}, RMSE {format_metric(rmse_test)}\"\n",
    "    print(f\"{epoch_str:20} | {train_metrics:65} | {test_metrics}\")\n",
    "\n",
    "total_time = time.process_time() - start_time\n",
    "\n",
    "# Plotting the loss curves\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, epochs+1), train_losses, label='Train Loss')\n",
    "plt.plot(range(1, epochs+1), [l for l in test_losses], label='Test Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Test Loss Curve')\n",
    "plt.legend()\n",
    "\n",
    "best_index = test_rmse.index(min(test_rmse))\n",
    "print(f\"Best rmse {test_rmse[best_index]}\")\n",
    "print(f\"100 epochs of training and evaluation took, {total_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log-Linear RFF to each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [ 1/300]       | Train: Loss 4.50864, R2 -0.24398, RMSE 2.06140                    | Test: Loss 3.52121, R2 0.05782, RMSE 1.87185\n",
      "Epoch [ 2/300]       | Train: Loss 2.32303, R2 0.36285, RMSE 1.49457                     | Test: Loss 1.26183, R2 0.64426, RMSE 1.11642\n",
      "Epoch [ 3/300]       | Train: Loss 1.37107, R2 0.61928, RMSE 1.16359                     | Test: Loss 1.15647, R2 0.68129, RMSE 1.07109\n",
      "Epoch [ 4/300]       | Train: Loss 1.24162, R2 0.65369, RMSE 1.10728                     | Test: Loss 1.04391, R2 0.69297, RMSE 1.01789\n",
      "Epoch [ 5/300]       | Train: Loss 1.14425, R2 0.68145, RMSE 1.06400                     | Test: Loss 0.99049, R2 0.71805, RMSE 0.98902\n",
      "Epoch [ 6/300]       | Train: Loss 1.06728, R2 0.70288, RMSE 1.02893                     | Test: Loss 0.89182, R2 0.74982, RMSE 0.93566\n",
      "Epoch [ 7/300]       | Train: Loss 1.07717, R2 0.69996, RMSE 1.03371                     | Test: Loss 0.88540, R2 0.75824, RMSE 0.93465\n",
      "Epoch [ 8/300]       | Train: Loss 1.02056, R2 0.71539, RMSE 1.00506                     | Test: Loss 0.92695, R2 0.73851, RMSE 0.95687\n",
      "Epoch [ 9/300]       | Train: Loss 1.00511, R2 0.71950, RMSE 0.99894                     | Test: Loss 0.85173, R2 0.75619, RMSE 0.91307\n",
      "Epoch [10/300]       | Train: Loss 0.99130, R2 0.72189, RMSE 0.99067                     | Test: Loss 0.85383, R2 0.76040, RMSE 0.91733\n",
      "Epoch [11/300]       | Train: Loss 0.99288, R2 0.72224, RMSE 0.99216                     | Test: Loss 1.00415, R2 0.73646, RMSE 0.99490\n",
      "Epoch [12/300]       | Train: Loss 0.98653, R2 0.72418, RMSE 0.98816                     | Test: Loss 0.88912, R2 0.73359, RMSE 0.93711\n",
      "Epoch [13/300]       | Train: Loss 0.98164, R2 0.72571, RMSE 0.98560                     | Test: Loss 0.88087, R2 0.76306, RMSE 0.93410\n",
      "Epoch [14/300]       | Train: Loss 0.97167, R2 0.72741, RMSE 0.98079                     | Test: Loss 0.85435, R2 0.77314, RMSE 0.92004\n",
      "Epoch [15/300]       | Train: Loss 0.94638, R2 0.73509, RMSE 0.96841                     | Test: Loss 0.88072, R2 0.74635, RMSE 0.93159\n",
      "Epoch [16/300]       | Train: Loss 0.97875, R2 0.72902, RMSE 0.98310                     | Test: Loss 0.80916, R2 0.77623, RMSE 0.89184\n",
      "Epoch [17/300]       | Train: Loss 0.94472, R2 0.73537, RMSE 0.96724                     | Test: Loss 0.84974, R2 0.76571, RMSE 0.91517\n",
      "Epoch [18/300]       | Train: Loss 0.93766, R2 0.73888, RMSE 0.96412                     | Test: Loss 0.89430, R2 0.76044, RMSE 0.93955\n",
      "Epoch [19/300]       | Train: Loss 0.93453, R2 0.73856, RMSE 0.96161                     | Test: Loss 0.93551, R2 0.75013, RMSE 0.96097\n",
      "Epoch [20/300]       | Train: Loss 0.92895, R2 0.73823, RMSE 0.95853                     | Test: Loss 0.82056, R2 0.76264, RMSE 0.90284\n",
      "Epoch [21/300]       | Train: Loss 0.93528, R2 0.73516, RMSE 0.96350                     | Test: Loss 0.80285, R2 0.76644, RMSE 0.88731\n",
      "Epoch [22/300]       | Train: Loss 0.91508, R2 0.74522, RMSE 0.95121                     | Test: Loss 0.82275, R2 0.76040, RMSE 0.90321\n",
      "Epoch [23/300]       | Train: Loss 0.91105, R2 0.74664, RMSE 0.94927                     | Test: Loss 0.87380, R2 0.73909, RMSE 0.92796\n",
      "Epoch [24/300]       | Train: Loss 0.90829, R2 0.74624, RMSE 0.94977                     | Test: Loss 0.85527, R2 0.75647, RMSE 0.92058\n",
      "Epoch [25/300]       | Train: Loss 0.89944, R2 0.74842, RMSE 0.94377                     | Test: Loss 0.96300, R2 0.74082, RMSE 0.96391\n",
      "Epoch [26/300]       | Train: Loss 0.90710, R2 0.74584, RMSE 0.94850                     | Test: Loss 0.87668, R2 0.76055, RMSE 0.93128\n",
      "Epoch [27/300]       | Train: Loss 0.89537, R2 0.75074, RMSE 0.94045                     | Test: Loss 1.14066, R2 0.75180, RMSE 0.98990\n",
      "Epoch [28/300]       | Train: Loss 0.87760, R2 0.75639, RMSE 0.93247                     | Test: Loss 0.79696, R2 0.78044, RMSE 0.88411\n",
      "Epoch [29/300]       | Train: Loss 0.87189, R2 0.75565, RMSE 0.92990                     | Test: Loss 0.82374, R2 0.77579, RMSE 0.90349\n",
      "Epoch [30/300]       | Train: Loss 0.88488, R2 0.75087, RMSE 0.93620                     | Test: Loss 0.92231, R2 0.72807, RMSE 0.95628\n",
      "Epoch [31/300]       | Train: Loss 0.86437, R2 0.75887, RMSE 0.92579                     | Test: Loss 0.78606, R2 0.77496, RMSE 0.88303\n",
      "Epoch [32/300]       | Train: Loss 0.85719, R2 0.75970, RMSE 0.92152                     | Test: Loss 0.79030, R2 0.78476, RMSE 0.88470\n",
      "Epoch [33/300]       | Train: Loss 0.86258, R2 0.75931, RMSE 0.92393                     | Test: Loss 0.85947, R2 0.77258, RMSE 0.91684\n",
      "Epoch [34/300]       | Train: Loss 0.85322, R2 0.76061, RMSE 0.91991                     | Test: Loss 0.75471, R2 0.77896, RMSE 0.85960\n",
      "Epoch [35/300]       | Train: Loss 0.84939, R2 0.76305, RMSE 0.91685                     | Test: Loss 0.78378, R2 0.79083, RMSE 0.88044\n",
      "Epoch [36/300]       | Train: Loss 0.86355, R2 0.75992, RMSE 0.92312                     | Test: Loss 0.94532, R2 0.74903, RMSE 0.96835\n",
      "Epoch [37/300]       | Train: Loss 0.84768, R2 0.76518, RMSE 0.91671                     | Test: Loss 0.75173, R2 0.79023, RMSE 0.85904\n",
      "Epoch [38/300]       | Train: Loss 0.81710, R2 0.77317, RMSE 0.89855                     | Test: Loss 0.78727, R2 0.79225, RMSE 0.88154\n",
      "Epoch [39/300]       | Train: Loss 0.84905, R2 0.76094, RMSE 0.91702                     | Test: Loss 0.79889, R2 0.78572, RMSE 0.88674\n",
      "Epoch [40/300]       | Train: Loss 0.83248, R2 0.76717, RMSE 0.90760                     | Test: Loss 0.74792, R2 0.79425, RMSE 0.85712\n",
      "Epoch [41/300]       | Train: Loss 0.80248, R2 0.77556, RMSE 0.89071                     | Test: Loss 0.76380, R2 0.78048, RMSE 0.86856\n",
      "Epoch [42/300]       | Train: Loss 0.81716, R2 0.77206, RMSE 0.89877                     | Test: Loss 0.76459, R2 0.78832, RMSE 0.86374\n",
      "Epoch [43/300]       | Train: Loss 0.80716, R2 0.77385, RMSE 0.89419                     | Test: Loss 0.77871, R2 0.78392, RMSE 0.87617\n",
      "Epoch [44/300]       | Train: Loss 0.80764, R2 0.77352, RMSE 0.89447                     | Test: Loss 0.75859, R2 0.79713, RMSE 0.86465\n",
      "Epoch [45/300]       | Train: Loss 0.79539, R2 0.77774, RMSE 0.88879                     | Test: Loss 0.75153, R2 0.79377, RMSE 0.86121\n",
      "Epoch [46/300]       | Train: Loss 0.80512, R2 0.77528, RMSE 0.89368                     | Test: Loss 0.78813, R2 0.78008, RMSE 0.88498\n",
      "Epoch [47/300]       | Train: Loss 0.79429, R2 0.77536, RMSE 0.88805                     | Test: Loss 0.77777, R2 0.78056, RMSE 0.87846\n",
      "Epoch [48/300]       | Train: Loss 0.79082, R2 0.77878, RMSE 0.88555                     | Test: Loss 0.72509, R2 0.79266, RMSE 0.84092\n",
      "Epoch [49/300]       | Train: Loss 0.78099, R2 0.78173, RMSE 0.87934                     | Test: Loss 0.73010, R2 0.80204, RMSE 0.84764\n",
      "Epoch [50/300]       | Train: Loss 0.78145, R2 0.78026, RMSE 0.88025                     | Test: Loss 0.74612, R2 0.78312, RMSE 0.85650\n",
      "Epoch [51/300]       | Train: Loss 0.78968, R2 0.77918, RMSE 0.88420                     | Test: Loss 0.71899, R2 0.79941, RMSE 0.84149\n",
      "Epoch [52/300]       | Train: Loss 0.78212, R2 0.77973, RMSE 0.88046                     | Test: Loss 0.73260, R2 0.79593, RMSE 0.84852\n",
      "Epoch [53/300]       | Train: Loss 0.79999, R2 0.77643, RMSE 0.88981                     | Test: Loss 0.71511, R2 0.80827, RMSE 0.83629\n",
      "Epoch [54/300]       | Train: Loss 0.75759, R2 0.78860, RMSE 0.86648                     | Test: Loss 0.74753, R2 0.79116, RMSE 0.86047\n",
      "Epoch [55/300]       | Train: Loss 0.77453, R2 0.78362, RMSE 0.87637                     | Test: Loss 0.72672, R2 0.80793, RMSE 0.84775\n",
      "Epoch [56/300]       | Train: Loss 0.75953, R2 0.78753, RMSE 0.86800                     | Test: Loss 0.72616, R2 0.80182, RMSE 0.84597\n",
      "Epoch [57/300]       | Train: Loss 0.75011, R2 0.79087, RMSE 0.86222                     | Test: Loss 0.75120, R2 0.79889, RMSE 0.85437\n",
      "Epoch [58/300]       | Train: Loss 0.75028, R2 0.79113, RMSE 0.86116                     | Test: Loss 0.84971, R2 0.79337, RMSE 0.89273\n",
      "Epoch [59/300]       | Train: Loss 0.74761, R2 0.79113, RMSE 0.86043                     | Test: Loss 0.72799, R2 0.79518, RMSE 0.84474\n",
      "Epoch [60/300]       | Train: Loss 0.75294, R2 0.78794, RMSE 0.86497                     | Test: Loss 0.74281, R2 0.80329, RMSE 0.85595\n",
      "Epoch [61/300]       | Train: Loss 0.74433, R2 0.79091, RMSE 0.85998                     | Test: Loss 0.71602, R2 0.79182, RMSE 0.84115\n",
      "Epoch [62/300]       | Train: Loss 0.73016, R2 0.79675, RMSE 0.85139                     | Test: Loss 0.71997, R2 0.79220, RMSE 0.84440\n",
      "Epoch [63/300]       | Train: Loss 0.72584, R2 0.79831, RMSE 0.84790                     | Test: Loss 0.70368, R2 0.80223, RMSE 0.83284\n",
      "Epoch [64/300]       | Train: Loss 0.74863, R2 0.79062, RMSE 0.86117                     | Test: Loss 0.69290, R2 0.80811, RMSE 0.82069\n",
      "Epoch [65/300]       | Train: Loss 0.73960, R2 0.79240, RMSE 0.85645                     | Test: Loss 0.77589, R2 0.78405, RMSE 0.87484\n",
      "Epoch [66/300]       | Train: Loss 0.73598, R2 0.79424, RMSE 0.85451                     | Test: Loss 0.70473, R2 0.80443, RMSE 0.82944\n",
      "Epoch [67/300]       | Train: Loss 0.72174, R2 0.79735, RMSE 0.84664                     | Test: Loss 0.72992, R2 0.79489, RMSE 0.84522\n",
      "Epoch [68/300]       | Train: Loss 0.72132, R2 0.79860, RMSE 0.84580                     | Test: Loss 0.74511, R2 0.79187, RMSE 0.85733\n",
      "Epoch [69/300]       | Train: Loss 0.71158, R2 0.80183, RMSE 0.83984                     | Test: Loss 0.73679, R2 0.79015, RMSE 0.84958\n",
      "Epoch [70/300]       | Train: Loss 0.70519, R2 0.80205, RMSE 0.83487                     | Test: Loss 0.75279, R2 0.80363, RMSE 0.85837\n",
      "Epoch [71/300]       | Train: Loss 0.70776, R2 0.80263, RMSE 0.83638                     | Test: Loss 0.73970, R2 0.79017, RMSE 0.85542\n",
      "Epoch [72/300]       | Train: Loss 0.70133, R2 0.80418, RMSE 0.83408                     | Test: Loss 0.78027, R2 0.78418, RMSE 0.87711\n",
      "Epoch [73/300]       | Train: Loss 0.69271, R2 0.80432, RMSE 0.82978                     | Test: Loss 0.75231, R2 0.79953, RMSE 0.86137\n",
      "Epoch [74/300]       | Train: Loss 0.72245, R2 0.79845, RMSE 0.84627                     | Test: Loss 0.72889, R2 0.79202, RMSE 0.84918\n",
      "Epoch [75/300]       | Train: Loss 0.69093, R2 0.80574, RMSE 0.82824                     | Test: Loss 0.70401, R2 0.79999, RMSE 0.83259\n",
      "Epoch [76/300]       | Train: Loss 0.69589, R2 0.80543, RMSE 0.83009                     | Test: Loss 0.78815, R2 0.78392, RMSE 0.88042\n",
      "Epoch [77/300]       | Train: Loss 0.68350, R2 0.80774, RMSE 0.82442                     | Test: Loss 0.70780, R2 0.80918, RMSE 0.83280\n",
      "Epoch [78/300]       | Train: Loss 0.68782, R2 0.80789, RMSE 0.82572                     | Test: Loss 0.70044, R2 0.80994, RMSE 0.82337\n",
      "Epoch [79/300]       | Train: Loss 0.69065, R2 0.80789, RMSE 0.82732                     | Test: Loss 0.70894, R2 0.80141, RMSE 0.83574\n",
      "Epoch [80/300]       | Train: Loss 0.67356, R2 0.81160, RMSE 0.81768                     | Test: Loss 0.82554, R2 0.77661, RMSE 0.90114\n",
      "Epoch [81/300]       | Train: Loss 0.67995, R2 0.80931, RMSE 0.82105                     | Test: Loss 0.76818, R2 0.79702, RMSE 0.86465\n",
      "Epoch [82/300]       | Train: Loss 0.68595, R2 0.80852, RMSE 0.82496                     | Test: Loss 0.74683, R2 0.79647, RMSE 0.85666\n",
      "Epoch [83/300]       | Train: Loss 0.68582, R2 0.80784, RMSE 0.82550                     | Test: Loss 0.74355, R2 0.78836, RMSE 0.85320\n",
      "Epoch [84/300]       | Train: Loss 0.66235, R2 0.81387, RMSE 0.81085                     | Test: Loss 0.71739, R2 0.79425, RMSE 0.83942\n",
      "Epoch [85/300]       | Train: Loss 0.66758, R2 0.81310, RMSE 0.81423                     | Test: Loss 0.72237, R2 0.79632, RMSE 0.84304\n",
      "Epoch [86/300]       | Train: Loss 0.68522, R2 0.80900, RMSE 0.82494                     | Test: Loss 0.74084, R2 0.79865, RMSE 0.85458\n",
      "Epoch [87/300]       | Train: Loss 0.66847, R2 0.81280, RMSE 0.81409                     | Test: Loss 0.71521, R2 0.79896, RMSE 0.83921\n",
      "Epoch [88/300]       | Train: Loss 0.65697, R2 0.81435, RMSE 0.80850                     | Test: Loss 0.69991, R2 0.80994, RMSE 0.82797\n",
      "Epoch [89/300]       | Train: Loss 0.66032, R2 0.81470, RMSE 0.80986                     | Test: Loss 0.74181, R2 0.79773, RMSE 0.85107\n",
      "Epoch [90/300]       | Train: Loss 0.64875, R2 0.81730, RMSE 0.80279                     | Test: Loss 0.73865, R2 0.79546, RMSE 0.85382\n",
      "Epoch [91/300]       | Train: Loss 0.64690, R2 0.81790, RMSE 0.80153                     | Test: Loss 0.74454, R2 0.78641, RMSE 0.85544\n",
      "Epoch [92/300]       | Train: Loss 0.63628, R2 0.82043, RMSE 0.79548                     | Test: Loss 0.71451, R2 0.80691, RMSE 0.83546\n",
      "Epoch [93/300]       | Train: Loss 0.66136, R2 0.81417, RMSE 0.81060                     | Test: Loss 0.96775, R2 0.68587, RMSE 0.92963\n",
      "Epoch [94/300]       | Train: Loss 0.64398, R2 0.81972, RMSE 0.79879                     | Test: Loss 0.71043, R2 0.79712, RMSE 0.83366\n",
      "Epoch [95/300]       | Train: Loss 0.64946, R2 0.81847, RMSE 0.80223                     | Test: Loss 0.77872, R2 0.79518, RMSE 0.87349\n",
      "Epoch [96/300]       | Train: Loss 0.64467, R2 0.82075, RMSE 0.79895                     | Test: Loss 0.69752, R2 0.80604, RMSE 0.82544\n",
      "Epoch [97/300]       | Train: Loss 0.62575, R2 0.82420, RMSE 0.78806                     | Test: Loss 1.10692, R2 0.76749, RMSE 0.95218\n",
      "Epoch [98/300]       | Train: Loss 0.63593, R2 0.82070, RMSE 0.79501                     | Test: Loss 0.83108, R2 0.76930, RMSE 0.90560\n",
      "Epoch [99/300]       | Train: Loss 0.65216, R2 0.81780, RMSE 0.80508                     | Test: Loss 0.69856, R2 0.79773, RMSE 0.83004\n",
      "Epoch [100/300]      | Train: Loss 0.63059, R2 0.82352, RMSE 0.79138                     | Test: Loss 0.70766, R2 0.80043, RMSE 0.83166\n",
      "Epoch [101/300]      | Train: Loss 0.62288, R2 0.82504, RMSE 0.78639                     | Test: Loss 0.71068, R2 0.79945, RMSE 0.82926\n",
      "Epoch [102/300]      | Train: Loss 0.60993, R2 0.82908, RMSE 0.77872                     | Test: Loss 0.83436, R2 0.78444, RMSE 0.90159\n",
      "Epoch [103/300]      | Train: Loss 0.62518, R2 0.82312, RMSE 0.78725                     | Test: Loss 0.70222, R2 0.79803, RMSE 0.82773\n",
      "Epoch [104/300]      | Train: Loss 0.61207, R2 0.82767, RMSE 0.77913                     | Test: Loss 0.70849, R2 0.80125, RMSE 0.83215\n",
      "Epoch [105/300]      | Train: Loss 0.61309, R2 0.82704, RMSE 0.77957                     | Test: Loss 0.69807, R2 0.80549, RMSE 0.82795\n",
      "Epoch [106/300]      | Train: Loss 0.61649, R2 0.82724, RMSE 0.78209                     | Test: Loss 0.74837, R2 0.80690, RMSE 0.85813\n",
      "Epoch [107/300]      | Train: Loss 0.60674, R2 0.82862, RMSE 0.77666                     | Test: Loss 0.67350, R2 0.80861, RMSE 0.80839\n",
      "Epoch [108/300]      | Train: Loss 0.59398, R2 0.83400, RMSE 0.76796                     | Test: Loss 0.75005, R2 0.80382, RMSE 0.85834\n",
      "Epoch [109/300]      | Train: Loss 0.59777, R2 0.83256, RMSE 0.77025                     | Test: Loss 0.76294, R2 0.78817, RMSE 0.86726\n",
      "Epoch [110/300]      | Train: Loss 0.59956, R2 0.83206, RMSE 0.77166                     | Test: Loss 0.70847, R2 0.79999, RMSE 0.83000\n",
      "Epoch [111/300]      | Train: Loss 0.60618, R2 0.82945, RMSE 0.77616                     | Test: Loss 0.72140, R2 0.80513, RMSE 0.84016\n",
      "Epoch [112/300]      | Train: Loss 0.59106, R2 0.83443, RMSE 0.76705                     | Test: Loss 1.01322, R2 0.78156, RMSE 0.92315\n",
      "Epoch [113/300]      | Train: Loss 0.59194, R2 0.83323, RMSE 0.76667                     | Test: Loss 0.75693, R2 0.79269, RMSE 0.86549\n",
      "Epoch [114/300]      | Train: Loss 0.58603, R2 0.83429, RMSE 0.76312                     | Test: Loss 0.71878, R2 0.79916, RMSE 0.84294\n",
      "Epoch [115/300]      | Train: Loss 0.59431, R2 0.83325, RMSE 0.76846                     | Test: Loss 0.68833, R2 0.81287, RMSE 0.81603\n",
      "Epoch [116/300]      | Train: Loss 0.58380, R2 0.83544, RMSE 0.76201                     | Test: Loss 0.76101, R2 0.78864, RMSE 0.86356\n",
      "Epoch [117/300]      | Train: Loss 0.57478, R2 0.83793, RMSE 0.75545                     | Test: Loss 0.71258, R2 0.80678, RMSE 0.83638\n",
      "Epoch [118/300]      | Train: Loss 0.57452, R2 0.83810, RMSE 0.75507                     | Test: Loss 0.76120, R2 0.77990, RMSE 0.85997\n",
      "Epoch [119/300]      | Train: Loss 0.58740, R2 0.83566, RMSE 0.76312                     | Test: Loss 0.74836, R2 0.80173, RMSE 0.85603\n",
      "Epoch [120/300]      | Train: Loss 0.57734, R2 0.83493, RMSE 0.75745                     | Test: Loss 0.85595, R2 0.79037, RMSE 0.90677\n",
      "Epoch [121/300]      | Train: Loss 0.58306, R2 0.83715, RMSE 0.76080                     | Test: Loss 0.79598, R2 0.79500, RMSE 0.88176\n",
      "Epoch [122/300]      | Train: Loss 0.58370, R2 0.83578, RMSE 0.76171                     | Test: Loss 0.71828, R2 0.80357, RMSE 0.84170\n",
      "Epoch [123/300]      | Train: Loss 0.57714, R2 0.83864, RMSE 0.75650                     | Test: Loss 0.82187, R2 0.80075, RMSE 0.88324\n",
      "Epoch [124/300]      | Train: Loss 0.57076, R2 0.83881, RMSE 0.75316                     | Test: Loss 0.71926, R2 0.80437, RMSE 0.84120\n",
      "Epoch [125/300]      | Train: Loss 0.58122, R2 0.83590, RMSE 0.75985                     | Test: Loss 0.73738, R2 0.80291, RMSE 0.85255\n",
      "Epoch [126/300]      | Train: Loss 0.56054, R2 0.84334, RMSE 0.74614                     | Test: Loss 0.70301, R2 0.80570, RMSE 0.83161\n",
      "Epoch [127/300]      | Train: Loss 0.56496, R2 0.84086, RMSE 0.75004                     | Test: Loss 0.69747, R2 0.80495, RMSE 0.82682\n",
      "Epoch [128/300]      | Train: Loss 0.55816, R2 0.84296, RMSE 0.74396                     | Test: Loss 0.83817, R2 0.74514, RMSE 0.89694\n",
      "Epoch [129/300]      | Train: Loss 0.56148, R2 0.84347, RMSE 0.74719                     | Test: Loss 0.73275, R2 0.80090, RMSE 0.85022\n",
      "Epoch [130/300]      | Train: Loss 0.55714, R2 0.84316, RMSE 0.74413                     | Test: Loss 0.71769, R2 0.81192, RMSE 0.84268\n",
      "Epoch [131/300]      | Train: Loss 0.55044, R2 0.84607, RMSE 0.73957                     | Test: Loss 0.68912, R2 0.81438, RMSE 0.81745\n",
      "Epoch [132/300]      | Train: Loss 0.55496, R2 0.84447, RMSE 0.74231                     | Test: Loss 0.71888, R2 0.80778, RMSE 0.83986\n",
      "Epoch [133/300]      | Train: Loss 0.55901, R2 0.84281, RMSE 0.74474                     | Test: Loss 0.78176, R2 0.79888, RMSE 0.87380\n",
      "Epoch [134/300]      | Train: Loss 0.56338, R2 0.84198, RMSE 0.74734                     | Test: Loss 0.73306, R2 0.80155, RMSE 0.84841\n",
      "Epoch [135/300]      | Train: Loss 0.54946, R2 0.84647, RMSE 0.73875                     | Test: Loss 0.67883, R2 0.81549, RMSE 0.80880\n",
      "Epoch [136/300]      | Train: Loss 0.55282, R2 0.84386, RMSE 0.74097                     | Test: Loss 0.71362, R2 0.80579, RMSE 0.83910\n",
      "Epoch [137/300]      | Train: Loss 0.53213, R2 0.84983, RMSE 0.72726                     | Test: Loss 0.72728, R2 0.80740, RMSE 0.84582\n",
      "Epoch [138/300]      | Train: Loss 0.54067, R2 0.84907, RMSE 0.73324                     | Test: Loss 0.74380, R2 0.79743, RMSE 0.85606\n",
      "Epoch [139/300]      | Train: Loss 0.53409, R2 0.85065, RMSE 0.72813                     | Test: Loss 0.72749, R2 0.79745, RMSE 0.84624\n",
      "Epoch [140/300]      | Train: Loss 0.52688, R2 0.85118, RMSE 0.72392                     | Test: Loss 0.68986, R2 0.81004, RMSE 0.82221\n",
      "Epoch [141/300]      | Train: Loss 0.52425, R2 0.85312, RMSE 0.72217                     | Test: Loss 0.68343, R2 0.81402, RMSE 0.81696\n",
      "Epoch [142/300]      | Train: Loss 0.52373, R2 0.85385, RMSE 0.72164                     | Test: Loss 0.69837, R2 0.79511, RMSE 0.82728\n",
      "Epoch [143/300]      | Train: Loss 0.53277, R2 0.84989, RMSE 0.72755                     | Test: Loss 0.74015, R2 0.80655, RMSE 0.85536\n",
      "Epoch [144/300]      | Train: Loss 0.52906, R2 0.85244, RMSE 0.72532                     | Test: Loss 0.77710, R2 0.79436, RMSE 0.87053\n",
      "Epoch [145/300]      | Train: Loss 0.53249, R2 0.85009, RMSE 0.72756                     | Test: Loss 0.72050, R2 0.80005, RMSE 0.83710\n",
      "Epoch [146/300]      | Train: Loss 0.51910, R2 0.85390, RMSE 0.71879                     | Test: Loss 0.70215, R2 0.80941, RMSE 0.82581\n",
      "Epoch [147/300]      | Train: Loss 0.51029, R2 0.85651, RMSE 0.71254                     | Test: Loss 0.70191, R2 0.79640, RMSE 0.82944\n",
      "Epoch [148/300]      | Train: Loss 0.51302, R2 0.85427, RMSE 0.71414                     | Test: Loss 0.72481, R2 0.80500, RMSE 0.84116\n",
      "Epoch [149/300]      | Train: Loss 0.51684, R2 0.85507, RMSE 0.71646                     | Test: Loss 0.78061, R2 0.78448, RMSE 0.87237\n",
      "Epoch [150/300]      | Train: Loss 0.50459, R2 0.85697, RMSE 0.70821                     | Test: Loss 0.73798, R2 0.80816, RMSE 0.84810\n",
      "Epoch [151/300]      | Train: Loss 0.51589, R2 0.85435, RMSE 0.71630                     | Test: Loss 0.77859, R2 0.77656, RMSE 0.86875\n",
      "Epoch [152/300]      | Train: Loss 0.51013, R2 0.85662, RMSE 0.71260                     | Test: Loss 0.72233, R2 0.80267, RMSE 0.84630\n",
      "Epoch [153/300]      | Train: Loss 0.49867, R2 0.85935, RMSE 0.70424                     | Test: Loss 0.74415, R2 0.78268, RMSE 0.85629\n",
      "Epoch [154/300]      | Train: Loss 0.50124, R2 0.85914, RMSE 0.70547                     | Test: Loss 0.83694, R2 0.76326, RMSE 0.90389\n",
      "Epoch [155/300]      | Train: Loss 0.50932, R2 0.85633, RMSE 0.71184                     | Test: Loss 0.81775, R2 0.78595, RMSE 0.89479\n",
      "Epoch [156/300]      | Train: Loss 0.49748, R2 0.85995, RMSE 0.70325                     | Test: Loss 0.70508, R2 0.80596, RMSE 0.82887\n",
      "Epoch [157/300]      | Train: Loss 0.49848, R2 0.85912, RMSE 0.70456                     | Test: Loss 0.74918, R2 0.79324, RMSE 0.85524\n",
      "Epoch [158/300]      | Train: Loss 0.49151, R2 0.86091, RMSE 0.69893                     | Test: Loss 0.72890, R2 0.79228, RMSE 0.84821\n",
      "Epoch [159/300]      | Train: Loss 0.48450, R2 0.86374, RMSE 0.69340                     | Test: Loss 0.77966, R2 0.79342, RMSE 0.87543\n",
      "Epoch [160/300]      | Train: Loss 0.49002, R2 0.86227, RMSE 0.69859                     | Test: Loss 1.10298, R2 0.76023, RMSE 0.95425\n",
      "Epoch [161/300]      | Train: Loss 0.48277, R2 0.86372, RMSE 0.69302                     | Test: Loss 0.79837, R2 0.77539, RMSE 0.88573\n",
      "Epoch [162/300]      | Train: Loss 0.49364, R2 0.86088, RMSE 0.70028                     | Test: Loss 0.73668, R2 0.79847, RMSE 0.85132\n",
      "Epoch [163/300]      | Train: Loss 0.48966, R2 0.86240, RMSE 0.69818                     | Test: Loss 0.68807, R2 0.80166, RMSE 0.81883\n",
      "Epoch [164/300]      | Train: Loss 0.48095, R2 0.86370, RMSE 0.69117                     | Test: Loss 0.71779, R2 0.79977, RMSE 0.83806\n",
      "Epoch [165/300]      | Train: Loss 0.47388, R2 0.86661, RMSE 0.68601                     | Test: Loss 0.68488, R2 0.81174, RMSE 0.81596\n",
      "Epoch [166/300]      | Train: Loss 0.47798, R2 0.86523, RMSE 0.68969                     | Test: Loss 0.91059, R2 0.68365, RMSE 0.91624\n",
      "Epoch [167/300]      | Train: Loss 0.48148, R2 0.86336, RMSE 0.69254                     | Test: Loss 0.72492, R2 0.80330, RMSE 0.84330\n",
      "Epoch [168/300]      | Train: Loss 0.47203, R2 0.86804, RMSE 0.68572                     | Test: Loss 0.77116, R2 0.79639, RMSE 0.87039\n",
      "Epoch [169/300]      | Train: Loss 0.48104, R2 0.86442, RMSE 0.69205                     | Test: Loss 0.75682, R2 0.78447, RMSE 0.86166\n",
      "Epoch [170/300]      | Train: Loss 0.48127, R2 0.86364, RMSE 0.69197                     | Test: Loss 0.71993, R2 0.79796, RMSE 0.83758\n",
      "Epoch [171/300]      | Train: Loss 0.47256, R2 0.86679, RMSE 0.68520                     | Test: Loss 0.72077, R2 0.80228, RMSE 0.83928\n",
      "Epoch [172/300]      | Train: Loss 0.47354, R2 0.86713, RMSE 0.68568                     | Test: Loss 0.72474, R2 0.80102, RMSE 0.83849\n",
      "Epoch [173/300]      | Train: Loss 0.47026, R2 0.86823, RMSE 0.68355                     | Test: Loss 0.71882, R2 0.80182, RMSE 0.84514\n",
      "Epoch [174/300]      | Train: Loss 0.45409, R2 0.87230, RMSE 0.67129                     | Test: Loss 0.76794, R2 0.77893, RMSE 0.86854\n",
      "Epoch [175/300]      | Train: Loss 0.46061, R2 0.86886, RMSE 0.67686                     | Test: Loss 0.73377, R2 0.79082, RMSE 0.85094\n",
      "Epoch [176/300]      | Train: Loss 0.45222, R2 0.87349, RMSE 0.67097                     | Test: Loss 0.71539, R2 0.80377, RMSE 0.84231\n",
      "Epoch [177/300]      | Train: Loss 0.45267, R2 0.87307, RMSE 0.67041                     | Test: Loss 0.80287, R2 0.77508, RMSE 0.88723\n",
      "Epoch [178/300]      | Train: Loss 0.46004, R2 0.86913, RMSE 0.67633                     | Test: Loss 0.73781, R2 0.79661, RMSE 0.84892\n",
      "Epoch [179/300]      | Train: Loss 0.46634, R2 0.86893, RMSE 0.68026                     | Test: Loss 0.74432, R2 0.79762, RMSE 0.85227\n",
      "Epoch [180/300]      | Train: Loss 0.47681, R2 0.86650, RMSE 0.68750                     | Test: Loss 0.86854, R2 0.71177, RMSE 0.90677\n",
      "Epoch [181/300]      | Train: Loss 0.46540, R2 0.86969, RMSE 0.68026                     | Test: Loss 0.76421, R2 0.78839, RMSE 0.86749\n",
      "Epoch [182/300]      | Train: Loss 0.44947, R2 0.87387, RMSE 0.66835                     | Test: Loss 0.71542, R2 0.81026, RMSE 0.83114\n",
      "Epoch [183/300]      | Train: Loss 0.44831, R2 0.87353, RMSE 0.66793                     | Test: Loss 0.73240, R2 0.79664, RMSE 0.84616\n",
      "Epoch [184/300]      | Train: Loss 0.45570, R2 0.87240, RMSE 0.67328                     | Test: Loss 0.73182, R2 0.79979, RMSE 0.84642\n",
      "Epoch [185/300]      | Train: Loss 0.45397, R2 0.87289, RMSE 0.67172                     | Test: Loss 0.72957, R2 0.80050, RMSE 0.84714\n",
      "Epoch [186/300]      | Train: Loss 0.45315, R2 0.87300, RMSE 0.67125                     | Test: Loss 0.76710, R2 0.78447, RMSE 0.86986\n",
      "Epoch [187/300]      | Train: Loss 0.44342, R2 0.87458, RMSE 0.66381                     | Test: Loss 0.74410, R2 0.79431, RMSE 0.85508\n",
      "Epoch [188/300]      | Train: Loss 0.43980, R2 0.87650, RMSE 0.66095                     | Test: Loss 0.83609, R2 0.77708, RMSE 0.90239\n",
      "Epoch [189/300]      | Train: Loss 0.44742, R2 0.87219, RMSE 0.66711                     | Test: Loss 0.84758, R2 0.77757, RMSE 0.91032\n",
      "Epoch [190/300]      | Train: Loss 0.44493, R2 0.87429, RMSE 0.66503                     | Test: Loss 0.79213, R2 0.76082, RMSE 0.88009\n",
      "Epoch [191/300]      | Train: Loss 0.42066, R2 0.88100, RMSE 0.64664                     | Test: Loss 0.74418, R2 0.79315, RMSE 0.85488\n",
      "Epoch [192/300]      | Train: Loss 0.43457, R2 0.87823, RMSE 0.65684                     | Test: Loss 0.80312, R2 0.79806, RMSE 0.88360\n",
      "Epoch [193/300]      | Train: Loss 0.42420, R2 0.87998, RMSE 0.64951                     | Test: Loss 0.73548, R2 0.79061, RMSE 0.85091\n",
      "Epoch [194/300]      | Train: Loss 0.42693, R2 0.87983, RMSE 0.65152                     | Test: Loss 0.73830, R2 0.79047, RMSE 0.85039\n",
      "Epoch [195/300]      | Train: Loss 0.43068, R2 0.87867, RMSE 0.65415                     | Test: Loss 0.77104, R2 0.79841, RMSE 0.87392\n",
      "Epoch [196/300]      | Train: Loss 0.44160, R2 0.87461, RMSE 0.66270                     | Test: Loss 0.74054, R2 0.79335, RMSE 0.85520\n",
      "Epoch [197/300]      | Train: Loss 0.42894, R2 0.88003, RMSE 0.65289                     | Test: Loss 0.73428, R2 0.79924, RMSE 0.84353\n",
      "Epoch [198/300]      | Train: Loss 0.42994, R2 0.87825, RMSE 0.65388                     | Test: Loss 0.75584, R2 0.79649, RMSE 0.86059\n",
      "Epoch [199/300]      | Train: Loss 0.42124, R2 0.88151, RMSE 0.64742                     | Test: Loss 0.72723, R2 0.79872, RMSE 0.84626\n",
      "Epoch [200/300]      | Train: Loss 0.43437, R2 0.87819, RMSE 0.65763                     | Test: Loss 0.74549, R2 0.79631, RMSE 0.85155\n",
      "Epoch [201/300]      | Train: Loss 0.41858, R2 0.88174, RMSE 0.64547                     | Test: Loss 0.74932, R2 0.78980, RMSE 0.85672\n",
      "Epoch [202/300]      | Train: Loss 0.42541, R2 0.87941, RMSE 0.65071                     | Test: Loss 0.80647, R2 0.71632, RMSE 0.88981\n",
      "Epoch [203/300]      | Train: Loss 0.41572, R2 0.88328, RMSE 0.64325                     | Test: Loss 0.75253, R2 0.79968, RMSE 0.86276\n",
      "Epoch [204/300]      | Train: Loss 0.41981, R2 0.88252, RMSE 0.64643                     | Test: Loss 0.83542, R2 0.78045, RMSE 0.89493\n",
      "Epoch [205/300]      | Train: Loss 0.41720, R2 0.88194, RMSE 0.64450                     | Test: Loss 0.75574, R2 0.79713, RMSE 0.86562\n",
      "Epoch [206/300]      | Train: Loss 0.41170, R2 0.88366, RMSE 0.64013                     | Test: Loss 0.79362, R2 0.78675, RMSE 0.88477\n",
      "Epoch [207/300]      | Train: Loss 0.40181, R2 0.88669, RMSE 0.63246                     | Test: Loss 0.75031, R2 0.79628, RMSE 0.85908\n",
      "Epoch [208/300]      | Train: Loss 0.40416, R2 0.88630, RMSE 0.63440                     | Test: Loss 0.79013, R2 0.78426, RMSE 0.87793\n",
      "Epoch [209/300]      | Train: Loss 0.40994, R2 0.88491, RMSE 0.63876                     | Test: Loss 0.74346, R2 0.76802, RMSE 0.85565\n",
      "Epoch [210/300]      | Train: Loss 0.39915, R2 0.88737, RMSE 0.63052                     | Test: Loss 0.75801, R2 0.79042, RMSE 0.86234\n",
      "Epoch [211/300]      | Train: Loss 0.40530, R2 0.88503, RMSE 0.63548                     | Test: Loss 0.75637, R2 0.78841, RMSE 0.86242\n",
      "Epoch [212/300]      | Train: Loss 0.39959, R2 0.88820, RMSE 0.63077                     | Test: Loss 0.74234, R2 0.79501, RMSE 0.85584\n",
      "Epoch [213/300]      | Train: Loss 0.40918, R2 0.88502, RMSE 0.63816                     | Test: Loss 0.76161, R2 0.79500, RMSE 0.86247\n",
      "Epoch [214/300]      | Train: Loss 0.39876, R2 0.88784, RMSE 0.63046                     | Test: Loss 0.76399, R2 0.78977, RMSE 0.86785\n",
      "Epoch [215/300]      | Train: Loss 0.40896, R2 0.88510, RMSE 0.63768                     | Test: Loss 0.85108, R2 0.76480, RMSE 0.90897\n",
      "Epoch [216/300]      | Train: Loss 0.41097, R2 0.88459, RMSE 0.63945                     | Test: Loss 0.76175, R2 0.79648, RMSE 0.86805\n",
      "Epoch [217/300]      | Train: Loss 0.41189, R2 0.88318, RMSE 0.63915                     | Test: Loss 0.82853, R2 0.76967, RMSE 0.90621\n",
      "Epoch [218/300]      | Train: Loss 0.38835, R2 0.89022, RMSE 0.62159                     | Test: Loss 0.77174, R2 0.78869, RMSE 0.85967\n",
      "Epoch [219/300]      | Train: Loss 0.39403, R2 0.88947, RMSE 0.62646                     | Test: Loss 0.74431, R2 0.78888, RMSE 0.85083\n",
      "Epoch [220/300]      | Train: Loss 0.38583, R2 0.89160, RMSE 0.61982                     | Test: Loss 0.81702, R2 0.77575, RMSE 0.89662\n",
      "Epoch [221/300]      | Train: Loss 0.39274, R2 0.88945, RMSE 0.62509                     | Test: Loss 0.94528, R2 0.74993, RMSE 0.94464\n",
      "Epoch [222/300]      | Train: Loss 0.40822, R2 0.88439, RMSE 0.63764                     | Test: Loss 0.76016, R2 0.78698, RMSE 0.86597\n",
      "Epoch [223/300]      | Train: Loss 0.40618, R2 0.88586, RMSE 0.63621                     | Test: Loss 0.77748, R2 0.76752, RMSE 0.87253\n",
      "Epoch [224/300]      | Train: Loss 0.38464, R2 0.89094, RMSE 0.61919                     | Test: Loss 0.74022, R2 0.79981, RMSE 0.85275\n",
      "Epoch [225/300]      | Train: Loss 0.37558, R2 0.89430, RMSE 0.61143                     | Test: Loss 1.02978, R2 0.77007, RMSE 0.96340\n",
      "Epoch [226/300]      | Train: Loss 0.38284, R2 0.89148, RMSE 0.61728                     | Test: Loss 0.74620, R2 0.79239, RMSE 0.85631\n",
      "Epoch [227/300]      | Train: Loss 0.38461, R2 0.89159, RMSE 0.61873                     | Test: Loss 0.80582, R2 0.76811, RMSE 0.89012\n",
      "Epoch [228/300]      | Train: Loss 0.37582, R2 0.89436, RMSE 0.61181                     | Test: Loss 0.79813, R2 0.77290, RMSE 0.88626\n",
      "Epoch [229/300]      | Train: Loss 0.38767, R2 0.89033, RMSE 0.62114                     | Test: Loss 0.73632, R2 0.80478, RMSE 0.84430\n",
      "Epoch [230/300]      | Train: Loss 0.37499, R2 0.89416, RMSE 0.61115                     | Test: Loss 0.82450, R2 0.76860, RMSE 0.89835\n",
      "Epoch [231/300]      | Train: Loss 0.38398, R2 0.89112, RMSE 0.61797                     | Test: Loss 0.73873, R2 0.79922, RMSE 0.84858\n",
      "Epoch [232/300]      | Train: Loss 0.37473, R2 0.89228, RMSE 0.61082                     | Test: Loss 0.80599, R2 0.77037, RMSE 0.89351\n",
      "Epoch [233/300]      | Train: Loss 0.36701, R2 0.89684, RMSE 0.60386                     | Test: Loss 0.77461, R2 0.78251, RMSE 0.86602\n",
      "Epoch [234/300]      | Train: Loss 0.37376, R2 0.89556, RMSE 0.60998                     | Test: Loss 0.75986, R2 0.79005, RMSE 0.86493\n",
      "Epoch [235/300]      | Train: Loss 0.36964, R2 0.89518, RMSE 0.60661                     | Test: Loss 0.74328, R2 0.78523, RMSE 0.85641\n",
      "Epoch [236/300]      | Train: Loss 0.37236, R2 0.89439, RMSE 0.60917                     | Test: Loss 0.79020, R2 0.78760, RMSE 0.87909\n",
      "Epoch [237/300]      | Train: Loss 0.36950, R2 0.89574, RMSE 0.60617                     | Test: Loss 0.76012, R2 0.79518, RMSE 0.86398\n",
      "Epoch [238/300]      | Train: Loss 0.36625, R2 0.89647, RMSE 0.60373                     | Test: Loss 0.77219, R2 0.78153, RMSE 0.86985\n",
      "Epoch [239/300]      | Train: Loss 0.36200, R2 0.89768, RMSE 0.60038                     | Test: Loss 0.75252, R2 0.79774, RMSE 0.85146\n",
      "Epoch [240/300]      | Train: Loss 0.36825, R2 0.89545, RMSE 0.60555                     | Test: Loss 0.85935, R2 0.78887, RMSE 0.90859\n",
      "Epoch [241/300]      | Train: Loss 0.35774, R2 0.89896, RMSE 0.59716                     | Test: Loss 0.90020, R2 0.75770, RMSE 0.93090\n",
      "Epoch [242/300]      | Train: Loss 0.35848, R2 0.89805, RMSE 0.59773                     | Test: Loss 0.74669, R2 0.79252, RMSE 0.85540\n",
      "Epoch [243/300]      | Train: Loss 0.36172, R2 0.89778, RMSE 0.60005                     | Test: Loss 0.84935, R2 0.77576, RMSE 0.90851\n",
      "Epoch [244/300]      | Train: Loss 0.35808, R2 0.89851, RMSE 0.59693                     | Test: Loss 0.75443, R2 0.80048, RMSE 0.85749\n",
      "Epoch [245/300]      | Train: Loss 0.36415, R2 0.89714, RMSE 0.60234                     | Test: Loss 0.73943, R2 0.79567, RMSE 0.84876\n",
      "Epoch [246/300]      | Train: Loss 0.36746, R2 0.89583, RMSE 0.60427                     | Test: Loss 0.81376, R2 0.78196, RMSE 0.89363\n",
      "Epoch [247/300]      | Train: Loss 0.35134, R2 0.90131, RMSE 0.59168                     | Test: Loss 0.80017, R2 0.78174, RMSE 0.88969\n",
      "Epoch [248/300]      | Train: Loss 0.34233, R2 0.90322, RMSE 0.58397                     | Test: Loss 0.78227, R2 0.78412, RMSE 0.87101\n",
      "Epoch [249/300]      | Train: Loss 0.36002, R2 0.89842, RMSE 0.59880                     | Test: Loss 0.77083, R2 0.78500, RMSE 0.87248\n",
      "Epoch [250/300]      | Train: Loss 0.34794, R2 0.90228, RMSE 0.58883                     | Test: Loss 0.81285, R2 0.78482, RMSE 0.89276\n",
      "Epoch [251/300]      | Train: Loss 0.34899, R2 0.90095, RMSE 0.58957                     | Test: Loss 0.76867, R2 0.78888, RMSE 0.86825\n",
      "Epoch [252/300]      | Train: Loss 0.35141, R2 0.90149, RMSE 0.59177                     | Test: Loss 1.17609, R2 0.75415, RMSE 0.98645\n",
      "Epoch [253/300]      | Train: Loss 0.34912, R2 0.90079, RMSE 0.58990                     | Test: Loss 0.86836, R2 0.76993, RMSE 0.91993\n",
      "Epoch [254/300]      | Train: Loss 0.35183, R2 0.90117, RMSE 0.59215                     | Test: Loss 0.75007, R2 0.78935, RMSE 0.85888\n",
      "Epoch [255/300]      | Train: Loss 0.35104, R2 0.90183, RMSE 0.59104                     | Test: Loss 0.76247, R2 0.79110, RMSE 0.85397\n",
      "Epoch [256/300]      | Train: Loss 0.34334, R2 0.90367, RMSE 0.58486                     | Test: Loss 0.77311, R2 0.78566, RMSE 0.86791\n",
      "Epoch [257/300]      | Train: Loss 0.34384, R2 0.90234, RMSE 0.58475                     | Test: Loss 0.81498, R2 0.76015, RMSE 0.89512\n",
      "Epoch [258/300]      | Train: Loss 0.33850, R2 0.90379, RMSE 0.58024                     | Test: Loss 0.75612, R2 0.79298, RMSE 0.85434\n",
      "Epoch [259/300]      | Train: Loss 0.34093, R2 0.90384, RMSE 0.58283                     | Test: Loss 0.74460, R2 0.79754, RMSE 0.84969\n",
      "Epoch [260/300]      | Train: Loss 0.34286, R2 0.90315, RMSE 0.58421                     | Test: Loss 0.74947, R2 0.79431, RMSE 0.85494\n",
      "Epoch [261/300]      | Train: Loss 0.33379, R2 0.90549, RMSE 0.57656                     | Test: Loss 0.82758, R2 0.75708, RMSE 0.90025\n",
      "Epoch [262/300]      | Train: Loss 0.33068, R2 0.90666, RMSE 0.57393                     | Test: Loss 0.76129, R2 0.78454, RMSE 0.86260\n",
      "Epoch [263/300]      | Train: Loss 0.33684, R2 0.90557, RMSE 0.57915                     | Test: Loss 0.80830, R2 0.78630, RMSE 0.88905\n",
      "Epoch [264/300]      | Train: Loss 0.34265, R2 0.90293, RMSE 0.58419                     | Test: Loss 0.79510, R2 0.77997, RMSE 0.88515\n",
      "Epoch [265/300]      | Train: Loss 0.34670, R2 0.90233, RMSE 0.58768                     | Test: Loss 0.87388, R2 0.76092, RMSE 0.92209\n",
      "Epoch [266/300]      | Train: Loss 0.34067, R2 0.90348, RMSE 0.58281                     | Test: Loss 0.73317, R2 0.78861, RMSE 0.84498\n",
      "Epoch [267/300]      | Train: Loss 0.33085, R2 0.90694, RMSE 0.57404                     | Test: Loss 0.76712, R2 0.78017, RMSE 0.86910\n",
      "Epoch [268/300]      | Train: Loss 0.32756, R2 0.90720, RMSE 0.57103                     | Test: Loss 0.83728, R2 0.78067, RMSE 0.90164\n",
      "Epoch [269/300]      | Train: Loss 0.33115, R2 0.90671, RMSE 0.57428                     | Test: Loss 0.85502, R2 0.73930, RMSE 0.90945\n",
      "Epoch [270/300]      | Train: Loss 0.32751, R2 0.90689, RMSE 0.57131                     | Test: Loss 0.77911, R2 0.78027, RMSE 0.87250\n",
      "Epoch [271/300]      | Train: Loss 0.32298, R2 0.90814, RMSE 0.56741                     | Test: Loss 0.75237, R2 0.78467, RMSE 0.85765\n",
      "Epoch [272/300]      | Train: Loss 0.32175, R2 0.90826, RMSE 0.56653                     | Test: Loss 0.78513, R2 0.78994, RMSE 0.87510\n",
      "Epoch [273/300]      | Train: Loss 0.32204, R2 0.90879, RMSE 0.56626                     | Test: Loss 0.78037, R2 0.78292, RMSE 0.87719\n",
      "Epoch [274/300]      | Train: Loss 0.32493, R2 0.90866, RMSE 0.56875                     | Test: Loss 0.76156, R2 0.78642, RMSE 0.86176\n",
      "Epoch [275/300]      | Train: Loss 0.31606, R2 0.90983, RMSE 0.56159                     | Test: Loss 0.82635, R2 0.77795, RMSE 0.89921\n",
      "Epoch [276/300]      | Train: Loss 0.32373, R2 0.90915, RMSE 0.56762                     | Test: Loss 0.78835, R2 0.78386, RMSE 0.87790\n",
      "Epoch [277/300]      | Train: Loss 0.32497, R2 0.90870, RMSE 0.56881                     | Test: Loss 0.78878, R2 0.79238, RMSE 0.87960\n",
      "Epoch [278/300]      | Train: Loss 0.32966, R2 0.90633, RMSE 0.57295                     | Test: Loss 0.76544, R2 0.77980, RMSE 0.87054\n",
      "Epoch [279/300]      | Train: Loss 0.32261, R2 0.90841, RMSE 0.56658                     | Test: Loss 0.79794, R2 0.73261, RMSE 0.88591\n",
      "Epoch [280/300]      | Train: Loss 0.31117, R2 0.91120, RMSE 0.55699                     | Test: Loss 0.81891, R2 0.77980, RMSE 0.89949\n",
      "Epoch [281/300]      | Train: Loss 0.31274, R2 0.91081, RMSE 0.55797                     | Test: Loss 0.77851, R2 0.78417, RMSE 0.87220\n",
      "Epoch [282/300]      | Train: Loss 0.31008, R2 0.91219, RMSE 0.55567                     | Test: Loss 0.80367, R2 0.78299, RMSE 0.88543\n",
      "Epoch [283/300]      | Train: Loss 0.31102, R2 0.91233, RMSE 0.55683                     | Test: Loss 0.76787, R2 0.77831, RMSE 0.86749\n",
      "Epoch [284/300]      | Train: Loss 0.30516, R2 0.91380, RMSE 0.55126                     | Test: Loss 0.77521, R2 0.78054, RMSE 0.86779\n",
      "Epoch [285/300]      | Train: Loss 0.31749, R2 0.91073, RMSE 0.56217                     | Test: Loss 0.77376, R2 0.77602, RMSE 0.87169\n",
      "Epoch [286/300]      | Train: Loss 0.30229, R2 0.91459, RMSE 0.54881                     | Test: Loss 0.79926, R2 0.78332, RMSE 0.88832\n",
      "Epoch [287/300]      | Train: Loss 0.30596, R2 0.91320, RMSE 0.55186                     | Test: Loss 0.79591, R2 0.78623, RMSE 0.88327\n",
      "Epoch [288/300]      | Train: Loss 0.32045, R2 0.90978, RMSE 0.56503                     | Test: Loss 0.91190, R2 0.78331, RMSE 0.92881\n",
      "Epoch [289/300]      | Train: Loss 0.30148, R2 0.91432, RMSE 0.54821                     | Test: Loss 0.84072, R2 0.78529, RMSE 0.90599\n",
      "Epoch [290/300]      | Train: Loss 0.31453, R2 0.91089, RMSE 0.55979                     | Test: Loss 0.81419, R2 0.78579, RMSE 0.89678\n",
      "Epoch [291/300]      | Train: Loss 0.30642, R2 0.91319, RMSE 0.55255                     | Test: Loss 0.84014, R2 0.76841, RMSE 0.90929\n",
      "Epoch [292/300]      | Train: Loss 0.30396, R2 0.91386, RMSE 0.55020                     | Test: Loss 0.76569, R2 0.79811, RMSE 0.86694\n",
      "Epoch [293/300]      | Train: Loss 0.29633, R2 0.91563, RMSE 0.54346                     | Test: Loss 0.82317, R2 0.78517, RMSE 0.89637\n",
      "Epoch [294/300]      | Train: Loss 0.30419, R2 0.91444, RMSE 0.55038                     | Test: Loss 0.78423, R2 0.78497, RMSE 0.87751\n",
      "Epoch [295/300]      | Train: Loss 0.30188, R2 0.91536, RMSE 0.54867                     | Test: Loss 0.84029, R2 0.76797, RMSE 0.90637\n",
      "Epoch [296/300]      | Train: Loss 0.31249, R2 0.91167, RMSE 0.55773                     | Test: Loss 0.86425, R2 0.76841, RMSE 0.91979\n",
      "Epoch [297/300]      | Train: Loss 0.30203, R2 0.91517, RMSE 0.54859                     | Test: Loss 0.82799, R2 0.75969, RMSE 0.90549\n",
      "Epoch [298/300]      | Train: Loss 0.29596, R2 0.91628, RMSE 0.54333                     | Test: Loss 0.78726, R2 0.78673, RMSE 0.86382\n",
      "Epoch [299/300]      | Train: Loss 0.30540, R2 0.91363, RMSE 0.55085                     | Test: Loss 0.78011, R2 0.77998, RMSE 0.87723\n",
      "Epoch [300/300]      | Train: Loss 0.29953, R2 0.91461, RMSE 0.54659                     | Test: Loss 0.82272, R2 0.74853, RMSE 0.90250\n",
      "Best rmse 0.8083908970539386\n",
      "100 epochs of training and evaluation took, 55.28125\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class LossFunction(nn.Module):\n",
    "    def __init__(self, num_tasks):\n",
    "        super(LossFunction, self).__init__()\n",
    "        self.num_tasks = num_tasks\n",
    "\n",
    "        self.loss_fns = [nn.MSELoss() for x in range(num_tasks)] \n",
    "\n",
    "    def forward(self, predictions, labels_task1):\n",
    "\n",
    "        #task 1\n",
    "        target = labels_task1\n",
    "        prediction = predictions\n",
    "        loss_fn = self.loss_fns[0]\n",
    "\n",
    "        task_loss = loss_fn(prediction, target)\n",
    "        \n",
    "        return task_loss\n",
    "    \n",
    "model = CATTransformer(rff_on=True,\n",
    "                       sigma=2,\n",
    "                       embed_size=160,\n",
    "                       heads=5,\n",
    "                       forward_expansion=8,\n",
    "                       pre_norm_on=False,\n",
    "                       mlp_scale_classification=8,\n",
    "                   \n",
    "                       decoder_dropout=0,\n",
    "                       classification_dropout=0.1,\n",
    "                       targets_classes=target_classes,\n",
    "                       n_cont=len(cont_columns),\n",
    "              \n",
    "                       embedding_scheme='log-linear_periodic',\n",
    "                       trainable=False,\n",
    "                       linear_on=True).to(device_in_use)\n",
    "\n",
    "def format_metric(value): # Used to format the metrics output\n",
    "    return f\"{value:.5f}\"\n",
    "\n",
    "loss_functions = LossFunction(1)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr = 0.0001) # Maybe try messing around with optimizers. try other torch optimizers with different configurations.\n",
    "epochs = 500 #Set the number of epochs\n",
    "\n",
    "train_losses = []\n",
    "train_rmse = [] \n",
    "test_losses = []\n",
    "test_rmse = []  \n",
    "\n",
    "\n",
    "#Time will be recorded for all 100 epochs - This means the results will not be comparable to Xgboost but that is ok, we will only compare between transformer models who will also train for 100 epochs\n",
    "start_time = time.process_time()\n",
    "\n",
    "for t in range(epochs):\n",
    "    train_loss, r2_train, rmse_train = train(train_dataloader, model, loss_functions, optimizer, device_in_use=device_in_use)\n",
    "    test_loss, r2_test, rmse_test = test(test_dataloader, model, loss_functions, device_in_use=device_in_use)\n",
    "    train_losses.append(train_loss)\n",
    "    train_rmse.append(rmse_train)\n",
    "    # train_accuracies_2.append(train_accuracy_2)\n",
    "    # train_recalls.append(train_recall) \n",
    "    # train_f1_scores.append(train_f1)\n",
    "    test_losses.append(test_loss)\n",
    "    test_rmse.append(rmse_test)\n",
    "    # test_accuracies_2.append(test_accuracy_2)\n",
    "    # test_recalls.append(test_recall)\n",
    "    # test_f1_scores.append(test_f1)\n",
    "    # Formatting for easier reading\n",
    "    epoch_str = f\"Epoch [{t+1:2}/{epochs}]\"\n",
    "    train_metrics = f\"Train: Loss {format_metric(train_loss)}, R2 {format_metric(r2_train)}, RMSE {format_metric(rmse_train)}\"\n",
    "    test_metrics = f\"Test: Loss {format_metric(test_loss)}, R2 {format_metric(r2_test)}, RMSE {format_metric(rmse_test)}\"\n",
    "    print(f\"{epoch_str:20} | {train_metrics:65} | {test_metrics}\")\n",
    "\n",
    "total_time = time.process_time() - start_time\n",
    "\n",
    "# Plotting the loss curves\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, epochs+1), train_losses, label='Train Loss')\n",
    "plt.plot(range(1, epochs+1), [l for l in test_losses], label='Test Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Test Loss Curve')\n",
    "plt.legend()\n",
    "\n",
    "best_index = test_rmse.index(min(test_rmse))\n",
    "print(f\"Best rmse {test_rmse[best_index]}\")\n",
    "print(f\"100 epochs of training and evaluation took, {total_time}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
