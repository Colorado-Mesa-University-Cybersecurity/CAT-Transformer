{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\smbm2\\AppData\\Local\\miniconda3\\envs\\ml-env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from rff.layers import GaussianEncoding #pip install random-fourier-features-pytorch\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import os\n",
    "import optuna\n",
    "from optuna.trial import TrialState\n",
    "from tab_transformer_pytorch import FTTransformer, TabTransformer\n",
    "import sys\n",
    "import time\n",
    "from torch import Tensor\n",
    "from typing import Literal\n",
    "\n",
    "device_in_use='cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train = pd.read_csv('/home/cscadmin/CyberResearch/CAT-Transformer/datasets/covertype/train.csv')\n",
    "# df_test = pd.read_csv('/home/cscadmin/CyberResearch/CAT-Transformer/datasets/covertype/test.csv')\n",
    "# df_val = pd.read_csv('/home/cscadmin/CyberResearch/CAT-Transformer/datasets/covertype/validation.csv') #READ FROM RIGHT SPOT\n",
    "\n",
    "df_train = pd.read_csv(r'C:\\Users\\smbm2\\projects\\CAT-Transformer\\datasets\\covertype\\train.csv')\n",
    "df_test = pd.read_csv(r'C:\\Users\\smbm2\\projects\\CAT-Transformer\\datasets\\covertype\\test.csv')\n",
    "df_val = pd.read_csv(r'C:\\Users\\smbm2\\projects\\CAT-Transformer\\datasets\\covertype\\validation.csv') #READ FROM RIGHT SPOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7]\n"
     ]
    }
   ],
   "source": [
    "cont_columns = ['Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology',\n",
    "       'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways',\n",
    "       'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm',\n",
    "       'Horizontal_Distance_To_Fire_Points', 'Wilderness_Area1',\n",
    "       'Wilderness_Area2', 'Wilderness_Area3', 'Wilderness_Area4',\n",
    "       'Soil_Type1', 'Soil_Type2', 'Soil_Type3', 'Soil_Type4', 'Soil_Type5',\n",
    "       'Soil_Type6', 'Soil_Type7', 'Soil_Type8', 'Soil_Type9', 'Soil_Type10',\n",
    "       'Soil_Type11', 'Soil_Type12', 'Soil_Type13', 'Soil_Type14',\n",
    "       'Soil_Type15', 'Soil_Type16', 'Soil_Type17', 'Soil_Type18',\n",
    "       'Soil_Type19', 'Soil_Type20', 'Soil_Type21', 'Soil_Type22',\n",
    "       'Soil_Type23', 'Soil_Type24', 'Soil_Type25', 'Soil_Type26',\n",
    "       'Soil_Type27', 'Soil_Type28', 'Soil_Type29', 'Soil_Type30',\n",
    "       'Soil_Type31', 'Soil_Type32', 'Soil_Type33', 'Soil_Type34',\n",
    "       'Soil_Type35', 'Soil_Type36', 'Soil_Type37', 'Soil_Type38',\n",
    "       'Soil_Type39', 'Soil_Type40']\n",
    "target = ['Cover_Type']\n",
    "\n",
    "#CHECKING TO MAKE SURE YOUR LIST IS CORRECT (NO NEED TO TOUCH)\n",
    "yourlist = cont_columns + target\n",
    "yourlist.sort()\n",
    "oglist = list(df_train.columns)\n",
    "oglist.sort()\n",
    "\n",
    "assert(yourlist == oglist), \"You may of spelled feature name wrong or you forgot to put on of them in the list\"\n",
    "\n",
    "target_classes = [max(len(df_train[target].value_counts()), len(df_val[target].value_counts()),len(df_test[target].value_counts()))]\n",
    "print(target_classes)\n",
    "# Create a StandardScaler and fit it to the cont features\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(df_train[cont_columns])\n",
    "\n",
    "# Transform the training, test, and validation datasets\n",
    "df_train[cont_columns] = scaler.transform(df_train[cont_columns])\n",
    "df_test[cont_columns] = scaler.transform(df_test[cont_columns])\n",
    "df_val[cont_columns] = scaler.transform(df_val[cont_columns])\n",
    "\n",
    "class SingleTaskDataset(Dataset):\n",
    "    def __init__(self, df : pd.DataFrame, num_columns,task1_column):\n",
    "        self.n = df.shape[0]\n",
    "        \n",
    "        self.task1_labels = df[task1_column].astype(np.int64).values\n",
    "\n",
    "        self.num = df[num_columns].astype(np.float32).values\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Retrieve features and labels from the dataframe using column names\n",
    "        num_features = self.num[idx]\n",
    "        labels_task1 = self.task1_labels[idx]\n",
    "\n",
    "        return num_features, labels_task1\n",
    "\n",
    "#Wrapping in Dataset\n",
    "train_dataset = SingleTaskDataset(df_train, cont_columns, 'Cover_Type')\n",
    "val_dataset = SingleTaskDataset(df_val, cont_columns, 'Cover_Type')\n",
    "test_dataset = SingleTaskDataset(df_test, cont_columns, 'Cover_Type')\n",
    "\n",
    "#This is a hyperparameter that is not tuned. Maybe mess with what makes sense here\n",
    "#Also try looking to see what other papers have done\n",
    "batch_size = 256\n",
    "\n",
    "# Wrapping with DataLoader for easy batch extraction\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_size, heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        self.head_dim = embed_size // heads\n",
    "        assert(self.head_dim * heads == embed_size), \"Embed size needs to be div by heads\"\n",
    "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.keys =nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.fc_out = nn.Linear(heads*self.head_dim, embed_size)\n",
    "\n",
    "\n",
    "    def forward(self, values, keys, query):\n",
    "        N = query.shape[0]\n",
    "        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
    "\n",
    "        values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
    "        keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n",
    "        queries = query.reshape(N, query_len, self.heads, self.head_dim)\n",
    "\n",
    "        values = self.values(values)\n",
    "        keys = self.keys(keys)\n",
    "        queries = self.queries(queries)\n",
    "\n",
    "        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
    "\n",
    "        attention = torch.softmax(energy / (self.embed_size ** (1/2)), dim=3) #(batch_size, head_dim, #query_embeddings, #key_embeddings)\n",
    "\n",
    "        # Calculate simplified attention scores\n",
    "        avg_attention = attention.mean(dim=0)  # Average across batches\n",
    "        # print(\"batch average\", avg_attention.shape)\n",
    "        avg_attention = avg_attention.mean(dim=0).squeeze(dim=0)\n",
    "        # print(\"head average\", avg_attention.shape)\n",
    "\n",
    "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(N, query_len, self.heads*self.head_dim) #(batch_size, n_features, embed_size)\n",
    "        out = self.fc_out(out)\n",
    "\n",
    "        return out, avg_attention\n",
    "    \n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_size, heads, dropout, forward_expansion, pre_norm_on):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "\n",
    "        self.pre_norm_on = pre_norm_on\n",
    "        if self.pre_norm_on:\n",
    "            self.pre_norm = nn.LayerNorm(embed_size)\n",
    "        self.attention = MultiHeadAttention(embed_size, heads)\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "\n",
    "        self.feed_forward = nn.Sequential(nn.Linear(embed_size, forward_expansion*embed_size),\n",
    "                                          nn.ReLU(),\n",
    "                                          nn.Linear(forward_expansion*embed_size, embed_size)\n",
    "                                          )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self,value,key,query):\n",
    "        if self.pre_norm_on:\n",
    "            query = self.pre_norm(query)\n",
    "            key = self.pre_norm(key)\n",
    "            value = self.pre_norm(value)\n",
    "            \n",
    "        attention, avg_attention = self.attention(value, key, query)\n",
    "        x = self.dropout(self.norm1(attention + query))\n",
    "        forward = self.feed_forward(x)\n",
    "        out = self.dropout(self.norm2(forward + x))\n",
    "        return out, avg_attention\n",
    "    \n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, embed_size, heads, forward_expansion, dropout, pre_norm_on):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "\n",
    "        self.attention = MultiHeadAttention(embed_size, heads)\n",
    "        self.norm = nn.LayerNorm(embed_size)\n",
    "        self.transformer_block = TransformerBlock(embed_size, heads, dropout, forward_expansion, pre_norm_on)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, value, key):\n",
    "        out, avg_attention = self.transformer_block(value, key, x)\n",
    "\n",
    "        return out, avg_attention\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 embed_size,\n",
    "                 num_layers,\n",
    "                 heads,\n",
    "                 forward_expansion,\n",
    "                 decoder_dropout,\n",
    "                 pre_norm_on\n",
    "    ):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "                [\n",
    "                    DecoderBlock(\n",
    "                        embed_size,\n",
    "                        heads,\n",
    "                        dropout=decoder_dropout,\n",
    "                        forward_expansion=forward_expansion,\n",
    "                        pre_norm_on=pre_norm_on\n",
    "                    )\n",
    "                    for _ in range(num_layers)\n",
    "                ]\n",
    "            )\n",
    "        self.avg_attention = None\n",
    "\n",
    "    def forward(self, class_embed, context):\n",
    "        for layer in self.layers:\n",
    "            # x is the classification embedding (CLS Token)\n",
    "            # context are the feature embeddings that will be used as key and value\n",
    "            x, self.avg_attention = layer(class_embed, context, context)\n",
    "  \n",
    "        return x \n",
    "\n",
    "class EmbeddingsRFFforIndividualFeatures(nn.Module):\n",
    "    def __init__(self, sigma, embed_size, input_size, n_cont,  num_target_labels, rff_on, linear_on:bool):\n",
    "        super(EmbeddingsRFFforIndividualFeatures, self).__init__()\n",
    "\n",
    "        self.rff_on = rff_on\n",
    "\n",
    "        self.linear_on = linear_on\n",
    "\n",
    "        if self.rff_on:\n",
    "            self.rffs = nn.ModuleList([GaussianEncoding(sigma=sigma, input_size=input_size, encoded_size=embed_size//2) for _ in range(n_cont)])\n",
    "            self.mlp_in = embed_size\n",
    "        else:\n",
    "            self.mlp_in = input_size\n",
    "\n",
    "        if self.linear_on:\n",
    "            self.cont_embeddings = nn.ModuleList([nn.Linear(in_features=self.mlp_in, out_features=embed_size) for _ in range(n_cont)])\n",
    "\n",
    "        # Classifcation Embeddings for each target label\n",
    "        self.target_label_embeddings = nn.ModuleList([nn.Embedding(1, embed_size) for _ in range(num_target_labels)])\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(2) #(batch_size, n_features) -> (batch_size, n_features, 1)\n",
    "        rff_vectors = []\n",
    "        if self.rff_on:\n",
    "            for i, r in enumerate(self.rffs):\n",
    "                input = x[:,i,:]\n",
    "                out = r(input)\n",
    "                rff_vectors.append(out)\n",
    "\n",
    "        embeddings = []\n",
    "        if self.linear_on:\n",
    "            x = torch.stack(rff_vectors, dim=1)\n",
    "            for i, e in enumerate(self.cont_embeddings):\n",
    "                goin_in = x[:,i,:]\n",
    "                goin_out = e(goin_in)\n",
    "                embeddings.append(goin_out)\n",
    "        else:\n",
    "            embeddings = rff_vectors\n",
    "\n",
    "        target_label_embeddings_ = []\n",
    "        for e in self.target_label_embeddings:\n",
    "            input = torch.tensor([0], device=x.device)\n",
    "            temp = e(input)\n",
    "            temp = temp.repeat(x.size(0), 1)\n",
    "            tmep = temp.unsqueeze(1)\n",
    "            target_label_embeddings_.append(temp)\n",
    "\n",
    "        class_embeddings = torch.stack(target_label_embeddings_, dim=1)\n",
    "\n",
    "        context = torch.stack(embeddings, dim=1)\n",
    "\n",
    "\n",
    "        return class_embeddings, context\n",
    "\n",
    "class EmbeddingsRFFforAllFeatures(nn.Module):\n",
    "    def __init__(self, sigma, embed_size, input_size, n_cont, num_target_labels, rff_on, linear_on:bool):\n",
    "        super(EmbeddingsRFFforAllFeatures, self).__init__()\n",
    "\n",
    "        self.rff_on = rff_on\n",
    "        self.n_cont = n_cont\n",
    "        self.linear_on=linear_on\n",
    "\n",
    "        if self.rff_on:\n",
    "            self.rff = GaussianEncoding(sigma=sigma, input_size=input_size, encoded_size=embed_size//2) \n",
    "            self.mlp_in = embed_size\n",
    "        else:\n",
    "            self.mlp_in = input_size\n",
    "\n",
    "        if self.linear_on:\n",
    "            self.cont_embeddings = nn.ModuleList([nn.Linear(in_features=self.mlp_in, out_features=embed_size) for _ in range(n_cont)])\n",
    "\n",
    "        \n",
    "        # Classifcation Embeddings for each target label\n",
    "        self.target_label_embeddings = nn.ModuleList([nn.Embedding(1, embed_size) for _ in range(num_target_labels)])\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(2) #(batch_size, n_features) -> (batch_size, n_features, 1)\n",
    "        rff_vectors = []\n",
    "        if self.rff_on:\n",
    "            for i in range(self.n_cont):\n",
    "                input = x[:,i,:]\n",
    "                out = self.rff(input)\n",
    "                rff_vectors.append(out)\n",
    "        \n",
    "        embeddings = []\n",
    "        if self.linear_on:\n",
    "            x = torch.stack(rff_vectors, dim=1)\n",
    "            for i, e in enumerate(self.cont_embeddings):\n",
    "                goin_in = x[:,i,:]\n",
    "                goin_out = e(goin_in)\n",
    "                embeddings.append(goin_out)\n",
    "        else:\n",
    "            embeddings = rff_vectors\n",
    "\n",
    "        target_label_embeddings_ = []\n",
    "        for e in self.target_label_embeddings:\n",
    "            input = torch.tensor([0], device=x.device)\n",
    "            temp = e(input)\n",
    "            temp = temp.repeat(x.size(0), 1)\n",
    "            tmep = temp.unsqueeze(1)\n",
    "            target_label_embeddings_.append(temp)\n",
    "\n",
    "        class_embeddings = torch.stack(target_label_embeddings_, dim=1)\n",
    "\n",
    "        context = torch.stack(embeddings, dim=1)\n",
    "\n",
    "        return class_embeddings, context\n",
    "    \n",
    "class PeriodicActivation(nn.Module):\n",
    "    def __init__(self, sigma, embed_size, n_cont, num_target_labels,trainable: bool, initialization: str, linear_on:bool):\n",
    "        super(PeriodicActivation, self).__init__()\n",
    "\n",
    "        self.n = embed_size\n",
    "        self.sigma = sigma\n",
    "        self.trainable = trainable\n",
    "        self.initialization = initialization\n",
    "        self.linear_on = linear_on\n",
    "        self.n_cont = n_cont\n",
    "\n",
    "        if self.initialization == 'log-linear':\n",
    "            coefficients = self.sigma ** (torch.arange(self.n//2) / self.n)\n",
    "            coefficients = coefficients[None]\n",
    "        else:\n",
    "            assert self.initialization == 'normal'\n",
    "            coefficients = torch.normal(0.0, self.sigma, (1, self.n//2))\n",
    "\n",
    "        if self.trainable:\n",
    "            self.coefficients = nn.Parameter(coefficients)\n",
    "        else:\n",
    "            self.register_buffer('coefficients', coefficients)\n",
    "\n",
    "        if self.linear_on:\n",
    "            self.cont_embeddings = nn.ModuleList([nn.Linear(in_features=embed_size, out_features=embed_size) for _ in range(n_cont)])\n",
    "\n",
    "        \n",
    "        # Classifcation Embeddings for each target label\n",
    "        self.target_label_embeddings = nn.ModuleList([nn.Embedding(1, embed_size) for _ in range(num_target_labels)])\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(2) #(batch_size, n_features) -> (batch_size, n_features, 1)\n",
    "\n",
    "        temp = []\n",
    "        for i in range(self.n_cont):\n",
    "            input = x[:,i,:]\n",
    "            out = torch.cat([torch.cos(self.coefficients * input), torch.sin(self.coefficients * input)], dim=-1)\n",
    "            temp.append(out)\n",
    "        \n",
    "        embeddings = []\n",
    "        if self.linear_on:\n",
    "            x = torch.stack(temp, dim=1)\n",
    "            for i, e in enumerate(self.cont_embeddings):\n",
    "                goin_in = x[:,i,:]\n",
    "                goin_out = e(goin_in)\n",
    "                embeddings.append(goin_out)\n",
    "        else:\n",
    "            embeddings = temp\n",
    "\n",
    "        target_label_embeddings_ = []\n",
    "        for e in self.target_label_embeddings:\n",
    "            input = torch.tensor([0], device=x.device)\n",
    "            temp = e(input)\n",
    "            temp = temp.repeat(x.size(0), 1)\n",
    "            tmep = temp.unsqueeze(1)\n",
    "            target_label_embeddings_.append(temp)\n",
    "\n",
    "        class_embeddings = torch.stack(target_label_embeddings_, dim=1)\n",
    "\n",
    "        context = torch.stack(embeddings, dim=1)\n",
    "\n",
    "        return class_embeddings, context\n",
    "\n",
    "\n",
    "class classificationHead(nn.Module):\n",
    "    def __init__(self, embed_size, dropout, mlp_scale_classification, num_target_classes):\n",
    "        super(classificationHead, self).__init__()\n",
    "        \n",
    "        #flattening the embeddings out so each sample in batch is represented with a 460 dimensional vector\n",
    "        self.input = embed_size\n",
    "        self.lin1 = nn.Linear(self.input, mlp_scale_classification*self.input)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.lin2 = nn.Linear(mlp_scale_classification*self.input, mlp_scale_classification*self.input)\n",
    "        self.lin3 = nn.Linear(mlp_scale_classification*self.input, self.input)\n",
    "        self.lin4 = nn.Linear(self.input, num_target_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.initialize_weights()\n",
    "\n",
    "    def initialize_weights(self): #he_initialization.\n",
    "        torch.nn.init.kaiming_normal_(self.lin1.weight, nonlinearity='relu')\n",
    "        torch.nn.init.zeros_(self.lin1.bias)\n",
    "\n",
    "        torch.nn.init.kaiming_normal_(self.lin3.weight, nonlinearity='relu')\n",
    "        torch.nn.init.zeros_(self.lin3.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x= torch.reshape(x, (-1, self.input))\n",
    "\n",
    "        x = self.lin1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.lin2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.lin3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.lin4(x)\n",
    "  \n",
    "        return x\n",
    "\n",
    "class CATTransformer(nn.Module):\n",
    "    def __init__(self, \n",
    "                 rff_on = True,\n",
    "                 sigma=4,\n",
    "                 embed_size=20,\n",
    "                 input_size=1,\n",
    "                 n_cont = 0,\n",
    "                 num_layers=1,\n",
    "                 heads=1,\n",
    "                 forward_expansion=4, # Determines how wide the MLP is in the encoder. Its a scaling factor. \n",
    "                 decoder_dropout=0,\n",
    "                 classification_dropout = 0,\n",
    "                 pre_norm_on = False,\n",
    "                 mlp_scale_classification = 4,\n",
    "                 targets_classes : list=  [3,8],\n",
    "                embedding_scheme =\"rff_unique\",\n",
    "                trainable=True,\n",
    "                linear_on=True\n",
    "                 ):\n",
    "        super(CATTransformer, self).__init__()\n",
    "\n",
    "        assert(embedding_scheme in ['rff_unique', 'rff', 'log-linear_periodic', 'normal_periodic']), \"wrong embedding_scheme\"\n",
    "\n",
    "        if embedding_scheme == 'rff_unique':\n",
    "            self.embeddings = EmbeddingsRFFforIndividualFeatures(rff_on=rff_on, sigma=sigma, embed_size=embed_size, input_size=input_size, \n",
    "                                     n_cont=n_cont, num_target_labels=len(targets_classes), linear_on=linear_on)\n",
    "        elif embedding_scheme == 'log-linear_periodic':\n",
    "            self.embeddings = PeriodicActivation(sigma=sigma, embed_size=embed_size, n_cont=n_cont, \n",
    "                                                 num_target_labels=len(target_classes),trainable=trainable,initialization='log-linear', linear_on=linear_on)\n",
    "        elif embedding_scheme == 'normal_periodic':\n",
    "            self.embeddings = PeriodicActivation(sigma=sigma, embed_size=embed_size, n_cont=n_cont, \n",
    "                                                 num_target_labels=len(target_classes),trainable=trainable,initialization='normal',linear_on=linear_on)\n",
    "        else:\n",
    "            self.embeddings = EmbeddingsRFFforAllFeatures(rff_on=rff_on, sigma=sigma, embed_size=embed_size, input_size=input_size, \n",
    "                                     n_cont=n_cont, num_target_labels=len(targets_classes),linear_on=linear_on)\n",
    "            \n",
    "        self.decoder = Decoder(embed_size=embed_size, num_layers=num_layers, heads=heads, forward_expansion=forward_expansion, \n",
    "                               decoder_dropout=decoder_dropout, pre_norm_on=pre_norm_on)\n",
    "        self.classifying_heads = nn.ModuleList([classificationHead(embed_size=embed_size, dropout=classification_dropout, \n",
    "                                                                   mlp_scale_classification=mlp_scale_classification, \n",
    "                                                                   num_target_classes=x) for x in targets_classes])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        class_embed, context = self.embeddings(x)\n",
    "\n",
    "        x = self.decoder(class_embed, context)\n",
    "        \n",
    "        probability_dist_raw = []\n",
    "        for i, e in enumerate(self.classifying_heads):\n",
    "            input = x[:, i,:]\n",
    "            output = e(input)\n",
    "            probability_dist_raw.append(output)\n",
    "        \n",
    "        return probability_dist_raw\n",
    "    \n",
    "    # Training and Testing Loops\n",
    "def train(dataloader, model, loss_function, optimizer, device_in_use):\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "\n",
    "    total_correct_1 = 0\n",
    "    total_samples_1 = 0\n",
    "    all_targets_1 = []\n",
    "    all_predictions_1 = []\n",
    "\n",
    "    total_correct_2 = 0\n",
    "    total_samples_2 = 0\n",
    "    all_targets_2 = []\n",
    "    all_predictions_2 = []\n",
    "\n",
    "    for (features,labels_task1) in dataloader:\n",
    "        features,labels_task1 = features.to(device_in_use),labels_task1.to(device_in_use)\n",
    "\n",
    "\n",
    "        task_predictions = model(features) #contains a list of the tensor outputs for each task\n",
    "\n",
    "        loss = loss_function(task_predictions, labels_task1)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        #computing accuracy for first target\n",
    "        y_pred_softmax_1 = torch.softmax(task_predictions[0], dim=1)\n",
    "        _, y_pred_labels_1 = torch.max(y_pred_softmax_1, dim=1)\n",
    "        total_correct_1 += (y_pred_labels_1 == labels_task1).sum().item()\n",
    "        total_samples_1 += labels_task1.size(0)\n",
    "        all_targets_1.extend(labels_task1.cpu().numpy())\n",
    "        all_predictions_1.extend(y_pred_labels_1.cpu().numpy())\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    avg_loss = total_loss/len(dataloader)\n",
    "    accuracy_1 = total_correct_1 / total_samples_1\n",
    "    # accuracy_2 = total_correct_2 / total_samples_2\n",
    "\n",
    "    # # precision = precision_score(all_targets, all_predictions, average='weighted')\n",
    "    # recall = recall_score(all_targets, all_predictions, average='weighted')\n",
    "    # f1 = f1_score(all_targets, all_predictions, average='weighted')\n",
    "\n",
    "    return avg_loss, accuracy_1\n",
    "\n",
    "def test(dataloader, model, loss_function, device_in_use):\n",
    "  model.eval()\n",
    "  total_loss = 0\n",
    "  \n",
    "  total_correct_1 = 0\n",
    "  total_samples_1 = 0\n",
    "  all_targets_1 = []\n",
    "  all_predictions_1 = []\n",
    "\n",
    "  total_correct_2 = 0\n",
    "  total_samples_2 = 0\n",
    "  all_targets_2 = []\n",
    "  all_predictions_2 = []\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for (features,labels_task1) in dataloader:\n",
    "      features,labels_task1 = features.to(device_in_use),labels_task1.to(device_in_use)\n",
    "\n",
    "      #compute prediction error\n",
    "      task_predictions = model(features) #contains a list of the tensor outputs for each task\n",
    "\n",
    "      loss = loss_function(task_predictions, labels_task1)\n",
    "      total_loss += loss.item()\n",
    "\n",
    "      #computing accuracy for first target\n",
    "      y_pred_softmax_1 = torch.softmax(task_predictions[0], dim=1)\n",
    "      _, y_pred_labels_1 = torch.max(y_pred_softmax_1, dim=1)\n",
    "      total_correct_1 += (y_pred_labels_1 == labels_task1).sum().item()\n",
    "      total_samples_1 += labels_task1.size(0)\n",
    "      all_targets_1.extend(labels_task1.cpu().numpy())\n",
    "      all_predictions_1.extend(y_pred_labels_1.cpu().numpy())\n",
    "\n",
    "  avg = total_loss/len(dataloader)\n",
    "  accuracy_1 = total_correct_1 / total_samples_1\n",
    "  # accuracy_2 = total_correct_2 / total_samples_2\n",
    "  # recall = recall_score(all_targets, all_predictions, average='weighted')\n",
    "  f1_1 = f1_score(all_targets_1, all_predictions_1, average='weighted')\n",
    "  # f1_2 = f1_score(all_targets_2, all_predictions_2, average=\"weighted\")\n",
    "\n",
    "  return avg, accuracy_1, all_predictions_1, all_targets_1, f1_1\n",
    "\n",
    "def format_metric(value): # Used to format the metrics output\n",
    "    return f\"{value:.4f}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log-Linear RFF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\sigma = -2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [ 1/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [ 2/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [ 3/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [ 4/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [ 5/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [ 6/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [ 7/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [ 8/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [ 9/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [10/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [11/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [12/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [13/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [14/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [15/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [16/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [17/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [18/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [19/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [20/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [21/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [22/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [23/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [24/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [25/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [26/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [27/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [28/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [29/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [30/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [31/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [32/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [33/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [34/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [35/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [36/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [37/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [38/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [39/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [40/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [41/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [42/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [43/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [44/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [45/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [46/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [47/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [48/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [49/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [50/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [51/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [52/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [53/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [54/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [55/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [56/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [57/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [58/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [59/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [60/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [61/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [62/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [63/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [64/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [65/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [66/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [67/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [68/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [69/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [70/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [71/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [72/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [73/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [74/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [75/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [76/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [77/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [78/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [79/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [80/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [81/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [82/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [83/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [84/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [85/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [86/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [87/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [88/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [89/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [90/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [91/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [92/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [93/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [94/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [95/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [96/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [97/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [98/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [99/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [100/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [101/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [102/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [103/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [104/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [105/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [106/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [107/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [108/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [109/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [110/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [111/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [112/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [113/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [114/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [115/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [116/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [117/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [118/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [119/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [120/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [121/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [122/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [123/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [124/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [125/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [126/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [127/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [128/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [129/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [130/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [131/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [132/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [133/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [134/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [135/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [136/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [137/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [138/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [139/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [140/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [141/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [142/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [143/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [144/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [145/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [146/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [147/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [148/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [149/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [150/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [151/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [152/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [153/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [154/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [155/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [156/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [157/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [158/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [159/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [160/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [161/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [162/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [163/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [164/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [165/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [166/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [167/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [168/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [169/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [170/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [171/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [172/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [173/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [174/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [175/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [176/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [177/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [178/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [179/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [180/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [181/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [182/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [183/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [184/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [185/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [186/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [187/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [188/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [189/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [190/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [191/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [192/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [193/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [194/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [195/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [196/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [197/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [198/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [199/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Epoch [200/200]      | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n",
      "Confusion Matrix for income\n",
      "[[42204     0     0     0     0     0     0]\n",
      " [56912     0     0     0     0     0     0]\n",
      " [ 7094     0     0     0     0     0     0]\n",
      " [  533     0     0     0     0     0     0]\n",
      " [ 1962     0     0     0     0     0     0]\n",
      " [ 3460     0     0     0     0     0     0]\n",
      " [ 4038     0     0     0     0     0     0]]\n",
      "Best accuracy 0.36319200020653514\n",
      "Best F1 0.19352876042998868\n",
      "100 epochs of training and evaluation took, 744.359375\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class LossFunction(nn.Module):\n",
    "    def __init__(self, num_tasks):\n",
    "        super(LossFunction, self).__init__()\n",
    "        self.num_tasks = num_tasks\n",
    "\n",
    "        self.loss_fns = [nn.CrossEntropyLoss() for x in range(num_tasks)] \n",
    "\n",
    "    def forward(self, predictions, labels_task1):\n",
    "\n",
    "        #task 1\n",
    "        target = labels_task1.long()\n",
    "        prediction = predictions[0]\n",
    "        loss_fn = self.loss_fns[0]\n",
    "\n",
    "        task_loss = loss_fn(prediction, target)\n",
    "        \n",
    "        return task_loss\n",
    "    \n",
    "model = CATTransformer(rff_on=True,\n",
    "                       sigma=-2,\n",
    "                       embed_size=160,\n",
    "                       heads=5,\n",
    "                       forward_expansion=8,\n",
    "                       pre_norm_on=False,\n",
    "                       mlp_scale_classification=8,\n",
    "                   \n",
    "                       decoder_dropout=0,\n",
    "                       classification_dropout=0.1,\n",
    "                       targets_classes=target_classes,\n",
    "                       n_cont=len(cont_columns),\n",
    "              \n",
    "                       embedding_scheme='log-linear_periodic',\n",
    "                       trainable=False,\n",
    "                       linear_on=True).to(device_in_use)\n",
    "\n",
    "def format_metric(value): # Used to format the metrics output\n",
    "    return f\"{value:.5f}\"\n",
    "\n",
    "loss_functions = LossFunction(1)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr = 0.0001) # Maybe try messing around with optimizers. try other torch optimizers with different configurations.\n",
    "epochs = 200 #Set the number of epochs\n",
    "\n",
    "train_losses = []\n",
    "train_accuracies_1 = [] \n",
    "test_losses = []\n",
    "test_accuracies_1 = [] \n",
    "test_f1_scores = [] \n",
    "\n",
    "\n",
    "#Time will be recorded for all 100 epochs - This means the results will not be comparable to Xgboost but that is ok, we will only compare between transformer models who will also train for 100 epochs\n",
    "start_time = time.process_time()\n",
    "\n",
    "for t in range(epochs):\n",
    "  train_loss, train_accuracy_1 = train(train_dataloader, model, loss_functions, optimizer, device_in_use=device_in_use)\n",
    "  test_loss, test_accuracy_1, all_predictions_1, all_targets_1, f1_1 = test(test_dataloader, model, loss_functions, device_in_use=device_in_use)\n",
    "  train_losses.append(train_loss)\n",
    "  train_accuracies_1.append(train_accuracy_1)\n",
    "  # train_accuracies_2.append(train_accuracy_2)\n",
    "  # train_recalls.append(train_recall) \n",
    "  # train_f1_scores.append(train_f1)\n",
    "  test_losses.append(test_loss)\n",
    "  test_accuracies_1.append(test_accuracy_1)\n",
    "  # test_accuracies_2.append(test_accuracy_2)\n",
    "  # test_recalls.append(test_recall)\n",
    "  test_f1_scores.append(f1_1)\n",
    "  # Formatting for easier reading\n",
    "  epoch_str = f\"Epoch [{t+1:2}/{epochs}]\"\n",
    "  train_metrics = f\"Train: Loss {format_metric(train_loss)}, Accuracy {format_metric(train_accuracy_1)}\"\n",
    "  test_metrics = f\"Test: Loss {format_metric(test_loss)}, Accuracy {format_metric(test_accuracy_1)}, F1 {format_metric(f1_1)}\"\n",
    "  print(f\"{epoch_str:20} | {train_metrics:65} | {test_metrics}\")\n",
    "\n",
    "total_time = time.process_time() - start_time\n",
    "\n",
    "# Plotting the loss curves\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, epochs+1), train_losses, label='Train Loss')\n",
    "plt.plot(range(1, epochs+1), [l for l in test_losses], label='Test Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Test Loss Curve')\n",
    "plt.legend()\n",
    "\n",
    "# Plotting the accuracy curves\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, epochs+1), train_accuracies_1, label='Train Accuracy')\n",
    "plt.plot(range(1, epochs+1), test_accuracies_1, label='Test Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Test Accuracy Curve')\n",
    "plt.legend()\n",
    "\n",
    "# Display confusion matrix for the first task (Traffic Type) on test data\n",
    "conf_matrix_1 = confusion_matrix(all_targets_1, all_predictions_1)\n",
    "print(\"Confusion Matrix for income\")\n",
    "print(conf_matrix_1)\n",
    "\n",
    "best_index = test_accuracies_1.index(max(test_accuracies_1))\n",
    "print(f\"Best accuracy {test_accuracies_1[best_index]}\")\n",
    "print(f\"Best F1 {test_f1_scores[best_index]}\")\n",
    "print(f\"100 epochs of training and evaluation took, {total_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\sigma = -1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [ 1/200]       | Train: Loss nan, Accuracy 0.36458                                 | Test: Loss nan, Accuracy 0.36319, F1 0.19353\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\smbm2\\projects\\CAT-Transformer\\rff_experiments\\covertype_loglinear.ipynb Cell 9\u001b[0m line \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/rff_experiments/covertype_loglinear.ipynb#X45sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mprocess_time()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/rff_experiments/covertype_loglinear.ipynb#X45sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/rff_experiments/covertype_loglinear.ipynb#X45sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m   train_loss, train_accuracy_1 \u001b[39m=\u001b[39m train(train_dataloader, model, loss_functions, optimizer, device_in_use\u001b[39m=\u001b[39;49mdevice_in_use)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/rff_experiments/covertype_loglinear.ipynb#X45sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m   test_loss, test_accuracy_1, all_predictions_1, all_targets_1, f1_1 \u001b[39m=\u001b[39m test(test_dataloader, model, loss_functions, device_in_use\u001b[39m=\u001b[39mdevice_in_use)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/rff_experiments/covertype_loglinear.ipynb#X45sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m   train_losses\u001b[39m.\u001b[39mappend(train_loss)\n",
      "\u001b[1;32mc:\\Users\\smbm2\\projects\\CAT-Transformer\\rff_experiments\\covertype_loglinear.ipynb Cell 9\u001b[0m line \u001b[0;36m4\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/rff_experiments/covertype_loglinear.ipynb#X45sZmlsZQ%3D%3D?line=404'>405</a>\u001b[0m \u001b[39mfor\u001b[39;00m (features,labels_task1) \u001b[39min\u001b[39;00m dataloader:\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/rff_experiments/covertype_loglinear.ipynb#X45sZmlsZQ%3D%3D?line=405'>406</a>\u001b[0m     features,labels_task1 \u001b[39m=\u001b[39m features\u001b[39m.\u001b[39mto(device_in_use),labels_task1\u001b[39m.\u001b[39mto(device_in_use)\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/rff_experiments/covertype_loglinear.ipynb#X45sZmlsZQ%3D%3D?line=408'>409</a>\u001b[0m     task_predictions \u001b[39m=\u001b[39m model(features) \u001b[39m#contains a list of the tensor outputs for each task\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/rff_experiments/covertype_loglinear.ipynb#X45sZmlsZQ%3D%3D?line=410'>411</a>\u001b[0m     loss \u001b[39m=\u001b[39m loss_function(task_predictions, labels_task1)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/rff_experiments/covertype_loglinear.ipynb#X45sZmlsZQ%3D%3D?line=411'>412</a>\u001b[0m     total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\smbm2\\AppData\\Local\\miniconda3\\envs\\ml-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mc:\\Users\\smbm2\\projects\\CAT-Transformer\\rff_experiments\\covertype_loglinear.ipynb Cell 9\u001b[0m line \u001b[0;36m3\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/rff_experiments/covertype_loglinear.ipynb#X45sZmlsZQ%3D%3D?line=375'>376</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/rff_experiments/covertype_loglinear.ipynb#X45sZmlsZQ%3D%3D?line=376'>377</a>\u001b[0m     class_embed, context \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membeddings(x)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/rff_experiments/covertype_loglinear.ipynb#X45sZmlsZQ%3D%3D?line=378'>379</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder(class_embed, context)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/rff_experiments/covertype_loglinear.ipynb#X45sZmlsZQ%3D%3D?line=380'>381</a>\u001b[0m     probability_dist_raw \u001b[39m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\smbm2\\AppData\\Local\\miniconda3\\envs\\ml-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mc:\\Users\\smbm2\\projects\\CAT-Transformer\\rff_experiments\\covertype_loglinear.ipynb Cell 9\u001b[0m line \u001b[0;36m2\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/rff_experiments/covertype_loglinear.ipynb#X45sZmlsZQ%3D%3D?line=265'>266</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_cont):\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/rff_experiments/covertype_loglinear.ipynb#X45sZmlsZQ%3D%3D?line=266'>267</a>\u001b[0m     \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m x[:,i,:]\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/rff_experiments/covertype_loglinear.ipynb#X45sZmlsZQ%3D%3D?line=267'>268</a>\u001b[0m     out \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mcat([torch\u001b[39m.\u001b[39;49mcos(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcoefficients \u001b[39m*\u001b[39;49m \u001b[39minput\u001b[39;49m), torch\u001b[39m.\u001b[39;49msin(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcoefficients \u001b[39m*\u001b[39;49m \u001b[39minput\u001b[39;49m)], dim\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/rff_experiments/covertype_loglinear.ipynb#X45sZmlsZQ%3D%3D?line=268'>269</a>\u001b[0m     temp\u001b[39m.\u001b[39mappend(out)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/rff_experiments/covertype_loglinear.ipynb#X45sZmlsZQ%3D%3D?line=270'>271</a>\u001b[0m embeddings \u001b[39m=\u001b[39m []\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class LossFunction(nn.Module):\n",
    "    def __init__(self, num_tasks):\n",
    "        super(LossFunction, self).__init__()\n",
    "        self.num_tasks = num_tasks\n",
    "\n",
    "        self.loss_fns = [nn.CrossEntropyLoss() for x in range(num_tasks)] \n",
    "\n",
    "    def forward(self, predictions, labels_task1):\n",
    "\n",
    "        #task 1\n",
    "        target = labels_task1.long()\n",
    "        prediction = predictions[0]\n",
    "        loss_fn = self.loss_fns[0]\n",
    "\n",
    "        task_loss = loss_fn(prediction, target)\n",
    "        \n",
    "        return task_loss\n",
    "    \n",
    "model = CATTransformer(rff_on=True,\n",
    "                       sigma=-1,\n",
    "                       embed_size=160,\n",
    "                       heads=5,\n",
    "                       forward_expansion=8,\n",
    "                       pre_norm_on=False,\n",
    "                       mlp_scale_classification=8,\n",
    "                   \n",
    "                       decoder_dropout=0,\n",
    "                       classification_dropout=0.1,\n",
    "                       targets_classes=target_classes,\n",
    "                       n_cont=len(cont_columns),\n",
    "              \n",
    "                       embedding_scheme='log-linear_periodic',\n",
    "                       trainable=False,\n",
    "                       linear_on=True).to(device_in_use)\n",
    "\n",
    "def format_metric(value): # Used to format the metrics output\n",
    "    return f\"{value:.5f}\"\n",
    "\n",
    "loss_functions = LossFunction(1)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr = 0.0001) # Maybe try messing around with optimizers. try other torch optimizers with different configurations.\n",
    "epochs = 200 #Set the number of epochs\n",
    "\n",
    "train_losses = []\n",
    "train_accuracies_1 = [] \n",
    "test_losses = []\n",
    "test_accuracies_1 = [] \n",
    "test_f1_scores = [] \n",
    "\n",
    "\n",
    "#Time will be recorded for all 100 epochs - This means the results will not be comparable to Xgboost but that is ok, we will only compare between transformer models who will also train for 100 epochs\n",
    "start_time = time.process_time()\n",
    "\n",
    "for t in range(epochs):\n",
    "  train_loss, train_accuracy_1 = train(train_dataloader, model, loss_functions, optimizer, device_in_use=device_in_use)\n",
    "  test_loss, test_accuracy_1, all_predictions_1, all_targets_1, f1_1 = test(test_dataloader, model, loss_functions, device_in_use=device_in_use)\n",
    "  train_losses.append(train_loss)\n",
    "  train_accuracies_1.append(train_accuracy_1)\n",
    "  # train_accuracies_2.append(train_accuracy_2)\n",
    "  # train_recalls.append(train_recall) \n",
    "  # train_f1_scores.append(train_f1)\n",
    "  test_losses.append(test_loss)\n",
    "  test_accuracies_1.append(test_accuracy_1)\n",
    "  # test_accuracies_2.append(test_accuracy_2)\n",
    "  # test_recalls.append(test_recall)\n",
    "  test_f1_scores.append(f1_1)\n",
    "  # Formatting for easier reading\n",
    "  epoch_str = f\"Epoch [{t+1:2}/{epochs}]\"\n",
    "  train_metrics = f\"Train: Loss {format_metric(train_loss)}, Accuracy {format_metric(train_accuracy_1)}\"\n",
    "  test_metrics = f\"Test: Loss {format_metric(test_loss)}, Accuracy {format_metric(test_accuracy_1)}, F1 {format_metric(f1_1)}\"\n",
    "  print(f\"{epoch_str:20} | {train_metrics:65} | {test_metrics}\")\n",
    "\n",
    "total_time = time.process_time() - start_time\n",
    "\n",
    "# Plotting the loss curves\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, epochs+1), train_losses, label='Train Loss')\n",
    "plt.plot(range(1, epochs+1), [l for l in test_losses], label='Test Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Test Loss Curve')\n",
    "plt.legend()\n",
    "\n",
    "# Plotting the accuracy curves\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, epochs+1), train_accuracies_1, label='Train Accuracy')\n",
    "plt.plot(range(1, epochs+1), test_accuracies_1, label='Test Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Test Accuracy Curve')\n",
    "plt.legend()\n",
    "\n",
    "# Display confusion matrix for the first task (Traffic Type) on test data\n",
    "conf_matrix_1 = confusion_matrix(all_targets_1, all_predictions_1)\n",
    "print(\"Confusion Matrix for income\")\n",
    "print(conf_matrix_1)\n",
    "\n",
    "best_index = test_accuracies_1.index(max(test_accuracies_1))\n",
    "print(f\"Best accuracy {test_accuracies_1[best_index]}\")\n",
    "print(f\"Best F1 {test_f1_scores[best_index]}\")\n",
    "print(f\"100 epochs of training and evaluation took, {total_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\sigma = -0.5$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossFunction(nn.Module):\n",
    "    def __init__(self, num_tasks):\n",
    "        super(LossFunction, self).__init__()\n",
    "        self.num_tasks = num_tasks\n",
    "\n",
    "        self.loss_fns = [nn.CrossEntropyLoss() for x in range(num_tasks)] \n",
    "\n",
    "    def forward(self, predictions, labels_task1):\n",
    "\n",
    "        #task 1\n",
    "        target = labels_task1.long()\n",
    "        prediction = predictions[0]\n",
    "        loss_fn = self.loss_fns[0]\n",
    "\n",
    "        task_loss = loss_fn(prediction, target)\n",
    "        \n",
    "        return task_loss\n",
    "    \n",
    "model = CATTransformer(rff_on=True,\n",
    "                       sigma=-0.5,\n",
    "                       embed_size=160,\n",
    "                       heads=5,\n",
    "                       forward_expansion=8,\n",
    "                       pre_norm_on=False,\n",
    "                       mlp_scale_classification=8,\n",
    "                   \n",
    "                       decoder_dropout=0,\n",
    "                       classification_dropout=0.1,\n",
    "                       targets_classes=target_classes,\n",
    "                       n_cont=len(cont_columns),\n",
    "              \n",
    "                       embedding_scheme='log-linear_periodic',\n",
    "                       trainable=False,\n",
    "                       linear_on=True).to(device_in_use)\n",
    "\n",
    "def format_metric(value): # Used to format the metrics output\n",
    "    return f\"{value:.5f}\"\n",
    "\n",
    "loss_functions = LossFunction(1)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr = 0.0001) # Maybe try messing around with optimizers. try other torch optimizers with different configurations.\n",
    "epochs = 200 #Set the number of epochs\n",
    "\n",
    "train_losses = []\n",
    "train_accuracies_1 = [] \n",
    "test_losses = []\n",
    "test_accuracies_1 = [] \n",
    "test_f1_scores = [] \n",
    "\n",
    "\n",
    "#Time will be recorded for all 100 epochs - This means the results will not be comparable to Xgboost but that is ok, we will only compare between transformer models who will also train for 100 epochs\n",
    "start_time = time.process_time()\n",
    "\n",
    "for t in range(epochs):\n",
    "  train_loss, train_accuracy_1 = train(train_dataloader, model, loss_functions, optimizer, device_in_use=device_in_use)\n",
    "  test_loss, test_accuracy_1, all_predictions_1, all_targets_1, f1_1 = test(test_dataloader, model, loss_functions, device_in_use=device_in_use)\n",
    "  train_losses.append(train_loss)\n",
    "  train_accuracies_1.append(train_accuracy_1)\n",
    "  # train_accuracies_2.append(train_accuracy_2)\n",
    "  # train_recalls.append(train_recall) \n",
    "  # train_f1_scores.append(train_f1)\n",
    "  test_losses.append(test_loss)\n",
    "  test_accuracies_1.append(test_accuracy_1)\n",
    "  # test_accuracies_2.append(test_accuracy_2)\n",
    "  # test_recalls.append(test_recall)\n",
    "  test_f1_scores.append(f1_1)\n",
    "  # Formatting for easier reading\n",
    "  epoch_str = f\"Epoch [{t+1:2}/{epochs}]\"\n",
    "  train_metrics = f\"Train: Loss {format_metric(train_loss)}, Accuracy {format_metric(train_accuracy_1)}\"\n",
    "  test_metrics = f\"Test: Loss {format_metric(test_loss)}, Accuracy {format_metric(test_accuracy_1)}, F1 {format_metric(f1_1)}\"\n",
    "  print(f\"{epoch_str:20} | {train_metrics:65} | {test_metrics}\")\n",
    "\n",
    "total_time = time.process_time() - start_time\n",
    "\n",
    "# Plotting the loss curves\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, epochs+1), train_losses, label='Train Loss')\n",
    "plt.plot(range(1, epochs+1), [l for l in test_losses], label='Test Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Test Loss Curve')\n",
    "plt.legend()\n",
    "\n",
    "# Plotting the accuracy curves\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, epochs+1), train_accuracies_1, label='Train Accuracy')\n",
    "plt.plot(range(1, epochs+1), test_accuracies_1, label='Test Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Test Accuracy Curve')\n",
    "plt.legend()\n",
    "\n",
    "# Display confusion matrix for the first task (Traffic Type) on test data\n",
    "conf_matrix_1 = confusion_matrix(all_targets_1, all_predictions_1)\n",
    "print(\"Confusion Matrix for income\")\n",
    "print(conf_matrix_1)\n",
    "\n",
    "best_index = test_accuracies_1.index(max(test_accuracies_1))\n",
    "print(f\"Best accuracy {test_accuracies_1[best_index]}\")\n",
    "print(f\"Best F1 {test_f1_scores[best_index]}\")\n",
    "print(f\"100 epochs of training and evaluation took, {total_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\sigma = 0.5$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [ 1/200]       | Train: Loss 0.70580, Accuracy 0.70025                             | Test: Loss 0.60297, Accuracy 0.73563, F1 0.72694\n",
      "Epoch [ 2/200]       | Train: Loss 0.56064, Accuracy 0.75698                             | Test: Loss 0.50810, Accuracy 0.78111, F1 0.77437\n",
      "Epoch [ 3/200]       | Train: Loss 0.47300, Accuracy 0.79773                             | Test: Loss 0.42186, Accuracy 0.81896, F1 0.81608\n",
      "Epoch [ 4/200]       | Train: Loss 0.39545, Accuracy 0.83233                             | Test: Loss 0.35248, Accuracy 0.85257, F1 0.85011\n",
      "Epoch [ 5/200]       | Train: Loss 0.33564, Accuracy 0.86036                             | Test: Loss 0.30231, Accuracy 0.87536, F1 0.87451\n",
      "Epoch [ 6/200]       | Train: Loss 0.29316, Accuracy 0.87935                             | Test: Loss 0.26234, Accuracy 0.89378, F1 0.89356\n",
      "Epoch [ 7/200]       | Train: Loss 0.26287, Accuracy 0.89196                             | Test: Loss 0.24842, Accuracy 0.89758, F1 0.89717\n",
      "Epoch [ 8/200]       | Train: Loss 0.24065, Accuracy 0.90115                             | Test: Loss 0.21681, Accuracy 0.91108, F1 0.91084\n",
      "Epoch [ 9/200]       | Train: Loss 0.22173, Accuracy 0.90891                             | Test: Loss 0.20094, Accuracy 0.91820, F1 0.91764\n",
      "Epoch [10/200]       | Train: Loss 0.20621, Accuracy 0.91629                             | Test: Loss 0.19453, Accuracy 0.92095, F1 0.92056\n",
      "Epoch [11/200]       | Train: Loss 0.19483, Accuracy 0.92050                             | Test: Loss 0.17994, Accuracy 0.92720, F1 0.92701\n",
      "Epoch [12/200]       | Train: Loss 0.18392, Accuracy 0.92539                             | Test: Loss 0.17425, Accuracy 0.92889, F1 0.92877\n",
      "Epoch [13/200]       | Train: Loss 0.17446, Accuracy 0.92893                             | Test: Loss 0.16087, Accuracy 0.93490, F1 0.93474\n",
      "Epoch [14/200]       | Train: Loss 0.16660, Accuracy 0.93200                             | Test: Loss 0.15285, Accuracy 0.93724, F1 0.93696\n",
      "Epoch [15/200]       | Train: Loss 0.15876, Accuracy 0.93517                             | Test: Loss 0.14846, Accuracy 0.93905, F1 0.93902\n",
      "Epoch [16/200]       | Train: Loss 0.15310, Accuracy 0.93753                             | Test: Loss 0.14670, Accuracy 0.94032, F1 0.94022\n",
      "Epoch [17/200]       | Train: Loss 0.14697, Accuracy 0.94045                             | Test: Loss 0.14323, Accuracy 0.94175, F1 0.94165\n",
      "Epoch [18/200]       | Train: Loss 0.14191, Accuracy 0.94217                             | Test: Loss 0.13985, Accuracy 0.94314, F1 0.94315\n",
      "Epoch [19/200]       | Train: Loss 0.13676, Accuracy 0.94436                             | Test: Loss 0.13613, Accuracy 0.94426, F1 0.94428\n",
      "Epoch [20/200]       | Train: Loss 0.13296, Accuracy 0.94600                             | Test: Loss 0.13135, Accuracy 0.94630, F1 0.94639\n",
      "Epoch [21/200]       | Train: Loss 0.12768, Accuracy 0.94835                             | Test: Loss 0.12496, Accuracy 0.94906, F1 0.94899\n",
      "Epoch [22/200]       | Train: Loss 0.12424, Accuracy 0.94973                             | Test: Loss 0.12614, Accuracy 0.94865, F1 0.94872\n",
      "Epoch [23/200]       | Train: Loss 0.12088, Accuracy 0.95099                             | Test: Loss 0.12168, Accuracy 0.95060, F1 0.95057\n",
      "Epoch [24/200]       | Train: Loss 0.11746, Accuracy 0.95205                             | Test: Loss 0.11555, Accuracy 0.95326, F1 0.95326\n",
      "Epoch [25/200]       | Train: Loss 0.11368, Accuracy 0.95409                             | Test: Loss 0.11663, Accuracy 0.95251, F1 0.95252\n",
      "Epoch [26/200]       | Train: Loss 0.11170, Accuracy 0.95487                             | Test: Loss 0.11406, Accuracy 0.95356, F1 0.95351\n",
      "Epoch [27/200]       | Train: Loss 0.10892, Accuracy 0.95567                             | Test: Loss 0.11250, Accuracy 0.95448, F1 0.95442\n",
      "Epoch [28/200]       | Train: Loss 0.10678, Accuracy 0.95629                             | Test: Loss 0.11055, Accuracy 0.95477, F1 0.95474\n",
      "Epoch [29/200]       | Train: Loss 0.10438, Accuracy 0.95753                             | Test: Loss 0.11046, Accuracy 0.95505, F1 0.95510\n",
      "Epoch [30/200]       | Train: Loss 0.10152, Accuracy 0.95875                             | Test: Loss 0.10821, Accuracy 0.95562, F1 0.95561\n",
      "Epoch [31/200]       | Train: Loss 0.10012, Accuracy 0.95948                             | Test: Loss 0.10733, Accuracy 0.95615, F1 0.95606\n",
      "Epoch [32/200]       | Train: Loss 0.09794, Accuracy 0.96006                             | Test: Loss 0.10360, Accuracy 0.95803, F1 0.95805\n",
      "Epoch [33/200]       | Train: Loss 0.09576, Accuracy 0.96110                             | Test: Loss 0.10618, Accuracy 0.95756, F1 0.95753\n",
      "Epoch [34/200]       | Train: Loss 0.09354, Accuracy 0.96181                             | Test: Loss 0.10653, Accuracy 0.95763, F1 0.95757\n",
      "Epoch [35/200]       | Train: Loss 0.09227, Accuracy 0.96219                             | Test: Loss 0.10231, Accuracy 0.95838, F1 0.95845\n",
      "Epoch [36/200]       | Train: Loss 0.09064, Accuracy 0.96298                             | Test: Loss 0.09903, Accuracy 0.96087, F1 0.96079\n",
      "Epoch [37/200]       | Train: Loss 0.08969, Accuracy 0.96326                             | Test: Loss 0.09820, Accuracy 0.96004, F1 0.96005\n",
      "Epoch [38/200]       | Train: Loss 0.08776, Accuracy 0.96422                             | Test: Loss 0.09931, Accuracy 0.96011, F1 0.96010\n",
      "Epoch [39/200]       | Train: Loss 0.08605, Accuracy 0.96472                             | Test: Loss 0.09902, Accuracy 0.96032, F1 0.96037\n",
      "Epoch [40/200]       | Train: Loss 0.08486, Accuracy 0.96512                             | Test: Loss 0.09796, Accuracy 0.96040, F1 0.96041\n",
      "Epoch [41/200]       | Train: Loss 0.08306, Accuracy 0.96584                             | Test: Loss 0.10287, Accuracy 0.95866, F1 0.95865\n",
      "Epoch [42/200]       | Train: Loss 0.08315, Accuracy 0.96605                             | Test: Loss 0.09497, Accuracy 0.96177, F1 0.96175\n",
      "Epoch [43/200]       | Train: Loss 0.08118, Accuracy 0.96678                             | Test: Loss 0.09750, Accuracy 0.95997, F1 0.96004\n",
      "Epoch [44/200]       | Train: Loss 0.07966, Accuracy 0.96711                             | Test: Loss 0.09504, Accuracy 0.96143, F1 0.96148\n",
      "Epoch [45/200]       | Train: Loss 0.07889, Accuracy 0.96769                             | Test: Loss 0.09663, Accuracy 0.96133, F1 0.96136\n",
      "Epoch [46/200]       | Train: Loss 0.07793, Accuracy 0.96835                             | Test: Loss 0.09569, Accuracy 0.96174, F1 0.96170\n",
      "Epoch [47/200]       | Train: Loss 0.07679, Accuracy 0.96843                             | Test: Loss 0.09469, Accuracy 0.96246, F1 0.96244\n",
      "Epoch [48/200]       | Train: Loss 0.07553, Accuracy 0.96907                             | Test: Loss 0.08981, Accuracy 0.96426, F1 0.96432\n",
      "Epoch [49/200]       | Train: Loss 0.07482, Accuracy 0.96936                             | Test: Loss 0.09260, Accuracy 0.96278, F1 0.96281\n",
      "Epoch [50/200]       | Train: Loss 0.07332, Accuracy 0.96998                             | Test: Loss 0.09322, Accuracy 0.96280, F1 0.96281\n",
      "Epoch [51/200]       | Train: Loss 0.07312, Accuracy 0.97036                             | Test: Loss 0.09171, Accuracy 0.96389, F1 0.96386\n",
      "Epoch [52/200]       | Train: Loss 0.07198, Accuracy 0.97031                             | Test: Loss 0.09294, Accuracy 0.96314, F1 0.96319\n",
      "Epoch [53/200]       | Train: Loss 0.07123, Accuracy 0.97062                             | Test: Loss 0.09061, Accuracy 0.96424, F1 0.96426\n",
      "Epoch [54/200]       | Train: Loss 0.07047, Accuracy 0.97114                             | Test: Loss 0.09225, Accuracy 0.96345, F1 0.96348\n",
      "Epoch [55/200]       | Train: Loss 0.06963, Accuracy 0.97131                             | Test: Loss 0.09160, Accuracy 0.96421, F1 0.96425\n",
      "Epoch [56/200]       | Train: Loss 0.06844, Accuracy 0.97172                             | Test: Loss 0.08966, Accuracy 0.96437, F1 0.96440\n",
      "Epoch [57/200]       | Train: Loss 0.06797, Accuracy 0.97238                             | Test: Loss 0.09215, Accuracy 0.96363, F1 0.96368\n",
      "Epoch [58/200]       | Train: Loss 0.06733, Accuracy 0.97208                             | Test: Loss 0.09129, Accuracy 0.96484, F1 0.96484\n",
      "Epoch [59/200]       | Train: Loss 0.06625, Accuracy 0.97296                             | Test: Loss 0.08930, Accuracy 0.96527, F1 0.96531\n",
      "Epoch [60/200]       | Train: Loss 0.06511, Accuracy 0.97321                             | Test: Loss 0.09128, Accuracy 0.96504, F1 0.96502\n",
      "Epoch [61/200]       | Train: Loss 0.06539, Accuracy 0.97319                             | Test: Loss 0.09291, Accuracy 0.96405, F1 0.96407\n",
      "Epoch [62/200]       | Train: Loss 0.06434, Accuracy 0.97352                             | Test: Loss 0.08713, Accuracy 0.96609, F1 0.96610\n",
      "Epoch [63/200]       | Train: Loss 0.06322, Accuracy 0.97402                             | Test: Loss 0.08771, Accuracy 0.96635, F1 0.96630\n",
      "Epoch [64/200]       | Train: Loss 0.06252, Accuracy 0.97469                             | Test: Loss 0.08926, Accuracy 0.96625, F1 0.96624\n",
      "Epoch [65/200]       | Train: Loss 0.06191, Accuracy 0.97455                             | Test: Loss 0.08729, Accuracy 0.96660, F1 0.96661\n",
      "Epoch [66/200]       | Train: Loss 0.06144, Accuracy 0.97467                             | Test: Loss 0.08776, Accuracy 0.96630, F1 0.96632\n",
      "Epoch [67/200]       | Train: Loss 0.06073, Accuracy 0.97512                             | Test: Loss 0.08678, Accuracy 0.96714, F1 0.96714\n",
      "Epoch [68/200]       | Train: Loss 0.06048, Accuracy 0.97553                             | Test: Loss 0.08760, Accuracy 0.96651, F1 0.96649\n",
      "Epoch [69/200]       | Train: Loss 0.05911, Accuracy 0.97565                             | Test: Loss 0.08933, Accuracy 0.96677, F1 0.96674\n",
      "Epoch [70/200]       | Train: Loss 0.05979, Accuracy 0.97559                             | Test: Loss 0.08995, Accuracy 0.96596, F1 0.96598\n",
      "Epoch [71/200]       | Train: Loss 0.05794, Accuracy 0.97633                             | Test: Loss 0.09366, Accuracy 0.96451, F1 0.96449\n",
      "Epoch [72/200]       | Train: Loss 0.05813, Accuracy 0.97595                             | Test: Loss 0.08797, Accuracy 0.96707, F1 0.96708\n",
      "Epoch [73/200]       | Train: Loss 0.05768, Accuracy 0.97629                             | Test: Loss 0.09085, Accuracy 0.96541, F1 0.96542\n",
      "Epoch [74/200]       | Train: Loss 0.05707, Accuracy 0.97655                             | Test: Loss 0.09113, Accuracy 0.96648, F1 0.96649\n",
      "Epoch [75/200]       | Train: Loss 0.05684, Accuracy 0.97657                             | Test: Loss 0.08824, Accuracy 0.96675, F1 0.96675\n",
      "Epoch [76/200]       | Train: Loss 0.05664, Accuracy 0.97702                             | Test: Loss 0.08812, Accuracy 0.96711, F1 0.96711\n",
      "Epoch [77/200]       | Train: Loss 0.05497, Accuracy 0.97728                             | Test: Loss 0.09194, Accuracy 0.96570, F1 0.96573\n",
      "Epoch [78/200]       | Train: Loss 0.05520, Accuracy 0.97716                             | Test: Loss 0.09051, Accuracy 0.96664, F1 0.96661\n",
      "Epoch [79/200]       | Train: Loss 0.05425, Accuracy 0.97776                             | Test: Loss 0.08814, Accuracy 0.96751, F1 0.96753\n",
      "Epoch [80/200]       | Train: Loss 0.05493, Accuracy 0.97757                             | Test: Loss 0.09032, Accuracy 0.96664, F1 0.96666\n",
      "Epoch [81/200]       | Train: Loss 0.05375, Accuracy 0.97807                             | Test: Loss 0.08969, Accuracy 0.96717, F1 0.96718\n",
      "Epoch [82/200]       | Train: Loss 0.05329, Accuracy 0.97803                             | Test: Loss 0.08883, Accuracy 0.96734, F1 0.96734\n",
      "Epoch [83/200]       | Train: Loss 0.05299, Accuracy 0.97797                             | Test: Loss 0.09489, Accuracy 0.96588, F1 0.96587\n",
      "Epoch [84/200]       | Train: Loss 0.05267, Accuracy 0.97840                             | Test: Loss 0.09085, Accuracy 0.96663, F1 0.96663\n",
      "Epoch [85/200]       | Train: Loss 0.05216, Accuracy 0.97861                             | Test: Loss 0.09008, Accuracy 0.96728, F1 0.96730\n",
      "Epoch [86/200]       | Train: Loss 0.05138, Accuracy 0.97902                             | Test: Loss 0.09079, Accuracy 0.96706, F1 0.96704\n",
      "Epoch [87/200]       | Train: Loss 0.05090, Accuracy 0.97921                             | Test: Loss 0.09032, Accuracy 0.96738, F1 0.96738\n",
      "Epoch [88/200]       | Train: Loss 0.05057, Accuracy 0.97923                             | Test: Loss 0.09125, Accuracy 0.96695, F1 0.96696\n",
      "Epoch [89/200]       | Train: Loss 0.04982, Accuracy 0.97956                             | Test: Loss 0.08793, Accuracy 0.96821, F1 0.96822\n",
      "Epoch [90/200]       | Train: Loss 0.04941, Accuracy 0.97975                             | Test: Loss 0.09135, Accuracy 0.96721, F1 0.96722\n",
      "Epoch [91/200]       | Train: Loss 0.04944, Accuracy 0.97996                             | Test: Loss 0.08935, Accuracy 0.96760, F1 0.96760\n",
      "Epoch [92/200]       | Train: Loss 0.04923, Accuracy 0.97973                             | Test: Loss 0.08832, Accuracy 0.96838, F1 0.96839\n",
      "Epoch [93/200]       | Train: Loss 0.04916, Accuracy 0.97980                             | Test: Loss 0.09182, Accuracy 0.96711, F1 0.96713\n",
      "Epoch [94/200]       | Train: Loss 0.04751, Accuracy 0.98050                             | Test: Loss 0.08853, Accuracy 0.96849, F1 0.96847\n",
      "Epoch [95/200]       | Train: Loss 0.04886, Accuracy 0.98006                             | Test: Loss 0.09207, Accuracy 0.96749, F1 0.96751\n",
      "Epoch [96/200]       | Train: Loss 0.04794, Accuracy 0.98018                             | Test: Loss 0.08932, Accuracy 0.96879, F1 0.96879\n",
      "Epoch [97/200]       | Train: Loss 0.04746, Accuracy 0.98066                             | Test: Loss 0.09196, Accuracy 0.96730, F1 0.96731\n",
      "Epoch [98/200]       | Train: Loss 0.04671, Accuracy 0.98085                             | Test: Loss 0.08921, Accuracy 0.96840, F1 0.96844\n",
      "Epoch [99/200]       | Train: Loss 0.04721, Accuracy 0.98048                             | Test: Loss 0.09349, Accuracy 0.96767, F1 0.96765\n",
      "Epoch [100/200]      | Train: Loss 0.04653, Accuracy 0.98112                             | Test: Loss 0.09021, Accuracy 0.96857, F1 0.96862\n",
      "Epoch [101/200]      | Train: Loss 0.04553, Accuracy 0.98120                             | Test: Loss 0.08926, Accuracy 0.96946, F1 0.96946\n",
      "Epoch [102/200]      | Train: Loss 0.04574, Accuracy 0.98105                             | Test: Loss 0.09068, Accuracy 0.96830, F1 0.96830\n",
      "Epoch [103/200]      | Train: Loss 0.04511, Accuracy 0.98163                             | Test: Loss 0.09156, Accuracy 0.96826, F1 0.96826\n",
      "Epoch [104/200]      | Train: Loss 0.04509, Accuracy 0.98144                             | Test: Loss 0.09154, Accuracy 0.96805, F1 0.96806\n",
      "Epoch [105/200]      | Train: Loss 0.04491, Accuracy 0.98186                             | Test: Loss 0.08741, Accuracy 0.96911, F1 0.96910\n",
      "Epoch [106/200]      | Train: Loss 0.04459, Accuracy 0.98174                             | Test: Loss 0.09276, Accuracy 0.96837, F1 0.96837\n",
      "Epoch [107/200]      | Train: Loss 0.04406, Accuracy 0.98196                             | Test: Loss 0.09290, Accuracy 0.96738, F1 0.96743\n",
      "Epoch [108/200]      | Train: Loss 0.04387, Accuracy 0.98204                             | Test: Loss 0.09084, Accuracy 0.96841, F1 0.96841\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\smbm2\\projects\\CAT-Transformer\\rff_experiments\\covertype_loglinear.ipynb Cell 13\u001b[0m line \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/rff_experiments/covertype_loglinear.ipynb#X53sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mprocess_time()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/rff_experiments/covertype_loglinear.ipynb#X53sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/rff_experiments/covertype_loglinear.ipynb#X53sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m   train_loss, train_accuracy_1 \u001b[39m=\u001b[39m train(train_dataloader, model, loss_functions, optimizer, device_in_use\u001b[39m=\u001b[39;49mdevice_in_use)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/rff_experiments/covertype_loglinear.ipynb#X53sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m   test_loss, test_accuracy_1, all_predictions_1, all_targets_1, f1_1 \u001b[39m=\u001b[39m test(test_dataloader, model, loss_functions, device_in_use\u001b[39m=\u001b[39mdevice_in_use)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/rff_experiments/covertype_loglinear.ipynb#X53sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m   train_losses\u001b[39m.\u001b[39mappend(train_loss)\n",
      "\u001b[1;32mc:\\Users\\smbm2\\projects\\CAT-Transformer\\rff_experiments\\covertype_loglinear.ipynb Cell 13\u001b[0m line \u001b[0;36m4\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/rff_experiments/covertype_loglinear.ipynb#X53sZmlsZQ%3D%3D?line=404'>405</a>\u001b[0m \u001b[39mfor\u001b[39;00m (features,labels_task1) \u001b[39min\u001b[39;00m dataloader:\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/rff_experiments/covertype_loglinear.ipynb#X53sZmlsZQ%3D%3D?line=405'>406</a>\u001b[0m     features,labels_task1 \u001b[39m=\u001b[39m features\u001b[39m.\u001b[39mto(device_in_use),labels_task1\u001b[39m.\u001b[39mto(device_in_use)\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/rff_experiments/covertype_loglinear.ipynb#X53sZmlsZQ%3D%3D?line=408'>409</a>\u001b[0m     task_predictions \u001b[39m=\u001b[39m model(features) \u001b[39m#contains a list of the tensor outputs for each task\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/rff_experiments/covertype_loglinear.ipynb#X53sZmlsZQ%3D%3D?line=410'>411</a>\u001b[0m     loss \u001b[39m=\u001b[39m loss_function(task_predictions, labels_task1)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/rff_experiments/covertype_loglinear.ipynb#X53sZmlsZQ%3D%3D?line=411'>412</a>\u001b[0m     total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\smbm2\\AppData\\Local\\miniconda3\\envs\\ml-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mc:\\Users\\smbm2\\projects\\CAT-Transformer\\rff_experiments\\covertype_loglinear.ipynb Cell 13\u001b[0m line \u001b[0;36m3\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/rff_experiments/covertype_loglinear.ipynb#X53sZmlsZQ%3D%3D?line=375'>376</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/rff_experiments/covertype_loglinear.ipynb#X53sZmlsZQ%3D%3D?line=376'>377</a>\u001b[0m     class_embed, context \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membeddings(x)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/rff_experiments/covertype_loglinear.ipynb#X53sZmlsZQ%3D%3D?line=378'>379</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder(class_embed, context)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/rff_experiments/covertype_loglinear.ipynb#X53sZmlsZQ%3D%3D?line=380'>381</a>\u001b[0m     probability_dist_raw \u001b[39m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\smbm2\\AppData\\Local\\miniconda3\\envs\\ml-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mc:\\Users\\smbm2\\projects\\CAT-Transformer\\rff_experiments\\covertype_loglinear.ipynb Cell 13\u001b[0m line \u001b[0;36m2\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/rff_experiments/covertype_loglinear.ipynb#X53sZmlsZQ%3D%3D?line=265'>266</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_cont):\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/rff_experiments/covertype_loglinear.ipynb#X53sZmlsZQ%3D%3D?line=266'>267</a>\u001b[0m     \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m x[:,i,:]\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/rff_experiments/covertype_loglinear.ipynb#X53sZmlsZQ%3D%3D?line=267'>268</a>\u001b[0m     out \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([torch\u001b[39m.\u001b[39mcos(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcoefficients \u001b[39m*\u001b[39m \u001b[39minput\u001b[39m), torch\u001b[39m.\u001b[39;49msin(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcoefficients \u001b[39m*\u001b[39;49m \u001b[39minput\u001b[39;49m)], dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/rff_experiments/covertype_loglinear.ipynb#X53sZmlsZQ%3D%3D?line=268'>269</a>\u001b[0m     temp\u001b[39m.\u001b[39mappend(out)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/smbm2/projects/CAT-Transformer/rff_experiments/covertype_loglinear.ipynb#X53sZmlsZQ%3D%3D?line=270'>271</a>\u001b[0m embeddings \u001b[39m=\u001b[39m []\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class LossFunction(nn.Module):\n",
    "    def __init__(self, num_tasks):\n",
    "        super(LossFunction, self).__init__()\n",
    "        self.num_tasks = num_tasks\n",
    "\n",
    "        self.loss_fns = [nn.CrossEntropyLoss() for x in range(num_tasks)] \n",
    "\n",
    "    def forward(self, predictions, labels_task1):\n",
    "\n",
    "        #task 1\n",
    "        target = labels_task1.long()\n",
    "        prediction = predictions[0]\n",
    "        loss_fn = self.loss_fns[0]\n",
    "\n",
    "        task_loss = loss_fn(prediction, target)\n",
    "        \n",
    "        return task_loss\n",
    "    \n",
    "model = CATTransformer(rff_on=True,\n",
    "                       sigma=0.5,\n",
    "                       embed_size=160,\n",
    "                       heads=5,\n",
    "                       forward_expansion=8,\n",
    "                       pre_norm_on=False,\n",
    "                       mlp_scale_classification=8,\n",
    "                   \n",
    "                       decoder_dropout=0,\n",
    "                       classification_dropout=0.1,\n",
    "                       targets_classes=target_classes,\n",
    "                       n_cont=len(cont_columns),\n",
    "              \n",
    "                       embedding_scheme='log-linear_periodic',\n",
    "                       trainable=False,\n",
    "                       linear_on=True).to(device_in_use)\n",
    "\n",
    "def format_metric(value): # Used to format the metrics output\n",
    "    return f\"{value:.5f}\"\n",
    "\n",
    "loss_functions = LossFunction(1)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr = 0.0001) # Maybe try messing around with optimizers. try other torch optimizers with different configurations.\n",
    "epochs = 200 #Set the number of epochs\n",
    "\n",
    "train_losses = []\n",
    "train_accuracies_1 = [] \n",
    "test_losses = []\n",
    "test_accuracies_1 = [] \n",
    "test_f1_scores = [] \n",
    "\n",
    "\n",
    "#Time will be recorded for all 100 epochs - This means the results will not be comparable to Xgboost but that is ok, we will only compare between transformer models who will also train for 100 epochs\n",
    "start_time = time.process_time()\n",
    "\n",
    "for t in range(epochs):\n",
    "  train_loss, train_accuracy_1 = train(train_dataloader, model, loss_functions, optimizer, device_in_use=device_in_use)\n",
    "  test_loss, test_accuracy_1, all_predictions_1, all_targets_1, f1_1 = test(test_dataloader, model, loss_functions, device_in_use=device_in_use)\n",
    "  train_losses.append(train_loss)\n",
    "  train_accuracies_1.append(train_accuracy_1)\n",
    "  # train_accuracies_2.append(train_accuracy_2)\n",
    "  # train_recalls.append(train_recall) \n",
    "  # train_f1_scores.append(train_f1)\n",
    "  test_losses.append(test_loss)\n",
    "  test_accuracies_1.append(test_accuracy_1)\n",
    "  # test_accuracies_2.append(test_accuracy_2)\n",
    "  # test_recalls.append(test_recall)\n",
    "  test_f1_scores.append(f1_1)\n",
    "  # Formatting for easier reading\n",
    "  epoch_str = f\"Epoch [{t+1:2}/{epochs}]\"\n",
    "  train_metrics = f\"Train: Loss {format_metric(train_loss)}, Accuracy {format_metric(train_accuracy_1)}\"\n",
    "  test_metrics = f\"Test: Loss {format_metric(test_loss)}, Accuracy {format_metric(test_accuracy_1)}, F1 {format_metric(f1_1)}\"\n",
    "  print(f\"{epoch_str:20} | {train_metrics:65} | {test_metrics}\")\n",
    "\n",
    "total_time = time.process_time() - start_time\n",
    "\n",
    "# Plotting the loss curves\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, epochs+1), train_losses, label='Train Loss')\n",
    "plt.plot(range(1, epochs+1), [l for l in test_losses], label='Test Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Test Loss Curve')\n",
    "plt.legend()\n",
    "\n",
    "# Plotting the accuracy curves\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, epochs+1), train_accuracies_1, label='Train Accuracy')\n",
    "plt.plot(range(1, epochs+1), test_accuracies_1, label='Test Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Test Accuracy Curve')\n",
    "plt.legend()\n",
    "\n",
    "# Display confusion matrix for the first task (Traffic Type) on test data\n",
    "conf_matrix_1 = confusion_matrix(all_targets_1, all_predictions_1)\n",
    "print(\"Confusion Matrix for income\")\n",
    "print(conf_matrix_1)\n",
    "\n",
    "best_index = test_accuracies_1.index(max(test_accuracies_1))\n",
    "print(f\"Best accuracy {test_accuracies_1[best_index]}\")\n",
    "print(f\"Best F1 {test_f1_scores[best_index]}\")\n",
    "print(f\"100 epochs of training and evaluation took, {total_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\sigma = 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossFunction(nn.Module):\n",
    "    def __init__(self, num_tasks):\n",
    "        super(LossFunction, self).__init__()\n",
    "        self.num_tasks = num_tasks\n",
    "\n",
    "        self.loss_fns = [nn.CrossEntropyLoss() for x in range(num_tasks)] \n",
    "\n",
    "    def forward(self, predictions, labels_task1):\n",
    "\n",
    "        #task 1\n",
    "        target = labels_task1.long()\n",
    "        prediction = predictions[0]\n",
    "        loss_fn = self.loss_fns[0]\n",
    "\n",
    "        task_loss = loss_fn(prediction, target)\n",
    "        \n",
    "        return task_loss\n",
    "    \n",
    "model = CATTransformer(rff_on=True,\n",
    "                       sigma=1,\n",
    "                       embed_size=160,\n",
    "                       heads=5,\n",
    "                       forward_expansion=8,\n",
    "                       pre_norm_on=False,\n",
    "                       mlp_scale_classification=8,\n",
    "                   \n",
    "                       decoder_dropout=0,\n",
    "                       classification_dropout=0.1,\n",
    "                       targets_classes=target_classes,\n",
    "                       n_cont=len(cont_columns),\n",
    "              \n",
    "                       embedding_scheme='log-linear_periodic',\n",
    "                       trainable=False,\n",
    "                       linear_on=True).to(device_in_use)\n",
    "\n",
    "def format_metric(value): # Used to format the metrics output\n",
    "    return f\"{value:.5f}\"\n",
    "\n",
    "loss_functions = LossFunction(1)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr = 0.0001) # Maybe try messing around with optimizers. try other torch optimizers with different configurations.\n",
    "epochs = 200 #Set the number of epochs\n",
    "\n",
    "train_losses = []\n",
    "train_accuracies_1 = [] \n",
    "test_losses = []\n",
    "test_accuracies_1 = [] \n",
    "test_f1_scores = [] \n",
    "\n",
    "\n",
    "#Time will be recorded for all 100 epochs - This means the results will not be comparable to Xgboost but that is ok, we will only compare between transformer models who will also train for 100 epochs\n",
    "start_time = time.process_time()\n",
    "\n",
    "for t in range(epochs):\n",
    "  train_loss, train_accuracy_1 = train(train_dataloader, model, loss_functions, optimizer, device_in_use=device_in_use)\n",
    "  test_loss, test_accuracy_1, all_predictions_1, all_targets_1, f1_1 = test(test_dataloader, model, loss_functions, device_in_use=device_in_use)\n",
    "  train_losses.append(train_loss)\n",
    "  train_accuracies_1.append(train_accuracy_1)\n",
    "  # train_accuracies_2.append(train_accuracy_2)\n",
    "  # train_recalls.append(train_recall) \n",
    "  # train_f1_scores.append(train_f1)\n",
    "  test_losses.append(test_loss)\n",
    "  test_accuracies_1.append(test_accuracy_1)\n",
    "  # test_accuracies_2.append(test_accuracy_2)\n",
    "  # test_recalls.append(test_recall)\n",
    "  test_f1_scores.append(f1_1)\n",
    "  # Formatting for easier reading\n",
    "  epoch_str = f\"Epoch [{t+1:2}/{epochs}]\"\n",
    "  train_metrics = f\"Train: Loss {format_metric(train_loss)}, Accuracy {format_metric(train_accuracy_1)}\"\n",
    "  test_metrics = f\"Test: Loss {format_metric(test_loss)}, Accuracy {format_metric(test_accuracy_1)}, F1 {format_metric(f1_1)}\"\n",
    "  print(f\"{epoch_str:20} | {train_metrics:65} | {test_metrics}\")\n",
    "\n",
    "total_time = time.process_time() - start_time\n",
    "\n",
    "# Plotting the loss curves\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, epochs+1), train_losses, label='Train Loss')\n",
    "plt.plot(range(1, epochs+1), [l for l in test_losses], label='Test Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Test Loss Curve')\n",
    "plt.legend()\n",
    "\n",
    "# Plotting the accuracy curves\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, epochs+1), train_accuracies_1, label='Train Accuracy')\n",
    "plt.plot(range(1, epochs+1), test_accuracies_1, label='Test Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Test Accuracy Curve')\n",
    "plt.legend()\n",
    "\n",
    "# Display confusion matrix for the first task (Traffic Type) on test data\n",
    "conf_matrix_1 = confusion_matrix(all_targets_1, all_predictions_1)\n",
    "print(\"Confusion Matrix for income\")\n",
    "print(conf_matrix_1)\n",
    "\n",
    "best_index = test_accuracies_1.index(max(test_accuracies_1))\n",
    "print(f\"Best accuracy {test_accuracies_1[best_index]}\")\n",
    "print(f\"Best F1 {test_f1_scores[best_index]}\")\n",
    "print(f\"100 epochs of training and evaluation took, {total_time}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
