{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../model/') #change path so that sys can access our model\n",
    "from updatedModel import CATTransformer, Combined_Dataset, train\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "device_in_use='cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All Continuous Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target classes [100]\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv(r'C:\\Users\\smbm2\\projects\\CAT-Transformer\\datasets\\helena\\train.csv')\n",
    "df_test = pd.read_csv(r'C:\\Users\\smbm2\\projects\\CAT-Transformer\\datasets\\helena\\test.csv')\n",
    "df_val = pd.read_csv(r'C:\\Users\\smbm2\\projects\\CAT-Transformer\\datasets\\helena\\validation.csv') #READ FROM RIGHT SPOT\n",
    "\n",
    "# df_train.columns\n",
    "cont_columns = ['V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',\n",
    "       'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20',\n",
    "       'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27']\n",
    "target = ['class']\n",
    "\n",
    "#CHECKING TO MAKE SURE YOUR LIST IS CORRECT (NO NEED TO TOUCH)\n",
    "yourlist = cont_columns + target\n",
    "yourlist.sort()\n",
    "oglist = list(df_train.columns)\n",
    "oglist.sort()\n",
    "\n",
    "assert(yourlist == oglist), \"You may of spelled feature name wrong or you forgot to put one of them in the list\"\n",
    "\n",
    "target_classes = [max(len(df_train[target].value_counts()), len(df_val[target].value_counts()),len(df_test[target].value_counts()))]\n",
    "print(\"target classes\",target_classes)\n",
    "# Create a StandardScaler and fit it to the cont features\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(df_train[cont_columns])\n",
    "\n",
    "# Transform the training, test, and validation datasets\n",
    "df_train[cont_columns] = scaler.transform(df_train[cont_columns])\n",
    "df_test[cont_columns] = scaler.transform(df_test[cont_columns])\n",
    "df_val[cont_columns] = scaler.transform(df_val[cont_columns])\n",
    "\n",
    "#Wrapping in Dataset\n",
    "train_dataset = Combined_Dataset(df_train, cat_columns=[], num_columns=cont_columns, task1_column='class')\n",
    "val_dataset = Combined_Dataset(df_val, cat_columns=[], num_columns=cont_columns, task1_column='class')\n",
    "test_dataset = Combined_Dataset(df_test, cat_columns=[], num_columns=cont_columns, task1_column='class')\n",
    "\n",
    "#This is a hyperparameter that is not tuned. Maybe mess with what makes sense here\n",
    "batch_size = 256\n",
    "\n",
    "# Wrapping with DataLoader for easy batch extraction\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [ 1/3]         | Train: Loss 3.973314587630373, Accuracy 0.10114599995617592      \n",
      "Epoch [ 2/3]         | Train: Loss 3.4268477922045317, Accuracy 0.20564454280518002     \n",
      "Epoch [ 3/3]         | Train: Loss 3.22518864956648, Accuracy 0.24651050682560202       \n"
     ]
    }
   ],
   "source": [
    "model = CATTransformer(n_cont=len(cont_columns),\n",
    "                       cat_feat=[], #pass cat_feat an emtpy list if dataset contains no cat features\n",
    "                       targets_classes=target_classes\n",
    "                       ).to(device_in_use)\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.0001)\n",
    "\n",
    "train_losses = []\n",
    "train_accuracies_1 = [] \n",
    "test_losses = []\n",
    "test_accuracies_1 = [] \n",
    "test_f1_scores = [] \n",
    "\n",
    "epochs = 3\n",
    "\n",
    "for t in range(epochs):\n",
    "    train_loss, train_acc = train(regression_on=False, \n",
    "                                   dataloader=train_dataloader, \n",
    "                                   model=model, \n",
    "                                   loss_function=loss_function, \n",
    "                                   optimizer=optimizer, \n",
    "                                   device_in_use=device_in_use)\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies_1.append(train_acc)\n",
    "\n",
    "    epoch_str = f\"Epoch [{t+1:2}/{epochs}]\"\n",
    "    train_metrics = f\"Train: Loss {(train_loss)}, Accuracy {(train_acc)}\"\n",
    "    print(f\"{epoch_str:20} | {train_metrics:65}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorical and Continous Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target classes [2]\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv(r'C:\\Users\\smbm2\\projects\\CAT-Transformer\\datasets\\income\\train.csv')\n",
    "df_test = pd.read_csv(r'C:\\Users\\smbm2\\projects\\CAT-Transformer\\datasets\\income\\test.csv')\n",
    "df_val = pd.read_csv(r'C:\\Users\\smbm2\\projects\\CAT-Transformer\\datasets\\income\\validation.csv') #READ FROM RIGHT SPOT\n",
    "\n",
    "cont_columns = ['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss',\n",
    "       'hours-per-week']\n",
    "cat_columns = ['workclass', 'education', 'marital-status', 'occupation',\n",
    "       'relationship', 'race', 'sex', 'native-country']\n",
    "target = ['income']\n",
    "\n",
    "#CHECKING TO MAKE SURE YOUR LIST IS CORRECT (NO NEED TO TOUCH)\n",
    "yourlist = cont_columns + cat_columns+target\n",
    "yourlist.sort()\n",
    "oglist = list(df_train.columns)\n",
    "oglist.sort()\n",
    "\n",
    "assert(yourlist == oglist), \"You may of spelled feature name wrong or you forgot to put on of them in the list\"\n",
    "\n",
    "cat_features = (10,16,7,16,6,5,2,43)\n",
    "\n",
    "target_classes = [max(len(df_train[target].value_counts()), len(df_val[target].value_counts()),len(df_test[target].value_counts()))]\n",
    "print(\"target classes\", target_classes)\n",
    "# Create a StandardScaler and fit it to the cont features\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(df_train[cont_columns])\n",
    "\n",
    "# Transform the training, test, and validation datasets\n",
    "df_train[cont_columns] = scaler.transform(df_train[cont_columns])\n",
    "df_test[cont_columns] = scaler.transform(df_test[cont_columns])\n",
    "df_val[cont_columns] = scaler.transform(df_val[cont_columns])\n",
    "\n",
    "\n",
    "#Wrapping in Dataset\n",
    "train_dataset = Combined_Dataset(df_train, cat_columns, cont_columns, 'income')\n",
    "val_dataset = Combined_Dataset(df_val, cat_columns, cont_columns, 'income')\n",
    "test_dataset = Combined_Dataset(df_test, cat_columns, cont_columns, 'income')\n",
    "\n",
    "#This is a hyperparameter that is not tuned. Maybe mess with what makes sense here\n",
    "#Also try looking to see what other papers have done\n",
    "batch_size = 256\n",
    "\n",
    "# Wrapping with DataLoader for easy batch extraction\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [ 1/3]         | Train: Loss 0.45052271503121105, Accuracy 0.7965134984936676     \n",
      "Epoch [ 2/3]         | Train: Loss 0.3966388021832082, Accuracy 0.818362631255667       \n",
      "Epoch [ 3/3]         | Train: Loss 0.3895877237195399, Accuracy 0.8222527713592092      \n"
     ]
    }
   ],
   "source": [
    "model = CATTransformer(n_cont=len(cont_columns),\n",
    "                       cat_feat=cat_features,\n",
    "                       targets_classes=target_classes\n",
    "                       ).to(device_in_use)\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.0001)\n",
    "\n",
    "train_losses = []\n",
    "train_accuracies_1 = [] \n",
    "test_losses = []\n",
    "test_accuracies_1 = [] \n",
    "test_f1_scores = [] \n",
    "\n",
    "epochs = 3\n",
    "\n",
    "for t in range(epochs):\n",
    "    train_loss, train_acc = train(regression_on=False, \n",
    "                                   dataloader=train_dataloader, \n",
    "                                   model=model, \n",
    "                                   loss_function=loss_function, \n",
    "                                   optimizer=optimizer, \n",
    "                                   device_in_use=device_in_use)\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies_1.append(train_acc)\n",
    "\n",
    "    epoch_str = f\"Epoch [{t+1:2}/{epochs}]\"\n",
    "    train_metrics = f\"Train: Loss {(train_loss)}, Accuracy {(train_acc)}\"\n",
    "    print(f\"{epoch_str:20} | {train_metrics:65}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9851]\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv(r'C:\\Users\\smbm2\\projects\\CAT-Transformer\\datasets\\california\\train.csv')\n",
    "df_test = pd.read_csv(r'C:\\Users\\smbm2\\projects\\CAT-Transformer\\datasets\\california\\test.csv')\n",
    "df_val = pd.read_csv(r'C:\\Users\\smbm2\\projects\\CAT-Transformer\\datasets\\california\\validation.csv') #READ FROM RIGHT SPOT\n",
    "\n",
    "cont_columns = [ 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup',\n",
    "       'Latitude', 'Longitude']\n",
    "target = ['MedInc']\n",
    "\n",
    "#CHECKING TO MAKE SURE YOUR LIST IS CORRECT (NO NEED TO TOUCH)\n",
    "yourlist = cont_columns + target\n",
    "yourlist.sort()\n",
    "oglist = list(df_train.columns)\n",
    "oglist.sort()\n",
    "\n",
    "assert(yourlist == oglist), \"You may of spelled feature name wrong or you forgot to put on of them in the list\"\n",
    "\n",
    "target_classes = [max(len(df_train[target].value_counts()), len(df_val[target].value_counts()),len(df_test[target].value_counts()))]\n",
    "print(target_classes)\n",
    "# Create a StandardScaler and fit it to the cont features\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(df_train[cont_columns])\n",
    "\n",
    "# Transform the training, test, and validation datasets\n",
    "df_train[cont_columns] = scaler.transform(df_train[cont_columns])\n",
    "df_test[cont_columns] = scaler.transform(df_test[cont_columns])\n",
    "df_val[cont_columns] = scaler.transform(df_val[cont_columns])\n",
    "\n",
    "#Wrapping in Dataset\n",
    "train_dataset = Combined_Dataset(df_train, [], cont_columns, 'MedInc')\n",
    "val_dataset = Combined_Dataset(df_val, [], cont_columns, 'MedInc')\n",
    "test_dataset = Combined_Dataset(df_test, [], cont_columns, 'MedInc')\n",
    "\n",
    "#This is a hyperparameter that is not tuned. Maybe mess with what makes sense here\n",
    "#Also try looking to see what other papers have done\n",
    "batch_size = 256\n",
    "\n",
    "# Wrapping with DataLoader for easy batch extraction\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [ 1/5]         | Train: Loss 4.378844148234317, RMSE 2.0547747214635215           \n",
      "Epoch [ 2/5]         | Train: Loss 2.935761913918612, RMSE 1.6912907027361685           \n",
      "Epoch [ 3/5]         | Train: Loss 1.4208068293437623, RMSE 1.1864062620882403          \n",
      "Epoch [ 4/5]         | Train: Loss 1.2781160086916203, RMSE 1.1247415814483375          \n",
      "Epoch [ 5/5]         | Train: Loss 1.2058105416465223, RMSE 1.0945611606564438          \n"
     ]
    }
   ],
   "source": [
    "model = CATTransformer(n_cont=len(cont_columns),\n",
    "                       cat_feat=[],\n",
    "                       regression_on=True #turn regression on if you are performing a regression task\n",
    "                       ).to(device_in_use)\n",
    "\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.0001)\n",
    "\n",
    "train_losses = []\n",
    "train_rmses = [] \n",
    "\n",
    "epochs = 5\n",
    "\n",
    "for t in range(epochs):\n",
    "    train_loss, train_rmse = train(regression_on=True, \n",
    "                                   dataloader=train_dataloader, \n",
    "                                   model=model, \n",
    "                                   loss_function=loss_function, \n",
    "                                   optimizer=optimizer, \n",
    "                                   device_in_use=device_in_use)\n",
    "    train_losses.append(train_loss)\n",
    "    train_rmses.append(train_rmse)\n",
    "\n",
    "    epoch_str = f\"Epoch [{t+1:2}/{epochs}]\"\n",
    "    train_metrics = f\"Train: Loss {(train_loss)}, RMSE {(train_rmse)}\"\n",
    "    print(f\"{epoch_str:20} | {train_metrics:65}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
